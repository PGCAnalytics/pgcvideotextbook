<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.353">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>PGC Video Textbook - Software Tutorials: PRS (Video Transcript)</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>


<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar docked nav-fixed">


<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">PGC Video Textbook</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link active" href="./index.html" rel="" target="" aria-current="page">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./contact.html" rel="" target="">
 <span class="menu-text">Contact us</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://pgc.unc.edu/" rel="" target="">
 <span class="menu-text">PGC Website</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools ms-auto">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item">Software Tutorials: PRS (Video Transcript)</li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
      <a href="./index.html" class="sidebar-logo-link">
      <img src="./pgc_logo_website_v3.jpeg" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
      <div class="sidebar-tools-main">
    <div class="dropdown">
      <a href="" title="" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" aria-label=""><i class="bi bi-github"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="">
            Source code
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="">
            Report a bug
            </a>
          </li>
      </ul>
    </div>
</div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./welcome.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome to the PGC Video Textbook!</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./toc.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Table of Contents</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Chapters</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 1: Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 2: The Genome</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 3: Technologies</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 4: Study designs</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 5: GWAS analysis</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 6: Polygenic Scores</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter7.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 7: Ancestry-Specific Analyses and Considerations</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter8.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 8: Post-GWAS bioinformatics</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter9.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 9: Advanced Topics</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 10: Other Considerations</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
 <span class="menu-text">Software Tutorials</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./software_cnvs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">CNVs</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./software_conditional.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Conditional Analysis</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./software_crossdisorder.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Cross-disorder Analysis</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./software_ewas.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">EWAS</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./software_geneset.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Gene Set Identification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./software_gwas.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">GWAS</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./software_genomicSEM.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Genomic SEM</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./software_imaging.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Imaging</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./software_MR.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Mendelian Randomization</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./software_prs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">PRS</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./software_correlation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">SNP Heritability and Genetic Correlation</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
 <span class="menu-text">Additional Resources</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./glossary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Glossary</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./resources.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Software Resources</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./addreading.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Additional Reading</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="https://pgc.unc.edu/for-researchers/download-results/" class="sidebar-item-text sidebar-link">
 <span class="menu-text">PGC Summary Statistics</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#comparison-of-prs-methods" id="toc-comparison-of-prs-methods" class="nav-link active" data-scroll-target="#comparison-of-prs-methods">Comparison of PRS methods</a></li>
  <li><a href="#prs-cs-and-prs-csx" id="toc-prs-cs-and-prs-csx" class="nav-link" data-scroll-target="#prs-cs-and-prs-csx">PRS-CS and PRS-CSx</a>
  <ul class="collapse">
  <li><a href="#prs-csx-to-perform-prs-analysis" id="toc-prs-csx-to-perform-prs-analysis" class="nav-link" data-scroll-target="#prs-csx-to-perform-prs-analysis">PRS-CSx to perform PRS analysis</a></li>
  <li><a href="#prs-in-ancestrally-diverse-populations" id="toc-prs-in-ancestrally-diverse-populations" class="nav-link" data-scroll-target="#prs-in-ancestrally-diverse-populations">PRS in Ancestrally-diverse Populations</a></li>
  </ul></li>
  <li><a href="#prsice2-and-lassosum" id="toc-prsice2-and-lassosum" class="nav-link" data-scroll-target="#prsice2-and-lassosum">PRSice2 and LASSOSUM</a></li>
  <li><a href="#ldpred2" id="toc-ldpred2" class="nav-link" data-scroll-target="#ldpred2">LDpred2</a></li>
  <li><a href="#sbayesr" id="toc-sbayesr" class="nav-link" data-scroll-target="#sbayesr">SBayesR</a></li>
  <li><a href="#vertical-transmission-and-genetic-nurture-with-prs" id="toc-vertical-transmission-and-genetic-nurture-with-prs" class="nav-link" data-scroll-target="#vertical-transmission-and-genetic-nurture-with-prs">Vertical Transmission and Genetic Nurture with PRS</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Software Tutorials: PRS (Video Transcript)</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<hr>
<section id="comparison-of-prs-methods" class="level1">
<h1>Comparison of PRS methods</h1>
<p><strong>Title</strong>: Comparison of PRS methods</p>
<p><strong>Presenter(s)</strong>: Guiyan Ni, Program in Complex Trait Genomics, Institute for Molecular Bioscience, University of Queensland</p>
<p>Welcome. The topic of this video is how to run polygenic risk score comparisons. Until now, we already have watched the individual talks on PRS-CS, LDpred2, and SBayesR, and other methods. I believe you all have a good understanding about polygenic score and each of those methods.</p>
<p>So this slide here is just to set up the scene to make sure that we are on the same page. Polygenic risk score of an individual is a weighted sum of the counted risk alleles. So, based on the GWAS summary statistical results, the basic method for polygenic risk score is p-value clumping and thresholding. This method is simple, but it doesn’t formally model different genetic architecture, so there are many new methods that try to model the genetic architecture for the trait of interest.</p>
<p>For example, using different priors in the Bayesian regression like our LDpred infinitesimal model, LDpred2, SBayesC, PRS-CS, and SBayesR. And also methods like Lassosum are using the lasso regression. MegaPRS is another method; It runs on different priors. For example, if it runs on a prior using a mixture of four normal distributions, it will be the same as SBayesR, or similar, and if it assumes all the SNPs have a contribution to the phenotype variance, then it will be similar to the LDpred infinitesimal model or SBLUP. It can also run a prior like BOLT-LMM, and also can run Lassosum regression. So the difference between MegaPRS and other methods is the expected per-SNP heritability can vary by LD and minor allele frequency. So in this talk, we will compare all those methods, and we know that when the method is proposed they already compared with other methods, but the fundamental question we are trying to answer here is: which method should we use in the PGC data?</p>
<p>We use the cross-validation, leave-one-cohort out cross-validation, to answer this question, and to compare the performance of different methods. Here’s the toy example showing the cross-validation. Each cell here is one cohort, and the pink cell is for the discovery cohort, and the green cell is for the target cohort. And in the first round of the analysis, we’re using the four pink discovery cohorts as a discovery set, and then validate the performance of each method in the target sample. Then we repeat this process until each of those cells, or each of those cohorts, serve as a target cohort. If it’s needed by the method, we have another cohort that will serve as a tuning sample to select the optimal hyperparameters. So, in the real data analysis, we use the GWAS summary statistics from schizophrenia 2 as a discovery sample and out of those data, we actually have access to 33 cohorts where the individual level genotypes are available, and we use each of them as a target sample. For the tuning cohort, we use four cohorts in turn to tune the hyperparameters, and then we can predict the polygenic score into each of the targets sample.</p>
<p>Here, we used three statistics to measure the performance of different methods: One is AUC, another one is proportion of variance explained in the liability scale, and third one is the odds ratio. I will go through each of them to show how to calculate each of those statistics.</p>
<p><em>AUC</em></p>
<p>So first start with AUC. Here is a toy example on how to calculate AUC by hand. So AUC is actually short for the “area under the ROC curve”, which is shaded by the pink here. So the ROC curve is made by plotting the true positive rate against the false positive rates at each possible cut off. So what that means is assume that this is the density of plot for the polygenic score in the control samples, and here is for the case samples, and this vertical line is the current cutoff. In this case, this graph can be divided into four groups: true negative, false negative, true positive, and false positive. And then we can calculate the proportion of each group, and then we can calculate the true positive rates, and the false positive positive rates, which are the coordinates used in the ROC curve. So in the in the current cutoff we use here, it means that we have roughly about 75 percent of cases correctly classified as case, and then there are about 10 percent of controls that are wrongly classified as case, which give us the coordinates for this dot here. And when we vary the this vertical line, this cutoff, we will get this ROC curve as shown in this slides here.</p>
<p><em>Variance explained in the liability scale</em></p>
<p>You’ve seen the first statistic we use to measure the performance of different methods, and the second one is variance explained in the liability scale when using ascertained case-control studies. So this variance is a function of variance explaining the observed scale, this R squared, observed case control study and another two parameters C and Theta so the variance explained on The observed scale is actually a function of two likelihoods from the new model and the phone full mode which is designed in this two equation and this parameter C is a function of k z and P and this K parameter is actually the proportion of the population that are diseased is also means the prevalence of the disease and Z parameter is a density at this threshold T here and this curve is a standard normal distribution and the p is a proportional case in your G was a result or in your case control study and Theta parameter is a function of the same kzt and threshold T but with different combination</p>
<p>so in this slides I just give the final result of how to calculate the variance explained in the liability scale the full Direction with of this equation can be found in this reference foreign statistic is called Odds ratio and also ratio is a ratio between two OS and the OS is a probability being a case over the probability being a control so here is a toy example showing how to calculate all the visual backhand and that saying that we are ordering the individual based on their polygen risk score from a lowest to highest and we are interested in the observation between the uh 10 stairs and um first day so with a number of cases and the controls show in this table so the always being a case in the first day so is 23 over 103 and us being a case in the 10 states of is 83 divided by the 43. the old ratio of between the two decimals is 9.3 so this this value means um when we order individual based on their polygender score the individual in the top 10 days in the top 10 percent or in the 10 states or have 9.3 Times Higher of us being a case compared to the individual in the bottom 10 percent and this this old version can be easily estimated from the logistical version using the logic link function so using the U1 cohort strategy we can access the AUC variance plan and also ratio for each of those Target cohort and here show the result for AUC and for cohort of each method and different colors here stand for different method we used and the y-axis here is a UC difference compared to the P plus T which is a benchmark we used and as you can see of course different validation cohorts there are lots of variations and that’s why we think our comparison is more robust compared to other comparison when they are only use one target cohort if we summarize these uh bar products by each group by method we can see we can observe this bar plot the y-axis here is AUC and each of the group stands for each of the method we compared and each of the bar in each of the group stands for different tuning cohort we use and we noticed that the methods that have formally modeled different genetic architecture they actually have quite</p>
<p>similar performance this is because the genetic architecture of psychiatric disorders they are quite polygenic if we look at the results for the Alzheimer’s disease Which is less polygenic compared to psychiatric disorder we will observe a big difference across different methods and then we also observed the similar pattern for our various explained in the laminate scale and also ratio between the top 10 percent and bottom 10 also the alteration between top 10 medium but uh we observed that LD player 2 as base R and mega PRS they rank the highest amount in most of the comparison and to summarize in this talk I show how to calculate AUC various explaining liability skill and also ratio backhand and based on the comparison we made we observed that for security disorders which are very polygenic all the methods are perform similar but some are rank higher than others for example I would refer to as base R and mega PRS so the result actually here is part of this study which is published recently published and in this paper we also did the comparison for major depression and also other sensitivity analysis and we also provide the code for each to run each other method and for each comparison and also each of the statistics statistic use for comparison and with this I would like to give a big thank you to Professor Norman Ray who always give me a huge support of whatever I needed and thanks to all other pccg members and thank you all.</p>
<hr>
</section>
<section id="prs-cs-and-prs-csx" class="level1">
<h1>PRS-CS and PRS-CSx</h1>
<section id="prs-csx-to-perform-prs-analysis" class="level2">
<h2 class="anchored" data-anchor-id="prs-csx-to-perform-prs-analysis">PRS-CSx to perform PRS analysis</h2>
<p><strong>Title</strong>: Hands-on tutorial of using PRS-CSx to perform multi-ancestry PRS analysis</p>
<p><strong>Presenter(s)</strong>: Tian Ge, Yunfeng Ruan, Stanley Center, Broad Institute</p>
<p><strong>Yunfeng Ruan</strong>:</p>
<p>hello everyone this is uniform I’m going to introduce how to calculate polygenic risk war with PRI CSX prssx combines both poetry was to increase the accuracy of polygenic risk score if you are interested in the algorithm please refer to operate on Mac archive or GitHub has a very detailed read means it covers almost all the aspects of how to run PRS says X if you are familiar with python you may figure out how to run it within no time yourself here I will walk you through the whole process and clarify some details in the addition to the readme</p>
<p>this is a workflow of the methods the input is G1 summary statistics from different populations prs6x adjusts the effect size of each population D was using their corresponding LD reference panel the adjustment of sleep effect size are performed to Snips that are shared by input G was LD reference panel and the target data therefore prss X needs the prefix of Target data so that it can read the Steep list from the dot Beam file the output is two sets of adjusted effect size</p>
<p>next you calculate the pris of your target sample based on each set of the adjusted effect size usually this is termed by plink finally you linearly combine the PRS from different operation G was using software like r or Matlab and that predicts the phenotype with the final result in pris you can also add more G was so that hopefully you can have a better result</p>
<p>now let’s see how to run prss X step by step PR status X is a command line python software you can use either person 2 or 3 and have to install Senpai and the H5 pi you can use command git clone to download and install in one step or you can click the code button to download it locally and unzip the compressed file you can test if the installation is successful by comment prr successx dash dash help it will print a long list like this</p>
<p>next download the LD reference we provide the pre-calculated LD reference panel Link at the readme getting started session we have early reference calculated from 1000 to genome and LD reference calculated from UK power bank samples you also need to download the Steep information and put it in the same directory with other audio reference files after you download all the software and the LD reference you can write your first analysis</p>
<p>you can download the formatted test data and the user example command from the readme test data to run a test the test will be finished in about one minute and you will have two txt files as the output if you use your own data the first thing to do is format your summary statistics file the information is available on using prssx session I want to highlight that the formatted the inputs must contain header line and have the right column order and the the available names now you can finally run the method here is a typical comment which contains</p>
<p>the path to the prss x the the directory of LD reference the prefix of the target data a list of formatted summary statistics please know that you put should comma between different items and there should be no space in the list and the list of G1 sample size and the list of population of the GEOS we highly recommend you run the prss x per chromosome to allow calculation in parallel you can specify which chromosome you want to adjust by specifying it in the Chrome option and then the hyper parameter file and the output</p>
<p>to address the effect size of one trait you need to run 88 jobs for each job PR CSX will print something like this on either screen or the log file you will get two outputs one for each population if you adjust each chromosome in parallel you can concatricate the result in one file use the command cat all together you will have eight sets of adjust effect size you can calculate pris based on the adjusted effect size using a little like a score function in clink the last step is to predict the video type with the PRS</p>
<p>first you normalize all the prns to mean equals to zero and the standard deviation equals to one then you optimize the hair parameter in validation data for each file you predict the phenotype with the normalized prsa and the normalized prsb and get the r squared you compare r squared from each Phi and learn the pacifier and the coefficient of the two pris under that file</p>
<p>then in the testing data you calculate the final result with the knowledge of the pacifier and the coefficient of the two pris you choose the pris under that pathfly and combine them with the Learned coefficients then you use the combination to of the PRS to calculate the r squared this will be your final result if you have any other question you can use an issue on the GitHub website and all team members and the data resources</p>
</section>
<section id="prs-in-ancestrally-diverse-populations" class="level2">
<h2 class="anchored" data-anchor-id="prs-in-ancestrally-diverse-populations">PRS in Ancestrally-diverse Populations</h2>
<p><strong>Title</strong>: PRS-CSx: Improving Cross-Population Polygenic Prediction using Coupled Continuous Shrinkage Priors</p>
<p><strong>Presenter(s)</strong>: Tian Ge</p>
<p>price past nine so I will you know start introduced him uh 10 is a good friend and a good colleague of me of mine as a general hospital and Harvard Medical School um he’s a assistant professor in Harvard Medical School and you know trained as a mathematician he’s contributed to many uh you know domains in science including the neural genetics neural Imaging uh statistical matters and the genetics of development so for today I’m very excited to have him talking about a PRS a new PRS method that is able to jointly model g16 summary statistics of medical ancestrys to improve the prediction accuracy so without further Ado uh Tien let’s get started</p>
<p><strong>Tian Ge</strong>:</p>
<p>um thanks for very generous introduction and for having me today so I’m going to talk about our recent work that extends our base in Partnership production framework which is known as prcs to prcsx which can now integrate G1 summary stats from multiple populations in order to improve cross population polygenic prediction um so to start I just like to briefly recap the idea of polygenic projection and then talk about the motivation and intuition behind the piscs work which might be useful to see how PR CSX works so as many of you already know um polygenic prediction summarizes the effects of genomic genetic markers to measure the genetic liability to a complex trait or Disorder so a conventional method to compute research for particular risk score is pruning and thresholding which is also known as the Columbians thresholding so basically to apply this method usually we set up a p-value threshold or screen a range of p-value thresholds and for each of these p-values thresholds we only consider Snips reaching the significance level level we then perform a procedure called LD clamping which basically retains the most significant snip in each genomic region and discards all the Snips that are in LD with the least nib so for example in this figure we will just keep the top snapping purple and then remove all the Snips that are in red orange green light blue because they are being LD was the lead snip um and then finally we sum up the genotypes of the remaining Snips weighted by the effect size estimates from external GEOS so this pruning and thresholding method is very widely used because it’s conceptually intuitive and also computationally very efficient but it has several limitations so for example it relies on margin of GEOS Association statistics and we know that most associated in European huge region may not be causal so a lot of times we might be using the sub-optimal tagging step to build PRS and because because of the LED clamping procedure we use the method also ignores many of the secondary and tertiary signals in each genomic region and finally when we sum up the Snips we use the effect size estimates directly from external GEOS without any adjustment or shrinkage a large effect size estimates in g-was may suffer from swingers curse and and most non-codile variants will have noisy non-zero effect size estimates so by including these nibs and using these G1 effect size estimates directory without any adjustment we are adding a lot of noise to our PRS and and all these um limitations or limit the predictive of predictive performance appearance um so to address these limitations and improve the conventional pruning thresholding method a more principled framework is to calculate the product generator score by joint remodeling the genetic markers across the genome without any arbitrary pruning and stress shorting and to do this we are basically fitting this linear regression problem where the phenotype Vector is regressed onto this genotype Matrix X and beta here is a vector of snap effect size and Epsilon is a vector that captures non-genetic effects so if we can join traffic with this regression model and get the effect size estimates which is denoted as beta head here we can take the estimate to to the Target data set and where the genotypes there and compute apology score so the methodological challenge here is that in genomic prediction we often have many more Snips than the number of samples we have so this is a ultra high dimensional regression problem and we we need to regularize the effects as estimate to avoid overfitting um and so we know that this pruning and thresholding Method can actually be considered as a specific way of regularizing and shrinking the snip effect size because it in essence shrinks the effect size of discarded snap to zero and performs no shrinkage on effect size estimates of selected Snips but we have discussed that this this shrinkage scheme may be arbitrary and sub-optimal so there are many more principles that is called Methods that can import this shrinkage so for example the frequentest approach is to fit a regularized regression using methods like lasso or Ridge regression or elastic net which often encourage sparse effects as estimates and penalize large effects so if you have so if you have heard of this measure called lassos um so that’s one of the polygenic prediction methods that applies lasso to build PRS um so in the past few years we also see many Bayesian polygenic prediction methods that have been developed and the Bayesian approach to tackle this High dimensional regression problem is to assign a prior on Snap effect sizes to impose screen coach so all the models basically fit the same regression and the difference is what prior distribution to use so the question here is you know how do we design a prior or which prior is optimal for this type of genomic prediction problem um so the most widely used prior is what we call infinite has monomial prior which is also known as The Basin Ridge regression so with the effect size of each snip follows a normal distribution so this model is very widely used in many classical statistical genetics methods including like gcdta and odd score regression so all these messages assume this underlying infinite test more normal genetic architecture and one major advantage of this price and also that’s why this price is so popular is that it’s mathematically trackable and there’s a closed form expression for the posterior um so here um Lambda is a penalty parameter or shrinkage parameter which depends on these two variants one is the per snip variance of the snip effects and the other one is is the residual variance here so you can see that if the noise the residual variance is large relative to the genetic signal and then we impose a strong shrinkage on the effect size and beta is shrunk towards zero so in the extreme case if you have no genetic signal and then the beta will be shrink to zero on the other hand if the genetic signal is relatively large to the noise and then the estimate will be closer to the least Square estimator and with this penalty from The Matrix is always invertible so this is a well-defined estimator so we also noticed that this is a multivariate estimate of snip effect sizes and X transpose times x here is proportional to the LD Matrix so it’s easy to incorporate LED information in this estimator and in practice you can always divide the genome into independent LED blocks and then within each block you can do this joint estimate of snip effects um so with this being said um there are also limitations of this prior so as you can see here the shrinkage parameter is a constant meaning that under this infinite has monomial prior all the Snips are treated equal and they are shrunk towards zero at the same constant rate so this is sub-optimal because ideally we want to impose various strong shrinkage on small and noisy non-codile signals but at the same time we don’t want to over shrink large and real signals so what we really want is the shrinkage scheme that is adaptive to different Snips and different G1 signals but this cannot be achieved by this infinite Tesla normal prior because the penalty parameter here is a constant which is not snip specific um it’s in an alternative way to look at this problem is to take a look at the shape of the prior distribution which is normal um and this non-adaptive nature of the prior is equivalent to say that you know for the normal distribution um when used as a prior there isn’t enough Mass around zero to impose strong enough shrinkage on noisy signals and and because of this normal distribution has exponentially decayed Tails so these tails are two things meaning that a priority we believe there’s very low probability of large effect sizes so we don’t have a prior that can accommodate those large effect size which often leads to overshrink of real signals so that’s why the Bayesian Rich regression or in this infinite testimon normal prior is not very adaptive to different genetic architectures and usually only works well for highly polygenic trades um so there are many work um trying to design a more flexible prior so that the polygenic model is more adaptive to varying genetic architectures and one idea is that in contrast to use a single normal distribution is the prior we can use a mixture of two or more distributions so for example one pioneering approach in this field LED prep uses this Spike and slab prior which assumes that a fraction of the Snips are now so they have no effect on the phenotype while the rest of the Snips are called or Snips and and their effects are this follow a normal distribution um so if we take a look at a density prior so this prior has two components so there’s a spike component or which is a very narrow distribution centered at zero which is often used to model small signals and there’s a slab component which is much wider and can be used to model large signals and then by varying this proportion of the chordal variance which is coded in pi here so this model can cover a wide range of genetic architectures um so although this prior is much more flexible than the infinite Tesla normal prior it also has two limitations so number one so this is a discrete mixture of two components uh so we call this type of Prior discrete mixture prior so in posterior inference you can see that each snip can either belongs to this now component or this normal component normal component so you can imagine that if there are a million Snips then we have a discrete model space of 2 to the power of medium possibilities which is you know almost unlikely to fully explore and number two so unlike the infinitesimal normal prior which has a closed form multivariary update of the snip effects so the spike and slab prior does not allow for a multivariate effect estimate so in post Theory inference one has to update effects like snip by snip which makes it very difficult to incorporate LD information in this model estimation procedure um so there are many other Bayesian polygenic projection methods that have been developed and used different priors but the majority of them are discrete mixture price so for example you can parameterize um just two normal mixture differently using a additive version or multiplicative version so you can also do a now component plus a t distribution which gives you a heavier tail to model larger signals say s base in R which is another method that receives a lot of attention recently uses a mixture of four normals and and each of these normals captures the effect size distribution distribute Distribution on a different scale which which makes the model even more flexible and then finally there are non-parametric models where the effect size distribution is assumed to be a mixture of an infinite number of normals and in posterior influence the data will determine the optimal number of mixtures um so these are different variations of this discrimination normals so there are all discrete mixtures of two or more distributions so they largely share the same advantage and limitations of LD print so just to quickly summarize um so we have discussed that so the infinite has more normal prior is computationally efficient and allows for multivari remodeling of l-dependence but it’s not robust to varying genetic architectures while this cream extra price on the other hand can create much more flexible models for the genetic architecture but they are computationally challenging and it’s often difficult to incorporate LD information so our motivation was to design a prior that can combine the advantages of these two types of price</p>
<p>so in our prcs work we introduced this conceptually different class of price which is called continuous shrinkage price and in contrast to this horizontal discrete mixture of normals we use the hierarchical scale mixture of normals and here Phi is a global shrinkage parameter which is similar to the Penalty parameter in Ridge regression and it is shared across all the Snips and models the overall sparseness of the genetic architecture and and different from this infinitech monomial prior we added this low cost shrinkage parameter so here J is the index of snips so this local shrinkage parameter is Snip specific and can adapt to different genetic signals and you can see that if we integrate out these hyper parameters the density function of this prior is continuous which can also be seen in this density plot on the right so here the dash black line is the normal prior for reference and then the red and blue lines are the two components of the spike and slab fire and the yellow line is the continuous shrinkage prior so you can see that unlike this two components I can slap prior the prior we used is one continuous density function but it can approximate the shape produced by this discrete mixture price and compared to the normal distribution so you can see we put substantial Mass near zero which can impose strong shrinkage on small anointing signals and in the meantime this is this this distribution has heavy polynomial Tails which can retain large and real signals so the continuous trinkage prior is almost as flexible as the discrete mixture prior but because of its continuous nature it also shares some advantages of the infinite test monomial prior that is it allows for the multivari modeling of LD patterns and it’s also computationally efficient so these are the motivation and some intuitions behind prscs so I’m not going to talk about other features of features of the method but the software is available on GitHub which you can download and test so we hope we have released both pre-computed Southern genomes and UK power bank reference panels for major populations which hopefully has made the application easier um and in the initial application of the prcs method so we applied it to some existing guas of six common complex diseases and six quantitative traits and then predicting to the Mass General Bergen biobank and you can see that kind of prsc is substantially improved the prediction over this conventional pruning and thresholding Method and often outperform LD print um and a second application that might be relevant to this group is the polygenic prediction of schizophrenia and in this study led by Amanda so we aggregated the schizophrenia cases and controls across for Healthcare Systems so Geisinger Mount Sinai Mass General Bergen and Vanderbilt so as part of the cycle emerge Consortium and you can see this polygenic risk score calculated by prscs correlates with the case prevalence and schizophrenia diagnosis and can be used to identify diseases that are genetically correlated with schizophrenia using a free watch design so that’s the review of the ideas behind prcs and some of its applications but one limitation of prcs is that it was designed and tested in homogeneous populations but now it is well well recognized that cross population predictive performance of polygen and growth spores decreases dramatically especially when the target sample is genetically distant from the training example due to the predominant European samples in the current guar studies so there are many factors that can limit the transferability of powers learned transferability of PRS learn from European GEOS so for example there may be population specific variants or variation in the snap effect size estimates across populations the allele frequencies and LD patterns are different across populations um and also the differences in the phenotyping OR environmental factors can all affect the prediction accuracy so in the past few years there have been many efforts to expand the scale of now European Duos and to diversify the samples in genomic research in general although the sample size of most non-europe NG was remain considerably smaller than European studies so they cannot right now they cannot be used to fully characterize the genetic architecture in non-european populations and dissect relative contributions of these factors to PRS predictive performance but one question we can ask is can we leverage these existing non-european Jewelers to improve trans ethnic prediction of the pris even if they are smaller than European unions um so we have been working on this um method called prscsx which is a very simple extension of the prcs framework so here to model existing now European guas we assume that we have data from K different populations and then we still use this continuous shrinkage prior to model snippet effects but this time this product is shared across populations so you can see that these shrinkage parameters do not depend on K which is the index of population so they are shared across populations and to use this coupled prior we have implicitly made assumption that causal variants are largely should across populations so we think this is a reasonable assumption given that many recent Studies have estimated the transdent genetic correlations for many complex traits and diseases to be moderate to high um and with this coupled prior so we can borrow information across summary statistics and increased accuracy of effects as estimation particularly for non-european populations whose G1 s emphasized relatively small um and the other advantage of this coupled prior is that we can leverage the Audi diversity across populations to better localize the G1 signal so this is very similar to the idea of transcending file mapping so although we are not doing any form of file mapping analysis here we we’re actually using this idea um so although we use this shared prior cross populations the effect size for snip are still allowed to vary across populations and so we believe this gives the model more flexibility so we don’t constrain effects has to be the same across populations and we also allow for population specific variants meaning that so if a snip is available in one population but absent in other populations due to for example the low frequency other populations we still include the snap in the model although in this case there’s no effects to couple or we still included Snips in the modeling um so finally um PR CSX incurts many features from prcs so it allows for this multivariate modeling of LD patterns using population specific reference panels and also computational efficient so in practice PR CSX takes the G1 summary stats and the ancestry matched LED reference panels from multiple populations it then joins a models or these data that fits the model and then I’ll put one set of the snap posterior effect size for each Discovery population and then these snap effect science estimates can then be taken to a validation data set and calculate one PRS for each population and we then learn an optimal linear combination of these PRS in the validation data set and evaluate the predictive performance of the final PRS in an independent testing data um so as a comparison and also in the results I’m going to show so we’re examine two alternative methods that can combine G1 summaries that’s from multiple populations uh one method which we call PT met here it performs a fixed effect meta-analysis of the GEOS and then applies the pruning and thresholding method to the method you was um and since the LD pattern is mixed after this meta-analysis so it has different LD reference panels in this case and then select the one with the best predictive performance to evaluate in the testing data set so the other method which we call PT multi this method was developed by August price group a few years back so they applied prunian thresholding separately to each G1 summary statistics and then the resulting PRS are linearly combined in the validation data set and then taken to the to the final PRS for for evaluation</p>
<p>okay um so here are some results so we selected um 17 quantitative traits that are shared between the UK power bank and about Bank Japan and in this analysis UK power bank G1 sample size is typically three to six times larger than the BBJ biobank japani was so we then train uh different PRS using BBJ g1s only so these are the PRS measures that applied to the wild Bank Japan g1’s only and then these are the PRS that was trained on UK biobank data only and then the last three methods are those PRS that combine the G1 summaries that’s from the UK bio bank and biobank Japan and then we train these different PRS and then predict into um different populations in the UK power bank that are independent of the UK power bank training to us so you can see so here in the first panel when the target sample is the UK power bank European population and you can see that PRS is trained with the ancestry match the UK power bank Gwas performs better than PRS trained with the BBJ juice which is expected and in this case combining the UK power bank and bbj-g was doesn’t help too much so you can see it is a very small probably five around five percent of improvement in prediction accuracy when we combine UK power bank and biobank Japan that’s likely because the UK power bank G was was already quite powerful so adding a smaller East Asian G was doesn’t help too much in a prediction uh in a prediction of the European samples um but when we predict into the UK biobank East Asian samples you can see PR CSX can increase the prediction accuracy um here the bar shows the median medium variance plane that was in increased by about 25 percent when comparing with When comparing prcsx with these PRS trained on the European GEOS and then if you compare with this ancestry matched PRS trained in the biobank Japan he was the Improvement was even larger it’s around 80 percent so these results show that we can leverage this large-scale European G was to improve the prediction in non-european populations and then when we predict into the UK biobank African samples the target population didn’t match any of the discovery sample the biobank Japan sample or the UK power bank sample and and both the European and East Asian samples are genetically distant from the African samples so in this case the Improvement in predictions was again limited and predictions in the African population remain quite low relative to the predictions in European and East Asian populations um so we asked whether we can add some African samples to the Discovery data set to improve the prediction in African population and among the 17 trades We examined here seven were also available in the page study which largely comprised of African-American and Hispanic Latino samples but you can see the sample size of the page study was much smaller than the UK power bank and baobank Japan so the question here is whether adding a small African jewels to the Discovery data set can improve projection um and you can see in the right panel of this figure so when integrating this UK power bank power bank depends and Page summary stats using PRN CSX the prediction the African sample was quite dramatically improved and Improvement in media and variance brain was about 70 When comparing with the prcsx applied to UK power bank and barber and Japan she was only and the prediction was also much better than the PRS train on Ancestry matched page study so these results suggest that we can leverage samples that have matched ancestry with the target population to improve prediction even if the now European training was a considerably smaller than European studies so adding the page study of the discovery data set also improved the prediction in other Target populations although the Improvement was to a much less extent so in the last example so we evaluated um different purse methods in the prediction of schizophrenia risk so in in this analysis we use the GEOS summary statistics derived from the pgc2 schizophrenia G was in the European samples and also the reasons schizophrenia G was in the East Asians and of course led by Max lamb Cheyenne and Hylian and colleagues so we have access to the individual level data of sixth East Asian cohorts um and we left out one cohort as the validation data set so this is the data set we use to learn hyper parameters or linear combinations of pis and then for the remaining six cohorts we apply the leave one out approach so meaning that we in turn use one of the six cohorts as the testing data set and then meta-analyze the remaining five cohorts with the other cohorts to generate the discovery to us for the East Asian population we then build again build different PRS using the East Asian GEOS only or using the European G was only or using methods that can combine these two guas and you can see that the prcsx can increase the prediction accuracy relative to messages trained on a single GEOS and then the um medium rarings playing here had approximately 50 increase relative to guas using ancestry match these Asian Duos and almost double the prediction accuracy here when the pris was trainually using European GEOS and on the right panel um you can see that at the tail of the PRS distribution and prcsx can also better stratify patients at top and bottom PRS percentiles relative to other methods um okay so I think I’ll stop here and send call my collaborators so in particular viewing phone has led many of the real data analysis in this project and how long has critical inputs in every aspect of the project and he also led the Stanley Center East Asia initiative which make the schizophrenia analysis in the East Asian cohorts possible and then our preprint is our Med archive and we have also released the software on GitHub so any feedback on comments will be much appreciated so I will stop here and I’m happy to take any questions great thanks a lot here and that’s a great talk so um we have 20 minutes for questions foreign terrific talk Tiana really appreciate it so um I’ll just start things off I’m sure there’s other questions um but I guess one question I have is it seems like with these methods uh Beyond pruning and thresholding one of the big barriers is that some of them are harder to implement than others Printing and thresholding is so easy um and so I was wondering if you could just say a little bit about um how difficult it is to implement this approach and what um what information people need to be able to use PRS CSX um yeah so um I guess in many of the analysis we still use pruning and thresholding as a baseline because it’s computationally faster and also a robust approach so you can use that as a comparison so in terms of those phasing methods um depending on the different implementations and different methods will require different computational costs and usually takes longer than premium stretch holding but um we have trying to you know hopefully make this software easier to use so we have released these reference panels so users usually don’t need to calculate their own reference panels and then we can parallel the computation of clouds chromosomes and then usually for the longest chromosome it takes around an hour or one and a half hours to finish so I think it’s</p>
<p>um populated and add too much computational burden on end user that’s really helpful that’s really useful right and so people just summarize statistics from multiple populations of course the other the other things are provided with your software yeah great so on that topic 10 uh could you perhaps discuss a little bit about the mixed population uh you know I think not every population has a relief reference panel so what are the considerations here and the what are things you know these people can do if they want their you know uh analysis being done you’ll see this matter great question there are a lot of challenges in terms of how to handle Amex populations because um you know the LD patterns might depend on specific studies and and the proportion of the samples in each study so I don’t think there’s a universal reference panel that can be used for all that makes populations um so right now so for example in this study when we try to model the page dual summaries that’s um so it’s sort of a mixture of African-American examples and also Latino Hispanic samples so it’s kind of atmixture so we try to use a African reference panel to approximate in this situation and turn out to work okay but clearly there are still a lot of work to do and think about how to better model the admix populations and how to build reference panels in this case all right maybe I can ask a question so um solely on the deal with on summary statistics it’s always simpler but at the same time more difficult right because you lose a lot of detail information uh so but I’m curious if you have individual level do not have data will you be able to do that better uh for the advanced population because you should be able to have much higher individual level resolution incredible uh population local structure right um right so I think there are two aspects of this question so number one is you know do we lose any information when we use summaries that’s relative to individual data whether it’s a homogeneous population or at next population right um so the question that the answer to that question is so if you only look at um only use the second order information it’s basically the LD information and then you assume you have a LD reference panel that can accurately approximate the true LD patterns in your dual sample and then there’s actually no information lost when we use the reference we use the summary stats data relative to the individual level data um so the question here is can we find a G1 ref can we find a reference panel that can closely approximate the LD patterns in your G1 sample um and so a lot of times you know when the G1 sample G was was conducted in a homogeneous population we think um you know the the reference panel was accurate enough but that also you know warrants you know if in the future it’s possible to release in Sample LD information with the geother summaries that we can of course do better and get more accurate LED patterns so we’ll have less information loss or less reference sample mismatch um in this case and then the second part of the question was you know if we have individual level data can we do better to handle app mix populations um and I think sure because with individual level data you can go beyond LD you can look at local ancestry and do those decomposition and build pris using those local ancestry information which can of course be much more accurate than uh treating the whole genome in a homogeneous way um so I think going forward releasing LD inflammation and local genetic information with the GEOS summaries that might be the key to further improve the polygenic prediction</p>
<p>these topics populations just follow the um so you have an implementation and to deal with individual level potato and we don’t actually but but if you have individual level data you can just compute the ink sample LD and then do a g was so then I’ve tried a message to the GEOS summary stats that should give you um you know highly similar results to a message using individual level information thank you question from Laura slootman we did but it was answered because I didn’t uh I thought the the answer was uh truncated before uh the individual level conversation uh I was going to ask specifically about what you just clarified actually girl this is could I ask a practical question um we usually transfer as ratio to look at ratio before calculating pis based on LD pruning and p-valuation approach so my question is do we need also do the same process in piscs because we know that the posterior effect size in this approach is relative small if we don’t um process the ratio to low guard ratio before calculated posterior effect size right so prcs can take alteration odds ratio estimates but it basically just take out ratio and take the lock and convert it to standardized beta so if you you know G1 summaries that’s it’s odd ratio then it’s by nprcs can take that I think I thought it was really encouraging to see how much better the prediction was with the page samples for the African ancestry individuals um do you have any ideas about why that worked as well as it did um yeah that’s a good question so I think number one so doors do population specific variants and information that data containing in the page study Geo summary is that that’s not available in the US of other populations the other possibility might be you know I integrate these Duos so because the LD pattern that the Audi block is smaller in African samples so we have a better localization of the Dual signal which also improved the production accuracy but I I think there’s much work to do to dissect these contributions and see</p>
<p>you know whether we can improve on that it’s very encouraging because I think sometimes when you see samples for European ancestries that are are over an order of magnitude or more larger than the other ancestries you think well maybe it’s not worth it including these other ancestries but it sounds like these data are suggesting that definitely you should I just have a question uh I think someone said about the phenotype I just wonder if you think that you know for the Latin population it’s because you know I think the sample size is small if if you have a better phenotype something like that should you have more chance to you know to I don’t know but to predict I think this meta-analysis or phenotype is something it’s a variable that matter here or not you know it’s just I don’t know if you start a project in Latin American that is totally mixed and you don’t know and you have simple there’s more simple size do you think it’s you know it’s good to spend I don’t know money or time having a deep phenotype or this is doesn’t matter so the video type typing that’s definitely influences the production accuracy of um pris and then if you have very different phenotyping in say or training and Target populations that might reduce the production accuracy and in many of our work so when we try different phenotyping methods so for example try</p>
<p>to predict depression and then there are different ways to you know type like using ICD codes um using any rule based or algorithm based definition of depression cases and controls and they do give us meaningfully different production accuracy and a lot of times um you know there’s a balance here because if you use the simpler icd-based method you get you know more cases and controls that you use the more Exchange in definition you get higher specificity but sometimes lower power because the case number is reduced so I think again there’s a manufacturers that contribute here the sample size and how the phenotyping matches between the discovery and Target data set and how specificity specifically the phenotyping algorithm is um a lot of times you know when you conduct the PRS analysis these phenotyping issues beyond our control because we only use Geo summary stats or tests the pr as the existing cohorts but if you have control over these genotyping algorithms um I think some kind of results phenotyping and sometimes boost the projection of pris okay so other question is just today I saw you know a talk in a conference that they use a family apology trios or families to predicted this polygenic risk score and then you know they have this analysis using trios what do you think about that it’s a good strategy or right I think Trio of family studies provide additional opportunities that can’t be done by uh PRS of unrelated individuals for example you can better control environmental factors and sometimes you can even decompo decompose and dissect the QR transmitted and non-transmitted abuse interesting questions that you can only do in family or truths that is so I think both methods are so so it’s so a lot of work we do is to do this PRS analysis in population-based cohort and trying to stratify patients for example but in terms of memory study they also give a different or unique aspect where you can look at the relative contribution of genetics environment so I think both study designs are useful and can be used to answer different questions it should your method can be used in this approach um that’s a good question so right now probably not because when we build the model we assume the G1 summaries does or calculated on a large unrelated G1 sample so if we want to conduct any um any PRS analysis that is specific to your family or Trio design probably need to look into you know more specific methods that can you know better address the questions there and those areas I remember of their paper about about estimating the cross population prediction activity</p>
<p>and um he said the most tricky part of the smart population predictions to different housing matrices and those variances so your message how these two parts are handled so I actually have some trouble here in this first part of the question well maybe I I see a louder a little bit better now I would say can you hear me the voice quality is not great um speaking up it seems like helps a little but it’s still a little bit difficult to hear um sorry I think it’s better sending the question in the chat because it’s okay then I will type the question sorry I don’t know sorry I don’t know if you I will have time but yeah no problem if you have any questions you can also you know email me afterwards [Music] yeah foreign how are the two matrices handled in your method um two different LD matrices um right so we use so there are you know if you have G Watson raised that’s from different populations you’ll have one without the some population specifically the matching that tensor trail that she was and then when we do this effects not estimates they are actually taken care of and then so different um effects sides in different populations were modeled by the matching LD reference panels and there’s a follow-up question that how are the variants with different minority frequency being handled into different matrices um how minor little frequencies are handled um so we don’t well so I’m not sure how to answer this question so how minor your frequencies are handling so when you come to the LD Matrix um they’re just using population specific reference panel to compute that penalty Matrix and that gives you um The Matrix for each population and then when we model the effect size</p>
<p>and those effect size are and the relationship between snaps in each population was mapped to the population</p>
<p>specific LD reference panel um I’m not sure if that answers the question all right so I think we’re at the hour thanks uh Tien for giving this great talk and thanks everyone for joining us for this uh for for this meeting and I we look forward to seeing each other again in amongst the next meeting is May 5th May 5th May 5th and I think believer back at the uh 105 p.m Eastern great thanks so much Jen and great to see everyone bye folks bye</p>
<hr>
</section>
</section>
<section id="prsice2-and-lassosum" class="level1">
<h1>PRSice2 and LASSOSUM</h1>
<p><strong>Title</strong>: PRSice: Basic Polygenic analysis</p>
<p><strong>Presenter(s)</strong>: Shing Wan Choi</p>
<p><strong>Shing Wan Choi</strong>:</p>
<p>so hi there um today we are going to talk about like a little bit on PRSice and because it’s a simple relatively simple software we’ll also talk about some of the basics of polygenic score analysis. so my name is Sam Sam Choi I’m from O’Reilly’s from uh I can school of medicine I’m on Sinai now first thing first we have to go to like the fundamental of polygenic score</p>
<p>so polygen school can be calculated as the sum of B like betas times uh x divided by n the betas are the effect sizes and the X are the individual genotypes so essentially it’s like the um weight score like of your genetic dosage you can notice that like there’s an N here which is the number of alleles this helps us to account for the number of missiness and it’s slightly different from what you usually see on in in projects so like to illustrate assume that we have three individuals that each have a different alleles some of them might be missing and then each of those we have something called the effective allele so for example for snip a the effective values is a so what we are saying is step four a little a it has an effect size of 0.1 relative to the reference which is like the non-effective value which is the value G for snip B the effective value is G and then it has a effect size of -2 so if X size doesn’t have to be all positive it can be positive and negative which has an effective value t with a value of four which is like huge but anyway let’s go ahead and see how we calculate the polygenic score so for each individuals we can basically just we can try to multiply the alleles with the corresponding effect size so for individual a it has two effective alleles so we will times the effect size by two so we get a 0.2</p>
<p>for the second individuals it has one effective allele so we’ll say that it has one uh it will multiply the beaters by one so point one and the third individuals has no effective value so all of this value is like non-effective so it’s a score of zero now we go ahead to excuse me the second snips is almost the same but when we have missing data we’ll usually just apply a zero to it or sometimes we do a mean implementations which use the populations allele frequency of the effective allele and and tomorrow in place of like zero so if this is our population for for example we’ll say that maybe we have like 0.25 a little frequency for G across because we only have two samples and there’s only one copy of values but here we’ll just say zero and then so on and so forth what we’ll then do is we try to sum up every single one of this score into one single total score so for individual a there will be 4 times 0.2 so 4.2 divided by the number of values so one two three four five six six alleles and same for the second one but for last sample because we have two missing alleles when we Define the number we’ll say We’ll divide by four instead of six now if we do it mean imputations we will still do it by six so that’s like some of the fine control of the allele frequency like how you do missingness imputations and some software like precise do have like functions like I’m playing you have functions for you to have a more fine control of how to do like this kind of implementations and some other software will just always imply zero score uh so after that we can just calculate the score so we can say that the first person have a score of 0.7 the second has a score of minus 0.317 and the last person has a score of two so this is the like General concept of how apologize calculated</p>
<p>now there are actually alternative formulas for PRS calculations uh one of the more common one is sum score instead of what we are using the average score the main difference is we we move the denominator from the equations and this is usually what was shown in the literature the second one Oops why why is it now but so the problem of this sum score is that it can be biased by the level of missingness so um if for example we have a sample like there’s an individuals with a large amount of individual genotype messiness and by definitions that samples score will be relatively smaller than other individuals and when you try to rank samples or try to distribute them across the populations this will introduce a bias that is like correlated with the level of genotype missingness and that might not be ideal um that’s why like for precise I’m playing the default settings is the average score not the sum score now another pop by a fair point uh formula is the standardized score what it does is take the average score and look at the whole populations and we try to standardize it so that has a mean zero and standard deviation of one uh the problem of the standardized score is is that for binary traits for case control samples when you standardize your score it can be affected by ascertainment so imagine in a data set where you have like 50 cases and 50 control for popular for trade with population preference of one percent your data is I’m rich with cases so when you calculate the mean and step deviations that the cases contribute more than what you will otherwise observe in the population and this introduced a bias uh it’s not as much of a problem quantity quantitative trade so that’s more specific for case control in uh precise we have an options where you can standardize your score using only the controls we assume the controls are a more representative more random that is something that you can do uh and and something that you should consider when you do the analysis</p>
<p>now as I just mentioned before there’s also different way of handling missiness now that’s the main implementations that I just mentioned which you can use the allele frequency of the your reference cleaner or your target sample data to estimate the binary frequency um the other one is zero centered so you try to make sure that the the missing sample all have a value of Zero by minusing the all the scores by the allele frequency so it’s similar now the main challenges in polygenic score analysis is actually that we are mainly concerning two factors the first one is the winners first when when you select a Snips or variants for PRS constructions they were selected because usually like because they’re significant or because they have a high effect sizes and that like unfortunately when we do selections those that were selected usually have inflator value that’s why they got selected and this is something we have to account for when we do the analysis another problem is uh linkage this equilibrium when we do G was linkage disequilibrium help us a lot because now we can reduce the number of tag Snips that we need and we can still have a good coverage of the genome but when we calculate a polygenic score if we have multiple alleles in the same regions in high LD then the effect size of that particular area will drop Will got like a rich and calculated many times and that will cause a bias now to deal with Winner’s curse we in Project score we usually do something called strength shrinkage what we essentially does is we push down the exact sizes by a certain factors using different methods and we are saying that by by we like strengthen or by pushing down these effect sizes we get a better representations of the true effects and hopefully will get a better estimate</p>
<p>another one is like linkage disequilibrium she wants this relies a lot on tagging so it doesn’t necessarily have genotype the causal variance it makes it quite difficult for us to say this variant is the true causal variance and we can just guess so and and variance this does lead to the situations where when we calculate polygenic score uh variance in LD to each other can lead to double Counting so imagine the most extreme scenario when two alleles are in perfect LD two variants are in perfect LD and you don’t like you when you calculate the polygenic score you are essentially calculating the same score like same alleles twice in the same model now if you imagine there’s a region of the genome that are in high LD like chromosome 6 like MHC regions along Le then you will get uh your score would be dominated by this kind of regions and you don’t want that because that causes bias</p>
<p>so we introduce a method called precise or more like a software called precise it’s a simple approach to projecting score analysis it mainly uses two methods the clumping method for accounting for LD and the pfl thresholding method as a form of shrinkage is computationally very efficient and relative to all the software is slightly easier to understand and comprehend although it does have a lower performance in terms of R square and P value of associations but we do argue that this is a very good starting point for you to get hold of your data get a general idea of what how how your publishing score will look like and then improve a point maybe using more complex more more advanced software like also some LD per space or MPR CSX or prcs all of which you might have exposed to in the PGC day this was first developed by Jack Houston in King’s College London and is now maintained by me uh and Dr Paul variety</p>
<p><em>Clumping</em></p>
<p>uh so with linkage this equilibrium PRSice performs something called clumping so imagine we have a bunch of variants I use this crappy DNA symbols to represent variants and and I rank them by their significance so that they are the most significant they are they are in LD so they’re linked together now if I calculate if I don’t do anything and just calculate the score then like this this alleles can get double counted or give a High proportions of effects to the project score what precise does is that we look at this variance we select the the most significant Snips for variance and then we remove any variance that are in LD with it and have a lower like a less significant p-value so here only the pink variance will remain in after clumping and those will be used for position score calculations</p>
<p><em>Thresholding</em></p>
<p>next up we do p-val thresholding so imagine we have a a a a g was like this is I think this is the PGC 108 season 3 Ninjas what we do is that we will perform some kind of uh thresholding is a way of heart thresholding what I meant is let’s say we only select Snips that has P value less than one e minus a it means that any variance that have subheaval higher than one U minus eight will get the effect size strength to zero so the effect size becomes zero this will iteratively change the change the pivotal threshold and then as you can see we we retain different portions of Snips in the project score and we strength the strength the rest of this score uh variance to have a effect size of zero the way precise select the P5 threshold is by means of at each threshold we’ll get a PRS and we’ll regress it against the phenotype of Interest we’ll then say that the regression model that gives us the best performance highest R square is the best threshold now a lot of people complain that this might be overfeed and and it does it does like because of this model optimizations it does cause some kind of overfitting but this over everything is inherent to polyogenic score model not just to precise</p>
<p><em>Empirical P-value</em></p>
<p>so what we developed is that in precise we try to do something called permutation like to calculate empirical p-value so what it does is we first calculate the we use the best PRS we look at like how what P value it does provide us we then suffer the phenotype to obtain a new p-value and we repeat it like 10 000 times and this gives us no distributions of possible p-values and then we we can then calculate the empirical P values as the number of time we observe a better more significant people than observed p-value this helped us to eliminate a little bit of the overfitting problem of of testing many parameters</p>
<p>so that’s the gist of it so before you like we’ll go through how like some general principle of running precise and some of the things that you need to think of when you do it so before you start precise analysis you’ll need the G1 summary statistics and you must have the snip ID call column the effect size effective allele and p-value column in the summary statistics those are the bare minimum the snip ID is for matching the the genotype data with the summary statistic data the effect size is giving us the effect size estimate the effective value tell us which allele just so that we can do strand flipping amplifiers for clumping now we are all sort of like require genotype data for the Target samples and these samples must not be included in the G was like otherwise you’ll get into trouble this is fundamental to all PRS method you must not have any of your target sample present energy was or you will get like highly inflated abnormal results now precise let’s accept like file in plank or bgm format so you can just directly use them which is fine uh you can also give like precise the phenotype data and the appropriate covariance if it was not already included in the fan file or the sample file um you can also give precise LD reference like thousand genome if your target sample size are small like less than 500 samples although that that’s usually like don’t have enough power PRS and that might be something useful now precise is mainly divided into two part the precise executable which is used for the core analyzes and the precise.r which is an R script that is just useful</p>
<p>before you start though you should from some basic quality controls on your data for example if you’re you have any duplicator Snips in your GEOS you should like make a decisions of whether you want to maintain one of those copies or do you want to remove both of those copies you should also do like common uh GEOS filtering on your target data for example we move with later samples remove samples with sex mismatch we both Snips on samples with excessive messiness etc etc to just ensure high quality data if you want more you can uh PRS tutorial paper or nature protocol and they will they are more unblocked outline of those so running precise is hopefully like intuitive uh we do try to make sure that our documentations is clear and our parameters as like makes as much sense as possible so running precisely it’s just type like precise and then you have to give us the target sample using the B file parameters you just give us the prefix and then you can give us the base which is the summary statistic file the GEOS if your GEOS has a standard iPhone map so for example the snip ID is called snip the effect size is called business a p-value is called P the effective allele column is called A1 then precise will automatically detect those otherwise you can always use the appropriate parameters uh minus minus A1 for the exact size effective value minus minus stats for effect sizes Etc to indicate to tell us like which columns correspond to which informations you can also give us the phenotype file using minus minus final and then use minus minus final column minus column to indicate which column you want if you have multiple columns uh you you would then tell need to tell us whether it is a continuous or non-con binary Target using binary Target uh if it is continuous precisely we use a linear regressions to identify the best threshold and if it is a binary phenotype then it would do a logistic regression you can then also give a covariate file and use minus minus cover code minus Co L to indicate the columns that you want to include in the covariance and then like one of the most asked about features is print Snips so people ask us like if there’s any way to show like the Snips included in the PRS and print slip options is your friend and finally to get the empirical copy value you use minus minus um ten thousand we recommend at least 10 000 because that’s when you get a relatively stable key value at 0.05 you can grow more but it’s very computationally intensive uh if you want to like get the plots all you need to do is just to add a I’ll use the r script instead sorry misspell should be our script no I’ll see yeah precise.all and then minus minus precise and then tell us where the executable is and all the other comments are identical now the most important one of the most important output from precise is the dot summary file which is show us the best model fit it will has informations of the phenotype the set like this is something related to preset usually it’s just space in precise which is genome y score threshold tells you which pfl threshold we use prsr square is the variance explained by only the PRS 4r squared is the variance explained by the full model including so including the convert and the PRS and then now R square is the variance explained only by the covariates</p>
<p>if you are if you have a binary traits you can give us the preference and then we’ll do an adjustment of the PRS to account for ascertainment bias if not then it would just be minus a coefficient is the regression coefficient it can be like huge reduced average score because the one unit change of polygenic score when you take the average and given the small effect size is a big change in phenotype usually so if you you will have a large value for coefficients if you want uh it can be smaller uh to be smaller than you standardized score will generally give you a smaller coefficients but it doesn’t shouldn’t change your P value or they’ll Square much uh p-value p is the p-value model fit standard errors that’s an error of the model number of snip is the number of simple included in the model and if you do the implementations then you will also get a empirical P column you can also look at the individual threshold model fit by looking at the top precise file but we generally found that not very informative so we ignore that quite a lot and then the other important output is the precise dot best file which is the contain the score at the best p-val threshold it has four columns for precise fib and ID are the individual ID provided and usually include in the plane file in regressions such as whether these samples were included in regression model so if your sample has a missing vary or missing phenotype we will still calculate the polygenic score but then in this best score folder we will tell you that it is not in in the regression model by having a node in this column and then finally the PRS which is like the poison score column</p>
<p>we if you use the r script you will also get some useful plots just for for your eyes so this is like the bar plot which is quite famous uh so you show sorry it shows you the R square at each different P value threshold and the corresponding P value at top and also colors so you can see immediately that the best P5 fashion is 0.4463 here with an R square roughly 0.05 and P value of 0.4.7 times 10 to the minus 18. alternatively we also have a high resolution score if you use the high resolution options of precise it shows you the p-value like model fit of each individual at each individual pivotal threshold the green line represent the same uh threshold that were included in the bar plot and this shows us like where the best pivotal threshold is and and the correspond P values so common problems uh we encounter with project score like precise we do try to capture as much of them as possible so we will launch our duplicate samples duplicate variance missing like excessive uh Missing call very uh wrong definitions of phenotypes Etc we try to do that but we cannot detect sample for that so this is something that you have to do it yourself but general rule of thumb if your results are too good to be true so for example everything is like very significant or like the R square is extremely high or something like that basically a flat plot and you know that you have sample for that most likely so this is almost the end of it if you want to get more informations of precise you can go to precise.info which is our official website or if you want to know about more General about polygenic score or how to do poison score or what are the concepts then you can go to our project score tutorial choicing one.github.io tutorial so that’s it thank you very much</p>
<hr>
</section>
<section id="ldpred2" class="level1">
<h1>LDpred2</h1>
<p><strong>Title</strong>: How to run LDpred2</p>
<p><strong>Presenter(s)</strong>: Florian Privé</p>
<p>hello my name is fran river i’m a postdoc at torres university and i will be talking today to this tutorial video about how to compute a project called using ldpride2 and this video was made for the pgc day at the wcpg2021 so if you go to this tutorial of lg pride 2 then i explain how to you know do the wall computation using some small data um so actually if you are if you don’t have enough data to um you know make your own ldl film so actually at least 2 000 individuals then we provide in the algebra to paper</p>
<p>we provide some ldl trends based on the uk bay bank so on the large sample size that you can use and here you can see the variants that are used in this in this data and basically this is a bit more than one million app map three variants where you have the information like chromosome position allows you also some pre-computed lg scar and also you have we we change the position so like two different builds so if you if you want to match with different um issues something for example some methods you have different builds in for the position then you can match directly using those okay so the uh so this is the lg reference that we provide so in this uh tutorial we just use some smoke small fake data just of the purpose of not to have to download like large files and also to run it quickly um</p>
<p>so you can download this tutorial data and zip it and it will like have like a directory called tmp data and then i just read the data from the bet format the Plink BED format in the format that is used in the big snipper package so this format is easy it’s just a list with basically three elements the first one is the geotab matrix which is stored using a special format that is a matrix format but stored on disk and then you have two data frames with information on the individuals and one with information on the identity virus i also read the summary stat stick so again it’s a fake summary statistic so here i’m using only 50000 genetic variants just to make it uh you know small and easy to run and actually the data that we need to run lgbt2 so we need the effect size beta the standard there are the effect size and also the sample size so if you have the sample size per variant then it’s better and also we need like the information a chromosome position and alice uh to match between your your summers that seek and and the data that you’re using so just for the purpose of this tutorial we just split the data that we have so only like 500 people uh we split in in validation set and test sets so the validation set will use will be used to tune the hyper parameters so choosing the parameter with the best prediction and the desktop is used to uh you know just uh see how it works in the final prediction in the test set</p>
<p>okay the first thing that we need to do is matching uh the gender data that we use with the summer statistics so we can match by chromosome position and adds so to do that there is this function called snip match and actually if you run this you will see that basically no almost no violence is matched so the problem here is that the sum stats and and the data is not in the position or not in the same build so yeah actually what you can do is modify the build using leftover and this function you can if you use the ldl form that we provide you can use one of the other columns for the position in different builds or you can instead of turning by positions you can join by the ass ids so for some pc we’ll do that here so now you can see that much more many more variants are matched okay now that we have matched the variants uh we need also some lg reference so you can you know either use the different iphones that we provide or just compute uh one from your data</p>
<p>uh in lgbt2 we recommend to use a windows site of three sensimorgan so you need to get the genetic position in this uh you know in centimeter so for that you can use this function um yeah just to convert the position and the physical position into a genetic position uh just for simplicity here i’ve pre-completed them and you can just get them from this field and then per chromosome we compute the correlation using this snip cap function using a three centimorgan window with this genetic position and in parallel it should be quite fast and then from this correlation matrix which is just you know a standard sparse r matrix we can get the ld scores because we’ll need the discard to analyze calculation later so again if you’re using the the ldl forms that we provide we provide some precomputed lg scores but remember that if you’re using alice correction with this uh this lds car you need to use the the the number of ions corresponding to this led car so the this total here not a subset of the matched violence so be aware of that and um</p>
<p>actually i convert this matrix into a another space matrix format but that is storm disk just to make it more efficient to parallelize over this this lg advance this compact culture is is quite new uh it’s make just the the matrix uh twice as efficient so now if you use the admap’s refinements the size of this data on this should be approximately 15 or 16 gigabyte gigabytes of just gigabytes and so you need to be able to store this to keep it in in cash so that it’s fast and you need also to store all your results from the you know all the outputs from algebra 2. so if using like 50 or 60 gigabytes it should be more than enough normally and then we can run many males so far the ancient thermal model uh which is um just a model assuming like that all the variants are causal and actually there is an analytical formula for that so it’s just a matter of solving a linear system so it should be quite fast um so you get the effect size then you can get the prediction but just you know project with the gen type matrix and then looked at the correlation between the prediction and the actual phenotype in the test sets and we get like 32 percent of college then we can run algebra to grade which is then so that our twitter parameters the irritability the sniper readability and um the proportion of causal variant p which can be smaller than one if you expect not the not all valence to be causal so for the italicity we use so the irritability estimate from lg score regulation here um and actually because the discoloration can be um not very precise when for example the trait that you are interested in is not very politic so we also try 0.7 times the this value and 1.4 times this value and actually we also have 0.3 times this value because uh in our smartphone we’ll see that if we use a small irritability estimate it’s induces more shrinkage morganization and it’s sometimes useful when the quality of the uh the mystery is not that way uh for p so sequence between 10 to f minus five to one on the log scale and then we have this parameter sparse uh if you use sparsical true it will just make sure that you um you have some effects that are exactly zero</p>
<p>uh to make the final position of sparse so so lots of uh some of the effect size will be exactly zero so the violence won’t be used um in the modern so we can compute the uh so the betas corresponding to all these models so a lot of models that are running in parallel um then get the prediction from all these models and then in the validation set we can compute some kind of score so here we use the t score from a linear version between the phenotype and the polishing scar so actually you can use uh some covariates here as well and if you’re using a binary trait then you can use a glm with a family cause binomial</p>
<p>and then we can look at all the models uh using gg plot if we want so we have like the itability the color the editability x-axis the proportion of causal variance this is the z-score that we get here and particles are splashing also we can look at the the best models uh so actually if you care about sparsity then you can maybe you know choose this models that is you know getting you almost the same score in the validation as the best model but then you have like a very sparse</p>
<p>[Music]</p>
<p>vector of fx size at the end we can get the best prediction and then for the best prediction we compute in the validation set we compute the the final position in the test set and look at the correlation uh with the phenotype and then we get some much better prediction than the infestable model so with algebra two it’s usually the case that you would get major better collection with l5 to play that compared to our early grade 2 off and then we have ldpre to auto which is</p>
<p>[Music]</p>
<p>so instead of trying many values for the hyper parameters it just um like estimate them from the data so the editability is invalidation proportional estimate directly from the data we run actually many models here as well because we run many chains with different initial p values for p uh so in parallel again so you get like here 30 different models you can look at one of them where you get like the betas the proportion of the impossible variants uh the pst made the h2 the snippet ability estimates and so on you can look at them in ggpr2 like the it’s the path of the like each iteration the values of each iteration in the gibson player and then you can get all the prediction and then you should have a way to filter the chains that for example didn’t convert or diverge um</p>
<p>so to do that we propose two methods so the first one is based on just computing the standard deviation of the predictors and just keeping the ones that are close to each other or you can also compare the irritability estimate and proportion of being causal estimates and look at the chains that are close to each other and then the final prediction is just the average over the chance that you keep and then if you look at the correlation with the final estimate final prediction with the phenotype that you get a very good prediction</p>
<p>and finally i will talk very quickly about lysosome 2 which is just a re-implementation of flashoscent uh but that uses the exact same parameters as alibrate2 so this ld matrix and and this uh you know beta and see and effect sizes no so effective standards exercises and sample sizes and so you can just run a snip lasso something like this with the exact same you know parameters then compute so far grid of a hyper parameter there are two electronic linear system you would get all the prediction then you can choose the best one again based on the validation set you can look at all of them you can choose the best one and actually what you could do is just you know take the grenade to greater and lasso sam2 and take the prediction the best prediction overall in the validation set yeah that’s that’s all</p>
<hr>
</section>
<section id="sbayesr" class="level1">
<h1>SBayesR</h1>
<p><strong>Title</strong>: How to run SBayesR</p>
<p><strong>Presenter(s)</strong>: Jian Zeng</p>
<hr>
</section>
<section id="vertical-transmission-and-genetic-nurture-with-prs" class="level1">
<h1>Vertical Transmission and Genetic Nurture with PRS</h1>
<p><strong>Title</strong>: Estimating Vertical Transmission and Genetic Nurture using Polygenic Scores</p>
<p><strong>Presenter(s)</strong>: Jared Balbona, Yongkang Kim, Matthew Keller</p>
<p>all right hi everyone uh thank you all for watching uh so my name is Jared balbona and today I’ll be talking about a project on estimating vertical transmission and genetic nurture using polygenic scores this is a project I’ve been working on with Matt Keller and young gone Kim here at the University of Colorado Boulder and it’s one that I’m very excited about so without further Ado I will start with a brief overview</p>
<p>as you all know parents and Offspring resemble one another in a variety of ways one of the most commonly used examples of which at least in our field is that Highly Educated parents tend to have Highly Educated offspring so there are a couple different explanations for the similarity between parents and kids occurs the first and perhaps most obvious of which is share genetic effects so the parent has a strong genetic predisposition for higher education which she transmits to her daughter giving her daughter that same predisposition however we can consider an alternative explanation in which being Highly Educated itself influences how parents raise their child so for example a highly educated parent may be more likely to encourage a positive attitude towards schoolwork or to redo their child all these different behaviors that collectively could have a potential influence on the child’s education level down the line so this effect of a parental phenotype on an offspring one is called vertical transmission and it’s mediated here by What’s called the offspring’s familial environment that is the environment in which the child is raised so of course this vertical transmission path and this shared genetic one are not mutually exclusive but rather they co-occur meaning that children with genes conducive to higher education are more likely to also have environments conducive education in other words vertical transmission means that a person’s genes and environment will covarry with one another a covariance that has recently been termed genetic nurture as you can probably guess this genetic nurture covariance makes it very challenging to tease apart genetic and environmental Pathways from one another which is something that we need to do in order to estimate and understand the relative importance of parenting in shaping who children become furthermore if unaccounted for genetic nurture will bias the estimates of heritability that we get from both twin based and genome-wide studies of genetic effects and it does so in some kind of unpredictable and counter-intuitive ways making it all the more important for us to tease apart</p>
<p>so to this end uh researchers have begun focusing not only on the genetic variants that parents transmit to their kids but also in the set of alleles that parents have but do not transmit to their children shown up here</p>
<p>so like these transmitted alleles the non-transmitted alleles still have the direct impact on the parental phenotype which will in turn influence the offspring’s familial environment and subsequently The Offspring phenotype however unlike the transmit alleles that are shared between parents and offspring these non-transmitted alleles do not have a direct effect on The Offspring so notice that there’s no Arrow right here so thus by essentially subtracting these pads in green from these pads in red we can get an unbiased estimate of this direct genetic effect that is the actual causal effect of these alleles on the phenotype and we can use that to in turn get an unbiased estimate of vertical transmission so recently Matt Young and I took this logic and instantiated it into a series of structural equation models for a paper we recently published so this is one of those models which contains all the pieces I just described namely it has these three y’s to represent uh the parents and Offspring phenotypes and for those of you who do structural equation modeling note the circles here so the parental phenotypes don’t actually need to be observed in our models they can be represented as latent variables if need be so in addition to these three phenotypes we also have these three big F’s representing each person’s familial environment that is the environment in which they were raised we also have these NTS two NTS representing quadronic scores made from the non-transmitted portions of each parent’s genotype and these T’s to represent positive scores made from the transmitted portions of the parental genotypes so note that these T’s have arrows going to both the parental and to The Offspring phenotypes so this model Compares relationships between all these different pieces to estimate parental effects and in doing so it builds on past work in a number of ways namely using only a polygenic score it can provide unbiased estimates of heritability vertical transmission and genetic nurture and it can do so even when the plyogenic score being used is not very predictive which uh is great considering that polygenic scores generally do explain a small proportion of trait heritability right now secondly uh they can properly account for various types of assorted of mating at both equilibrium and disequilibrium three they are not in principle biased by missing data if those data are missing at random so that’s uh these models do not require complete trios if we just have father child or mother child pairs that works too you’ll just need a larger sample size to achieve equivalent power fourth as will hopefully become clear over the course of this demonstration these models are very easy to use relying on relatively simple math and five which I hope will also become clear they can allow for a vast number of extensions to best fit the trait and data being worked with so uh that was a really quick overview</p>
<p>um but moving on from there I’ll now pivot a bit and start talking about how these estimates are actually derived using openmx so openmx is a package in R that’s great for working with structural equation models and it’s one that I’m guessing a lot of you are probably already familiar with so we’ll be running a script I wrote in openmx on some example data that I simulated so you can download that the code and the practice data from here tinyurl.com scmpgs2021 that should redirect you to my Google Drive um but if for whatever reason this a nice little short link doesn’t work out you can also try to type in this link here which will also take you to the same place where you can download the data and the script I’ll be going over here</p>
<p>so using the simulated data will be trying to answer the question how does parental educational attainment influence Offspring educational attainment as with just about any structural equation modeling this process begins with choosing the actual model</p>
<p>so like we described in our paper there are several different variations of our models each designed to address different types of questions with different types of data and they might entail some different assumptions along the way so because of that there are some questions to consider when you start thinking about the type of test you’d like to run first is this a trait that parents maybe is sorting on and has this assortment likely been consistent across Generations so in other words is this a quality that partners are more likely to be similar on such as education or height and is it possible that uh the importance of this trait in mid-selection has changed over time so for height probably not tall people have probably always preferred tall people short people with short people but for Education it’s very possible that it’s become increasingly relevant over the past 100 years or so secondly there are good existing estimates of heritability for this trait and could the heritability be age or sex dependent three how many trios and parent Offspring pairs do I have available do they all have measured phenotypic data four do I have any other nuclear family relatives like grandparents siblings or spouses in my data so uh one thing to point out for these first two points which are a little bit more conceptual uh you don’t need to have actual answers to those questions these are just things to consider and are also things that you could test using our models if they’re of Interest um and I should also point out this is not by any means an exhaustive list or this is just something to you know consider as you’re getting started so moving back to those models uh we can stick with the model that I showed earlier this one right here which makes two important assumptions first it assumes that there is a sort of demating in both the parental and Grand parental Generations so following the educational attainment example we’re working with here it assumes that uh in the parental generation and in the grand parental generation spouses prefer to mate with people who are phenotypically similar to them who have similar education levels</p>
<p>so secondly this model also assumes that the polygenic score explains the full trait heritability which of course is not true for any trait um at present so this specific model is mostly useful didactically I wouldn’t recommend using it or the example script on actual data to estimate parental effects but uh we do have other models that will relax the second assumption which will be more effective uh for actual analyzes so just something to point out okay</p>
<p>so as mentioned earlier uh our models require five pieces of excuse me five observed pieces of information so first The Offspring is phenotype must be observed so in this case it’s a value indicating the amount of schooling completed by each individual in the sample we also have uh our models also require two educational attainment polygenic scores for each offspring one from the set of alleles they inherited from their dad one from the set of alleles they inherited from their mom as well as two pledging scores made from the non-transmitted portions of the parental genotypes</p>
<p>finally um as I touched on earlier we can incorporate observed parental phenotypes if we’d like which will increase our</p>
<p>power but they are not necessary and for that reason they’re not included in the example script I’ll be going over so these are our observed values that we feed in openmx so if I switch over to R this is the script uh that’s posted on Google Drive if we read in the data here and take a look at it you can see that the data looks uh exactly the same as what as what I showed on that PowerPoint slide so one column for each um excuse me uh one column for each parameter phenotype and the four plugin scores one row for each individual so you can see for example this individual has a lower polygenic score and lower phenotypic score than say this individual here and all the columns for this particular script do need to be in this specific order which is something you can adjust when you do this yourself so uh once we’ve written the data for this script we’ll specify a couple of options uh you can play around with which options you’d like to use some might be better students text specific types of tests and others however one that is required for is a requirement for our models is this NP solo Optimizer option which I’ll explain in a second</p>
<p>all right so the first thing to do is to create the different variables that our models will be estimating which we’re doing here so each of these will uh we’ll get an estimate for each of these down the line we’re explaining that they’re free to be estimated we’re not constraining them to a specific value and below that for some of those variables we’ll be giving algebraic constraints so these algebraic constraints are derived from path tracing they’re all described in uh pretty great detail in our paper if that’s of interest to you and one important thing to note you can see for example that v y is a function of VF and VF is a function of v y similarly G is a function of omega omega as a function of G this type of circularity is a Hallmark of a lot of causal models and it’s definitely present in our models these are called non-linear constraints and to be estimated uh we would be that’s where this NP solid Optimizer comes in so MP cell is a software package that openmx implements to estimate nonlinear constraints and it does so in a way that’s really efficient and effective uh and works really well for these kinds of analyzes</p>
<p>so once we create the variables and provide algebraic constraints we’ll also provide some more quality constraints in this case just saying that these variables are equal in value so the algebra here so nothing’s super interesting in this case but for other more complicated tests that you might want to run you can play around with the quality constraints here you can say things like I want the genetic values to be constrained uh for parents and for their offspring or you can relax that assumption and say that you don’t want the parental and offspring’s genetic values to be the same or you can do that for assortment for Parental effects whatever you want um as with everything in the script you can play around with uh different options to get different results so once you’ve specified all the different variables we will then provide an expected covariance Matrix here so the idea behind that and we’ll go to PowerPoint for this is that uh openmix will take all the different variables that are inside of the expected covariance Matrix and simultaneously get estimates for each one and a manner that minimizes the distance between the estimates and The observed excuse me the observed values as much as possible so it’ll try to come up say with values of Delta g w and K that approximate 0.28 as closely as possible for example and then they’ll do that for all of these so moving back to the r script specify all these options and if I don’t get to anything hopefully the annotations will describe what’s</p>
<p>going on but if you have any questions feel free to reach out to me so here you can see we have a couple of Nest to for loops these are trying out different combinations of start values for Delta v e and f and if we run that so using these different start values open Max will iteratively test out different combinations of parameter estimates in a way that uh like I said minimizes the distance between the expected and The observed covariance matrices so once we run that we get a couple of warning messages here and fortunately these are don’t look like anything to really worry too much about most are saying that the start values are not feasible which is to be expected if we’re telling openmx that phenotypic variance is very heavily dependent on uh genetic factors and that it’s also very heavily dependent on on environmental factors it’s possible that those two things can’t co-exist at the same time and in which case it will produce this warning and Nas in your data which is totally fine</p>
<p>so once we tested out all those different combinations of start values those final step step four we’ll select the one with the lowest log likelihood and produce it at the bottom in a data frame called result summary shown here so uh firm result summary this is really what we’re what the whole script is all about we can see the phenotypic variance and the relative amounts of variance due to additive genetics parental effects and unique environment and uh it has to have a key here at the bottom to explain what these different columns mean um I made you forget so in these results we can see that parental effects are explaining a relatively small proportion of the phenotypic variance especially compared to additive genetics and unique environment so if these results are to be believed uh it appears that parents have very little in or I shouldn’t say very relatively little influence on the amount of schooling that their children choose to pursue later in life of course it’s possible that if you tried a different model that makes different assumptions uh tests different constraints it may fit even better than this one and you might see it a slight change and the relative importance of genetics and environment so that’s something to consider</p>
<p>and also I know not to belabor the point I know I’ve said this a couple times but again these are simulated data they’re totally made up um actual empirical evidence suggests that parents do have a relatively substantial influence on their offspring’s education level so take these numbers with a big grain of salt</p>
<p>okay so that is the script and moving back to the PowerPoint I can close now by talking about uh a couple of the other models we built and the types of questions that they can address so importantly a lot of these models are part of upcoming Publications in our lab so they haven’t been posted on GitHub just yet but hopefully they will very soon so first we have models that can leverage sporadic relatedness in large samples so you can derive estimates of Parental effects without needing to actually recruit a large number of trios so for example um there aren’t a ton of trios in the UK biobank but there are a large number of siblings and spouses so you can use those toe Drive estimates um which is a lot easier than getting a hold of actual Trio data two these models can be used bivariately without complicating the knot too much so we can test the effect of a parental trait on a different Offspring trait to get an unbiased estimate which has never been done before we can also model various types of assorted mating like primary phenotypic assortment genetic homogamy or social homogamy four we can test whether effects are sex dependent so this can be used to address questions like do fathers have a greater influence on Offspring substance use than others are these effects dependent on whether they have a son or a daughter things like that and similarly we can examine how effects have changed across previous generations so you can look at how mate preferences made preferences have changed across Generations like I said earlier but you can also look at whether the influence of parenting has increased or decreased over time so maybe pure influence becomes more important for certain traits or maybe uh parenting remains equally important across time so a lot of different avenues for future studies a lot of things that we’re really excited about and uh hopefully we’ll start to release data on in the upcoming future</p>
<p>so with that I’d like to thank my co-authors Matt and young and all of you for listening I know that was a lot of information in a pretty short period of time so feel feel free to email us if you have any questions we’re happy to help out to help sort of troubleshoot anything we can do to help so thank you all very much</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
          // target, if specified
          link.setAttribute("target", "_blank");
          // default icon
          link.classList.add("external");
      }
    }
});
</script>
</div> <!-- /content -->



</body></html>