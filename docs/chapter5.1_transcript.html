<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.353">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>PGC Video Textbook - 5.1 Quality Control (Video Transcript)</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>


<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar docked nav-fixed">


<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">PGC Video Textbook</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link active" href="./index.html" rel="" target="" aria-current="page">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./contact.html" rel="" target="">
 <span class="menu-text">Contact us</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://pgc.unc.edu/" rel="" target="">
 <span class="menu-text">PGC Website</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools ms-auto">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item">5.1 Quality Control (Video Transcript)</li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
      <a href="./index.html" class="sidebar-logo-link">
      <img src="./pgc_logo_website_v3.jpeg" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
      <div class="sidebar-tools-main">
    <div class="dropdown">
      <a href="" title="" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" aria-label=""><i class="bi bi-github"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="">
            Source code
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="">
            Report a bug
            </a>
          </li>
      </ul>
    </div>
</div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./welcome.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome to the PGC Video Textbook!</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./toc.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Table of Contents</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Chapters</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 1: Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 2: The Genome</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 3: Technologies</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 4: Study designs</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 5: GWAS analysis</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 6: Polygenic Scores</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter7.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 7: Ancestry-Specific Analyses and Considerations</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter8.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 8: Post-GWAS bioinformatics</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter9.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 9: Advanced Topics</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 10: Other Considerations</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
 <span class="menu-text">Software Tutorials</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./software_cnvs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">CNVs</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./software_conditional.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Conditional Analysis</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./software_datasets.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Datasets</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./software_ewas.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">EWAS</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./software_geneset.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Gene Set Identification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./software_gwas.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">GWAS</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./software_genomicSEM.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Genomic SEM</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./software_MR.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Mendelian Randomization</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./software_mtag.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">MTAG</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./software_prs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">PRS</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./software_correlation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">SNP Heritability and Genetic Correlation</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
 <span class="menu-text">Additional Resources</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./glossary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Glossary</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./resources.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Tools/Software Resources</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./addreading.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Additional Reading</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="https://pgc.unc.edu/for-researchers/download-results/" class="sidebar-item-text sidebar-link">
 <span class="menu-text">PGC Summary Statistics</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#sec-video1" id="toc-sec-video1" class="nav-link active" data-scroll-target="#sec-video1">Quality control: Introduction</a></li>
  <li><a href="#sec-video2" id="toc-sec-video2" class="nav-link" data-scroll-target="#sec-video2">Running Quality Control on Genotype Data</a></li>
  <li><a href="#sec-video3" id="toc-sec-video3" class="nav-link" data-scroll-target="#sec-video3">Considerations for Genotyping QC</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">5.1 Quality Control (Video Transcript)</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<hr>
<section id="sec-video1" class="level1">
<h1>Quality control: Introduction</h1>
<p><strong>Title</strong>: Quality control</p>
<p><strong>Presenter(s)</strong>: Katrina Grasby (katrina.grasby@qimrberghofer.edu.au) and Lucía Colodro Conde (Lucia.ColodroConde@qimrberghofer.edu.au), from the 2021 International Statistical Genetics Workshop hosted by the Institute for Behavioral Genetics at the University of Colorado, Boulder.</p>
<p>Thanks for joining me for this session on Quality Control. In this recording I’m going to be talking about the quality control or QC steps that we apply to genetic data. So this is in the very early stages of a study. We’ve collected our DNA, it’s being transformed into data. We’re going to clean that data up and then we will impute and then we can do our statistical analyses. So there’s many points in a study that will be applying QC, but these steps that we’ll be discussing here and in the tutorial, are the quality control steps that we apply to our genetic data.</p>
<p><em>Why do quality control?</em></p>
<p>So why do quality control? Essentially, poor quality data is going to contribute to false positives and false negatives in our results. So we want robust results. We’re going to need to clean our data up. So we’ll be removing essentially genotyping errors. These can be errors in the calling of genotypes, or the translation of DNA into data. They can be due to lots of different factors. One of the pictures that I like to bring to my own mind was a story given to me by a woman that I work with who was involved in a project where they posted out two spit kits to a couple who were participating in a project, and somewhere in that delivery one of the kits went missing or was damaged. And the couple thought or were trying to be helpful and both of them spat into the same kit and posted that back to us --to her. In doing so they also included a letter to say what they had done, but it was a classic example of DNA contamination. It’s an example of human error. After all, we ended up with no usable data from two people instead of having usable data from one person. There is no way that we can disentangle that DNA in that spit kit and say this belongs to that person and this belongs to that person. It’s also an example of contaminated DNA, and even if they had not included a letter to say what they had done, the steps that we will go through in the tutorial would be able to identify a problem like this. So we can actually go OK, this isn’t a clear indication of data from a person, a specific person. We can remove that it doesn’t interfere with our analyses.</p>
<p>So one of the other things that will be doing in the tutorial is, after we’ve cleaned up our data, we’re going to have a look at the relationship structure within our data, and whilst that’s not necessarily a quality control step, it is a necessary aspect of coming to understand our data so that we can apply appropriate analyses and that is going to be important for minimizing our false positives and false negatives. So how do we go from DNA to data?</p>
<p><em>DNA to data</em></p>
<p>I’m a behavior geneticist. I use statistics to analyze data. I have no experience working in a laboratory, actually processing the DNA into data. But it is still useful for me to have an idea of these many different steps that are involved and an appreciation of what are the possible sources of error and what exactly does my data represent. So we are able to post out spit kits to participants who can spit into that kit at home and post it back. The sample is then processed so that the DNA is fragmented, it’s chopped up into little pieces. And then it’s amplified, so we’ve got more of it. And then DNA is extracted. We can store some and then we can plate some on to SNP chips or genotyping arrays. For it to be then further analyzed. So this down the bottom here. These images come from the Illumina website. This is an example here at A of a SNP chip or a genotyping array. So there are many different forms of SNP chips. The technology has improved overtime and I’m sure it will continue to improve. This here is an example of the bead technology. On this particular chip, there is space for information, DNA, from 12 different individuals. These horizontal bars here are each [for] a different individual. Now, if you’re thinking then, looking at this SNP chip, then if you got information from multiple individuals, and you’ll have many chips and they might be sent off to a DNA a genotyping company for processing in different batches. If you are thinking from an experimental point of view, and when you’ve got cases and controls, you want to have your cases and controls randomly allotted to both the chips and the batch runs that they’re being processed under. In a similar way, if you have males and females you want to randomized them across your chips and also your batch runs. That way we can ensure that we can actually pick up any particular batch effects in our data once we’ve got our data at the end.</p>
<p>So back to the chip. For each of these individuals, there will be hundreds of thousands of probes in order to test the alleles at hundreds of thousands of points in the genome. Many many many loci. So each of these wells have has a bead. This here is a schematic of a bead. So this bead is targeting an allele at a locus. So it has a particular sequence here, an address, so this is the order of bases. And then this here is the locus of interest. So once you’ve got your fragmented DNA, it’s going to come along, if it’s the right location in the genome, it will bind to this bead and then depending on if this allele here bonds to this C, so G will bond to C, this bead will fluoresce green. A different bead it might bond to, if there’s an A at that location and a T here, then it will bond this way and it will fluoresce red. So this is how we’re establishing at that locus. You might have a G or you might have a T and if it’s a G it’s going to bond to the C and fluoresce green. If it’s a T it will bond to the A and fluoresce red. So this is translating the DNA into a color, it is called an intensity. So if you’ve got you’ve got DNA coming from your biological father from your biological mother. You’ve got two alleles at that locus. If your two alleles are the same, you have two G alleles. They’re both going to be fluorescing green. Nice solid green color. If they both, if you’ve got two T alleles, they’re both going to be bonding to these A beads. They’re both going to be a nice solid red color. If you’ve got a G coming from one parent and a T coming from the other, then some of the beads that are C you’re going to fluoresce green. Some of the beads that are A are going to fluoresce red and that person is going to be heterozygous and they’ll have this yellow color. So these colors are then representing the three possible genotypes at that locus in the genome. And then these here are for hundreds of thousands of different loci in the genome. What I’ve got in this particular slide are examples of genotyping intensities. genotyping intensities So this is how we’re going to look at the color clusters representing the different genotypes. And see whether or not there are any problems. Now, this will likely, this is typically done by a genotyping company, you will probably not be doing this. But they will give you information about these first steps of quality control at this stage so you know what’s going on with your data. It’ll be there in a report from the company.</p>
<p>This top left-hand corner is a really good example of what we’re looking for. We have three nice, separated clusters. This is a homozygous A allele, This is a heterozygous group of individuals, and this is homozygous for the other allele. And in these two examples, with their little black Xs they are representing missing data. So missing data may not be terribly problematic if there’s just a little bit of missingness and it’s across all the different genotypes. However, if it is biased to one allele or one genotype, then that’s going to interfere with our allele frequencies in our sample, and that is going to mean that it’s not going to be representative of the population, it’s not representative in terms of how we can actually test for this genotype against this phenotype. We don’t want to have biased information about allele calling or genotype calling. Down here in the bottom left hand corner of it we have an example of a very rare allele. Sorry, a rare genotype, or it is a rare allele as well as a rare genotype. So there’s only one individual here who’s homozygous for the A allele. Very few heterozygous. In the middle down the bottom, this would be an example of a monomorphic group at this locus, so it really isn’t a useful locus for us to have genotyped. Or it could be that just this population is, there’s no variation in this population at this locus. And in the right hand bottom corner we have an example where there’s really been a failure to call the genotypes correctly. There is no indication of any red color, which is representing the heterozygous group of people. We’ve got these two kind of green clusters and the missingness is all off on this cluster it’s a complete fail.</p>
<p><em>Checking the data</em></p>
<p>So the steps that we’re going to be going through with our quality control tutorial is we’re going to start off by checking the data. We’re going to have a look at the file format. How is data coded? How is missingness coded? We’re going to look at the build, so that we know what assembly our data is on. The genotyping company would have provided us with that, but you might not always have access to that information, so there are ways that we can check that out ourselves. This is a very useful resource, which we will use in the tutorial to do that. And... Knowing what build your data is on is very important, particularly for meta-analysis, but also if you’re going to do any follow-up analyses with, or follow-up work with, your results. We’ll be doing a sex check, which is to check that the sex that we can infer from the genetic sex check information is matching the sex reported by the individuals. So this check is looking at the heterozygosity of the X chromosome. And we have different expectations depending on whether an individual has one or two X chromosomes. So if the individual is reporting their sex and the genetic information comes back and it doesn’t match, and that happens for a lot of your sample, then you might have a problem with the information that is, matching your genetic information that has been returned after genotyping to your participant IDs. Bear in mind this is about biological sex and not about gender.</p>
<p><em>Genotyping call rate</em></p>
<p>We will be checking for missingness. So there’s two types of missingness that will check for. One is this one, the genotyping call rate. This is where SNPs are missing information on individuals. So for each SNP we want to have information coming from most of our individuals. If there is too much missing data for that SNP, so too many individuals did not have information that was called correctly for that SNP, then that SNP might not be a good SNP for us to be using in our analyses.</p>
<p><em>Hardy Weinberg equilibrium</em></p>
<p>We will have a look at the Hardy-Weinberg equilibrium, to see whether or not our allele frequencies are matching what we expect. So this can highlight whether we’ve got some bias in terms of the frequency of alleles, or perhaps in our terms of calling genotypes appropriately, thinking back to those genotype Gwise intensities. Will be checking the minor allele frequency. So this is to make sure that we have enough information to do statistical analyses. If it’s too rare, then our GWAS is not the appropriate tool to use perhaps for this particular locus.</p>
<p><em>Sample Call Rate</em></p>
<p>We’ll be having a look at sample call rate. So this is another form of missingness. This is to say, do all of our individuals have information across almost all of their SNPs. So we don’t want individuals to be missing too much information across many SNPs.</p>
<p><em>Heterozygosity</em></p>
<p>We’ll be looking at the proportion of heterozygosity. So this is a way of checking-- Think back to that sample where we had two people spitting into the same kit. That’s going to give us too much variation. There will be way too much variation in that DNA sample. So heterozygosity would be excessive. Inversely, reduced heterozygosity could be an example of inbreeding, but it could also just be that we had lots of missing data.</p>
<p><em>Reduced Heterozygosity</em></p>
<p>So that’s one of the reasons we’re going to check out our missingness first before we do our heterozygosity check. Because we don’t want to be making, or we don’t want to be setting ourselves up, to potentially making inferences that have social consequences that are negative. So if you’ve got missing data and that’s the reason you have reduced heterozygosity, you don’t want to end up looking at your sample going “oh, there’s lots of inbreeding here”.</p>
<p><em>Relationship structure</em></p>
<p>Towards the end of the tutorial, after we cleaned it will then have a look at the relationship structure in our data. So we might have lots of families or we might have extended families. We want to know whether or not our individuals are related so that we can apply the right type of statistical analyses.</p>
<p><em>Population structure</em></p>
<p>And finally, we’ll be having look at population structure or stratification. So that will be talked about more in another one of the sessions, but this is when we have a look at a little frequencies. There is differences in allele frequencies across different groups or different populations and that is an important thing for us to be aware of and to be including appropriately in our analysis. Elsewise, we’re going to get false positives and false negatives. If your population structure is also correlated in some way with your outcome of interest, that’s where we’re going to get a problem. And that’s when we’re going to talk about it in terms of population stratification.</p>
<p>So these are going to be out checklist for our key steps in QC that will be running through the tutorial.</p>
<hr>
</section>
<section id="sec-video2" class="level1">
<h1>Running Quality Control on Genotype Data</h1>
<p><strong>Title</strong>: How to run Quality Control on Genome-Wide Genotyping Data</p>
<p><strong>Presenter(s)</strong>: Jonathan Coleman</p>
<p>Hello, I’m Joni Coleman and in this brief presentation I’m going to discuss some key points concerned with running quality control on genome-wide genotype data which is a common first step in running a GWAS.</p>
<p><em>Overview</em></p>
<p>I’m going to provide a theoretical overview, addressing the overarching reasons why we need to do QC. Highlighting some common steps, and discussing a few pitfalls the data might throw up.</p>
<p>I'm not going to talk about conducting imputation, or GWAS analyses, or secondary analyses. &nbsp;Nor am I going to talk at great length about the process of genotyping and ensuring the quality of genotyping calls. I'll similarly not go into any deep code or maths, however, if you are starting to run your own qc and analyses I recommend the PGC’s RICOPILI automated pipeline as a starting point. There are also some simple scripts on my group’s github that may be useful as well. They follow a step-by-step process with codes and explanations. We’re currently updating this repository, so look out for some video tutorials there as well.</p>
<p><em>The beginning: genome-wide genotypes</em></p>
<p>So here is our starting point. I’ll be using this graph on the top right several times through this talk, and this is a genotype calling graph with common homozygotes in blue, heterozygotes in green, and rare homozygotes in red. Hopefully your data will already have been put through an automated genotype calling pipeline, and if you're really lucky, an overworked and under-appreciated bioinformatician might have done some manual recalling to ensure the quality of the data is as high as possible.</p>
<p>But in point of fact the data you will be using won’t be in this visual form but rather as a numeric matrix like the one below, with SNPs, and individuals. This might be in the form of a PLINK genotype file or it’s binary equivalent, or it’s in some similar form that can be converted to the PLINK format.</p>
<p><em>The desired endpoint: clean, complete data</em></p>
<p>Where we want to go is clean data with variants that are called in the majority of participants in your study, and won’t cause biases in downstream analyses.</p>
<p>That should give a nice clean Manhattan plot from GWAS was like the one below rather than the starry night effect of this poorly QC’d Manhattan plot above. &nbsp;However, something I’d like to emphasize across this talk is that QC is a data informed process, and what works for one cohort won’t necessarily be exactly right for another. Good QC requires the analyst to investigate and understand the data.</p>
<p><em>"Rare" variants</em></p>
<p>Often the first step is to remove rare variants, and this is because we cannot be certain of variant calls. Consider the variance in the circle on the right. Are these outlying common homozygotes or are they heterozygotes? We cannot really tell because there aren’t enough of them to form a recognizable cluster. Typically, we might want to exclude variants with a low minor allele count for example five. There are many excellent automated calling methods to increase the amount of certainty you have in these variants but it’s also worth noting that many analytical methods don’t deal well with rare variants anyway.</p>
<p>Again, the demands of your data determine your QC choices. It may be more useful for you to call rare variants even if you’re uncertain of them. Or you may wish to remove them and be absolutely certain of the variants that you retain.</p>
<p><em>Data missingness</em></p>
<p>Next we need to think about missing data. genotyping is a biochemical process and like all such processes it goes wrong in some cases, and a call cannot be made. this can be a failure of the genotyping probe or poor quality of DNA or a host of other reasons but such calls are unreliable and they need to be removed.</p>
<p><em>Missingness</em></p>
<p>Missingness is best dealt with iteratively. To convince you of that, let’s examine this example data. We want to keep only the participants (which are the rows in this example) with complete or near-complete data on the eight variants we’re examining (which here are shown in the columns). So, we could remove everyone with fewer than seven SNPs, but when we do that - oh dear, we’ve obliterated our sample size.</p>
<p><em>Iterative Missingness</em></p>
<p>So instead let’s do things iteratively. So, we’ll remove the worst SNP again, variant seven goes, and then we remove the worst participant, bye bye Dave, then we remove the next first SNP, so that’s SNP two, and now everyone has near complete data and we’ve retained nearly all of our cohort. So this was obviously a simple example, how does this look with real data?</p>
<p><em>Real data missingness</em></p>
<p>So here we have some real data, and it’s it’s pretty good data most variants are only missing in a small percentage of the cohort, but there are some that are missing in as much as 10 of the cohort. So let’s do that initiative thing removing variants missing in 10% of the individuals and then individuals who have more than 10% missing variants and then 9% and so on down to one percent. when we do this the data looks good. Nearly all of the variants are zero percent missingness and those that aren’t are present in at least 578 of the 582 possible participants, and we’ve lost around 25 participants for about 22 and a half thousand SNPS. but what if we didn’t do the iterative thing and we just went straight for 99 complete data.</p>
<p>So when we do that the distribution of variance looks good again, arguably it looks even better, and we’ve retained an additional 16 000 variants, but we’ve lost another 40 participants which is about six percent more of the original total than we lost with the iterative method. Typically, participants are more valuable than variants which can be regained through imputation anyway, but this again is a data-driven decision. If coverage is more important than cohort size in your case, you might want to prioritize well-genotyped variants over individuals.</p>
<p><em>Hardy-Weinberg equilibrium</em></p>
<p>So we’ve addressed rare variants where genotyping is uncertain, and missingness where the data is unreliable. but sometimes calling is simply wrong and, again there are many reasons that could be. we can identify some of these implausible genotype calls by using some simple population genetic theory. so from our observed genotypes we can calculate the allele frequency at any bioluelic snip we’ve called. so here the frequency of the a allele is twice the frequency of the AA calls (those are our common homozygotes in blue) plus the frequency of AB calls (our heterozygotes in green) and we can do the equivalent as you see on the slide for the frequency of the B allele.</p>
<p>Knowing the frequency of the A and the B allele we can use Hardy and Weinberg’s calculation for how we expect alleles at a given frequency to be distributed into genotypes, to generate an expectation for the genotypes we expect to observe at any given allele frequency. We can then compare how our observed genotypes i.e the blue, green, and red clusters fit to that expectation, and we can test that using a chi-squared test.</p>
<p>Now Hardy-Weinberg equilibrium is an idealized mathematical abstraction, so there are lots of plausible ways it can be broken, most notably by evolutionary pressure. As a result, in case control data it’s typically best to assess it just in controls, or to be less strict with defining violations of Hardy-Weinberg in cases. That said, in my experience genotyping errors can produce very large violations of Hardy-Weinberg, so if you exclude the strongest violations you tend to be removing the biggest genotyping errors.</p>
<p><em>Sex mislabelling</em></p>
<p>The previous steps are mostly focused on problematic variants, but samples can also be erroneous. One example is the potential for sample swaps, either through sample mislabeling in the lab, or correctly entered data in phenotypic data.</p>
<p>These are often quite hard to detect, but one way to detect at least some of these is to compare self-reported sex with X chromosome homozygosity, which is expected to differ between males and females. In particular males have one X chromosome, they’re what’s known as hemizygous so when you genotype them they appear to be homozygous on all SNPs on the X chromosome. Females on the other hand have two X chromosomes, they are holozygous, and they have a normal X distribution centered around zero which is the sample mean in this case. you could also look at chromosome Y SNPs for the same reason, however Y chromosome genotyping tends to be a bit sparse and is often not of fantastic quality, so there are benefits to using both of these methods. it’s also worth noting that potential errors here are just that - potential. Where possible it’s useful to confirm these with further information. For example if there isn’t a distinction between self-reported sex and self-reported gender in your phenotype data then known transgender individuals may be being removed unnecessarily. The aim here is to determine places where the phenotypic and genotypic data is discordant, as these may indicate a sample swap, and this might indicate the genotype to phenotype relationship has been broken and that data is no longer useful to you.</p>
<p><em>Homozygosity and the inbreeding coefficient</em></p>
<p>Average variant homozygosity can also be applied across the genome, where this metric is sometimes referred to as the breeding coefficient. it’s called that because high values of it can be caused by consanguinity. related individuals having children together, which increases the average homozygosity of the genome. there can also be other violations of expected homozygosity, so it’s worth examining the distribution of values and investigating or excluding any outliers that you see.</p>
<p><em>Relatedness</em></p>
<p>Examining genetic data also gives us the opportunity to assess the degree of relatedness between samples. For example, identical sets of variants implied duplicates or identical twins. 50% sharing implies a parent offspring relationship or siblings, and those two things can be separated by examining how often both alleles of a variant are shared. Specifically, we would expect parents and offspring to always share one allele at each variant, whereas siblings may share no alleles, they may share one allele, or they may share two alleles. lower amounts of sharing imply uncles and aunts, and their cousins, and grandparents, and so on down to more and more distant relationships. in some approaches to analysis, individuals are assumed to be unrelated, so the advice used to be to remove one member of each pair of related individuals.</p>
<p>However, as mixed linear models have become more popular in GWAS, and mixed linear models are able to retain and include related individuals in analyses, related individuals, therefore, should be retained if the exact analysis method isn’t known. Again, it’s worth having some phenotypic knowledge here. Unexpected relatives are a potential sign of sample switches and need to be examined, confirmed, and potentially removed if they are truly unexpected. and once again it’s important to know your sample, the data shown in this graph does not, despite what the graph appears to suggest, come from a sample with a vast amount of cousins, instead it comes from one in which a minority of individuals were from a different ancestry and that biases this metric. I’ll talk a little more about that in just a moment.</p>
<p><em>Average relatedness</em></p>
<p>Relatedness can also be useful for detecting sample contamination. Contamination will result in a mixture of different DNAs being treated as a single sample, and this results in an overabundance of heterozygote calls. This in turn creates a signature pattern of low-level relatedness between the contaminated sample and many other members of the cohort. These samples should be queried with the genotyping lab to confirm whether or not a contamination event has occurred, and potentially be removed if an alternative explanation for this odd pattern of inter-sample relatedness can’t be found.</p>
<p><em>Population structure</em></p>
<p>Finally, a word on genetic ancestry. Because of the way in which we have migrated across our history, there is a correlation between the geography of human populations and their genetics. This can be detected by running principal component analyses on genotype data pruned for linkage disequilibrium. For example this is the UK biobank data, you can see subsets of individuals who cluster together and who share European ethnicities, other subsets who share African ethnicities, and subsets who share different Asian ethnicities, and in a more diverse cohort you will be able to see other groupings as well. this kind of 2D plot isn’t the best way of visualizing this, for example here it isn’t really possible to distinguish these South Asian and admixed American groupings, and you don’t get the full sense of the dominance of European ancestry data in this cohort. The Europeans in this case account for around 95% of the full cohort but because of over plotting i.e.&nbsp;the same values being plotted on top of each other in this 2D plot, you don’t really appreciate that. Looking across multiple principal components helps for that.</p>
<p>Ancestry is important to QC. Many of the processes I've talked about rely on the groups being assessed fairly of being fairly homogeneous. As such, if your data is multi-ancestry it’s best to separate those ancestries out and re-run QC in each group separately.</p>
<p><em>Take-aways</em></p>
<p>So that was a brief run-through of some of the key things to think about when running QC.</p>
<p>I hope I've got across the need to treat this as a data informed process, and to be willing to re-run steps, and adjust approaches to fit cohorts. Although we’ve got something resembling standard practice in genotype QC, I think there are still some unresolved questions. So get hold of some data, look online for guides and automated pipelines, and enjoy your QC.</p>
<p>Thank you very much for listening, I’m doing a Q &amp; A at 9 30 EST, otherwise please feel free to throw questions at me on twitter where I live, or at the email address on screen which I occasionally check. Thank you very much.</p>
<hr>
</section>
<section id="sec-video3" class="level1">
<h1>Considerations for Genotyping QC</h1>
<p><strong>Title</strong>: Considerations for genotyping, quality control, and imputation in GWAS</p>
<p><strong>Author</strong>: Ayşe Demirkan (a.demirkan@surrey.ac.uk)</p>
<p>hello everyone my name is aisha demerka i’m affiliated at the university of</p>
<p>roningam from the netherlands and university of surrey from the uk this is a pre-recorded lecture</p>
<p>in the second lecture of on-demand sessions introduction to the statistical analysis of genome-wide association</p>
<p>studies i will be talking about considerations for genotyping quality control and</p>
<p>imputation in genomic association studies jivas</p>
<p>so here you see an overview of the lecture we will shortly go over genotyping platforms</p>
<p>Lecture outline</p>
<p>and options quality control then i will talk about definition and purpose of imputation and how it is done</p>
<p>and this is going to include reference data tools analysis of imputed data</p>
<p>imputation accuracy and accusing</p>
<p>Genotyping and platforms Genotyping is the process of determining differences in the genetic make- up (genotype) of an individual by examining the individual’s DNA Sequence</p>
<p>what we call is genotyping is the process of determining differences in the genetic makeup</p>
<p>hence the genotype of an individual by examining the individual’s dna sequence</p>
<p>of course the technology used for genotyping depends on the structural properties of the genetic variation</p>
<p>whether it is a single nucleotide polymorphism or a copy number variation or other structural variations</p>
<p>it also depends on the project rationale or scientific question and your budget</p>
<p>mainly and related to that of course how many snips</p>
<p>you want to genotype if it is a genomic association study and number of individuals you would like to include</p>
<p>depending on your study design you will also be limited with your dna</p>
<p>sample quality and quantity</p>
<p>so here on this slide you see the most common approach used for genotyping</p>
<p>Common approaches</p>
<p>synips and depending on your study you will be most likely using one of these what are those illuminati matrix arrays</p>
<p>so on the y-axis you see the number of snips that are easily captured by the arrays and on the x-axis you see the</p>
<p>number of individuals and then what do we have we have pcr rflp sequence</p>
<p>pyrosequencing and fluidicum platforms and tacman</p>
<p>um for instance one of the best examples are the illuminae arrays for whole</p>
<p>genome scans um whole genome genotyping by these arrays provide an overview of the entire</p>
<p>genome and enable you know white discoveries and associations so you using a high throughput</p>
<p>next generation sequencing and microarray technologies you can obtain a deeper understanding of the genome</p>
<p>because you are covering a very wide proportion of the genome so you can use</p>
<p>one of their selection of this illumina or f metrics arrays which you think may be suitable for your study</p>
<p>there are many options and so for s4 illumina there are genome-wide genotyping is for 18 species</p>
<p>at the moment so number of markers on each array it changed by products for human up to four minion markers per</p>
<p>sample are possible now and then there is an infinium low cost screening</p>
<p>array so for this one for example includes 600 000 markers on it</p>
<p>you can use start from 200 nanogram genomic dna and what you can also do you</p>
<p>can add some custom marker panels there is an add-on capacity up to</p>
<p>50 50k markers</p>
<p>and then there is this omni family of illumina arrays</p>
<p>Omni family of Illumina arrays</p>
<p>here you see a simple description of their coverage and the inclusion of genetic markers in relation to their</p>
<p>minor alleged frequencies so these expressed chips on the left include only common variation with minor</p>
<p>life frequencies higher than five percent some include cmes and some include snips with lower minor allied</p>
<p>frequencies so which one to choose among those will depend on your question research</p>
<p>question and population you want to screen for instance are you looking for a rare or common variation in terms of</p>
<p>snips are you looking for cmes are you looking are you working with a rare or common disease and what is your sample</p>
<p>size and your budget now</p>
<p>i listed some websites here please take 10 20 minutes to check on</p>
<p>the technologies mentioned in the first section using these websites</p>
<p>Quality control (QC) of genotyping From machine to dataset: genotype calling</p>
<p>now let’s talk about genotyping quality control qc you designed your study you chose a</p>
<p>proper array platform service you used or you used a service from your institute</p>
<p>so one critical initial step from chemically induced intensity signals and data analysis is a transfer and</p>
<p>qc of genotypes determined towards your computer this critical step is called genotype</p>
<p>calling so genotype calling algorithms are always implemented in the probability</p>
<p>software accompanying the genotyping platform you choose so you don’t need to invent them yourself</p>
<p>so it’s typical calling software uses a sort of mathematical clustering</p>
<p>algorithm to inter to analyze the row intensity data and it estimates the probability that</p>
<p>their genotype is one of the a a a b or bb for a given individual for a given b</p>
<p>allelic marker locus so one method of checking initial synopquality is visually inspecting the</p>
<p>intensity clustering of a particular snip in the overall population and depending on this one can decide</p>
<p>whether a snip is characterized by a clear signal or not so</p>
<p>here on the left of these figures you see a clear intensity clustering of a</p>
<p>scene in the population so you see that the common variant is</p>
<p>depicted by red on the left there is some hetero heterozygous in the middle</p>
<p>depicted by purple and the homozygous um people for the less common allele are</p>
<p>depicted as blue and the table on the right it shows the row values that this plot</p>
<p>is figured from so here the plot shows a tight</p>
<p>clustering of genotypes and there is not moist much noise in the measurement of</p>
<p>this cinema following that there are imputed the</p>
<p>important data qc steps one of them is</p>
<p>to work on uh replicates so for inspecting plating issues and by</p>
<p>looking at you know type concordance this would be a a good thing to do to include the same</p>
<p>dna sample on different batches of experiments and then there are mendelian areas to</p>
<p>control for for instance transmission and the inconsistencies for example snips with more than 10 percent manual</p>
<p>area rate can be excluded this would be based on the number of trios that you would include in your</p>
<p>experiment unfortunately this option obviously is only available for family</p>
<p>based and trio designs only another thing another qc measure we use</p>
<p>is snip call rate this is basically the missing genotype rate</p>
<p>1 minus the missing genotype rate per snip so this can depend on the quality of tsa</p>
<p>and this is generally between 95 percent and 99 is this is a very standard thing</p>
<p>to include in your qc another thing is the hardy weinberg</p>
<p>equilibrium uh deviance of your snip so this is another method for checking the quality and</p>
<p>exclusion of this of your snips this will be explained in the next slide another one is the sample call right so</p>
<p>this is a sample based uh qc method this is a good indication of sample</p>
<p>success so different platforms have different thresholds but this will this is will be mainly</p>
<p>determined by your initial dna quality and uh it will somehow will be in</p>
<p>relation to with a snip color so once you do snip call rate you could do sample call rate and you may want to</p>
<p>repeat snip call rate depending on that and another thing to do is sample gender</p>
<p>check for this quality measure you need x chromosome information to calculate this</p>
<p>and you may want to add this as an additional sanity check in your data to make sure that there is</p>
<p>a perfect overlap with your phenotype files in terms of sex and</p>
<p>another important one is sample heterozygosity this is to check for example outliers</p>
<p>for example samples with more heterozygosity than expected um can be an indication of contamination</p>
<p>in your samples and you also want to do something in on top of all of that you</p>
<p>you need to check samples cryptic relatedness and unexpected uh twinning</p>
<p>and whether there is actually a relatedness and structure in the data</p>
<p>but this will be more covered in the lecture of redik magi</p>
<p>so let’s talk about hard wineback equilibrium shortly so as occurrence of uh two allies of a</p>
<p>Hardy-Weinberg equilibrium</p>
<p>snip in the same individual are two independent events the distribution of the genotypes across</p>
<p>individuals should be more or less in equilibrium with the frequencies of the alas of a b allelic snip</p>
<p>so this is only possible in ideal conditions of course which would be random mating</p>
<p>no selection equal survival no migration no mutation and selection based on</p>
<p>mutation no inbreeding and large population size</p>
<p>so under these conditions above deviations from high divine back equilibrium is an indication of</p>
<p>genotyping calling problems and a commonly used threshold for</p>
<p>genotypes variance is a p value of hardy weinberg equilibrium uh that is less than ten to the minus five</p>
<p>is an indication of a deviation from hardy weinberg equilibrium and you may</p>
<p>want to take a look at these snips or you you may want to exclude them from your</p>
<p>data set another important thing to always</p>
<p>Genome builds and alignments</p>
<p>consider is genome bills and alignments so the characterization of the human</p>
<p>genome is an ongoing effort and a genome build tells us the positions of the snips in the genome on</p>
<p>the genome so the latest build is called build 38 but the most commonly used one at the</p>
<p>moment is still built 37 for instance the head map was released on build 35</p>
<p>and bill 36 so you need to be aware of issues relating to merging and meta-analyzing</p>
<p>data from different genome builds also for when preparing your data for</p>
<p>imputation this is very important because you need to make sure that your data is coded according to the same</p>
<p>genome build between the target set and the reference data set</p>
<p>so there are tools uh for that one they are called liftover tools for instance there is one from oxford that we use for</p>
<p>purpose and i provide the link to that here</p>
<p>Commonly used software for QC plink...</p>
<p>so all of these qc steps i shortly went over here are pretty standard and there</p>
<p>are a couple of widely used tools one very commonly used tool that we also</p>
<p>use for data storage analysis and qc is called plink</p>
<p>here on this slide i made snapshot of some of the blink</p>
<p>options that i also covered during the lecture and these functions are implemented in</p>
<p>the plink software and you can use it for the qc of</p>
<p>your genetic data so first thing to be able to use bling</p>
<p>to obviously install plink and you will need to read your genotype call data in plink in the form of a map or pad files</p>
<p>and then you can perform qc at the snip level remove or extract snips and you</p>
<p>can perform qc at the sample level you can remove or extract individuals and under the summary statistics option</p>
<p>here there are functions listed to check for call rate missingness hardy-weinberg</p>
<p>equilibrium highlight frequencies and mandel errors you can also perform</p>
<p>sex checks what billing can also do is to extract genetic principle components and</p>
<p>identify cryptically related individuals or twinnings in the data</p>
<p>and and the genetic structure of the data and uh can be which you can then use to determine ethnic outliers in your</p>
<p>data sets i will not talk about this because this is a part of the lecture of redick muggy</p>
<p>of the next session now</p>
<p>Intermezzo</p>
<p>here i put two websites here one of them is for</p>
<p>blink and how to use blink for qc and the other one is</p>
<p>for a beat studio which i mentioned is one of the algorithms that you could</p>
<p>use for a genotype calling so now take 10-20 minutes to have a look at</p>
<p>this website and try to grasp what you can do with them</p>
<p>Genetic data missingness</p>
<p>now let’s talk about imputation why do we need imputation we need imputation to</p>
<p>address missingness in the genetic data this is all about missing values in the genetic data where do the missing values</p>
<p>come from so during the qc we already set some values to missing right and also during</p>
<p>genotype calling you could set some data points to missing but actually most of</p>
<p>the missing values come from the initial targeted coverage of the genotyping</p>
<p>chips and platforms we used so remember that there are many types of arrays some of more dense less dense</p>
<p>there are arrays made specifically for oncological studies like onco arrays there is a metabolic</p>
<p>chip that is designed for metabolic disease especially and there are areas focused on focusing</p>
<p>on mainly snips with higher minor area frequency or their whereas focusing on cnvs</p>
<p>but even the dense snip areas do not cover all of the genetic variation they</p>
<p>cover much less than you would imagine and in addition to that snips included in one array may not be included in the</p>
<p>other one and for many variable positions on the genome we do not have</p>
<p>matching information across genotype set of individuals for instance</p>
<p>look what i try to depict here i think of three individuals</p>
<p>first two are typed on the array x and the third one is typed on array y</p>
<p>so hence they have different missing data points and when you try to</p>
<p>pull their data for a pooled analysis or to be using meta-analysis you’re going</p>
<p>to have even more missingness in this data because of the non-overlapping positions</p>
<p>and you will not be able to replicate findings from one of data set and in the</p>
<p>other one so additionally we will be analyzing only half of the</p>
<p>genetic variation and we may miss causal variance in the analysis this is this is</p>
<p>because of all these reasons we use a genetic data imputation</p>
<p>Imputation principle</p>
<p>so what do we do in principle in principle it means estimating the most likely genotypes in</p>
<p>an individual at the missing positions by looking at the correlated snip values</p>
<p>from a more complete data set and based on that writing the</p>
<p>writing over the missing values in the target data set so how does it work</p>
<p>first of all we need a data set where dense genotypes are directly measured this can be a density array or it could</p>
<p>be a set of sequence individuals this we call a reference panel</p>
<p>then we use an imputation software or service and by looking at the correlation structure of the density</p>
<p>types or sequence snips we estimate them in the target data set</p>
<p>so at the end these are probabilities and we end up with dosage information for alleles or</p>
<p>genotypes rather than hard genotypes calls and this dosage information</p>
<p>which accounts for the immunity in the estimation is then included in the</p>
<p>genomic association study analysis so to sum up the purpose of imputation</p>
<p>Purpose of imputation</p>
<p>is to increase power because obviously the reference panel is more likely to contain the causal</p>
<p>variance than a less dense geos array to improve fine mapping because</p>
<p>imputation provides a higher resolution overview of an association signal across</p>
<p>a locus and then to enable meta-analysis because imputation is going to allow viewers</p>
<p>typed with different arrays to be combined up to variance in the reference panel</p>
<p>Historical milestones 2010-2018</p>
<p>going over the historical milestones in terms of imputation also summarizes the</p>
<p>theoretical and technological advancements in human data immune genetic data imputation</p>
<p>so one important advance in all of these was the generation of reference panels so the first reference panel was hepmap</p>
<p>and the headmap2 was the most commonly used release of hapmap it consists of a limited sample of individuals from</p>
<p>diverse genetic backgrounds 60 yoruba indians 90 hein chinese and japanese and six</p>
<p>individuals that were utah residents descending of european ethnic origin</p>
<p>now it sounds funny to think that we imputed thousands of people based on the genetic material of 60 euro residents</p>
<p>only talking about the europeans but actually this yielded a lot of success and actually this is what we had um only</p>
<p>for a long time so we could only impute up to 3 million</p>
<p>snips with headmap at the time and then came the 1000 genome reference panel which included at the</p>
<p>end 2500 individuals from multiple ethnic groups</p>
<p>and later on and currently the most widely used reference panel is the panel</p>
<p>of haplotype reference consortium hrc recall shortly this is a combined set of</p>
<p>whole genome and exome sequence data for more than 30 000 individuals and use 39 million</p>
<p>snips after imputation of course this this is going to depend uh this on the</p>
<p>scaffold that you use for imputation as well many of these snips will not be include imputed with the good quality</p>
<p>but in the ideal conditions you can go up to 39 million and finally we now have a reference</p>
<p>panel from the transomics for precision medicine topmed program and this consists of almost 100k deeply sequenced</p>
<p>human genomes and it can yield up to 308 genetic variants</p>
<p>to be identified one technical milestone is mentioning</p>
<p>was prefacing of haplotypes so genetic imputation is a highly</p>
<p>computationally intensive process because of the probabilistic framework and high rate of missing data that we</p>
<p>are trying to deal with one of the major milestones is to reduce the computational</p>
<p>burden was introduction of prephasing so this idea involves a two-step</p>
<p>imputation process so there is one initial step of previousing which is actually haplotype</p>
<p>estimation of the geos genotypes and a subsequent step of imputation into the estimated steady haplotypes</p>
<p>so this reduces complexity of the imputation process and speeds it up the current version of all imputation</p>
<p>software can deal with the prefacing approach</p>
<p>and what is very important is a choice of reference panel</p>
<p>so it is shown that making use of the all ancestry’s reference panels rather than</p>
<p>ethnic specific reference panel improves imputation accuracy for rare variants in</p>
<p>any population and formatted reference panels for impude and minimax can be</p>
<p>downloaded from the software websites and it’s very important to make sure</p>
<p>that genotype scaffold and reference panels are aligned to the same build of the human genome i will get back to that</p>
<p>later as well</p>
<p>so another and very important and current technological advancement that makes our</p>
<p>lives easier is the imputation services these are freely available services such</p>
<p>as the michigan and sanger imputation services you can simply format and upload your data in a secure way to this</p>
<p>server and get the data imputed and face genotypes back in a few days</p>
<p>and this depends on the speed and how busy the server is and depending on the</p>
<p>sample size you are trying to impute of course</p>
<p>Historical milestones -Sanger</p>
<p>so in parallel to michigan imputation server uh there is also a sanger institute uh has</p>
<p>a similar service in this service also you can upload your data in a vca format and optionally</p>
<p>perform pre-phasing using beagle or shaping software and current reference panels</p>
<p>in the sanger imputation server includes hrc uk 10k and 1000 genomes</p>
<p>as i said there is also a server dedicated to the</p>
<p>topmat this is all very self</p>
<p>explanatory uh this is how uh the sanger imputation server would like</p>
<p>you to prepare the data so there is a whole a bunch of instructions there that you would like to use</p>
<p>um so the use of these services comes with instruction and manuals so feel free to make an account there and run</p>
<p>some test data says in there you will need to qcn format the data as required</p>
<p>in the instructions you will need to match the coordinates and reference level of the genome bills and prepare one file for each chromosome this is for</p>
<p>sanger imputation server and another important thing in terms of imputation is of course the speed</p>
<p>Speed-Impute 5 PLOS GENETICS</p>
<p>so increasing reference panel size improves accuracy of markers with low minor allele frequencies but</p>
<p>this positive every increase in computational challenges for imputation methods so recently a new imputation software</p>
<p>input 5 was introduced from the same group so it does memory efficient imputation by selecting haplotypes using</p>
<p>the positional borrows wheeler transform so using hrc reference panel</p>
<p>the developers of the software uh showed that input 5 is up to 30 times faster</p>
<p>than minimax 4 and up to 3 times faster than a beagle</p>
<p>5.1 and uses less memory than both of these methods</p>
<p>Example framework</p>
<p>so using all the mentioned considerations up until now you can</p>
<p>build an insico framework similar to this one so you can use for instance blink functions for the first two steps</p>
<p>of genetic data qc then you can check cheap information and</p>
<p>strength issues using rhino tools and if needed you can update your genome</p>
<p>build by using leftover tool and you can then preface by using shape it and</p>
<p>finally imputed in-house or using one of the servers are mentioned so two links</p>
<p>to this software are given here</p>
<p>now take time</p>
<p>probably hours to explore these three imputation services uh of</p>
<p>sanger michigan and topmatz uh you will be asked to make an account</p>
<p>and perhaps it will be need to be improved so take your time to do so</p>
<p>Imputation QC</p>
<p>and the next topic is imputation related qc so there are two qc steps um</p>
<p>around imputation one is pre-imputation qc um so we have already discussed standard</p>
<p>qc after genotyping and on top of that you you may want to exclude snips with less than one percent minor allyl</p>
<p>frequency and a post imputation quality is assessed</p>
<p>by information measures which is in some value in the range of 0</p>
<p>to 1 and it is typical to filter snips by this</p>
<p>value less than 0.8 for a strict filtering or less than 0.4</p>
<p>and in the impute software this is called infoscore and in the minimax</p>
<p>imputation software this is called r square per snip so it’s important to check quality of</p>
<p>type snips in the scaffold in the region also by visual inspection of cluster</p>
<p>plots and you may also want to produce quality plots per chromosome</p>
<p>varying by minor allele frequency strata and a position</p>
<p>on the chromosomes for instance is an example figure this</p>
<p>Imputation quality vs MAF</p>
<p>shows a typical relationship between minor allyl frequency and imputation quality</p>
<p>so on the y-axis you see the imputation accuracy as a determined by imputation</p>
<p>quality as by r square on infoscore from different softwares and on the</p>
<p>x-axis you see the minor allele frequency you see that the accuracy is top when</p>
<p>the minor alarm frequency is high when the allyl is common and more common and then the accuracy</p>
<p>goes lower where the minority frequency goes lower as well you still have some</p>
<p>snips which you still have some well-imputed snips</p>
<p>among the rare ones as well but most of the low quality snips are going to come from a low minor added frequency</p>
<p>snips so keep in mind that when you filter by imputation quality you will be filtering out a lot of rare snips as</p>
<p>well so what are the factors affecting imputation quality</p>
<p>Factors affecting imputation</p>
<p>so at the genome-wide level the number of individuals imputed has something to</p>
<p>do with it for this reason we merge scaffold data sets before imputation if we are going to impute</p>
<p>more than one so the the more the merrier</p>
<p>and the second factor is the reference panel</p>
<p>the choice of the reference panel as well as the whole idea is to use the correlation between snips</p>
<p>across different populations and this may be different from population to population</p>
<p>you want to go for a large multi-ethnic panel if you’re not able to go for a</p>
<p>large ethnic specific panel and uh finally at the snip level uh the</p>
<p>lower the minor life frequency the lower the quality of the imputation</p>
<p>is going to be and how to analyze the imputed data</p>
<p>Analysis of imputed genotypes</p>
<p>so for each individual imputation provides probability distribution of</p>
<p>possible genotypes for each untyped variant these properties can be converted into</p>
<p>best guest genotypes but this is not something really generally recommended as it increases false</p>
<p>positives and it reduces power but also you want to filter your best</p>
<p>best guess genotypes you want to put a strict filtering on the best uh guest genotypes and this would result</p>
<p>more nas in your data set so it’s better to convert the uh</p>
<p>probabilities to expected allele counts and analyze uh by taking the uncertainty</p>
<p>in the imputation into account that’s really important and to do that you need to match the</p>
<p>data formats to the software not all software uses all types of data and you may need to do</p>
<p>data conversions um and um software called epex snip test to</p>
<p>and link to supports the knowledge information and you need to check the lecture from</p>
<p>medic magi for analysis of genome-wide data</p>
<p>Messages</p>
<p>um so this is the last slide of this lecture so the take-home message is that</p>
<p>is we are dealing with hypothesis free approaches here unfortunately it all comes down to the bittersweet money and</p>
<p>uh resources we have so you need to think what is the best and most cost effective way of getting</p>
<p>genetics done in large sample size and the answer is combining a dense genome scan array with imputation as the reference panels are free at the moment and cost of arrays are going lower as well but you really need to think as the in silico part of doing so also will cause staff and a computational resources to some level and what else should you consider so in comparison you want to know depending on your research question of course whether there is a better array for you or perhaps an array or a metabold chip if you are going to conduct your research in a in a very restricted field and the most importantly what are the future uses of this data because obviously you don’t want to build something that you’re going to use only a couple of for only a couple of years and finalize the research on that you want to you ideally want to invest in big data uh so are you gonna invest in population based cohort or disease-based cohort is it going to be a short-term project or is it going to be a follow-up study that’s going to be likely build up and extended extended throughout the years and by inclusion of new phenotypes so and finally who do you want to collaborate with which consortia which disease</p>
<p>um yeah so i hope this lecture will be useful for your research and future studies and for the people who are interested and to have a better and in-depth understanding of imputation every year two times we have a jivas course organized by university of surrey in collaboration with imperial college and university of tartu from estonia this is a hands-on this includes a hands-on workshop as well as theoretical uh lectures where we teach these concepts and matters in more detail so the last one was in five to ten july 2021 and for information there is an email address you can connect to thank you very much and have a nice conference of the remaining time</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
          // target, if specified
          link.setAttribute("target", "_blank");
          // default icon
          link.classList.add("external");
      }
    }
});
</script>
</div> <!-- /content -->



</body></html>