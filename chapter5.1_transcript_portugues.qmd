---
title: "Capítulo 5.1: Controle de Qualidade"
---

# ***Controle de qualidade*** {#sec-section1}

**Título**: Controle de qualidade

**Apresentador(es)**: Katrina Grasby, PhD (The Psychiatric Genetics Group, QIMR Berghofer Medical Research Institute)

**Coautor(es):** Lucia Colodro Conde, PhD (The Psychiatric Genetics Group, QIMR Berghofer Medical Research Institute)

**Katrina Grasby:**

Obrigada por se juntar a mim nesta sessão sobre Controle de Qualidade. Nesta gravação falarei sobre as etapas de controle de qualidade ou QC que aplicamos aos dados genéticos. Portanto, isso está nos estágios iniciais de um estudo. Coletamos nosso DNA, ele está sendo transformado em dados. Vamos limpar esses dados e depois imputá-los e então poderemos fazer nossas análises estatísticas. Portanto, há muitos pontos em um estudo que aplicarão o QC, mas essas etapas que discutiremos aqui e no tutorial são as etapas de controle de qualidade que aplicamos aos nossos dados genéticos.

*Por que fazer controle de qualidade?*

Então, por que fazer o controle de qualidade? Essencialmente, dados de baixa qualidade contribuirão para falsos positivos e falsos negativos em nossos resultados. Portanto, queremos resultados robustos. Precisaremos limpar nossos dados. Portanto, removeremos essencialmente erros de genotipagem. Podem ser erros na chamada de genótipos ou na tradução do DNA em dados. Eles podem ser devidos a vários fatores diferentes. Uma das imagens que gosto de trazer à minha mente foi uma história que me foi contada por uma mulher com quem trabalho e que estava envolvida em um projeto onde eles postaram dois kits de cuspe para um casal que estava participando de um projeto, e em algum lugar daquela entrega, um dos kits desapareceu ou foi danificado. E o casal pensou ou estava tentando ser útil e os dois cuspiram no mesmo kit e postaram de volta para nós – para ela. Ao fazê-lo, incluíram também uma carta para dizer o que tinham feito, mas foi um exemplo clássico de contaminação por DNA. É um exemplo de erro humano. Afinal, acabamos sem dados utilizáveis ​​de duas pessoas, em vez de ter dados utilizáveis ​​de uma pessoa. Não há como desembaraçar o DNA daquele kit de saliva e dizer que isso pertence àquela pessoa e que isso pertence a essa pessoa. É também um exemplo de DNA contaminado, e mesmo que não tivessem incluído uma carta para dizer o que fizeram, os passos que percorreremos no tutorial seriam capazes de identificar um problema como este. Então, podemos realmente dizer, esta não é uma indicação clara de dados de uma pessoa, de uma pessoa específica. Podemos retirar que isso não interfere em nossas análises.

Então, uma das outras coisas que faremos no tutorial é, depois de limparmos nossos dados, daremos uma olhada na estrutura de relacionamento dentro de nossos dados e, embora isso não seja necessariamente uma etapa de controle de qualidade, é um aspecto necessário para compreender nossos dados para que possamos aplicar análises adequadas e isso será importante para minimizar nossos falsos positivos e falsos negativos. Então, como passamos do DNA aos dados?

*DNA para dados*

Sou geneticista comportamental. Eu uso estatísticas para analisar dados. Não tenho experiência de trabalho em laboratório, processando efetivamente o DNA em dados. Mas ainda é útil para mim ter uma ideia dessas muitas etapas diferentes envolvidas e uma apreciação de quais são as possíveis fontes de erro e o que exatamente representam meus dados. Assim, podemos enviar kits de cuspe para os participantes que podem cuspir nesse kit em casa e enviá-lo de volta. A amostra é então processada para que o DNA seja fragmentado, cortado em pequenos pedaços. E então é amplificado, então temos mais disso. E então o DNA é extraído. Podemos armazenar alguns e então colocar alguns em chips SNP ou matrizes de genotipagem, para que seja posteriormente analisado. Então isso aqui embaixo. Essas imagens vêm do site da Illumina. Este é um exemplo aqui em A de um chip SNP ou de uma matriz de genotipagem. Portanto, existem muitas formas diferentes de chips SNP. A tecnologia melhorou com o passar do tempo e tenho certeza de que continuará melhorando. Este aqui é um exemplo da tecnologia de beads. Neste chip específico há espaço para informações, DNA, de 12 indivíduos diferentes. Essas barras horizontais aqui são cada uma para um indivíduo diferente. Agora, se você está pensando então, olhando para este chip SNP, então se você obtiver informações de vários indivíduos, você terá muitos chips e eles podem ser enviados para uma empresa de genotipagem de DNA para processamento em lotes diferentes. Se você está pensando de um ponto de vista experimental e quando tem casos e controles, deseja que seus casos e controles sejam distribuídos aleatoriamente tanto para os chips quanto para as execuções de lote nas quais eles estão sendo processados. De maneira semelhante, se você tiver homens e mulheres, você deseja randomizá-los em seus chips e também em lotes. Dessa forma, podemos garantir que podemos realmente captar quaisquer efeitos de lote específicos em nossos dados, uma vez que os tenhamos no final.

Então, de volta ao chip. Para cada um desses indivíduos, haverá centenas de milhares de sondas para testar os alelos em centenas de milhares de pontos do genoma. Muitos, muitos, muitos loci. Então cada um desses poços tem uma bead. Este aqui é um esquema de uma bead. Portanto, esta bead tem como alvo um alelo num locus. Então tem uma sequência específica aqui, um endereço, então essa é a ordem das bases. E então este aqui é o locus de interesse. Então, uma vez que você tenha seu DNA fragmentado, ele aparecerá, se estiver no local certo no genoma, ele se ligará a essa bead e, dependendo se esse alelo aqui se ligar a esse C, então G se ligará a C, esta bead ficará fluorescente em verde. Uma bead diferente à qual ela pode se ligar, se houver um A naquele local e um T aqui, então ela se ligará dessa maneira e ficará fluorescente em vermelho. Então é assim que estamos estabelecendo esse locus. Você pode ter um G ou um T e se for um G, ele se ligará ao C e ficará fluorescente em verde. Se for um T, ele se ligará ao A e ficará vermelho fluorescente. Então isso é traduzir o DNA em uma cor, isso é chamado de intensidade. Então, se você tem, você tem DNA vindo do seu pai biológico, da sua mãe biológica. Você tem dois alelos nesse locus. Se seus dois alelos forem iguais, você terá dois alelos G. Ambos ficarão verdes fluorescentes. Bela cor verde sólida. Se ambos, se você tiver dois alelos T, ambos estarão ligados a essas beads A. Ambos terão uma bela cor vermelha sólida. Se você tiver um G vindo de um dos pais e um T vindo do outro, então algumas das beads que são C terão fluorescência verde. Algumas das beads que são A ficarão fluorescentes em vermelho e essa pessoa será heterozigota e terá essa cor amarela. Portanto, essas cores representam os três genótipos possíveis naquele locus do genoma. E então estes aqui são para centenas de milhares de loci diferentes no genoma. O que tenho neste slide específico são exemplos de intensidades de genotipagem. intensidades de genotipagem. Então é assim que veremos os agrupamentos de cores que representam os diferentes genótipos. E veja se há ou não algum problema. Agora, isso provavelmente será feito por uma empresa de genotipagem, provavelmente você não fará isso. Mas eles lhe darão informações sobre essas primeiras etapas do controle de qualidade nesta fase, para que você saiba o que está acontecendo com seus dados. Estará lá em um relatório da empresa.

Este canto superior esquerdo é um bom exemplo do que procuramos. Temos três clusters bonitos e separados. Este é um alelo A homozigoto. Este é um grupo heterozigoto de indivíduos e este é homozigoto para o outro alelo. E nestes dois exemplos, com os seus pequenos X pretos, representam dados faltantes. Portanto, a falta de dados pode não ser terrivelmente problemática se houver apenas um pouco de falta e estiver em todos os diferentes genótipos. No entanto, se for tendencioso para um alelo ou um genótipo, então isso irá interferir com as nossas frequências alélicas na nossa amostra, e isso significará que não será representativo da população, não é representativo em termos de como podemos realmente testar esse genótipo contra esse fenótipo. Não queremos ter informações tendenciosas sobre a chamada de alelos ou de genótipos. Aqui embaixo, no canto inferior esquerdo, temos um exemplo de um alelo muito raro. Desculpe, um genótipo raro ou é um alelo raro e também um genótipo raro. Portanto, há apenas um indivíduo aqui que é homozigoto para o alelo A. Muito poucos heterozigotos. Do meio para baixo, este seria um exemplo de um grupo monomórfico neste locus, então realmente não é um locus útil para genotiparmos. Ou pode ser que apenas esta população exista, não haja variação nesta população neste locus. E no canto inferior direito temos um exemplo onde realmente houve uma falha ao nomear os genótipos corretamente. Não há indicação de qualquer cor vermelha, que represente o grupo de pessoas heterozigotas. Temos esses dois tipos de clusters verdes e a falta está em todo este cluster, é uma falha completa.

*Verificando os dados*

Portanto, as etapas que seguiremos com nosso tutorial de controle de qualidade são: começaremos verificando os dados. Vamos dar uma olhada no formato do arquivo. Como os dados são codificados? Como a falta é codificada? Vamos dar uma olhada na construção para sabermos em qual montagem nossos dados estão. A empresa de genotipagem teria nos fornecido isso, mas nem sempre você terá acesso a essas informações, portanto, existem maneiras de verificarmos isso por nós mesmos. Este é um recurso muito útil, que usaremos no tutorial para fazer isso. E... Saber em que base estão seus dados é muito importante, especialmente para meta-análise, mas também se você for fazer qualquer análise ou trabalho de acompanhamento com seus resultados. Faremos uma verificação de sexo, que visa verificar se o sexo que podemos inferir a partir das informações genéticas da verificação de sexo corresponde ao sexo relatado pelos indivíduos. Portanto, esta verificação analisa a heterozigosidade do cromossomo X. E temos expectativas diferentes dependendo se um indivíduo tem um ou dois cromossomos X. Então, se o indivíduo está relatando seu sexo e a informação genética retorna e não corresponde, e isso acontece com grande parte da sua amostra, então você pode ter um problema com a informação, isto é, com a correspondência da sua informação genética que foi retornado após a genotipagem para seus IDs de participante. Tenha em mente que se trata de sexo biológico e não de gênero.

*Taxa de chamadas de genotipagem*

Estaremos verificando se há falta. Portanto, há dois tipos de faltas que serão verificadas. Uma é esta, a taxa de chamadas de genotipagem. É aqui que faltam informações sobre os indivíduos nos SNPs. Portanto, para cada SNP queremos obter informações provenientes da maioria dos nossos indivíduos. Se houver muitos dados faltantes para esse SNP, muitos indivíduos não tiverem informações que foram chamadas corretamente para esse SNP, então esse SNP pode não ser um bom SNP para usarmos em nossas análises.

*Equilíbrio de Hardy-Weinberg*

Daremos uma olhada no equilíbrio de Hardy-Weinberg para ver se nossas frequências alélicas correspondem ou não ao que esperamos. Portanto, isso pode destacar se temos algum viés em termos da frequência dos alelos, ou talvez em termos de chamar os genótipos de maneira adequada, pensando nas intensidades dos genótipos. Estarei verificando a frequência do alelo menor. Portanto, isso é para garantir que tenhamos informações suficientes para fazer análises estatísticas. Se for muito raro, então nosso GWAS não é a ferramenta apropriada para ser usada neste locus específico.

*Taxa de chamada de amostra*

Estaremos dando uma olhada em uma taxa de chamada das amostras. Portanto, esta é outra forma de falta. Isto é, todos os nossos indivíduos possuem informações em quase todos os seus SNPs. Portanto, não queremos que os indivíduos percam muita informação em muitos SNPs.

*Heterozigosidade*

Estaremos observando a proporção de heterozigosidade. Então esta é uma forma de verificar... Pense naquele exemplo em que tínhamos duas pessoas cuspindo no mesmo kit. Isso vai nos dar muita variação. Haverá muita variação nessa amostra de DNA. Portanto, a heterozigosidade seria excessiva. Inversamente, a heterozigosidade reduzida poderia ser um exemplo de endogamia, mas também poderia ser apenas o fato de termos muitos dados em falta.

*Heterozigosidade reduzida*

Então essa é uma das razões pelas quais vamos verificar primeiro as faltas na nossa amostra, antes de fazermos a verificação da heterozigosidade. Porque não queremos fazer, ou não queremos nos preparar, para potencialmente fazer inferências que tenham consequências sociais negativas. Então, se você tem dados faltantes e essa é a razão pela qual você reduziu a heterozigosidade, você não quer acabar olhando para sua amostra dizendo “ah, há muita endogamia aqui”.

*Estrutura de relacionamento*

No final do tutorial, depois de limpá-lo, daremos uma olhada na estrutura de relacionamento em nossos dados. Portanto, podemos ter muitas famílias ou podemos ter famílias extensas. Queremos saber se nossos indivíduos estão ou não relacionados para que possamos aplicar o tipo certo de análise estatística.

*Estrutura populacional*

E, finalmente, examinaremos a estrutura ou estratificação populacional. Então isso será falado mais em outra sessão, mas é aí que daremos uma olhada em algumas frequências alélicas. Existem diferenças nas frequências alélicas entre diferentes grupos ou diferentes populações e isso é importante para estarmos cientes e incluí-los adequadamente em nossa análise. Caso contrário, obteremos falsos positivos e falsos negativos. Se a sua estrutura populacional também estiver correlacionada de alguma forma com o resultado de interesse, é aí que teremos um problema. E é aí que vamos falar sobre isso em termos de estratificação populacional.

Portanto, esta será nossa lista de verificação para nossas principais etapas de controle de qualidade que serão executadas no tutorial.

------------------------------------------------------------------------

# ***Como executar o controle de qualidade em dados de genotipagem de todo o genoma*** {#sec-section2}

**Título**: Como executar o controle de qualidade em dados de genotipagem de todo o genoma

**Apresentadores**: Jonathan Coleman, PhD (Social, Genetic, and Developmental Psychiatry Centre, King’s College London)

**Jonathan Coleman:**

Olá, sou Joni Coleman e nesta breve apresentação discutirei alguns pontos-chave relacionados à execução do controle de qualidade em dados genotípicos de todo o genoma, que é um primeiro passo comum na execução de um GWAS.

*Visão geral*

Fornecerei uma visão geral teórica, abordando as razões abrangentes pelas quais precisamos fazer QC. Destacando algumas etapas comuns e discutindo algumas armadilhas que os dados podem apresentar.

Não vou falar sobre conduzir imputação, ou análises GWAS, ou análises secundárias. Também não vou falar muito sobre o processo de genotipagem e sobre a garantia da qualidade das chamadas de genotipagem. Da mesma forma, não entrarei em nenhum código profundo ou matemática, no entanto, se você estiver começando a executar seu próprio controle de qualidade e análises, recomendo o pipeline automatizado RICOPILI do PGC como ponto de partida. Existem também alguns scripts simples no github do meu grupo que também podem ser úteis. Eles seguem um processo passo a passo com códigos e explicações. No momento, estamos atualizando este repositório, então procure alguns tutoriais em vídeo lá também.

*O começo: genótipos de todo o genoma*

Então aqui está nosso ponto de partida. Usarei este gráfico no canto superior direito várias vezes durante esta palestra, e este é um gráfico de chamada de genótipo com homozigotos comuns em azul, heterozigotos em verde e homozigotos raros em vermelho. Esperançosamente, seus dados já terão sido colocados em um pipeline automatizado de chamada de genótipos e, se você tiver muita sorte, um bioinformático sobrecarregado e pouco apreciado poderá ter feito alguma recuperação manual para garantir que a qualidade dos dados seja a mais alta possível.

Mas, na verdade, os dados que você usará não estarão nesta forma visual, mas sim como uma matriz numérica como a mostrada abaixo, com SNPs e indivíduos. Isso pode estar na forma de um arquivo de genótipo PLINK ou em seu equivalente binário, ou em alguma forma semelhante que pode ser convertida para o formato PLINK.

*O endpoint desejado: dados limpos e completos*

Queremos chegar a dados limpos com variantes que são chamadas pela maioria dos participantes do seu estudo e que não causarão vieses nas análises posteriores.

Isso deve dar um agradável e limpo gráfico de Manhattan do GWAS, como o mostrado abaixo, em vez do efeito de noite estrelada deste gráfico de Manhattan com controle de qualidade ruim acima. No entanto, algo que gostaria de enfatizar nesta palestra é que o QC é um processo baseado em dados, e o que funciona para um grupo não será necessariamente correto para outro. Um bom QC exige que o analista investigue e compreenda os dados.

*Variantes “raras”*

Frequentemente, o primeiro passo é remover variantes raras, e isso ocorre porque não podemos ter certeza das chamadas de variantes. Considere a variação no círculo à direita. Esses são homozigotos comuns periféricos ou são heterozigotos? Não podemos realmente dizer por que não há um número suficiente deles para formar um aglomerado reconhecível. Normalmente, podemos querer excluir variantes com uma contagem baixa de alelos secundários, por exemplo cinco. Existem muitos métodos de chamada automatizada excelentes para aumentar a certeza que você tem nessas variantes, mas também é importante notar que muitos métodos analíticos não lidam bem com variantes raras.

Novamente, as demandas dos seus dados determinam suas escolhas de QC. Pode ser mais útil chamar variantes raras, mesmo se você não tiver certeza sobre elas. Ou você pode querer removê-los e ter certeza absoluta das variantes que mantém.

*Falta de dados*

Em seguida, precisamos pensar nos dados faltantes. a genotipagem é um processo bioquímico e, como todos esses processos, em alguns casos dá errado e não é possível tomar uma decisão. isto pode ser uma falha da sonda de genotipagem ou má qualidade do DNA ou uma série de outras razões, mas tais chamadas não são confiáveis ​​e precisam de ser removidas.

*Falta*

A falta é melhor tratada iterativamente. Para convencê-lo disso, vamos examinar os dados deste exemplo. Queremos manter apenas os participantes (que são as linhas neste exemplo) com dados completos ou quase completos sobre as oito variantes que estamos examinando (que aqui são mostradas nas colunas). Então, poderíamos remover todos com menos de sete SNPs, mas quando fazemos isso - nossa, obliteramos o tamanho da nossa amostra.

*Falta iterativa*

Então, em vez disso, vamos fazer as coisas iterativamente. Então, vamos remover o pior SNP novamente, a variante sete vai, e então removemos o pior participante, tchau, Dave, então removemos o próximo pior SNP, então esse é o SNP dois, e agora todos têm dados quase completos e nós mantivemos quase todo o nosso grupo. Então este foi obviamente um exemplo simples, como isso fica com dados reais?

*Falta em dados reais*

Portanto, aqui temos alguns dados reais, e são dados muito bons. A maioria das variantes está faltando apenas em uma pequena porcentagem da coorte, mas há algumas que estão faltando em até 10 da coorte. Então, vamos tomar essa iniciativa removendo variantes ausentes em 10% dos indivíduos e, em seguida, indivíduos que têm mais de 10% de variantes ausentes e depois 9% e assim por diante, até um por cento. quando fazemos isso, os dados parecem bons. Quase todas as variantes apresentam zero por cento de falta e aquelas que não estão presentes em pelo menos 578 dos 582 participantes possíveis, e perdemos cerca de 25 participantes em cerca de 22 mil e quinhentos SNPS. Mas e se não fizéssemos a coisa iterativa e fossemos direto para 99 dados completos.

Então, quando fazemos isso, a distribuição da variância parece boa novamente, provavelmente parece ainda melhor, e mantivemos 16.000 variantes adicionais, mas perdemos outros 40 participantes, o que é cerca de seis por cento mais do total original do que perdíamos com o método iterativo. Normalmente, os participantes são mais valiosos do que as variantes que podem ser recuperadas através de imputação de qualquer maneira, mas esta é novamente uma decisão baseada em dados. Se a cobertura for mais importante do que o tamanho da coorte no seu caso, você pode querer priorizar variantes bem genotipadas em vez de indivíduos.

*Equilíbrio de Hardy-Weinberg*

Portanto, abordamos variantes raras onde a genotipagem é incerta, e faltas onde os dados não são confiáveis. Mas às vezes chamadas são simplesmente erradas e, novamente, há muitos motivos que podem ser. Podemos identificar algumas dessas chamadas de genótipos implausíveis usando alguma teoria genética populacional simples. Então, a partir de nossos genótipos observados, podemos calcular a frequência alélica em qualquer recorte bialélico que chamamos. Então aqui a frequência do alelo A é o dobro da frequência das chamadas AA (esses são nossos homozigotos comuns em azul) mais a frequência das chamadas AB (nossos heterozigotos em verde) e podemos fazer o equivalente como você vê no slide para a frequência do alelo B.

Conhecendo a frequência do alelo A e B, podemos usar o cálculo de Hardy-Weinberg para saber como esperamos que os alelos em uma determinada frequência sejam distribuídos em genótipos, para gerar uma expectativa para os genótipos que esperamos observar em qualquer frequência alélica. Podemos então comparar como nossos genótipos observados, ou seja, os clusters azul, verde e vermelho, se ajustam a essa expectativa, e podemos testar isso usando um teste qui-quadrado.

Ora, o equilíbrio de Hardy-Weinberg é uma abstração matemática idealizada, portanto há muitas maneiras plausíveis de quebrá-lo, principalmente por pressão evolutiva. Como resultado, nos dados de controle de casos, normalmente é melhor avaliá-los apenas nos controles ou ser menos rigoroso na definição de violações de Hardy-Weinberg nos casos. Dito isto, na minha experiência, erros de genotipagem podem produzir violações muito grandes de Hardy-Weinberg, portanto, se você excluir as violações mais fortes, tenderá a remover os maiores erros de genotipagem.

*Rotulagem incorreta de sexo*

As etapas anteriores concentram-se principalmente em variantes problemáticas, mas as amostras também podem estar erradas. Um exemplo é o potencial para trocas de amostras, seja por meio de rotulagem incorreta de amostras no laboratório ou de dados inseridos corretamente em dados fenotípicos.

Estes são muitas vezes bastante difíceis de detectar, mas uma forma de detectar pelo menos alguns deles é comparar o sexo auto-relatado com a homozigosidade do cromossomo X, que se espera que seja diferente entre homens e mulheres. Em particular, os homens têm um cromossomo X, eles são conhecidos como hemizigotos, então quando você os genotipa, eles parecem ser homozigotos em todos os SNPs no cromossomo X. As mulheres, por outro lado, têm dois cromossomos X, são holozigotas e têm uma distribuição X normal centrada em zero, que é a média da amostra neste caso. Você também pode observar os SNPs do cromossomo Y pelo mesmo motivo; no entanto, a genotipagem do cromossomo Y tende a ser um pouco esparsa e muitas vezes não é de qualidade fantástica, portanto, há benefícios em usar esses dois métodos. Também é importante notar que os erros potenciais aqui são apenas isso - potenciais. Sempre que possível, é útil confirmá-los com mais informações. Por exemplo, se não houver uma distinção entre sexo autodeclarado e gênero autodeclarado em seus dados fenotípicos, então indivíduos transexuais conhecidos podem estar sendo removidos desnecessariamente. O objetivo aqui é determinar locais onde os dados fenotípicos e genotípicos são discordantes, pois estes podem indicar uma troca de amostra, e isso pode indicar que a relação genótipo para fenótipo foi quebrada e que os dados não são mais úteis para você.

*Homozigosidade e coeficiente de endogamia*

A homozigosidade da variante média também pode ser aplicada em todo o genoma, onde esta métrica é por vezes referida como coeficiente de reprodução. É chamado assim porque valores elevados podem ser causados ​​​​por consanguinidade. indivíduos aparentados que têm filhos juntos, o que aumenta a homozigosidade média do genoma. Também pode haver outras violações da homozigose esperada, por isso vale a pena examinar a distribuição de valores e investigar ou excluir quaisquer valores discrepantes que você observar.

*Relacionamento*

O exame dos dados genéticos também nos dá a oportunidade de avaliar o grau de parentesco entre as amostras. Por exemplo, conjuntos idênticos de variantes implicavam duplicatas ou gêmeos idênticos. O compartilhamento de 50% implica um relacionamento entre pais e filhos ou irmãos, e essas duas coisas podem ser separadas examinando a frequência com que ambos os alelos de uma variante são compartilhados. Especificamente, esperaríamos que os pais e os descendentes partilhassem sempre um alelo em cada variante, ao passo que os irmãos podem não partilhar nenhum alelo, podem partilhar um alelo ou podem partilhar dois alelos. quantidades mais baixas de partilha implicam tios e tias, e seus primos, e avós, e assim por diante, até relacionamentos cada vez mais distantes. em algumas abordagens de análise, presume-se que os indivíduos não sejam relacionados, portanto, o conselho costumava ser remover um membro de cada par de indivíduos relacionados.

No entanto, como os modelos lineares mistos se tornaram mais populares no GWAS, e os modelos lineares mistos são capazes de reter e incluir indivíduos relacionados nas análises, os indivíduos relacionados, portanto, devem ser retidos se o método de análise exato não for conhecido. Novamente, vale a pena ter algum conhecimento fenotípico aqui. Parentes inesperados são um sinal potencial de troca de amostras e precisam ser examinados, confirmados e potencialmente removidos se forem realmente inesperados. E mais uma vez é importante conhecer a sua amostra, os dados mostrados neste gráfico não provêm, apesar do que o gráfico parece sugerir, de uma amostra com uma grande quantidade de primos, mas sim de uma amostra em que uma minoria de indivíduos foram de uma ancestralidade diferente e isso distorce essa métrica. Falarei um pouco mais sobre isso daqui a pouco.

*Relacionamento médio*

A relação também pode ser útil para detectar a contaminação da amostra. A contaminação resultará em uma mistura de diferentes DNAs sendo tratada como uma única amostra, e isso resulta em uma superabundância de chamadas heterozigotas. Isto, por sua vez, cria um padrão característico de relacionamento de baixo nível entre a amostra contaminada e muitos outros membros da coorte. Estas amostras devem ser consultadas com o laboratório de genotipagem para confirmar se ocorreu ou não um evento de contaminação, e potencialmente ser removidas se uma explicação alternativa para este estranho padrão de relacionamento entre amostras não puder ser encontrada.

*Estrutura populacional*

Finalmente, uma palavra sobre ancestralidade genética. Devido à forma como migramos ao longo da nossa história, existe uma correlação entre a geografia das populações humanas e a sua genética. Isto pode ser detectado executando análises de componentes principais em dados genotípicos podados para desequilíbrio de ligação. Por exemplo, estes são os dados do UK Biobank, você pode ver subconjuntos de indivíduos que se agrupam e que compartilham etnias europeias, outros subconjuntos que compartilham etnias africanas e subconjuntos que compartilham diferentes etnias asiáticas, e em uma coorte mais diversificada você será capaz de ver outros agrupamentos também. Este tipo de gráfico 2D não é a melhor maneira de visualizar isso, por exemplo, aqui não é realmente possível distinguir esses agrupamentos do sul da Ásia e dos americanos miscigenados, e você não tem a noção completa do domínio dos dados de ancestralidade europeia nesta coorte. Os europeus, neste caso, representam cerca de 95% da coorte completa, mas devido ao excesso de plotagem, ou seja, os mesmos valores sendo plotados uns sobre os outros neste gráfico 2D, você realmente não aprecia isso. Examinar vários componentes principais ajuda nisso.

A ancestralidade é importante para o QC. Muitos dos processos sobre os quais falei dependem de os grupos serem avaliados de forma justa e serem bastante homogêneos. Dessa forma, se seus dados forem de múltiplas ancestralidades, é melhor separar essas ancestralidades e executar novamente o QC em cada grupo separadamente.

*Aprendizado*

Esse foi um breve resumo de algumas das principais coisas a serem consideradas ao executar o QC.

Espero ter passado a necessidade de tratar isso como um processo baseado em dados e de estar disposto a executar novamente as etapas e ajustar as abordagens para se adequar às coortes. Embora tenhamos algo semelhante à prática padrão no controle de qualidade de genótipos, acho que ainda existem algumas questões não resolvidas. Portanto, obtenha alguns dados, procure guias e pipelines automatizados on-line e aproveite seu controle de qualidade.

Muito obrigado por ouvir, estou fazendo perguntas e respostas às 9h30 EST, caso contrário, sinta-se à vontade para me fazer perguntas no Twitter onde moro ou no endereço de e-mail na tela que verifico ocasionalmente. Muito obrigado.

------------------------------------------------------------------------

# ***Considerações sobre genotipagem, controle de qualidade e imputação em GWAS*** {#sec-section3}

**Título**: Considerações sobre genotipagem, controle de qualidade e imputação em GWAS

**Apresentador(es)**: Ayşe Demirkan, PhD (School of Biosciences, Surrey Institute for People-Centred Artificial Intelligence, University of Surrey)

**Ayşe Demirkan**:

Olá a todos, meu nome é Ayşe Demirkan. Sou afiliada à Universidade de Groningen, da Holanda, e à Universidade de Surrey, do Reino Unido. Esta é uma palestra pré-gravada na segunda sessão de sessões sob demanda: “Introdução à análise estatística de estudos de associação genômica em larga escala”. Falarei sobre considerações para o controle de qualidade da genotipagem e imputação em estudos de associação genômica.

Então aqui você tem uma visão geral da palestra. Em breve abordaremos plataformas de genotipagem e opções de controle de qualidade. A seguir falarei sobre a definição e finalidade da imputação e como ela é feita. Isto incluirá dados de referência, ferramentas, análise de dados imputados, precisão de imputação e avaliação.

*Genotipagem e Plataformas*

O que chamamos de genotipagem é o processo de determinação de diferenças na composição genética, portanto no genótipo, de um indivíduo, examinando a sequência de DNA do indivíduo. É claro que a tecnologia utilizada para a genotipagem depende das propriedades estruturais da variação genética, quer se trate de um polimorfismo de nucleotídeo único ou de uma variação do número de cópias ou de outras variações estruturais. Depende também da justificativa do projeto ou da questão científica e do seu orçamento, principalmente. Relacionado a isso, é claro, está quantos SNPs você deseja genotipar. Se for um estudo de associação genômica em larga escala e o número de indivíduos que você gostaria de incluir, dependendo do desenho do seu estudo, você também estará limitado pela qualidade e quantidade da sua amostra de DNA.

*Abordagens Comuns*

Então, aqui neste slide, você vê as abordagens mais comuns usadas para genotipagem de SNPs, e dependendo do seu estudo, você provavelmente usará um deles, geralmente arrays Illumina/Affymetrix. Assim, no eixo y, você vê o número de SNPs que são facilmente capturados pelos arrays, e no eixo x, você vê o número de indivíduos. Então, o que temos? Temos plataformas PCR, RFLP, sequenciamento, pirosequenciamento, plataformas Fluidigm e TaqMan. Por exemplo, um dos melhores exemplos são os arrays Illumina para varreduras de genoma completo. A genotipagem do genoma completo por essas matrizes fornece uma visão geral de todo o genoma e permite novas descobertas e associações. Assim, usando tecnologias de sequenciamento e microarray de próxima geração de alto rendimento, você pode obter uma compreensão mais profunda do genoma porque está cobrindo uma proporção muito ampla do genoma.

Portanto, você pode usar uma de suas seleções de matrizes Illumina ou Affymetrix, que você acha que podem ser adequadas para o seu estudo. Existem muitas opções e, para as plataformas de matrizes da Illumina, existem atualmente matrizes de genotipagem de todo o genoma para 18 espécies. O número de marcadores em cada matriz varia de acordo com o produto. Para humanos, são possíveis até quatro milhões de marcadores por amostra. Depois, existe uma matriz de triagem de baixo custo Infinium. Por exemplo, este inclui 600.000 marcadores. Você pode começar com 200 nanogramas de DNA genômico. O que você também pode fazer é adicionar alguns painéis de marcadores personalizados. Há uma capacidade adicional de até 50 mil marcadores.

*Família Omni de matrizes Illumina*

E há também a família Omni de matrizes Illumina. Aqui você vê uma descrição simples de sua cobertura e a inclusão de marcadores genéticos em relação às suas frequências alélicas menores. Portanto, essas matrizes à esquerda incluem apenas variações comuns com frequências alélicas menores superiores a cinco por cento. Alguns incluem CNVs (Variações de Número de Cópia) e alguns incluem SNPs com frequências alélicas menores menores. Qual escolher entre eles dependerá da sua questão de pesquisa e da população que você deseja rastrear. Por exemplo, você está procurando variações raras ou comuns em termos de SNPs? Você está procurando CNVs? Você está trabalhando com uma doença rara ou comum e qual é o tamanho da sua amostra e seu orçamento? Agora, listei alguns sites aqui. Reserve de 10 a 20 minutos para verificar as tecnologias mencionadas na primeira seção usando esses sites.

*Controle de qualidade (QC) da genotipagem: da máquina ao conjunto de dados - chamada de genótipo*

Agora vamos falar sobre o controle de qualidade da genotipagem (QC). Você elaborou seu estudo, escolheu uma plataforma ou serviço de array adequado e usou um serviço de seu instituto. Portanto, uma etapa inicial crítica dos sinais de intensidade induzidos quimicamente e da análise de dados é a transferência e o controle de qualidade dos genótipos determinados para o seu computador. Esta etapa crítica é chamada de chamada de genótipo. Os algoritmos de chamada de genótipos são sempre implementados no software proprietário que acompanha a plataforma de genotipagem escolhida. Então, você não precisa inventá-los sozinho. Normalmente, o software de chamada usa uma espécie de algoritmo matemático de agrupamento para analisar os dados brutos de intensidade e estimar a probabilidade de seu genótipo ser AA, AB ou BB para um determinado indivíduo para um determinado locus de marcador bialélico. Um método para verificar a qualidade inicial do SNP é inspecionar visualmente o agrupamento de intensidade de um determinado SNP na população geral. Dependendo disso, pode-se decidir se um SNP é caracterizado por um sinal claro ou não.

Aqui, à esquerda dessas figuras, você vê um claro agrupamento de intensidade de um SNP na população. Você vê que a variante comum é representada em vermelho, o heterozigoto em roxo no meio e os homozigotos para o alelo menos comum são representados em azul. A tabela à direita mostra os valores brutos a partir dos quais este gráfico é calculado. Então, aqui o gráfico mostra um agrupamento compacto de genótipos, e não há muito ruído na medição deste SNP. Depois disso, há etapas importantes de controle de qualidade de dados. Uma delas é trabalhar em réplicas. Para inspecionar questões de plaqueamento e observar a concordância genotípica, isto seria uma boa coisa a fazer – incluir a mesma amostra de DNA em diferentes lotes de experimentos. Depois, há erros mendelianos para controlar, por exemplo, inconsistências de transmissão. Por exemplo, SNPs com taxa de erro mendeliana superior a 10% podem ser excluídos. Isso seria baseado no número de trios que você incluiria em seu experimento.

Infelizmente, esta opção obviamente está disponível apenas para designs familiares e de trio. Outra coisa, outra medida de controle de qualidade que usamos é a taxa de chamadas SNP. Esta é basicamente a taxa de genótipos ausentes, 1 menos a taxa de genótipos ausentes por SNP. Portanto, isso pode depender da qualidade das amostras de DNA, e geralmente fica entre 95% e 99%. Isso é algo muito padrão para incluir em seu controle de qualidade. Outra coisa é o desvio de equilíbrio de Hardy-Weinberg do seu SNP. Este é outro método para verificar a qualidade e exclusão dos seus SNPs. Isso será explicado no próximo slide. Outra é a taxa de chamada de amostra, portanto este é um método de controle de qualidade baseado em amostra. Esta é uma boa indicação do sucesso da amostra. Plataformas diferentes têm limites diferentes, mas isso será determinado principalmente pela qualidade inicial do seu DNA e, de alguma forma, estará em relação à taxa de chamada do SNP. Depois de definir a taxa de chamada SNP, você poderá fazer a taxa de chamada de amostra e poderá querer repetir a taxa de chamada SNP dependendo disso.

Outra coisa a fazer é a verificação de sexo na amostra. Para esta medida de qualidade, você precisa de informações do cromossomo X para calcular isso, e você pode querer adicionar isso como uma verificação adicional de sanidade em seus dados para garantir que haja uma sobreposição perfeita com seus arquivos de fenótipo em termos de sexo.

Outro importante é a heterozigosidade da amostra. Isto serve para verificar, por exemplo, valores discrepantes – por exemplo, amostras com mais heterozigosidade do que o esperado podem ser uma indicação de contaminação nas suas amostras. E você também quer fazer algo além de tudo isso. Você precisa verificar o relacionamento enigmático e a geminação inesperada das amostras, e se há realmente relacionamento e estrutura nos dados. Mas isso será mais abordado na palestra do Reedik Mägi.

*Equilíbrio de Hardy-Weinberg*

Vamos falar sobre o Equilíbrio de Hardy-Weinberg. Como a ocorrência de dois alelos de um SNP no mesmo indivíduo são dois eventos independentes, a distribuição dos genótipos entre os indivíduos deve estar mais ou menos em equilíbrio com as frequências dos alelos de um SNP bialélico. Portanto, isto só é possível em condições ideais, claro, que seriam acasalamento aleatório, sem seleção, sobrevivência igual, sem migração, sem mutação, e seleção baseada em mutação, sem endogamia, e grande tamanho populacional.

Portanto, sob essas condições, os desvios acima do equilíbrio de Hardy-Weinberg são uma indicação de problemas de chamada de genotipagem, e um limite comumente usado para a variância genotípica é um valor p do equilíbrio de Hardy-Weinberg que é menor que dez elevado a menos cinco. Isso é uma indicação de um desvio do equilíbrio de Hardy-Weinberg, e você pode querer dar uma olhada nesses SNPs ou excluí-los do seu conjunto de dados.

\*\*Outra coisa importante a sempre considerar são as construções e alinhamentos do genoma.\*\*

A caracterização do genoma humano é um esforço contínuo, e a construção do genoma nos informa as posições dos SNPs no genoma. Portanto, a versão mais recente é chamada de versão 38, mas a mais usada no momento ainda é a versão 37. Por exemplo, o HapMap foi lançado na versão 35 e na versão 36. Portanto, você precisa estar ciente dos problemas relacionados à junção e meta-análise de dados de diferentes construções de genoma. Além disso, ao preparar seus dados para imputação, isso é muito importante porque você precisa ter certeza de que seus dados são codificados de acordo com a mesma construção do genoma entre o conjunto de destino e o conjunto de dados de referência.

\*\*Então, existem ferramentas para isso. Uma delas é chamada liftover. Por exemplo, existe um de Oxford que usamos para esse fim, e forneço o link para ele aqui.\*\*

*Software comumente usado para controle de qualidade: PLINK*

Portanto, todas essas etapas de controle de qualidade que abordei aqui são bastante padronizadas e há algumas ferramentas amplamente utilizadas. Uma ferramenta muito comumente usada que também usamos para armazenamento, análise e controle de qualidade de dados é chamada PLINK. Aqui neste slide, fiz um print de algumas das opções do PLINK que também abordei durante a palestra, e essas funções estão implementadas no software PLINK. Você pode usá-lo para o controle de qualidade de seus dados genéticos.

\*\*A primeira coisa para poder usar o PLINK é obviamente instalar o PLINK, e você precisará ler os dados de chamada do seu genótipo no PLINK na forma arquivos map ou ped. Em seguida, você pode realizar o QC no nível do SNP – remover ou extrair SNPs – e pode realizar o QC no nível da amostra – você pode remover ou extrair indivíduos.\*\*

E na opção de estatísticas resumidas aqui, há funções listadas para verificar a falta de taxa de chamada, equilíbrio de Hardy-Weinberg, frequências alélicas e erros Mendelianos. Você também pode realizar verificações de sexo. O que o PLINK também pode fazer é extrair componentes principais genéticos e identificar indivíduos criptograficamente relacionadas nos dados e na estrutura genética dos dados. Você pode então usar isso para determinar valores discrepantes étnicos em seus conjuntos de dados. Não vou falar sobre isso porque faz parte da palestra do Reedik Mägi da próxima sessão.

*Intermezzo*

Aqui coloquei dois sites aqui. Um deles é para PLINK e como usar PLINK para QC, e o outro é para BEAGLE, que mencionei é um dos algoritmos que você pode usar para chamada de genótipos. Então agora, reserve de 10 a 20 minutos para dar uma olhada neste site e tente entender o que você pode fazer com eles.

*Falta de dados genéticos*

Agora, vamos falar sobre imputação. Por que precisamos de imputação? Precisamos de imputação para resolver a falta de dados genéticos. Trata-se de valores ausentes nos dados genéticos. De onde vêm os valores ausentes? Então, durante o QC, já colocamos alguns valores como faltantes, certo? E também, durante a chamada do genótipo, você pode definir alguns pontos de dados como ausentes. Mas, na verdade, a maior parte dos valores em falta provém da cobertura inicial direcionada dos chips e plataformas de genotipagem que utilizamos. Lembre-se de que existem muitos tipos de arrays – alguns são mais densos, menos densos. Existem matrizes feitas especificamente para estudos oncológicos, como matrizes onco. Existe um chip metabólico projetado especialmente para doenças metabólicas, e existem matrizes focadas principalmente em SNPs com maior frequência de alelos menores, ou há aquelas focadas em CNVs. Mas mesmo as matrizes densas de SNP não cobrem toda a variação genética. Eles cobrem muito menos do que você imagina. E, além disso, os SNPs incluídos em um array podem não estar incluídos no outro. E para muitas posições variáveis ​​no genoma, não temos informações correspondentes entre conjuntos genotípicos de indivíduos. Por exemplo, veja o que tento retratar aqui. Penso em três indivíduos: os dois primeiros são digitados na matriz X e o terceiro é digitado na matriz Y. Portanto, eles têm diferentes pontos de dados ausentes. E quando você tenta extrair seus dados para uma análise agrupada ou para serem usados ​​em meta-análise, você terá ainda mais faltas nesses dados por causa das posições não sobrepostas. E você não será capaz de replicar as descobertas de um conjunto de dados em outro. Portanto, além disso, analisaremos apenas metade da variação genética e poderemos perder variantes causais na análise. Isso ocorre por todos esses motivos. Usamos imputação de dados genéticos.

*Princípio de Imputação*

Então, o que fazemos em princípio? Em princípio, significa estimar os genótipos mais prováveis ​​em um indivíduo nas posições faltantes, observando os valores de SNP correlacionados de um conjunto de dados mais completo e, com base nisso, anotando os valores faltantes no conjunto de dados alvo. Então, como isso funciona? Em primeiro lugar, precisamos de um conjunto de dados onde os genótipos densos sejam medidos diretamente. Pode ser uma matriz densa ou um conjunto de indivíduos sequenciados. Isso chamamos de painel de referência. Em seguida, usamos um software ou serviço de imputação e, observando a estrutura de correlação dos genótipos densos ou SNPs de sequência, os estimamos no conjunto de dados alvo. Então, no final, essas são probabilidades, e acabamos com informações de dosagem para alelos ou genótipos, em vez de identificações rígidas de genótipos. E esta informação de dosagem, que explica a incerteza na estimativa, é então incluída na análise do estudo de associação genômica. Portanto, para resumir, o objetivo da imputação é aumentar o poder porque, obviamente, é mais provável que o painel de referência contenha as variantes causais do que uma matriz de genotipagem menos densa. Para melhorar o mapeamento fino porque a imputação fornece uma visão geral de maior resolução de um sinal de associação através de um locus. E então, para permitir a meta-análise porque a imputação permitirá que dados digitados com matrizes diferentes sejam combinados em variantes no painel de referência.

*Marcos históricos 2002-2010*

Repassar os marcos históricos em termos de imputação também resume os avanços teóricos e tecnológicos na imputação de dados genômicos humanos. Então, um avanço importante em tudo isso foi a geração de painéis de referência. O primeiro painel de referência foi o HapMap, e o HapMap2 foi a versão do HapMap mais comumente usada. Consiste em uma amostra limitada de indivíduos de diversas origens genéticas: 60 iorubás, 90 han chineses e japoneses e seis indivíduos residentes em Utah, descendentes de origem étnica europeia. Agora, parece engraçado pensar que imputamos milhares de pessoas com base apenas no material genético de 60 residentes de Utah, falando dos europeus. Mas, na verdade, isso rendeu muito sucesso e, na verdade, foi isso que tivemos por muito tempo. Portanto, só podíamos imputar até 3 milhões de SNPs ao HapMap na época.

E então veio o painel de referência do 1000 Genomes, que incluiu no final 2500 indivíduos de vários grupos étnicos. E mais tarde, e atualmente, o painel de referência mais utilizado é o painel do Haplotype Reference Consortium (HRC). Lembre-se brevemente, este é um conjunto combinado de dados de sequência completa do genoma e do exoma para mais de 30.000 indivíduos e usa 39 milhões de SNPs após a imputação, é claro. Isso também vai depender da estrutura que você usa para imputação. Muitos desses SNPs não serão imputados com boa qualidade, mas em condições ideais, podem chegar a 39 milhões. E, finalmente, temos agora um painel de referência do programa Trans-Omics for Precision Medicine (TOPMed), que consiste em quase 100000 genomas humanos profundamente sequenciados e pode produzir até 308 milhões de variantes genéticas a serem identificadas. 

*Marcos históricos 2010-2018*

Um marco técnico que vale a pena mencionar foi a pré- faseamento dos haplótipos. A imputação genética é um processo altamente intensivo em termos computacionais devido à estrutura probabilística e à alta taxa de dados faltantes com os quais estamos tentando lidar. Um dos principais marcos para reduzir a carga computacional foi a introdução da pré-faseamento. Essa ideia envolve um processo de imputação em duas etapas. Portanto, há uma etapa inicial de pré- faseamento, que na verdade é a estimativa de haplótipos dos genótipos, e uma etapa subsequente de imputação nos haplótipos em fases estimados. Então, isso reduz a complexidade do processo de imputação e o acelera. A versão atual de todos os softwares de imputação pode lidar com a abordagem de pré- faseamento. 

E o que é muito importante é a escolha do painel de referência. Assim, mostra-se que o uso de painéis de referência para todas as ancestralidades, em vez de painéis de referência étnicos específicos, melhora a precisão da imputação para variantes raras em qualquer população. E painéis de referência formatados para IMPUTE e Minimac podem ser baixados dos sites de software. E é muito importante garantir que a estrutura genotípica e os painéis de referência estejam alinhados com a mesma construção do genoma humano. Voltarei a isso mais tarde também.

Então, outro avanço tecnológico muito importante e atual que facilita a nossa vida são os serviços de imputação. Estes são serviços disponíveis gratuitamente, como os serviços de imputação de Michigan e Sanger. Você pode simplesmente formatar e fazer upload de seus dados de forma segura para este servidor e recuperar os dados imputados e os genótipos faseados em alguns dias. E isso depende da velocidade e do quão ocupado o servidor está e do tamanho da amostra que você está tentando imputar, é claro.

*Marcos históricos - Sanger*

Assim, paralelamente ao servidor de imputação de Michigan, o Sanger Institute também possui um serviço semelhante. Neste serviço, você também pode fazer upload de seus dados em formato VCF e opcionalmente realizar pré-faseamento usando o software BEAGLE ou SHAPEIT. Os painéis de referência atuais no servidor de imputação Sanger incluem HRC, UK 10K e 1000 Genomes. Como disse, também existe um servidor dedicado ao TOPMed. Tudo isso é muito autoexplicativo. É assim que o servidor de imputação Sanger gostaria que você preparasse os dados. Portanto, há um monte de instruções que você gostaria de usar. O uso desses serviços vem com instruções e manuais, então fique à vontade para criar uma conta lá e executar alguns conjuntos de dados de teste. Você precisará formatar os dados conforme exigido nas instruções. Você precisará corresponder as coordenadas as construções do genoma da referência e preparar um arquivo para cada cromossomo. Isto é para o servidor de imputação Sanger. 

*Velocidade - Impute5 PLOS GENÉTICA*

E outra coisa importante em termos de imputação é, claro, a rapidez. Portanto, aumentar o tamanho do painel de referência melhora a precisão dos marcadores com baixas frequências alélicas menores, mas esse efeito positivo aumenta os desafios computacionais para métodos de imputação. Recentemente, um novo software de imputação, Impute 5, foi introduzido pelo mesmo grupo. Ele faz imputação com eficiência de memória selecionando haplótipos usando a transformação posicional de Burrows-Wheeler. Assim, usando o painel de referência HRC, os desenvolvedores do software mostraram que o Impute 5 é até 30 vezes mais rápido que o Minimac 4 e até 3 vezes mais rápido que o BEAGLE 5.1 ​​e usa menos memória que ambos os métodos.

*Estrutura de exemplo*

Portanto, usando todas as considerações mencionadas até agora, você pode construir uma estrutura in-silico semelhante a esta. Assim, você pode usar, por exemplo, funções PLINK para as duas primeiras etapas do QC de dados genéticos. Em seguida, você pode verificar as informações do chip e os problemas de fitas usando as ferramentas do software R. E se necessário, você pode atualizar a construção do seu genoma usando a ferramenta liftover. E você pode então fazer a pré-faseamento usando SHAPEIT. E por último, impute internamente ou utilizando um dos servidores citados, em que os links para este software são fornecidos aqui. Agora reserve um tempo, provavelmente horas, para explorar esses três serviços de imputação (Sanger, Michigan e TOPMed), vão te pedir para fazer uma conta para imputar então tome seu tempo para fazer isso.

*QC da imputação*

E o próximo tópico QC relacionado a imputação. Há dois passos de QC relacionados com imputação. Um deles é o QC pré-imputação, então já discutimos o QC depois da genotipagem e além disso você pode querer excluir SNPs com frequência do alelo menor menor que 1 por cento. E também o QC pós-imputação, que acessa medidas de informação que são números entre 0 e 1, em que é comum filtrar SNPs por esse valor, menor que 0.8 para um filtro mais restrito ou 0.4. No software IMPUTE esse valor é chamado de “info score” e no minimac de “r2 metrics”, então é importante checar a qualidades dos SNPs imputados no andaime da região por inspeção visual de cluster plots e você também pode querer produzir gráficos de qualidade por cromossomo variando de acordo com frequência alélica menores , estratos e posição nos cromossomos. Por exemplo, aqui está uma figura de exemplo. Esta figura de qualidade de imputação versus MAF mostra uma relação típica entre a frequência do alelo menor e a qualidade de imputação. Portanto, no eixo y, você vê a precisão da imputação conforme determinada pela qualidade da imputação, como pelo r2 ou info score dos diferentes softwares, e no eixo x, você vê a frequência do alelo menor. Você vê que a precisão é maior quando a frequência do alelo menor é alta, quando o alelo é mais comum. Então, a precisão diminui onde a frequência do alelo menor também diminui. Você ainda tem alguns SNPs bem imputados entre os raros, mas a maioria dos SNPs de baixa qualidade virá de SNPs de baixa frequência alélica menor. Portanto, lembre-se de que ao filtrar por qualidade de imputação, você também filtrará muitos SNPs raros.

*Fatores que afetam a imputação*

Então quais fatores afetam a qualidade da imputação? Assim, ao nível do genoma, o número de indivíduos imputados tem algo a ver com isso. Por esse motivo, mesclamos conjuntos de dados de andaime antes da imputação, se quisermos imputar mais de um. Quanto mais melhor. E o segundo fator é o painel de referência. A escolha do painel de referência, assim como toda a ideia, é utilizar a correlação entre SNPs em diferentes populações, e isso pode ser diferente de população para população. Você deseja optar por um grande painel multiétnico, se não puder optar por um grande painel étnico específico. Finalmente, no nível do SNP, quanto menor a frequência do alelo menor, menor será a qualidade da imputação.

*Como analisar os dados imputados: análise de genótipos imputados*

Assim, para cada indivíduo, a imputação fornece uma distribuição de probabilidade de possíveis genótipos para cada variante não digitada. Essas probabilidades podem ser convertidas em genótipos de melhor estimativa, mas isso geralmente não é recomendado, pois aumenta os falsos positivos e reduz o poder. Além disso, ao converter as probabilidades em contagens de alelos esperadas, você deseja filtrar seus genótipos mais palpáveis. Você deseja aplicar uma filtragem rigorosa nos genótipos mais bem avaliados, e isso resultaria em mais NAs em seu conjunto de dados. Portanto, é melhor converter as probabilidades em contagens de alelos esperadas e analisar levando em consideração a incerteza na imputação. Isso é muito importante. E para fazer isso, você precisa combinar os formatos de dados com o software. Nem todo software usa todos os tipos de dados e pode ser necessário fazer conversões de dados. Um software chamado EPACTS, SNPTEST e PLINK 2 suporta as informações de dosagem, e você precisa conferir a palestra do Reedik Mägi para a análise de dados do genoma.

*Mensagens*

Então, este é o último slide desta palestra. A mensagem principal é que estamos lidando aqui com abordagens livres de hipóteses. Infelizmente, tudo se resume ao dinheiro e aos recursos que temos. Portanto, você precisa pensar sobre qual é a melhor e mais econômica maneira de fazer a genética em uma amostra grande. E a resposta é combinar uma matriz densa de varredura do genoma com imputação, já que os painéis de referência são gratuitos no momento e o custo das matrizes também está diminuindo. Mas você realmente precisa pensar na parte in silico de fazer isso, pois isso também requer pessoal e recursos computacionais até certo nível. E o que mais você deve considerar? Em comparação, você quer saber, dependendo da sua questão de pesquisa, é claro, se existe um array melhor para você, ou talvez um array ou um chip metabold se você for conduzir sua pesquisa em um campo muito restrito. E o mais importante, quais são os usos futuros desses dados? Porque obviamente você não quer construir algo que usará apenas por alguns anos e finalizar a pesquisa sobre isso. O ideal é que você queira investir em big data. Então, você vai investir em uma coorte de base populacional ou em uma coorte de doença? Será um projeto de curto prazo ou um estudo de acompanhamento que provavelmente se desenvolverá e se estenderá ao longo dos anos, incluindo novos fenótipos? E finalmente, com quem você deseja colaborar? Quais consórcios? Quais doenças?

Portanto, espero que esta palestra seja útil para suas pesquisas e estudos futuros, e para as pessoas que estão interessadas em ter uma compreensão melhor e mais aprofundada da imputação. Todos os anos, duas vezes, temos um curso GW[AS]{.smallcaps} organizado pela Universidade de Surrey em colaboração com o Imperial College e a Universidade de Tartu da Estônia. Este curso inclui um workshop prático e também aulas teóricas, onde ensinamos esses conceitos e assuntos com mais detalhes. A última foi de 10 de maio a 5 de julho de 2021. Para mais informações, existe um endereço de e-mail ao qual você pode se conectar. Muito obrigado e boa conferência pelo tempo restante.
