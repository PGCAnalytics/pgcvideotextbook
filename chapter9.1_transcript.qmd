---
title: "Chapter 9.1: Copy Number Variation (Video Transcript)"
---

**Title**: How to run Copy Number Variation (CNV) analysis

**Presenter(s)**: Daniel Howrigan, Broad Institute

just hello my name is Daniel Howrigan and today I'll be talking about how to run copy number variation analysis today's talk I'll address a couple of questions namely what is a copy number variant and how do we detect it with genotype data what does the CNB file format look like and what is CNB analysis output look like finally how do I use this to run CNB burden and Association tests

so what is a copy number variant well I Define it here as a subset of structural variation involving a gain or duplication or a loss slash deletion of genomic sequence now I call it a subset of structural variation because structural variation can Encompass basically any change in the length of a genomic sequence that can be as small as a single base pair insertion or deletion now most Snips that we we think of are just substitutions so the actual size of the genome isn't changed with a single nucleotide polymorphism but a structural variant can Encompass anything from very small all the way to entire chromosomes what we call cmvs well it being kind of a general name falls into the category of roughly at least defined here at minimum one kilobase to sub microscopic with array data we generally say something around 10 to 100 kilobases given our sensitivity to detect these cnvs whereas larger cmvs usually are greater than 500 kilobases up to multiple megabases and then we get into much larger events now I'm not going to get into the details of the mechanisms that cause it I've listed a few here you can look them up but there are different hot spots in the genome that are more prone to these copy number variants due to repeat regions or often ways in the Machinery that can make mistakes and lead to these gains or losses of genomic sequence how do we detect it using genotype data so when we run across the genome collecting a bunch of snips I show here kind of the basic mechanism or at least a figure showing how we take light intensity from different experiments looking at capturing either you know allele a I have a a as allele a or the B allele

being TT here and then a heterozygous you can see a mix of both red and green now we can leverage these allele frequencies in what we call the B allele frequency or at least the frequency of of the T in this case now normally when there's no copy number variant these will be roughly equal and so be around 50 percent when we see something like a large deletion or any sort of deletion we should see a loss of heterozygosity and therefore we will see a gap in these be alleles at least at any site that you would normally be heterozygote at with the duplication these would be a little bit off so it would be more like two-thirds one-third and you would see maybe a movement kind of akin to where these pink lines may be going even though they don't quite Define that you would see in a duplication that the B allele frequencies move off 50 but not to one and zero now that's looking at the B alleles the other thing we use to detect it is the log ratio and this is basically measuring the light intensity and when you see a drop here where we would see maybe in in all of these calls you should see a drop of say 50 percent when we have a deletion and you will see subsequently a rise of about 33 percent for a duplication now I'm not going to get into the details of the collars that are used to detect these copy number variants suffice to say this is kind of the foundation that it's built upon what I'm going to talk about next is given that you've run copy number variant callers

here are the kind of QC calls that you're moving into the analysis stage now I use plink to analyze the CNB data so you can get a bunch of different file formats copy number variants from different callers but you would let you know if you want to use plink you would convert this into this particular file format it's definitely which is the dot CMB file and it's basically individual identifiers the chromosome the start and end position of this CNB the type one being a deletion three being a duplication and then also there's a few other fields here the score and sites field in this example I use the score being say the number of copy number variant callers that agreed upon the Skip and call and it could range up to six sites here being the number of snips that are used to call the CMV now I note here that score and sites are not forced into a particular convention you could say replace score with the number of genes overlapping the CNB um or the site being some other variable that you would be interested in measuring now along with this file plink creates a cnb.map file and this is basically

breaking down the break points of each cnv into akin to a map file similar to what we have for snip data and you can see here I note that every different position is mapped even the the end position but also maybe a single position after that end because you may want to do a test an additional test after the end of a CNB to see how things have changed note that the CNB file format commands are not available in plink 1.9 but the initial version granted the speed ups that you get using the newer version of plink aren't all that applicable here because we're generally dealing with rare variation and so these file sizes generally not too large and the sorts of computing that you use is not too heavy so what does CNB analysis output look like usually whenever you run a command and plink looking at your CND files you get out.cnv.indiv file so this is per sample file where you say the number of CNB segments that this individual has the number of kilobases that these segments cover and the average kilobase is covered per segment you also get a cnv.summary file this is akin to the map file that is summarizing the number of affected and unaffected individuals at any given break point or start and and plus one of a c and b and this is basically you know the the output here well it seems quite simple I'll show how with these files you can do quite sophisticated analysis by using a lot of different filters so a lot of the Magic in plink is all the flags that you can use to subset your list of cmvs so what I have here is a kind of verbose command just to show the optionality available in plink and for each of these I describe what that function is doing so with plink you have dash dash C file this reads in the CNB and CNB map file I want to select only deletions I want to select cnvs that are at least 100 kilobases in length I want cnbs with a score of four or higher and at least 50 sites

I want to exclude cnbs that overlap a particular region so I can insert a different text file with a list of chromosome and start and positions here and I want to make sure in this exclusion that the cmvs must overlap by at least 50 percent to be excluded I can also look at frequency where at least 10 cmvs overlap I would like to exclude those because maybe I'm more interested in very rare uh cnbs I can also write out the frequencies of of these of these cnbs as well just so I can guarantee which cnbs are being dropped which cnbs are being kept and then I can run a basic burden test using a permutation model and here I just set the number of permutations to these ten thousand so you can see there's a lot of different flags here and it's manipulating a lot of these flags that can give you uh just what you would like in terms of your analysis now granted uh using the burden tests in plink doesn't handle covariates it basically just looks at something like case control status and so what I recommend is taking the output particularly the CND cnd.indiv file and reading that into say python or R I prefer R to run more sophisticated models and so you can see when you put more filters here if I go back the number of segments will change depending on what filters and obviously subsequently the number of kilobases covered by these segments will change as a function and it's kind of iteratively reading in these files at different filtering steps that can produce a wide range of tests

so I've shown a couple figures here that we published when looking at CMB burden in the PGC schizophrenia so I took in those dot cnv.indiv files into R I ran a logistic regression predicting schizophrenia status and adding a number of covariates such as principal components genotype platforms and then basically as I iteratively ran different uh commands in plink to look at KB C and B counts lengths frequencies whether or not they're in or not in a particular regions you can build up a number of tests of overall burden so in this example here this would be all cnvs deletions and duplications stratified by different genotyping platforms and then all together and then B here I'm stratifying by different frequencies and say previously implicated cmvs as a region I want to say in the blue bars we're looking at enrichment here in the green and blue bars but you can see the big deviation here there's a lot of enrichment when we talk about cnbs at this size or at least this frequency I mean and they go away because most of these have been implicated so I've excluded these regions rerun the burden test and you can see that we've captured much of the signal already with previously implicated cnbs so how do we use this to run CNB

Association tests at individual CMB loci or at individual break points of cnbs basically I would run a very similar command I could use all the same filters basically get rid of a number of commands in particular you're getting rid of this CNB in indiv.perm Step but you still run a permutation test and what you'll get out is a dot cmd.summary dot and perm file and at each base position you can run an association test here using permutation this would be the pointwise permutation value but there's also a family-wise permutation p-value that corrects for all the tests here so this Association is run at all possible start and in N plus one positions and one of the things you can do if you like to include covariates in your data at least what I've done in the past is maybe run your logistic regression model with a lot of your covariates pull out the residuals and then use this as a quantitative trait and you can run Association mapping in plank to get p-values that way so what does this look like I think having a figure kind of in is instructive here so I plotted uh through a browser I'll break this down this is our signal at the Direction one gene so in red we have our deletions uh light red deletions in our schizophrenia cases dark red deletions and our schizophrenia controls I also have duplications in blue which don't make up much of the signal here but as you can see I've also plotted the log negative log 10 p-value and if you kind of look closely you can see each little break point you can see a different test being run and you can see at you know a spot like this where there's many different break points a lot of granularity you can run many different tests and kind of get a shape of the association around this Gene now you can also collapse another test that we've done is collapsing across the all the exons of this Gene and so this would be Akin more to like a gene burden test where you collapse this region and then you test for overlap at that region run a similar model and then you can kind of aggregate all the cnbs and cases and controls to report say a gene-based p-value so that's a very quick overview of how to run burden and association with cnv data and a few considerations is that you know one of the things that you don't have access to when looking at cnbs is imputation and so it is a consideration to think about you know there isn't kind of a reference uh you know reference haplotypes or a larger data set to do additional QC so there can be additional challenges particularly with subpar data that they can't be rescued in the way that imputation uh can rescue snip genotypes and on that note you know genotyping chip does matter very much because you can't impute a bunch of different new sites the variability in terms of the number of snips particularly for smaller cnbs is a very kind of important

consideration and so you know you can think of in a particular genotyping chip if you don't have a good case control balance there you may not have the sensitivity to properly detect uh cmvs and so there's a lot of work that goes into determining at what length of cmvs uh or at least at what case control balances given your genotyping chip do you have the right amount of power and sensitivity to do a proper case Control Association test another thing too ancestry PCS most of the CNB hotspots and Associated cnv that we see in in psychiatric disease aren't very impacted and mainly that's because these are recurrent de novo CNB areas where there's a higher mutation rate but this is not really ancestrally um you know defined in terms of the fact that there's not a large difference in the allele frequency across different ancestries but it is still useful to include particularly as you get to higher frequencies and you get more inherited cnbs and finally multiple testing correction permutation is more one of the more robust ways to account for the multiple tests because the nature of CNB data is such that given the type of genotyping Chip that you use the size of your data set um no no particular study is going to be very similar to similar to another study and that kind of leveraging the correlation structure within your own data set given you know your ability to detect cnvs at various of various sizes and frequencies using permutation is usually the best way to go about properly testing for Association so if you have any questions feel free

to email me I've also put down some sites you could search here for the PGC CNB paper that uh we did in 2017 and also I think that the write-up and plink on how to do this is is really good and very descriptive of all its functionality thank you
