[
  {
    "objectID": "chapter5.4_transcript.html",
    "href": "chapter5.4_transcript.html",
    "title": "Chapter 5.4: Meta-Analysis (Video Transcript)",
    "section": "",
    "text": "Title: Genome-wide association study design and interpretation\nPresenter(s): Gina Peloso, Broad Institute"
  },
  {
    "objectID": "chapter3.html#sec-section1",
    "href": "chapter3.html#sec-section1",
    "title": "Chapter 3: Technologies",
    "section": "3.1 SNP array genotyping",
    "text": "3.1 SNP array genotyping\nTitle: SNP Chips (Introduction to genomics theory)\nDescription: This part is focused on understanding the SNP chip technology. We talk about the general idea behind them, and how they work and why are they useful.\nPresenter(s): Gábor Mészáros, Genomics Boot Camp\nLength: 28:05\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\nPowerpoint slides for this video are available here."
  },
  {
    "objectID": "chapter3.html#sec-section2",
    "href": "chapter3.html#sec-section2",
    "title": "Chapter 3: Technologies",
    "section": "3.2 Next Generation Sequencing",
    "text": "3.2 Next Generation Sequencing\n\nHow to sequence the human genome\nTitle: How to sequence the human genome\nDescription: Your genome, every human’s genome, consists of a unique DNA sequence of A’s, T’s, C’s and G’s that tell your cells how to operate. Thanks to technological advances, scientists are now able to know the sequence of letters that makes up an individual genome relatively quickly and inexpensively. Mark J. Kiel takes an in-depth look at the science behind the sequence.\nPresenter(s): Mark J. Kiel\nLength: 5:04\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\n\nNext Generation Sequencing: A Step-by-Step Guide to DNA Sequencing\nTitle: Next Generation Sequencing: A Step-by-Step Guide to DNA Sequencing\nDescription: Next Generation Sequencing (NGS) is used to sequence both DNA and RNA. Billions of DNA strands get sequenced simultaneously using NGS. Whereas with Sanger Sequencing, only one strand is sequenced at a time. While the Human Genome Project took over 30 years to sequence the human genome for the first time. Now with Next Generation Sequencing, a whole human genome can be sequenced in just one day. This video describes the library preparation process, cluster generation, the sequencing reaction and filtering and alignment of the sequencing data. This video focuses on the primary sequencing method used by Illumina, Sequencing by Synthesis (SBS).\nPresenter(s): ClevaLab\nLength: 7:37\n\n\n\n\n\n\n\n\n\nLink to video transcript here."
  },
  {
    "objectID": "chapter2.html#sec-section1",
    "href": "chapter2.html#sec-section1",
    "title": "Chapter 2: The Genome",
    "section": "2.1 Organization of the genome",
    "text": "2.1 Organization of the genome\nTitle: An Introduction to the Human Genome\nDescription: A brief overview of the human genome: What is DNA? How is DNA organized? How is DNA regulated? What are genes?\nPresenter(s): HMX Genetics, Harvard University\nLength: 5:35\n\n\n\n\n\n\n\n\n\nLink to video transcript here."
  },
  {
    "objectID": "chapter2.html#sec-section2",
    "href": "chapter2.html#sec-section2",
    "title": "Chapter 2: The Genome",
    "section": "2.2 Genetic variation",
    "text": "2.2 Genetic variation\nTitle: Genetic Variation and Mutation\nDescription: A brief overview on genetic variation (SNPs, CNVs)\nPresenter(s): Precision Health\nLength: 4:59\n\n\n\n\n\n\n\n\n\nLink to video transcript here."
  },
  {
    "objectID": "chapter2.html#sec-section3",
    "href": "chapter2.html#sec-section3",
    "title": "Chapter 2: The Genome",
    "section": "2.3 Evolutionary signatures",
    "text": "2.3 Evolutionary signatures\n\nOrigins of Genetic Variation\nTitle: Origins of Genetic Variation\nDescription: A brief overview on genetic variation (SNPs, CNVs)\nPresenter(s): Jessica Pamment, Professor, DePaul University\nLength: 4:47\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\n\n\nNatural Selection & Human Genetic Variation\nTitle: MPG Primer: Natural selection & human genetic variation\nDescription: A brief overview on genetic variation (SNPs, CNVs)\nPresenter(s): Stephen Schaffner, Computational Biologist, Broad Institute\nLength: 54:09\n\n\n\n\n\n\n\n\n\nLink to video transcript here."
  },
  {
    "objectID": "chapter2.html#sec-section4",
    "href": "chapter2.html#sec-section4",
    "title": "Chapter 2: The Genome",
    "section": "2.4 Linkage disequilibrium",
    "text": "2.4 Linkage disequilibrium\n\nWhat is linkage disequilibrium?\nTitle: What is linkage disequilibrium?\nDescription:\nPresenter(s): Gábor Mészáros, PhD\nLength: 12:52\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\n\n\nMeasuring linkage disequilibrium\nTitle: How to measure linkage disequilibrium?\nDescription:\nPresenter(s): Gábor Mészáros, PhD\nLength: 11:31\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\n\n\n(Advanced) Measuring linkage disequilibrium\nTitle: How to measure linkage disequilibrium? (ADVANCED)\nDescription:\nPresenter(s): Gábor Mészáros, PhD\nLength: 9:28\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\n\n\nComputing linkage disequilibrium\nTitle: Compute linkage disequilibrium (Part 1)\nDescription:\nPresenter(s): Gábor Mészáros, PhD\nLength: 10:18\n\n\n\n\n\n\n\n\n\nLink to video transcript here."
  },
  {
    "objectID": "chapter10.1_transcript.html",
    "href": "chapter10.1_transcript.html",
    "title": "Chapter 10.1: A Career in Psychiatric Genetics (Video Transcript)",
    "section": "",
    "text": "Title: How does Genetics Affect our Mental Health?\nPresenter(s): Cathryn Lewis, Alex Curmi\nYou have found the “Thinking Mind” podcast [Music].\nAlex: Professor Catherine Lewis, thank you for coming on the show.\nCathryn: Pleasure, Alex. Thank you for the invitation.\nAlex: I’m sure there’s a lot that we’re going to get to talk about in terms of genetics, psychiatry, psychiatric conditions, psychology, and how we can, using the art of maths and statistics, come to unravel some of these mysteries. But one of the great things about talking to interesting guests such as yourself is the ability to learn about Career Development and career paths, and to learn that career paths aren’t always linear. And maybe you could tell us a bit about how, how do you become a professor of genetics, statistics, epidemiology,\nCathryn: yeah, yeah, So my background is, it’s not in Psychiatry or psychology, but in maths. I did a first degree in very pure maths, um, and because I was passionate about maths when I was at school and really enjoyed it. But as the studies went on, I realized that that sort of esoteric theoretical side of Academia was not really what I was interested in. I wanted things that were much more practical, that were much more aligned with solving problems in everyday life. And so I moved on to do a master’s degree in statistics and did a lot of data analysis. Statistics is a fabulous area to study because it gives you access not just to the math side, but so many different areas that people use statistics in. And I was lucky enough during my PhD to start analyzing genetic data and realized that I’m was really interested in it and also that it was a blossoming, developing area of medical research. And so that launched me in my research career.\nUm, I should say that I’ve worked across different medical areas during my career. I started working in cancer, looking for genes for breast cancer, and I’m a very, very minor author in the paper that found the BRCA1 gene with high-risk variants for breast cancer, for example, and I’ve worked in autoimmune disorders, and really applying my statistical techniques where they can be useful. But what I’ve focused on in the last 15 years is working in the genetics of mental health disorders and of Psychiatry. And like many things, this was a very serendipitous chance. I remember presenting a poster at a conference on autoimmune disorders, and a psychiatrist came up to me and said we could do this in schizophrenia, and I started working with him in schizophrenia. And then Peter McGuffin, who was head of the social genetic and developmental Psychiatry Center where I am now am, saw the potential to bring my skills into mental health genetics and approached me, and 15 years later, I’m, um, you know, fully embedded in the genetics of Psychiatry.\nAlex: Have you found that serendipity is an important part of forging a career path?\nCathryn: Um, for me, it certainly has been. I know people have different approaches to planning their career; mine has always been quite short-termist. I’ve always, you know, I’ve always really enjoyed what I’ve done, and I’ve taken decisions based on, “Oh, that sounds like a really good project to be involved in,” “Oh, I’d really like to work with them.” So, I’ve never had a, you know, a long-term goal of what I would achieve, and I know that’s not the sort of advice that,\nAlex: it’s not the standard, right?\nCathryn: It’s not... it’s worked very well for me, and I’ve really enjoyed the flexibility of academic careers to hop around and to build up experience and to learn, you know, from different projects and different people - things that really produce a richness to the next, uh, the next thing that you do. And I should also say, perhaps, that I worked, I worked part-time for 10 years, years while my children were young, and again, really enjoyed that flexibility to continue with the bits of the role that were important to me.\nAlex: So you had the kind of experimental attitude towards your career, trying different things, and you were guided by your sense of fascination, which is actually career advice I give to people all the time because I think we’re conditioned by our educational system really to meet goals that are set by other people. And I also think that fascination is a kind of sense you can develop. The more you’re led by it, the more you can actually sense how actually interested or engaged am I in this particular thing. But so many people do things that they’re not engaged with for their whole lives, so it’s nice to hear that you’ve been able to escape that particular trap,\nCathryn: yeah, yeah, now that’s really good to hear that perspective on it and I agree with that kind of fascination, and of course, that’s what scientific research and exploration are all about. But maybe I can put in a plea, you know, I come from a maths background, and a lot of science now is about large numbers, it’s about AI, it’s having the skills to not only do molecular work (you know, in the lab or working with people) but to be able to analyze that data and put things together. And I would recommend anyone that gets the chance to develop their skills further in maths or statistics, that really does open doors to the breadth of different things that you can get involved in. And you know, particularly, young people, if they get the chance to study maths, you know, at a level beyond the age of 16 where it’s compulsory in the UK, you know, even downstairs dream that really opens doors having those skills and that showing your ability to be able to think analytically. And as a mathematician, of course, I’m biased, I do think mathematical skills open doors in many fields.\nAlex: Absolutely. So maybe you could tell us a bit about statistics. How does statistics help us to unravel some of the mysteries, particularly in Psychiatry, that we’re trying to figure out?\nCathryn: So one of the things that statistics enables you to get a handle on, is the risk of something happening, and that’s what we’re very interested in as individuals. You know, starting something very basic that we think about a lot is family history of psychiatric disorders. So we know, you know, we know from scientific studies, from our personal experience, that mental health disorders tend to run in families. We very often see that someone who’s diagnosed with depression, one of their parents or a more distant relative also has depression. And looking at families, we’ve been able to get risks of people developing a disorder themselves given their family history.\nBut I think people are not always very good at interpreting risks appropriately, and we see this very much so in newspapers, for example. We see that something is told that it increases our risk of something really bad happening by five times, and that’s quite scary to see, to know that, “Oh, I’ve drunk too much coffee, I might be at five times the risk of something else happening.” But that five times is a relative risk, that’s compared to it not happening. The other piece of information that we are not good at looking at is the absolute risk. So if the chance of something happening is very rare, say happening one in ten thousand times, then of five times, it’s still very unusual that it happens. And I think we’re very bad at interpreting risks because we need to put together that relative risk, which is often quite scary, with the absolute risk, which can bring that back to being quite reassuring.\nAlex: yes, so, so relative risk is like how, how much more likely are you to develop a condition given a particular state of affairs? That’s relative risk…\nCathryn: Exactly.\nAlex: Risk is just in the population, who’s going to develop this? How likely is it to develop a condition,\nCathryn: yeah, exactly. And so, to bring that back to mental health, so in schizophrenia, for example, we know that someone who has a parent who’s been diagnosed with schizophrenia is, you know, maybe eight or ten times more likely to develop schizophrenia themselves, and that’s a really scary number, and I know there’s a lot of concern, quite justifiably about that, but to put that in context, if the population risk of schizophrenia is about one percent, then being at an eight-fold increased risk of developing schizophrenia is a risk of eight percent. So yes, that eight percent is higher than the population risk, but that still means that there’s a 92 percent chance that that doesn’t happen, so it’s actually doesn’t develop schizophrenia.\nAlex: so really, we don’t… when we see genetic data, particularly I find an issue is when scientific data is filtered through the media because the media obviously are incentivized to produce something eye-catching or sensational. But leaving that aside, we don’t have good intuitions about how to evaluate risk as we see it as reported by scientific data, it sounds like.\nCathryn: Yes I completly agree with you, and there is, you know, as you say, the newspapers, journalists have to, you know, sell their news, and so they tend to report the things that are most likely to be eye-catching, you know, more sort of clickbait. And so they tend to report the relative risk rather than the more reassuring absolute risk.\nAlex: The other thing, I guess, the very traditional view of genetics is that our DNA is quite has a very high determination on what happens. You know, if you’re, if you have the particular genes for condition, you’re gonna get that condition. That’s how we tend to think of it classically. But we know more and more that there are some conditions which have a huge amount, a huge variety, of genetic influence, so much more than one gene. Say if you have a condition like Huntington’s disorder, that’s one gene, and if you have the gene, you’re going to develop that condition. But most things, is it fair to say that most things aren’t like Huntington’s?\nCathryn: It’s very fair to say so. So some traits are exactly like Huntington’s, like cystic fibrosis or variants in the BRCA1 gene for breast cancer, where those genetic changes on their own put someone at a very high risk of developing a genetic disorder, and that’s the sort of genetics that we’re taught in school, and it leads us to think about genetics as a black and white, yes, no, you know, we’re at risk or we’re not at risk.\nAlex: You have the fat gene, exactly, exactly.\nCathryn: Well, and can I say we all have the fat gene, we all carry that gene, but what differs between us is the changes in that gene, so specific changes at the level of the DNA variants that we carry, and that’s what makes a difference between us.\nAlex: Tell me more about that.\nCathryn: So, our genes are made up of a long string of DNA variants, so the base pairs A, C, G, and T, and about 99% of our genome is exactly the same. You know, you, me, and everyone listening to this podcast will have exactly the same base pair at exactly the same place in our DNA. But one percent of our genome may differ between us, and our genome is over three billion base pairs long, so one percent still gives us quite a lot of opportunities to differ, and some of those changes have a very big impact on our risk of different diseases, different conditions. But most of those changes that we see in our DNA have a very modest effect on disease risk, and much of my research is in the genetics of depression, and for the last 10 years, we’ve been carrying out studies to identify the genetic attributions to depression. And we do this in a classic epidemiological design, that’s a case-control study. We collect, we perform recent studies with groups of people who’ve been diagnosed with depression, and we compare their DNA to people either from the general population or people that we know have not been diagnosed with depression to date. And we step along the genome looking at multiple different places on the genome, and we look at the frequency of that change, and we say, okay, this site in the genome, people either have a C allele or an A allele, there. What’s the frequency of that A allele in people with depression, and with people without depression? And then we do a very simple statistical test and say, does that differ? And we do that at several million places through the genome, and by carrying out really careful statistical tests, we can identify specific changes that seem to increase people’s risk of developing depression,\nAlex: so specific, very specific genetic differences that are more correlated with someone having depression.\nCathryn: Exactly, but the actual risk conferred by each of those individual changes is very, very low. We’re not in the single gene Mendelian disorder territory that we’ve been with Huntington’s disease. We’re now in a polygenic genetic model, which is what seems to be underpinning depression, schizophrenia, bipolar, or all and also most physical disorders that really have an impact on people’s lives. We’re not looking for single genetic changes; we’re looking for genetic changes spread across the genome, each of which have a very modest effect on people’s risk of developing a disorder. But when we put them together across the genome, that gives us much more information on risks.\nAlex: So really, the role of statistics in all this is to churn through huge amounts of data and figure out what are the patterns, what are the meaningful patterns that we can extract from this data?\nCathryn: Exactly, and this really is big data analysis. And it’s big data in two ways. Our genome is very big, and we need to look, you know, across the whole of that genome. And also, because the risks conferred by each change are very modest, we need really large studies here to be able to identify those changes. So in depression genetics, 10 years ago, we carried out a very large international study through the Psychiatric Genomics Consortium that pulled together internationally 10,000 people with depression and 10,000 people without depression. And we were really excited by this. It was the largest study that had ever been performed in depression genetics. And we found absolutely nothing.\nAlex: Wow.\nCathryn: And this was a massive study; there were millions of research funding that had gone into this. And of course, it was, you know, 10,000 people, you know, their time and their energy, their participation in the study, and that was hugely disappointing at the time. And we stepped back and we thought about this and we looked at what had happened in other studies like in schizophrenia and bipolar and in studies in heart disease and rheumatoid arthritis that have been more successful. And we started to think, well, what is it that’s different about depression? And we realized a couple of things. That first of all, you know, depression is much more difficult to diagnose and other many of those other disorders where genetic studies were being successful 10 years ago. And so there was probably a lot of heterogeneity in those studies that we weren’t taking account for. And also, the genetics of depression… genetics is a much more modest contribution to risk in depression than it does in schizophrenia. So that when we look at twin studies, the heritability of depression is less than 40 percent compared to up to 80 in schizophrenia. So that means that in addition to genetics contributing, there’s much more scope for other risk factors to contribute. And we think of this, I think of this in the context of the biopsychosocial model. So we can investigate the bio part, but we need to acknowledge, particularly in depression, that there’s much, much more going on from the psychosocial part of this.\nAlex: This is such an interesting conversation because obviously you’re coming from the big data side of things, and I’m a clinician. And it’s so interesting because from what you’re telling me, it makes sense in light of my clinical experience. That, from my anecdotal clinical experience, it makes sense to me when you’re telling me that there’s a huge or a bigger psychosocial influence in depression with schizophrenia because with depression, for example, you can see very straightforwardly and intuitively how lifestyle factors, whether someone has a job, whether they have a good relationship, whether they have friends, whether they have whether they have meaningful work can influence whether or not they become depressed. And one so, let’s, let’s continue on the polygenic front. So, are we close to figuring out, for example, how many genetic changes might be involved in a condition like schizophrenia, or is that still mysterious, the total number of genetic changes involved?\nCathryn: We are making substantial progress. So, I talked about the depression study where we’d found absolutely nothing.\nAlex: You know, why did you guys find nothing, do you think?\nCathryn: Because um, because um, statistically, studies are always a mix of signal and noise, and you need to get enough signal to stand out against the noise. And that study, even though it had 9,000 people who generously contributed to our research, it just wasn’t enough. And so, having stepped back and thought about this, it had… we had the confidence to say we just need to continue, this is about sample size and uh, and so we went out, we increased the sample size substantially, we collaborated with a lot more people, and our latest genetic study, which is still unpublished, we’ve been talking about it in conferences, we now have identified 500 genetic variants across the genome which we believe play a role in depression.\nAlex: How many people did it take?\nCathryn: That’s a really good question. It’s taken half a million people with depression to find that. And some people, thinking of this in the biopsychosocial model, some people say, “Well, why do you bother? If you really need that many people to find these very modest genetic changes, you know, well, shouldn’t we be focusing our time and our energy and our money on doing different things to solve depression?” And I think my pushback on that is that we need to be doing everything that we can in our depression and all mental health disorders. You know, it’s a major problem for society, for those people who’ve been diagnosed, and the families that care for them. And we need to be doing everything we can, taking every angle that might help us diagnose, treat, maybe even prevent these disorders. And genetics allows us access to that underlying biology, and that’s really difficult to get a handle on in mental health disorders. Exactly, and we have no biomarkers; you know, we can’t do a blood test and identify people who are at high risk or use that for a diagnosis. And so, genetics, I’m hoping, will give us a handle on that underlying biology. It’ll tell us more about why people get depressed, what the pathways are for that, what are the changes in the brain, in the body, that can lead towards depression. And then, of course, that gives us perspectives on how we could help prevent that. You know, does it give us new new drug targets that we might be able to develop\nAlex: or other interventions like brain stimulation is becoming more involved?\nCathryn: Yeah, exactly, yeah.\nAlex: It’s really interesting. So, I talked to this week, I talked to Nolan Williams, who is a psychiatrist at Stanford, and he is an expert in brain stimulation. And he actually said from his work, which is doing very intense brain stimulation treatments, magnetic stimulation, he’s found out, by doing that and studying people in MRI scans, that there’s a subset of people with depression where the depression seems to be correlated with, if not caused by, one area of the brain activating before another area of the brain - the anterior cingulate cortex activating before the dorsolateral prefrontal cortex (I know that’s a mouthful, I’m sorry). And through stimulation, you can reverse that temporality, where one area gets activated before the other and regulates it, and that seems to have this amazing effect on people’s clinical presentation; they get a lot less depressed.\nSo absolutely, the biology is foundational. Do we know, do we have a sense of how many, or how many changes have we found genetic variances with conditions like schizophrenia, bipolar, and is there an overlap between those two?\nCathryn: For schizophrenia and bipolar disorder, we’ve found over 200 variants for each of those. But for all of these, we know that that’s the tip of the iceberg. You know, there is a lot more to find. There is overlap between them, there’s overlap between all psychiatric disorders, and the model, the way that we think about this currently is that there are probably some genetic changes, some genes that are involved that are relevant to the brain in general and across all different diagnoses. And then there are likely to be other variants that are unique to maybe subgroups of disorders or single disorders and conditions themselves. And so, but working across diagnoses and working transdiagnostically as well, genetics again should help us to do that and show where the similarities and where the differences lie.\nAlex: Because really, what this genetic picture (I’m not sure if you’re aware of this) is very challenging to our diagnostic framework as clinicians because we put diagnoses in categories or buckets. We say, this person has a psychotic condition such as schizophrenia, or this one has a mood disorder such as bipolar when it seems to be somewhere in between those two, we call it schizoaffective disorder or depression, or what have you. And this picture where it’s very continuous, where it’s loads of different genes and somewhere can be at any point on a spectrum, and then those genes, which I’d like to talk to you about later, have to interact with environmental stimuli. That paints a picture where people really aren’t in one category or another, but everything is kind of a spectrum. Would you agree with that notion?\nCathryn: Yes, and probably a multi-dimensional spectrum, and none of this is to be easy, and how might genetics help with that? Well, I mean, you’ve talked about people not fitting in with a particular diagnostic group, and I know people’s diagnoses change across life, and their symptoms change. And I would hope that genetics might be able to give us a perspective on that because our genetics stay constant all through life. And that’s one of the real values of genetic tests, is that the variants that we’re talking about, we inherit those from our parents, and they remain stable all through life. And so if we could get some information on how genetics might help predict prognosis, how might people’s mental health play out across their lifetime, what does their genetics indicate they might be at highest risk of developing, I would hope that that sort of information might become useful diagnostically. It’s never going to be the whole answer; nothing is going to be the whole answer in psychiatry; it’s all too complex. But we need to be taking, we need to be working together. I, as a geneticist, need to be working with you as a psychiatrist. We need to be working with people who do brain stimulation and do neuroimaging and working with psychologists and sociologists and looking at this in its entirety to see how we can make progress.\nAlex: I mean, as a clinician, I think having more genetic information about a given patient would be incredibly helpful. So if, because we get wrapped up in diagnostic mysteries all the time, and it can be very puzzling, and of course, people change subtly from day to day if they’ve had enough sleep, what medication they’re on, but if we can get some hard data, for example, tests which are sophisticated enough to say, “This person has 150 of the 200 changes that are associated with bipolar affective disorder, and they have 50 of the ones associated with schizophrenia,” for example, I can really see that that would provide a huge value to clinicians and to patients themselves. Because a lot of the trouble of being unwell is the huge amount of the mental suffering of being unwell is the mystery. And obviously, like you’re saying, we don’t want to take a too hard line approach where genetics is everything, but anything that can help us shine the light on these complexities can be very therapeutic I think.\nCathryn: That’s a really good point, and we’re starting to do that through polygenic scores. Now, these individual variants that we have, the 500 variants for depression, over 200 for schizophrenia and bipolar, individually, they give us tiny amounts of information. We can’t use them on their own. But when we put them together across the genome, we can construct polygenic scores, which does exactly what you’ve just said. It looks at how many of those risk variants does someone carry. Now, you know, we all carry some of those risk variants, and most people will carry an average number, so their genetics of depression won’t tell them very much; they’ll have an average loading for depression. But some people might carry high polygenic scores for depression because, by chance, they’ve inherited more of those risk variants from their parents than other people. And so that’s something that we’re starting to look at: how these scores allow us to look at the spread of risk in the population. So, so far, the amount of risk that they explain is quite low. For example, in schizophrenia, where perhaps we have the strongest genetic profiles to date, if you look at the people who have the highest polygenic scores, so the 10% of the population who are at the highest risk and the 10% who are at the lowest risk, the difference in their risk of schizophrenia is about 16-fold, to use a relative risk.\nAlex: So the 16 times more likely.\nCathryn: yes, exactly. So the 10% of people with the highest risk are 16 times more likely to develop schizophrenia than the 10% with the lowest\nAlex: which is fairly substantial because the risk of schizophrenia is about one percent at baseline, so that has a 16 percent,\nCathryn: but that does leave out the middle 80 percent of that distribution and another way of looking at that is to say, well, what might we want to do about this? Another perspective on this is to say, well, if we could identify people who were at the highest one percent of risk, so those one in a hundred people who have got the very highest genetic loading for schizophrenia, and we compared their risk to everyone else, what would that be? How big would that be? Is it really worth knowing, identifying that the very, very highest? And there, there’s a six-fold increase in risk, so that’s much more modest. It’s really saying that even though we know quite a lot about the genetics of schizophrenia, it’s not in itself giving us enough signal yet that it would be useful.\nAlex: But did you say, so the one percent, the top one percent, have a six-fold increase compared to everyone else? Which proportion had the 16-fold?\nCathryn: Uh, so the 16-fold risk was looking at the top and bottom ten percent. Okay, okay. So they’re looking at the real extremes, and this is, you know, it’s statistically valid, but it’s a statistic that says, “Right, if we look at the real extremes, what does that tell us?” And that’s really useful information for research studies, we might use that a lot to say to compare, you know, people who’ve enrolled in research studies who have the highest and lowest risk. But that’s not so useful for you as a clinician. And as a clinician, I think, you know, focusing on the people at the very highest risk and saying, “Does this tell me very much?” and at a population level, I think it doesn’t. But where it might be useful is, you know, where someone’s showing symptoms. So maybe someone’s had their first episode of psychosis or in an ultra-high-risk group, you know, including genetics together with all the other information you have on your patients, that I think where it might be useful.\nAlex: And it could help to advise individuals or their doctors on what lifestyle factors to encourage or avoid. So something we talk about a lot on this podcast is the association between cannabis and psychosis, which is a bit of a tricky one to explain in that the majority of people who use cannabis won’t encounter any issues, which is important when you’re thinking about legal policy and things like that. But a substantial minority or a significant minority can develop a psychotic illness, and this has been studied epidemiologically by Marta de Forte in a huge multi-centered trial where there’s a clear correlation between cannabis use and development of psychotic symptoms. And I think even anecdotally, many, many people know that one person when they were growing up who smoked a lot of cannabis and ended up developing a mental health condition of some kind. So do you think these polygenic risk scores, particularly when they’re more sophisticated, when we know more of the genetic variants, we’ll be able to tell us, “You need to stay away from this, that, or the other?”\nCathryn: I think that would be really useful, and we’re certainly heading in that direction. Um, and I have the pleasure of working with Marta de Forte and hearing about her wonderful research. And I think that’s really useful or potentially useful in a public health message. Um, but the reason that it’s useful there is because we have the genetic signal and we have an intervention. You know, we know that, well, with a really high genetic loading, maybe you really shouldn’t smoke cannabis. So knowing the genetics is not enough; we need to know what we do about that, and I think the genetics of schizophrenia and cannabis is the perfect example of that.\nAlex: Or someone at high risk of depression, say as a teenager, you could teach them certain skills, cognitive-behavioral skills around mental flexibility, because mental flexibility is really important for treating and preventing depression, for instance.\nCathryn: Yeah, so exactly. And also maybe the potential of exercise, which many people would say has a role in both preventing and hastening recovery from depression. Um, and it’s very easy for us sitting here to come up with these potential examples of where it might, where genetics might be useful. But, you know, we’re not, we’re not very good at assessing risks; we’re not very good at dealing with genetic information because as a society, we’ve never really had to do that before.\nAlex: It’s counterintuitive.\nCathryn: It is. And I think I worry slightly about if we’re going to start telling teenagers, “Well, you shouldn’t smoke cannabis because you have a high genetic risk of developing schizophrenia or psychosis.” That’s a really difficult message to give a teenager, and I think…\nAlex: it’s not easy marketing.\nCathryn: We’ve got a huge way to go to make sure that our communications of genetics are done in a way that’s helpful to people, that is not scary, that’s going to be advantageous, that’s not going to increase stigma, but it’s going to be helpful to everyone involved.\nAlex: And I suppose in a way that’s accurate because, you know, we’re having a whole long phone conversation, just to unpick the different pieces of this, and we have historical examples, obviously, of genetics and a genetic argument being misrepresented in order to justify an ideological goal or a political goal, things along those lines. So, there’s one issue with our species, I think, is we develop technology a lot faster than we develop an ability to understand it ethically. If we develop atomic weapons and then we have to figure out how we’re going to think about this in an ethical framework, and I think the same could be said with genetics because maybe I could ask you, do you think there’s a potential dark side to genetics and improving our genetic understanding, that it could be used for harmful means in the future?\nCathryn: I think the potential is certainly there, and we need to be really careful about how we talk about genetics and increasing people’s genetic literacy, so that they understand that genetics is not just a yes-no, that people are at high risk of a disorder or not, but it is a continuous measure. Measuring someone’s genetic liability to depression, to schizophrenia, you know, it’s like measuring someone’s height; it’s not a yes-no, it’s a, you know, people are most people are average risk, some are at high risk, and some are at low risk. But we’ve also seen, I think, areas where genetics can be misused. So, we’ve seen the genetics between different population groups around the world being used as a justification for discrimination, being jumped on by the far right to justify their perspectives. Another area that we’re seeing that worries many of us in genetic research is these polygenic risks for disorder being used in places that are not appropriate. And we see, we’ve talked about the schizophrenia risk being useful, but not on its own; it needs to be in context with everything else. But there are some companies that are offering genetic risks as part of embryo screening in in-vitro fertilization, and they do that because we have the technology, and they’re applying that. But I think we have to be really careful in those cases that what we’re that what, the way that we’re applying our genetic information is really underpinned by sound ethical and statistical concerns.\nAlex: Do you think, in the case of those companies, is the issue that they’re falsely representing how useful the technology is, how accurate it is, or is it just that it is actually has proven utility in screening for certain conditions, but they’re just kind of marching ahead without any kind of thought as to the ethics?\nCathryn: I wouldn’t accuse any of those companies of marching ahead without considering the ethics; that would be unethical of me. But from what I have read about the science and the risks conferred, it is very difficult for me to see that making a decision about which embryos to implant, on the basis of, for example, a polygenic score for schizophrenia, is justifiable. And of course, the concerns downstream are, you know, the impact on stigma, the impact on this idea that we could remove mental disorders by a simple genetic test. It’s really worrying, it’s completely inappropriate, and I think we need much broader societal discussions.\nAlex: And it sounds like there’s still too much mystery in this whole process to make those claims and to market these things as sort of… I can easily fantasize about how the marketing has a higher degree of certainty.\nCathryn: Exactly, and I think that there is so much noise. I mean, to put this in context, the risks that we have for schizophrenia account for about seven percent of overall understanding of risks of schizophrenia. So we can estimate what’s going on in that seven percent, that leaves 93 percent that we’re not including in that calculation, in that assessment.\nAlex: Um, how does the environment interact with our genetics to influence a particular presentation?\nCathryn: That’s a really good question, and we’ve this concept of gene-environment interaction has has been a part of research studies in for decades now, and our understanding is that for most examples, that it’s not an interaction but it is an additive, a combined effect and so what I mean there is that the genetics and the environment combine together to cumulatively give a risk of disorder, but they don’t interact on a statistical level. Yes, so what I mean by interacting here is that, well, let’s look at this the other way around, that genes and environment seem to contribute independently, so someone’s genetic risk to a disorder will give them a certain amount of risk, their environmental component will give you another risk, and we can just add those together. If there’s an interaction, that’s saying that the environment, the way that the environment contributes depends on the genetics, and that’s not what we’re seeing in most cases. So, you know, our intuitive understanding, I think, or a useful thought was, well, if people have really high genetic risk and a poor environment, maybe that will make their risks soar, you know, maybe the combination of two bad things will really lead to a much, much higher risk.\nAlex: You think of a compound,\nCathryn: yeah, but that’s not what we’re seeing, we’re seeing that there’s an additive effect, and the evidence for that is still quite modest in mental health disorders, but that’s what we’re seeing in heart disease, say, where we’ve got in a much better handle on traditional risk effects, you know, from obesity, from cholesterol, and we combine that with the genetics, and they do seem to be additive. So, I always, as a statistician, push back when people talk about interactions because what we seem to be seeing is more of an additive joint effect.\nAlex: that maybe doesn’t fit, maybe there’s a that doesn’t fit with the what we understand about some of the biology because my understanding is that certain genes can be switched on or off depending on environmental triggers, but that concept, the idea that genes can be switched on or off, does that fit with your statistical understanding?\nCathryn: I think it does just because these models, you know, are complex, and I don’t think that that model as you’ve described it would conflict with an additive effect. But we have a long way to go, I mean, you know, we’ve talked about the rapid progress we’ve made in genetic studies over the last 10 years, and as our understanding improves, as we know more about the genetics and the biology, we may, you know, we may well find interactive effects, but what I think we can say is that they’re not, you know, they’re not the overwhelming signal we’re finding.\nAlex: because I think maybe the way I was using interaction was different from the way you I think you meant interacting mathematically and I meant interacting as in the environment, the environmental stimuli switching on a gene or off.\nCathryn: Yes, you’re right, yes, the statistical and this biological terms are slightly different and emphasize that fact we need to work together, we need to combine all these different perspectives.\nAlex: So we’ve talked a lot about psychiatric conditions, you know pathological states, but let’s talk about a non-pathological but still psychological state like personality. Do does genetics have an influence on our personality?\nCathryn: Genetics does have an influence on our personality, and what a question that we should really be asking is how big is the genetic influence? Um, and the influence on personality is fairly high, and of course, that has an impact for mental health as well. So neuroticism as a personality trait is highly correlated with depression.\nAlex: And what is neuroticism? I should point out to our listeners. Okay, there’s two things I want to point out. First thing is, there’s the way personality is used technically. I mean, that’s the way we use personality colloquially, and we run into this when we, for example, talk about this or personality disorders. When we’re talking about personality technically, we mean a set of traits that are stable across the lifetime that influence someone’s interaction with themselves, the world, and other people. That’s my elevator pitch definition of personality. Um, I think that’s really important to keep in mind. That was the second thing I was going to point out, which I can’t remember now. Um, neuroticism is used in many different contexts in Psychology and psychoanalytic literature. What do we mean when we talk about neuroticism in this context?\nCathryn: So in this context, neuroticism through a series of questions that, um,\nAlex: so I think of it as a sensitivity to stress. So how much emotional dysregulation do you experience per unit stress? Like you take person A and person B, they both lose their jobs, person A will have a particular emotional response and person B will have a particular emotional response, and it’s quite likely they’ll be different. People are different, they respond emotionally differently to roughly equal units of stress. But neuroticism doesn’t have a great name for PR, so that’s a problem that’s happened, but, um, yeah, how, so how, on average, what kind of influence is genetics having on our personality?\nCathryn: So genetics does have an influence on personality, and we see that from studies all around the world, from twin studies and genetic studies. You know, we know there’s a big contribution, um, in exactly the same way as there’s a contribution towards mental health disorders, and it’s exactly the same sort of model that it is polygenic. You know, that we’re starting to identify individual variants that contribute to personality traits, to well-being, to whatever way we want to measure someone’s personality. But again, each of those individual variants has a very small effect, um, and we’re again starting to identify those polygenic profiles that that contribute to someone’s personality.\nAlex: I think people will have an inherent resistance to the idea that there is some genetic influence on personality because it has that flavor of biological determination, and personality is really important. So if you take a trait by conscientiousness, that’s people conscientiousness is highly correlated with life success, and that subdivides into industriousness and ordinariness. So industriousness is literally how hard you work. So obviously, you could see how that would have a correlation with success. If people were to find out that whether genetically or otherwise they have a particular, because you can do a test and find out what your personality is or what traits you’re high in or low in, is that is that a, is that a prison sentence? Are you bound by that? Or are there things that you can do to change your personality traits?\nCathryn: I want to push back on that phrase prison sentence, absolutely not. None of the genetics that we’re talking about in this context, these polygenic models, none of them are deterministic. You know, they are probabilistic, and they combine with the environment, with your family, you know, with society that you’re in. They can give you a certain inherited predisposition that might push you down a certain road to start with, but that genetics is always going to be mixed with everything else that you’re exposed to.\nAlex: What are some tips or maybe principles you’ve discovered for building a successful career in science? Which is a very difficult thing to achieve. What are some things you’ve learned along that along that road?\nCathryn: So building a successful career, so many different things. I think flexibility and resilience is one of it. Science itself is serendipitous, you know what, whether the studies you’re involved in, you know, get funding, have a successful result, are fashionable, and are popular in the scientific environment, all of those depend quite a lot on luck. It depends a lot on who we work with, you know, having supportive mentors and supervisors who can assess your strengths and see, you know, how you can develop best and what direction you could perhaps be moving in that would help you thrive. Um, it is really important. But also that ability to bounce back, you know, to be working in that personal environment, that professional environment where when your paper is rejected by a journal for the third time or your grant application doesn’t get funded again, you know, that being able to think “right, what’s Plan B here? where do I go next? and how can people help me achieve what I want to achieve?”\nAlex: So your attitude towards failure is very important.\nCathryn: Yes, I mean, you know, I sit here as an academic, as a professor, but working in a university is only one of the places where excellent science takes place. A lot of my PhD students and a lot of my postdocs, you know, don’t stay in academia but move into industry, into the pharmaceutical industry, into biotech, into policy work, into government and third sector organizations. There are so many different fascinating careers from a scientific basis where our trainees can thrive, and I encourage that breadth of discovery across all the options available.\nAlex: And the other thing I’ve observed about science from the outside, I’ve never done research myself. Is that it requires this very delicate balance of curiosity and open-mindedness on the one hand, and then being skeptical and detail-oriented on the other hand. And those two things don’t often go together, but from what I’ve observed, the very best scientists seem to be able to merge both, or at least the teams and labs. The best teams and labs have people who are good at both. Has that been your experience, or has it been something different?\nCathryn: I like that perspective of looking on science. Um, and you’ve talked about one person having both of those. For me, much of this has been about teamwork, you know. So you get a wonderful discovery, you know, you press the button on the computer and you get your highly significant p-value, and you jump up and down an enthusiasm with this in the team meeting. And then, you know, the good scientists that work with you will be saying, well, what about this, and did you account for this, and have you controlled for this? And it will be that balance of that excitement, that moderation, that checking, that doing the sensitivity analyses.\nAlex: Where do you see yourself? Are you the enthusiast or are you the moderating influence or are you both?\nCathryn: I’m a statistician. I am a skeptic, and I’m a skeptic about every single number that people present to me, and they have to justify me that and show me that, you know, whatever they do to their data, whatever perspective they have on that study, that effect does not disappear, then I will believe it.\nAlex: Okay, we should work together then because I’m definitely an enthusiast.\nCathryn: It would be a great partnership.\nAlex: Um, you were a listener of this podcast, and I am constantly trying to make it better, and I’m constantly asking, you know, what is it that you get from listening to it and how could it be better?\nCathryn: Um, I’m a huge fan of the podcast. I’ve learned so much, um, both within my field and well outside my field that has really helped me, you know, develop as a scientist and working in mental health, and really getting an appreciation. Um, I mean, as we’ve talked about, I’m a statistician. I often come at things from a numeric perspective rather than a people perspective, and listening to podcasts like this has really made me appreciate the real diversity of work that goes on and the value of people with lived experience, and how, you know, getting people involved in research, whatever their experience is, is essential for people like me to do good research. So, I would say just carry on that diversity.\nAlex: I need your moderating more [laughter]\nCathryn: More genetics, more biology, but it’s that diversity that, for me, it’s the real power of your work,\nAlex: Professor Lewis. Thank you so much.\nCathryn: My pleasure. Thank you for having me, Alex."
  },
  {
    "objectID": "software_correlation.html",
    "href": "software_correlation.html",
    "title": "SNP Heritability and Genetic Correlation",
    "section": "",
    "text": "LAVA\nTitle: LAVA Tutorial\nPresenter(s): Josefin Werme\nLevel: Intermediate\nLength: 16:08\n\n\n\n\n\n\n\n\n\n\n\nLDSC\n\n\n\n\n\n\n\n\n\n\n\nGCTA-GREML\nTitle: Estimating SNP-based heritability with GCTA-GREML\nPresenter(s): Jian Yang\nLevel: Intermediate\nLength: 18:28"
  },
  {
    "objectID": "chapter1.2_transcript.html",
    "href": "chapter1.2_transcript.html",
    "title": "Chapter 1.2: Epidemiology (Video Transcript)",
    "section": "",
    "text": "Till Andlauer:\nMost of the work of the psychiatric genomics consortium is about genome-wide association studies, or GWAS in short. But, what is a GWAS?\nSo first you need to consider that complex disorders, like most psychiatric disorders, are polygenic. So we don’t have single causal mutations that confer risk as is the case for monogenic disorders, but many many genetic variants with small individual contributions to disorder risk.\nGWAS typically analyze single nucleotide variants or polymorphisms, SNVs or SNPs in short, such a SNP is a position in the genome where the genotype can vary. In this example, the majority of people in a given population carry a “G”, and the minority an “A”, so “A” is the minor allele. The frequency of the minor allele can also differ between patients suffering from a disease and healthy controls. So that’s what GWAS is all about. So you take a group of patients and a group of healthy controls, and you determine the genotype of hundreds of thousands of SNPs using microarrays.\nThen you compare the frequency of alleles for each of these variants between cases and controls using logistic regression. You could also conduct the GWAS for a quantitative trait, for example body mass index or brain volume, and analyze associations using linear regression.\nGWAS results are presented as a Manhattan plot. Here you see one on Depression from the PGC. All the analyzed SNPs are shown on the x-axis, ordered by chromosome. And on the y-axis you see the minus log10 association p-value. Thus, the smaller the p-value, the higher the tower in the Manhattan plot. In this GWAS, you can see 44 such towers reaching above the red line. This red line is the genome-wide significance threshold, a p-value of 5 x 10-8, which corresponds to Bonferroni correction for multiple testing of 1 million variants.\nAnd why do you get these towers? Because of linkage disequilibrium. Nearby SNPs are correlated. They get inherited together more often than expected by chance. Thus, clusters of correlated variants show similar associations leading to the towers in the Manhattan plots.\nNow, published GWAS results typically don’t come from a single analysis; instead separate GWAS are conducted in dozens of cohorts, and the results of each of them is combined using meta-analysis. And that’s what the PGC does. In this manner, over the last years, hundreds of genetic loci associated with psychiatric disorders have been identified. And the list is constantly increasing. However, a lot of the work only begins after the GWAS has been conducted, and that is trying to annotate the function of the identified SNPs. There are many books and articles that provide you more information about GWAS and this book on psychiatric genetics is a good example."
  },
  {
    "objectID": "chapter5.3_transcript.html",
    "href": "chapter5.3_transcript.html",
    "title": "Chapter 5.3: Association Testing (Video Transcript)",
    "section": "",
    "text": "The Biometrical Model & Basic Statistics\nTitle: Biometrical Model and Basic Statistics\nPresenter(s): Benjamin Neale (bneale@partners.org)\nHello, I’m Ben Neale. I’m one of the course directors for the International Statistical Genetics Workshop and I’m here today to talk to you about the biometrical model and basic\nstatistics. Some really core theoretical underpinnings of how we think about twins and families,\nheritability estimation. All of this stretches back to an intellectual tradition that goes,\nyou know, to Mendel. And Galton and Fisher all people that we’ll talk about, [working]\nover the course of, say, from the mid 1800s. You know, forward to today. Now a number of\nthe scientists that I’m going to refer to are also eugenicists. I’m not going to talk about eugenics here. I know that can be very triggering for many. If you would like to\nlearn more about eugenics or understand the relationship between polygenic inheritance and polygenicity and eugenics theory, I’d refer you to Lea Davis talk in 2021 workshop\nabout precisely that. But nevertheless, let’s move on and and focus on the science in this\nparticular session. So a natural starting point when thinking about Human Genetics or\nMendel\nreally any genetics is to start with Mendel and start with this idea of inheritance of\nphysical characteristics. And that was what Mendel was really interested in. Sort of working out and figuring out and understanding was how is it that, you know, parents and offspring\nshare some kinds of traits and. In Mendel’s case he was doing breeding experiments using\npea plants, and here we’ve got the little picture of this sort of toy example of a yellow\nsmooth skinned pea crossed with a green wrinkly skinned pea. And if you cross peas of those\nparticular phenotypic characteristics, all you end up with in the first generation are smooth peas that are yellow. That kind of denoted here by a kind of imagined genotype\nof AABB. For “AA” “BB” for yellow and smooth and then little a “b” for green and wrinkly.\nNow this particular phenotype is operating in a recessive sort of fashion. If you think\nabout green or wrinkly, that is to say you have to be homozygous or identical at the\ngenotype. You have to have two copies of the genetic variant 1 from your mom and one from your dad to express the green wrinkly phenotype for the green or the wrinkly. Going to type\nthere on independent chromosomes and this is the law of independent asortment that Mendel\nwas also getting at. You can juxtapose that against the yellow smooth pea, which will behave in a dominant gene action form where that is to say the big A or big B means that\nif you have just one copy of that particular genetic variant that is governing the phenotype,\nthat you’re looking at the trait of yellow or the trait of smooth skin pea. You end up,\nyou know, with crossing those getting an F1 generation that has all the same phenotypes\nthat are all yellow peas and they’re all smooth skin. And then if you cross within that F1 generation, so you take the F1 generation and you mate across the different plants in\nthat fashion you end up with the 3/4 of the time being a yellow pea and 1/4 of the time\nbeing a green pea and 3/4 of the time being a smooth skin pea and 1/4 of the time. Being\na wrinkly skin pea. Now this wrinkly, smooth and yellow green is operating either in a\ndominant fashion for the yellow or recessive fashion for the green. And it’s a really single\ngenetic variant that’s governing this trait in this particular toy example of a pea plant.\nAnd Mendel experimentally showed this, you know, like actually, you know, bred a bunch of peas and did the you know, analysis to get to a place where we arrived at Mendel’s\nlaws. Of independent assortment and segregation, but also that like half your genetic material\nis coming from your mom and half your genetic material is coming from your dad. Now Mendel\ndid this when there wasn’t really much of an appreciation of statistics, but we’re going to spend a lot of time talking about as well in this course and throughout the rest of\nthis lecture. And so as a result his sums came out exactly at the proportion, so they\nstopped the experiment. When they got to the place where they thought the answer was, because there wasn’t really a notion of randomness. In the context of this particular experiment,\nbut nevertheless, those kinds of laws, Mendel’s laws that I’m sure you learned about in your introduction to biology course. Those Mendelian laws are still governing how genetic variation\nis transmitted from parent to offspring in humans in basically every other species in\nthe population, there are lots of different mechanisms of reproduction in biology, and\nso you know you can get into all of that that complexity, but we’re going to focus our attention on the way things work in humans during this course because I think we’re interested in\nunderstanding a little bit more about trait variation in the population or populations\nthat we’re studying now. When thinking about Mendelian genetics. You can also think about\nMendelian Genetics\nthis sort of interesting intermediate case. This case where you have white flowers, and you have red flowers, and you cross them. And then in the first generation after that\nyou see nothing but pink flowers, so you see no flowers like either the white flower side\nor the red flower side. You only see pink flowers and then if you take that pink flower generation and cross it with another white flower then what you end up with is a 50/50\nmix of pink flowers and white flowers. Whereas if you cross within just the pink flowers\nyou end up with a quarter white flowers, half pink flowers and a quarter red flowers. Now\nthis is a sort of co-dominance, the sort of intermediate circumstance for Mendelian genetics,\nright? So there’s this idea that the phenotype is not one of two forms, but actually maybe.\nSlightly more on a continuum of white to red and like you get the halfway point in that\nsort of space when you cross with, you know red flowers and and white flowers, and so\nit could be that the two genetic variants are, in a sense equally balancing one another rather than necessarily purely expressing or not expressing their particular phenotype.\nNow that’s a you know, complicated case, but let’s take something that’s also sort of intuitive\nHeight\nand important. And let’s talk about height and if we think about height in the population,\nthis guy over here, Francis Galton is, you know, been he did a great many different things\nscientifically. But perhaps one of his most important contributions was this observation\nthat parents that are tall, or parents that are short tend to have kids that are tall\nor have kids that are short and actually not only do they tend to have, you know, tall\nparents tend to have tall kids, and short parents tend to have short kids, but the kids don’t seem to be quite as short. As the parents are when you look at the mid parental height,\nwhich is this line from A to B on this picture, that’s the distribution of heights for parental\npairs. Taking the midpoint of those, and then if you look at the kids of, you know, those\nparents with sort of the shortest stature. They don’t have kids that are short that they\nhave this C to D line, so they actually tend to regress a little bit towards the the mean\nand. So that’s actually where we get our term regression from. So when we do a linear regression,\nit’s actually from this kind of drawing a line concept that that Galton was doing when\nhe was writing about height and stature and parents and offspring. But there’s a natural\nquestion that arises when you think about something like height. If you think about your own height, do you think there’s like a single genetic variant that tells you how\ntall you are? I don’t, that doesn’t seem to make so much sense for something that’s like. Genius, why would you have a single variant that like ends you up at say 64 like I like\nmy height. No, it’s actually a little bit more complicated than that, right? Just intuitively,\nthis idea like there’s this kind of continuous variation in height in the population. There’s\nthe kind of distribution of height sort of. Almost looks like a normal distribution, and\nso how do we square this inheritance of discrete physical characteristics that Mendel observed\nwith this idea of continuous variation now Galton was also heavily influenced by Charles\nDarwin. You know they were relatives. There was a lot of sort of. You know, kind of intellectual\ncuriosity around trait variation and kind of genetics and biology. More generally, right,\nlike this was all happening again in that kind of mid 1800s, so a huge amount of of\nchange in the scientific literature, and a lot of data being collected about things like you know people’s heights and their kids and their families, and these ideas of how do\nwe take something like height? How do we take a continuous trait like height and you, you know, integrate that with what we understand about discrete inheritance of characteristics\nfrom Mendel because that was really Mendel’s main point is that there was something - some discrete thing being transmitted from parent to offspring. So in 1915, there’s a really\nbeautiful paper by east looking at Corolla length in nicotiana longiflora, or the tobacco\nplant, and the experiment that East did. Was he concentrated the longest Corolla length\nand the shortest Corolla length plants, and then made those the founding parental generations\nand then crossed those and ended up with a distribution. This F1 distribution there was sort of in the midpoint. You know, sort of between those two distributions of Corolla\nlength from the kind of long Corolla length and the short Corolla length tobacco plants,\nand he had this F1 distribution. And then he mated within that F1 generation to get the F2 generation. Same sort of thing is with the peas or the flowers and what you can see\nquite, you know distinctly is that the distribution spreads out quite a bit from F1 to F2, and\nthat observation. That, like the increase in the variability of the distribution from\nF1 to F2, points to the idea that there is maybe more than one genetic factor being inherited\nhere, that there’s maybe something that’s adding a little bit of, you know, jumbling up and and actually you know what’s actually happening that we can know and appreciate\nnow and sort of obviously indicate is that amongst the tall parents you’ve homozygozed, or you’ve taken the two alleles that are the long form of whatever it is. Particular allele\nit is. And in the shorter length plants you’ve homozygozed, the short form. And so when you\ndo the F1 generation you get a lot of heterozygous sites, so that is to say one instances where\nyou have one of the long form and one of the short form of the particular genetic variant\nthat is having an impact on corolla length in the tobacco plant. And as a result of them\nall being homozygozed, they’re actually very similar genetically in F1. And then when you\ngo to F2 you then have the binomial resampling, so some of the heterozygous sites are now turning into the homozygous long form, or the homozygous short form in that F two generation\nnow not content with simply of looking at the increase in the distribution as you get to the F2 generation East then went and sampled from different points across that distribution\nand then. In a sense created an F3 generation, so just take a bunch of plants with a similar\nsort of height and then see what happens there and there. You can see that the mean more\nor less tracks with the mean of where the sampling was happening in that subsequent\nF3 generation, and so that’s the idea that there is actually a genetic contribution to the phenotypic variance in Corolla length in the tobacco plant, and that you could see\nit through these kinds of breeding experiments. This is very important work and it kind of\narticulated this idea that polygenic inheritance was, you know, perhaps the most natural explanation,\nbut there wasn’t really a clear mathematical formulation that made everything sort of airtight\nand coherent, and for that to hit the scene was a paper that Ronald Fisher wrote in 1916\nFisher\nand actually submitted to the Royal Society in London in 1916. It was rejected, and then\nit kind of got passed around in the academic publishing press of the day into this paper\nin 1918 where it was. Described in the Royal Society of Edinburgh instead. And as a result\nof the sort of observations that Lee is talking about, and many others were making throughout\nthe scientific literature at the time, Fisher wrote this treatise on the correlation between\nrelatives on the supposition of Mendelian inheritance, and this paper is an extremely rich, dense text. It has a huge number of ideas that are still relevant today, a century\non, so this is more than a century ago, the theoretical underpinnings of quantitative genetic theory were really articulated and laid out in a very, very clear and precise\nmathematical framework by Fisher. And in you know, just something like 30 pages or so.\nThere’s all kinds of ideas about how to think about genetic variance, the definition of\ngenetic variance, the definition of variance that we use today, the idea of partitioning variance indeed, prior to this paper, partitioning variance, and even like ANOVA like ideas hadn’t\nreally been invented, and that’s Fisher’s clear contribution here. How do we try and disentangle really complicated ideas? What we can maybe partition? Now again, this is\na statistical tool. This is a scientific tool. It is a model description of the world. It\nis not a complete and rich description of the world, and whenever we talk about partitioning variance, it’s important that it is a statement that genetic variation influences a phenotype.\nBut it is by no means the only mechanism by which a particular phenotypic value can arise,\nand indeed the environment may matter. a great deal changes in the environment can have massive changes on. Phenotypes, and that’s not really captured in a kind of idealized toy model\nthat Fisher was talking about in the context of the biometric model. But nevertheless,\nadditive genetic variance, dominance, worries about a sort of mating epistasis. How to think\nabout multiple alleles? How to think about all kinds of different forces that would shape\nthe genetic landscape of a phenotype in a population were really given some deep thorough\nmathematical treatment by Fisher and this paper is so important that it’s still the sort of primary way that we think about the definition of heritability. Indeed, the definition\nof heritability goes back to. Precisely this paper. OK, so how did Fisher get there? How\nCentral Limit Theorem\ndid he reconcile this idea of discrete inheritance from Mendel with continuous variation like\nheight? And what Galton was doing with the looking at the parents and kids and measuring\ntheir height and showing that they were the same? Well, what Fisher did was he invoked something called the central limit theorem and what the central limit theorem states\nis that if you have a bunch of independent factors, that sort of are summing together\nto create some outcome then a normal distribution will emerge and we can see this if we create\na sort of toy example of thinking about coin tosses and what we’re going to do when we\nthink about coin tosses, we’re going to think about just the binomial chance, and in a sense, if you think about your parents, they have genetic variation. They have a lot of places\nwhere they’re heterozygous where they have one form of a genetic variant and another form of a genetic variant, and it’s a random toss of the coin. Which form of that genetic\nvariant. You yourself get and so thinking about that you know coin toss. Well if there’s\njust one coin like there was for like wrinkly or smooth in terms of the peas or yellow skin\nor green skin for the peas then that might you know just be a binomial chance exactly\nlike this. But what happens when we start to add coins? Well when we start to add coins we see different distributions of outcomes in those coin tosses coming. And if you keep\nadding coins. What you see is, and you know a distribution emerge and that distribution\nthat emerges is this normal distribution that was written about by de Moivre in the 18th\ncentury and Gauss in the 19th century, and this normal distribution here. That’s like\nif you have an infinite number or a very large number of outcomes, you end up with that normal distribution, but it’s worth remembering that if you just look at ten coin tosses as your\noutcomes, you’re already getting pretty close to a normal-ish distribution in the population.\nOK, so how do we relate the normal distributions to something like diabetes or schizophrenia?\nWell, we invoke something called the liability threshold model that was really articulated\nin advance of the Fisherian idea of the quantitative genetic theory, the biometrical model that\nI’m talking to about so Pearson working with Alice Lee in 1901. Sort of articulated this\nidea that if you have an underlying distribution of liability. Of you know some sort of risk\nfor phenotype like schizophrenia or diabetes then and you’re above that threshold. Then\nsuddenly you’ll have. You’ll present with that illness or that disease or that binary trait. And if you’re below it then you won’t have that trait and so you can actually think\nabout some discrete binary phenotype as really having an underlying continuous distribution.\nNow. Pearson sort of elevated Alice Lee in the early 20th century. It was not often the\ncase that women coauthored scientific papers because of gatekeeping by many different male\nscientists and institutions, and a lot of institutionalized sexism, but Pearson, I think,\nwas a little bit more of a let the sort of people who actually created the ideas get\nthe credit, and so he advocated for Alice Lee to be recognized, in the scientific papers and I think there’s a lot of contributions that have gone unspoken in the history, and\nso it’s nice to sort of recognize one where there was actually the Coauthorship extended\nto the key intellectual partner for developing these ideas. Now the way that Pearson and\nLee got to this set of ideas was actually thinking about horses and thinking about horse coat color, which you could you know, see as discrete characteristics ranging from like\na Black Horse all the way to a White Horse. But if you lined up all of those horses from\nthe lightest shades to the darkest shades or the dark shades, the lightest shades. Then there might be some underlying distribution of horse coat color that was maybe slightly\nmore normally distributed in the population, and that was the whole idea. That was how they got to this notion that there could be some hidden distribution that we can’t see,\nbut that we just see this kind of binary outcome at the end, and this liability threshold model is still very powerful tool for modeling discrete outcomes, particularly when they are multifactorial\nwhen they have lots of contributing causes. Just as we saw with the central limit theorem\nideas with Fisher. OK, so Fisher didn’t just define you know variance and partitioning\nGenetic Action\nvariance and the analysis of variance in the 1918 paper. He also set forth a model to describe\nhow genetic action might operate in the population and these are slides that Manuel Ferreira made many years ago that I’m still using because I just find them so exquisitely clear. And\nso what we’ve got here are three genotype classes, “aa” “Aa” and “AA” and they are Attached\nto different means in the population, the white circle, the yellow circle, and the red circle have different means in the population conditional on what your genotype is that\nyou have. And that’s because this genetic variant. If you hold like everything else constant, has some impact on the phenotype. It has some mild change that will maybe make\nyou slightly taller or shorter. Thinking about Galton’s example data set. And you can see\nthat the genetic effect here that Fisher wrote down is “little a” and “little a” is like\nreally not the best naming convention, As it’s a really difficult collision of terms, but it is what it is. That’s the way it’s written. So here’s how we’re teaching it.\nAnd so the kind of genotype mean in the population is “minus a” for the “aa” [genotype] “d” for\n“Aa” [genotype] and this is in a context where D or the dominance deviation, which is to\nsay how far away from the midpoint value. Of the two homozygote classes, you are the\n“aa” or “BB”. Where is that “Aa” genotype in relation to that midpoint. It’s well in this instance it’s dominance equals zero, and so the “Aa” [genotype] has a mean of of\n0 and then “AA” has a mean of “plus a” and those are the sort of genotype conditional\nmeans and. So here’s now a picture of normal distributions layered on top of the genotype.\nDistribution, so we see the red “aa” distribution has some mean trait value. The Blue “Aa” distribution\nhas some mean trait value and the green “AA” has some trait value in there, you know separated\nby an “a’s” worth of distance and they are now a source of variation in the trait in\nthe overall population is actually quite a big source of variation. In this particular example. Now, what happens if we have some dominance deviation, right? So maybe everything\nisn’t purely additive. Maybe additivity doesn’t explain the universe perfectly well. Well,\nin that circumstance we’ll see this d. Now move the genotype mean of “Aa”. Note that\nthe midpoint is still term 0 is the midpoint between the two homozygotes. classes and Fisher\ndid that to make the algebra a little tidier, and I think we all appreciate the tidiness of the algebra When we get there. OK, so this is the dominance deviation, so this is allowing\nfor non-additivity in the genetic effect and there is a a lot of supporters of non-additivity\nin the Biological Sciences because there are a lot of sort of recessive acting phenotypes sort of like we saw with the yellow green pea color and the green peas are sort of recessive\nmode of action and so that necessity is an important observation in biology and so this\nnon additivity has got a long strong. Intellectual tradition in in this space. OK, so that’s\nBasic Statistics\nwhat happens under additivity. That’s what happens under dominance. Now let’s talk a little bit about some of the statistics that are used, and these are really just your elementary\nfirst level statistics. Very, very basic statistics ways of describing distributions of traits,\nand so here we’ve got some idealized simulated trait on the X axis on the right, and then this red line of mean it’s got a mean of zero that you know I’ve artificially fixed to be\n0, based on R, and then the frequency is the count. Of the number of individuals with the\ntrait value, and here we have it in normal units on the X axis, and the mean is just\nsimply defined as the sum of the observations that X_i those individual trait values divided\nby the total number of individuals or N. Pretty pretty simple. Pretty basic statistics. Hopefully\nyou all remember how to calculate a mean. Now let’s talk about the variance. The measure of spread in the distribution. Well, the variance is now summing up the deviations from the\nmean and collecting those and aggregating those over the entire distribution. And Fisher\nused the deviation from the mean squared because he found that that was the most consistent\nestimator when thinking about trying to define the measure of the spread. So what that means\nis that it has the least variability of an estimator of the spread of the distribution.\nWhich is why we favor X minus the mean quantity squared for each individual divided by now\n“N - 1”. And the reason that we have an “N - 1” there is because we’ve had to give up a degree of freedom to the mean, and I’m not going to go much further into that. But degrees\nof freedom are a bit fiddly like that, but this is a way to make the estimator unbiased.\nOK, so the covariance of a distribution is now thinking about not just one trait, but\ntwo traits, which is to say that we’ve got trait one on the X axis here and trait two on the Y axis. And these traits have some relationship to each other. There’s some you\nknow you can see this red line. That’s the regression line that I fit on this particular data set and they are a bit correlated or have some covariance. Now all the covariances\nis it’s just the way of again summing up the deviations from the mean. But now instead\nof doing it in one dimension, we’re doing it in two dimensions. So we’ve got the X_i minus Mu of X, which is the individual trait value of X for the i-th individual minus the\nmean of that for the overall data set. And then we do the same thing for the Y trait\nas we did for the X trait, and then we multiply those deviations for each individual together,\nand we divide that by the number of paired entries that we have minus one, and that gives us just the covariance, and so this is just in the scale that X is on or Y is on when\nX and Y are standard normals. When they are, have, you know means of 0 and variance of\n1. This turns into the correlation, but there’s also a way to turn the covariance into correlation\nas well. OK, so how much mean and variance? Well, we can think about the contribution\nMean Variance\nof the QTL to the mean, and, really, that’s just a way of making sure that the mean squares\nwith how we’ve defined our genotype classes and so we take the number of individuals with\na given Trait value times the frequency of individuals with that trait value. And you\nmight remember how there were different means for the different traits that we saw in the picture a little while ago, with the “AA”, “Aa” and “aa”. Really well, those mean points\nwe’re going have to come up with a grand mean for our total data set. Now. This might be something like cholesterol levels in the population, or to be something like height. You know any\ncontinuous phenotype. And really it can have any shape be any distribution and it will\nstill have some mean. Now here we’ve got our “AA” “Aa” “aa” genotypes. The effect of the\nQTL, the quantitative trait locus. That’s the genetic variant that is having an impact\non the trait or phenotype, and that effect here is For the “AA” genotype is just +a For\n“Aa” it’s D and then for “aa” it’s -a. That’s the conditional mean. That’s the mean of the\nphenotype Conditional on carrying that genotype and then there are the frequencies that those\ngenotype classes. Another bit of notation introduced. So we have a tendency to define the frequency of the genotype as “p” and “q” for the other allele. So one allele, so one\nform of the genetic variant gets the frequency of “p”, the other form of the genetic variant gets a frequency of “q” which equals “1-p” and you end up with “p^2” “2pq” and “q^2”\nfor the frequency of the genotype classes, if the. You know, Hardy Weinberg rules follow,\nand the Hardy Weinberg is basically just a way of saying that there isn’t an unexpected amount of correlation amongst your parents in terms of their genotype. So if you have\nrandom mating in the population more or less, then you’ll have. That sort of frequencies\nbe p^2 “AA” 2pq for “Aa” And q^2 for “aa”. Now the mean is just going to be taking those\nconditional genotype means times the frequency of the genotype and summing those all together.\nAnd so that’s what we see down here in the mean of X and that is gives us a grand mean that we’re going to use in the context of our calculation of the variance. OK, so when\nwe calculate the variance again, we’re looking at the squared deviation from the mean. Now\nnote this is in the population and so we don’t have to worry about the “n-1”. It’s funny because it’s thinking about everybody rather than thinking about an estimate and that little\nestimator thing is just a little bit of nuance around statistics. But again, we have this\n(x_i minus mu)^2, so the ith individual’s trait value of X minus the mean squared times\nthe frequency of that genotype class. And so here we just work through some more algebra. So the the variance is taking this effect mean for “AA” multiplying it, you know taking\noff the grand mean that we calculated on the previous slide and multiplying it by the frequency of that genotype in the population. So we have (A - mu)^2 * p^2 + (d - m)^2 * 2pq (-a\n- m)^2 * q^2. And that’s how we define the variance of the QTL. That’s what Fisher partitioned\nas the variance of the QTL, all the way back in 1918. Again, before there was the structure\nof the genome before the nucleic acids were really understood before we had any real notion of what was actually going on with DNA itself. But we understood that there were genetic\nvariants operating in the population that they were. A source of variation and this\nsource of variation could be a a tractable, quantifiable thing. This Vqtl this variance\nof the QTL. And the heritability of trait X at this locus is just simply taking the\nvariance of the QTL divided by the total variance of the phenotype. Now this is all worked out\nfor just one genetic effect, but remember, we could have many, many genetic effects, and so if we sum up all of those genetic effects together, then that might get us to the sum\ntotal of the variance of QTL, which is VA and then divide that by the sum total of the\nphenotypic variance. Just the variance of the trait in the population more generally. And that’s how you get to your heritability. OK, so let’s work through a bit more of the\nalgebra, so we’ve got this variance. Here we get this (a - m)^2 * p^2 + (d - m)^2 * 2pq\n+ (-a - m)^2 * q^2, and we can actually partition the variance of the QTL into a part that is\nadditive and a part that is dominance, and so that’s what been done here through rearranging is taking the VA of the QTL, and that’s the main effect that if you take a genotype and\njust run a regression. The genotype against a phenotype you will end up with this additive\ngenetic variance as your estimator. Fisher made it really convenient and really nice for us in the derivation of the math and kind of thought it through that way and then if\nyou add that second term and you encode a dominance deviation from that additivity,\nthen you can get this VD of the QTL, or the dominance contribution to variance. So that’s\njust the deviation from the purely additive model all predicated on where that. Heterozygous\nclass goes. OK, now that’s one genetic effect, but remember for something like height, things\nGenetic Distribution\nare a bit more complicated, right? We don’t have a single genetic effect necessarily. We actually maybe have many many genetic effects, and so we can kind of develop those ideas\na bit further and say that there might be some distribution to the genetic effects. This isn’t what you know. This is what Fisher implied in his work. He said that let’s assume\nthat there are polygenes we can assume a distribution of those single nucleotide polymorphisms.\nThose specific genetic effects that you’re going to learn about. A little bit later on, in the course we can then use that to generate an estimate of the heritability, and that’s\nexactly what the tool GCTA does. It’s also what we did in the context of the LD score regression. So this has just been an introduction to heritability, additive, genetic variance\ndominance, genetic variance, means variances, and covariances. These are the most fundamental\nbasic building blocks that the rest of the two weeks will be built on, and I hope you’ve\nenjoyed this introduction to how we think about partitioning, phenotypic variance, and a little bit on why we do it so as to try and understand a little bit more about the\nworld around us. Thank you.\nEnglish\nAllFrom International Statistical Genetics WorkshopStatisticsBiologyRelatedListenableWatched\n\n\nHypothesis Testing, Effect Sizes, and Statistical Power\nTitle: Hypothesis Testing, Effect Sizes, and Statistical Power\nPresenter(s): Brad Verhulst: bverhulst@vcu.edu\nHello and welcome to the 2022 Boulder Workshop where we’re going to discuss hypothesis testing, effect sizes and statistical power. My name is Brad Verhulst from Texas A&M University and I’m going to walk you through some of the important components of these concepts.\nSo the first thing that we’re going to start with is statistical power, and just to quickly define it, statistical  power is the probability of correctly rejecting the null hypothesis. Importantly, statistical power depends on 4 components. The first thing is sample size. This is often what we’re trying to calculate. The second is our alpha level, and typically we set alpha at .05 or the P value of alpha equals .05 or less. The third thing is our beta level, our power level, the probability that we’re going to reject the null hypothesis if the null hypothesis is actually false and the final thing that we’re going to do is we’re going to look at effect sizes. And so, in order to do this, we’re going to start right at the beginning and think about how statistical power relates to the basic components of hypothesis testing.\nHypothesis testing\nSo when we’re thinking about hypothesis testing, we really have three steps. The first step is to define what the null hypothesis is. Oftentimes, this is an hypothesis, no difference. So Group A is equal to Group B, or the parameter of interest that we’re looking for, say, our heritability coefficient is equal to 0. Of course, at that point we then define what  would be considered sufficient evidence to reject the null hypothesis. Say P is less than .05, for example. In in a genome wide significance framework, we might want to say P is less than 5 * 10 to the negative 8, and that would be our threshold for rejecting the null hypothesis and the final thing that we do is we go and collect data and then we actually conduct our  analysis and see where the parameter is fall. So when we’re thinking about hypothesis testing then, We can imagine a distribution of our test statistic under the null hypothesis.\nThe next thing that we need to do is define this evidence that we’re going to use to reject the null hypothesis. So in a standard situation where alpha is set to .05, anything that falls in this blue shaded region, we would claim is inconsistent with the null hypothesis, and therefore we will reject it. Now, even if it is part of the null hypothesis, we will observe that approximately 5% of the time; i.e. a P of 5%. The second thing that we really need to think about is the distribution of the test statistic under our alternative hypothesis. Now, most of the time, if we’re going to conduct a study, we’re not thinking, oh, nothing’s going to happen, we’re thinking, oh, something’s going to happen. And what we believe is going to happen is that our test statistic is going to fall in this distribution: In the alternative hypothesis distribution. And so the probability that we’re able to reject the null hypothesis given that were in the alternative hypothesis drawing our our statistic from the alternative hypothesis distribution is going to be our power and this red shaded area is the beta component here. 1 minus our power Which we can quite easily say is sometimes even if our statistic is drawn from the alternative hypothesis. Sometimes that statistic still won’t breach our level of evidence that we’re required to reject that it’s part of the null hypothesis distribution.  \nThe second component here that we really need to talk about is effect sizes. So effect size is a measure of the strength of a phenomenon in the population. And most of the time when we’re thinking about effect size as we’re thinking about, effect size is independent of the sample size or other components of statistical power. In a lot of cases, this helps us communicate the result in everyday language, especially if the scale is meaningful on a practical level. For example, we might want to say that people who take zyban or bupropion smoked 5 less cigarettes per day, or people who take Liponox will lose 28.16 pounds in eight weeks. That would be an effect size measure. Effect sizes are agnostic to whether the effect is real or not, so it could be a real effect, or it could be a false effect and the effect size doesn’t really care whether  it’s one or the other. Because they’re not associated with P values as they don’t incorporate any components of sample size into them.\nWhen we’re thinking about effect sizes and we’re thinking about the distribution of the null and the alternative hypothesis, the difference between the mean of the null and the mean of the alternative hypothesis distribution is what we’re really thinking about when we’re thinking about the effect sizes. So if we’ve got a regression coefficient of .2, this difference between these of 0 and .2 under null and alternative hypotheses, respectively, would be our effect size. So what are the conventional sizes of that that we think of when we’re talking about effect sizes? Well, most of the stuff comes from Cohen’s classic 1988 book where he provided some standards for interpreting effect sizes, and it’s really important to be cautious when we’re interpreting effect sizes because in a rather counterintuitive way, large effect sizes are not necessarily more theoretically interesting, and instead tend to be rather obvious. So when Cohen wrote his book, he noted that, well, a small effect, something with an R-squared around 1% is likely to be something that we need to do some sort of statistical analysis to detect. We really do need to do some sort of modeling of. Of our data in order to extract the association, it’s not going to be observable just from  from walking around. A medium effect size or an R-squared of about .1 is going to be apparent upon careful inspection. Might not be completely obvious, but it’s going to be apparent if you were to really look carefully at the world around you. And then we’ve got large effect sizes, or an R-squared of about .25. And this is really going to be obvious at a superficial glance. Things like men tend to be taller than women or or something completely obvious like that on average. That doesn’t mean that all men are taller than women, but that on average we wouldn’t really need to do a statistical test in order to get this.\nOK, so now that we’ve defined several of the components of statistical power, let’s talk about what that really means. So statistical power is typically used to do two types of power analyses. The first type of power analysis is called an a priori or a prospective power analysis. We typically do an a priori power analysis in order To figure out how many responses are nestled necessary to fairly test our null hypothesis. This is typically done for grant applications or things of that sort where where we have to justify a sample size that we’re planning on collecting. A second type of power analysis is called a post hoc or a retrospective power analysis, and in this type of power analysis, what we’re going to do is we’re going to explore whether the effects that we’ve observed can be reasonably expected to reject the null if it’s actually false. if we were to, say, add more people, how much power did we have in our test, or is our test based on about 20% power, 50% power, et cetera et cetera. And knowing how much power you had to test? Your your null hypothesis is really an essential element in understanding the the likelihood that you’re going to replicate your results.  \nSo if we think about the likelihood that you observe a true, you reject the null or you fail to reject the null. We can think of a situation where we know what the truth is So we can say that the null hypothesis is true or the null hypothesis is false. Of course, in reality we never know what the truth is, but we can set this up as kind of a straw man and then if the null hypothesis is true, but we reject the null hypothesis. We’re committing a Type 1 error or a false positive. If, by contrast, the null hypothesis is actually false and we fail to reject the null hypothesis, then we’re committing a type 2 error or a beta error. In this case, it’s a false negative. Of course, if the null hypothesis is false and we reject it, or if the null hypothesis is true and we fail to reject the null hypothesis, then we’re in a good state of affairs.\nSo what is a Type 1 error? A type 1 error is a false positive. The rejection region that we’re focusing on here is this red shaded region in this figure, and if our test statistic falls in this region, we will reject it even if the effect isn’t true. Basically, in this case we just sort of got lucky. So, given that our alpha is fixed probably by our discipline or or at least exogenously from the experiment, this is the basic significance level that we’re trying to test. By contrast, a beta error is. The probability of failing to reject the null hypothesis when it’s actually false. In this case, our statistic is drawn from the distribution on the right hand side here, but it just happened to be really far out in the lower tail of our our effect size distribution and what that means is that it doesn’t  exceed the necessary alpha threshold to reject the null hypothesis in this case, this is a false negative. So when we think about the standard conceptualization of statistical power, we’re really thinking about those four elements. We’ve got our effect sizes, our sample sizes, and the alpha and beta levels. So if we take this simple equation and we rearrange it, we can show that we’ve got our alpha level, our beta level, and the difference between alpha and beta is equal to the square root of our sample size times our effect size. And this works pretty well for a lot of cases when we’re looking at differences of means or we’re looking at sort of correlations or something like that.\nOnce we get into things like twin models where we’re looking at differences between distributions of correlations. Things get a little bit more complicated, so instead of thinking about the standard power calculations, twin modeling tends to use 2 Possible methods for calculating statistical power. The first method is a simple Monte Carlo simulation method where you simulate a model under the alternative hypothesis numerous times, say 1000 times, and then you count how many times you observe a test statistic for your parameter of interest that exceeds the critical value that you’re looking for. So .05 for example, and the proportion of times you get this significant result is your statistical power. It’s pretty simple to do. The downside of it is it can be very time consuming. Also. with models that are that are complex, this can take a lot of time and you can end up with a lot of model failures that may or may not affect your statistical power.  \nAn alternative method is is to use what we’d call non-centrality parameters. Because we’re working with parametric tests, we know or we assume that the distribution of the test statistic follows, say, a standard  chi squared or a standard normal distribution, and we can leverage this assumption to more directly calculate statistical power without doing just an absolute ton of replication. So we can do it once. And then calculate power from there rather than doing it 1000 times and looking at the proportions. And because of this time difference, the number of times you have to do it. This can be done relatively quickly. So we’re going to focus on non-centrality parameters.\nSo the non-centrality parameter is the sum of the mean of the test statistic distribution under the alternative hypothesis with a given set of degrees of freedom. That’s a little complicated, but basically what we’re talking about is the difference between the distributions that we’ve been discussing already: that effect size distribution. So there’s two points that are especially important for calculating statistical   power using noncentrality parameters. The first is as the effect size gets larger, the mean of the test statistic distribution gets larger and therefore the NCP gets larger. And as the NCP gets larger, we have more statistical power. The second component is as sample sizes increase the standard deviation of the null, and the alternative distributions get tighter. And therefore the NCP, the non-centrality parameter gets larger as well in both cases, as effect sizes get larger or as a sample sizes get larger, we are increasing our statistical power.  \nOK. So when we’re calculating power with non-centrality parameters in twin models, we’ve got four basic steps that we’re going to follow. The first thing that we’re going to do is we’re going to simulate twin data that corresponds  with the alternative hypothesis. Say we want to test the power to detect an additive genetic variance component of .4. How much power would we need to do that? And so that would be something that we want to test. Of course, the level of your effect sizes, for example how big your  additive genetic variance component is, should be based on the literature as far as possible. Of course, if you’re doing something really novel, you might not know how heritable it is, and so you’ll have to take a guess.  \nThe second step is to fit the full and the reduced models to the simulated data to obtain a kind of squared value from the likelihood ratio test. So if we’re going to test that the heritability or the additive genetic component, what we want to do is we want to simulate the data in step one and in Step 2 we’d run the ACE model and in Step 2 we’d also compare that against a reduced model the CE model. And we would be able to tell based on that, how significant that a parameter was.\nOnce we’ve got this chi squared test from the likelihood ratio chi squared value from the likelihood ratio test for the full and the reduced we can calculate the average contribution of each observation to the chi squared. So in order to do that, we take the difference that we have observed from the likelihood ratio test and simply divide it by the total number of observations. In this case, if we had MZ twins, we divide it by the total number of MZ plus the total number of DZ twins: twin pairs. And this will give us what I call the weighted non-centrality parameter. And then we can go ahead and in step four we can actually calculate that we can use this weighted non-centrality parameter to calculate the non-centrality parameter for a range of statistical sample sizes. And then we can just basically multiply it by any given sample size to get what our chi square value would be for that particular value of N.\nSo if we had a chi squared value, for example of 10 with 1000 observations, the weighted non-centrality parameter would be 10, or our chi squared value divided by 1000, which is our number of observations to give us .01. Therefore, on average, each observation contributes about .01 to the NCP. Because the NCP scales linear with sample size, if we had 2000 observations, we would simply multiply this .01 this weighted non-centrality parameter by 2000 and we would get a chi square value of 20 if we had 500 observations, we would multiply this weighted non centrality parameter of .01 by 500 and we get a chi square value of five and it’s really that easy.\nSo all of the stuff that I’ve told you today comes from this paper that I wrote in 2017, for the Boulder Workshop and I’m going to walk you through quickly a power analysis based on the script that we’ve put together. All of the functions can be found in this powerFun.R script, and as a quick note, you’re going to need to have the powerFun.R script in your current working directory or the powerScript.R will not be able to find the functions - because all the functions are to show how this script works and show how you can  calculate power using the non-centrality parameter in twins and everything’s pretty well boxed up, but of course.\nThe devil’s in the details, so we’ll go through some of those details now. So the first thing that we’re going to do is we’re going to want to require the necessary R packages, and there’s two in particular that we’re going to use OpenMx and MASS. So OpenMx is going to help us to specify and fit our twin models. And MASS is going to help us to simulate the data. Remember, that’s the key step at the beginning. And then what we’re going to do is we’re just going to source all of these functions and running these three lines of code will allow us to start playing with some of the possible Power analyses that we’re going to want to look at.\nSo I’ve set out a series of power analyses that we might find interesting as twin modelers. So the first question that we might want to know is what is the power to detect A, or the additive genetic component, in the univariate model as C, the shared environmental component, increases. So what I’ve done here is I’ve specified 3 models, one where the additive genetic path coefficient is .6. For each of the models and then the common environmental path coefficient goes from .3 to .5 to .7. And note here that we’re assuming that the sample size for the MZ and DZ twins is equal. We’ve set it arbitrarily to 1000, but this is actually something that it’s much more important to get the proportion of twins right. So in this case we’re having equal proportions. Then to actually specify a specific number. Sometimes when you are fitting rather esoteric models, you might want to bump this up to say, 10,000 or 100,000 twins in each group in order to get more precise estimates of the average. Non-centrality parameter or the weighted non-centrality parameter.\nSo if we run these three lines of code. We’ve basically set ourselves up to get all of the information that we’re going to need, and we can look inside this object. Same modA1. And what we’ll see here is 3 bits of information. First, we can see on the left hand column of the of the top table the estimates that we that we specify. So we wanted a .6 A .3 and then R just computes what the rest of the path coefficients would be. So .6 and .3 would leave an E component of about .74 and you can see how closely these are being. Estimated to what we’re asking for, and we can see what the standard errors are and this kind of tells us what the results from our twin model would have been under this situation. We want to make sure that these estimates here match what we’re seeing or what we’re asking for in our function. If they deviate too far, then our simulation didn’t work, and we probably have to do again, perhaps with the larger sample size. The two key pieces of information here that we want to know are going to relate to the A and the C parameters. So our weighted non-centrality parameter of A here is going to be this value here, .0121 et cetera and the value of C is going to be .00105 or so. And we can see that this value here for A and this value here for C correspond, or at least proportional, to the non-centrality parameters, giving us some suggestion that what we’re seeing is is going to be useful so. Now that we’ve actually calculated the weighted non-centrality parameters for A we can then just plot them.\nAnd so the powerPlot function that is cooked into this powerFun functions will allow us to plot all of the various non-centrality parameter power analysis that we’re interested in doing, and if we just run those four lines of code, it’s going to give us a legend as well. And what we can see here is that on the X axis  we’ve got the sample size that we’re looking for, and on the Y axis we’ve got the power to reject the null hypothesis or the power to detect a significant parameter. Now if we look at the black line here, that was our first situation where a was equal to .6 and C was equal to .3. So we can see here that as we increase the power, we finally get that magic value of power equals .8. When we get about 625 individuals. By contrast, if we increase C from .3 to .5 which is this red line here We can see that the power to detect the A component increases much faster and if we increase it again to .7 it is increasing even faster. Basically what we can see here is that the power to detect a depends on the assumptions that we have about the power to detect C, which is very interesting and it has a big effect on this. If we look in our R console window what we can see is our 80% power. To detect an A of .6 when C is .3, we would need about 645  twin pairs. Split evenly between MZs and DZs. If we had a C of .5 and an A of .6, what we’d have is about 400 twin pairs necessary. So 200 MZs and 200 DZs. Now if we have C of .7 and an A of .6, which is explaining pretty much all of the variation with either the additive or the common environment additive genetic or common environment, what we see here is we only need about 51 MZ and 51 DZ twin pairs, which is an astronomically low number of twin pairs.\nOK, so, The next question is what’s the reverse? How does the power to detect C vary as we increase A?  So instead of increasing C from .3 to .5 to .7, we increased A and we kept pretty much all of the other parameters the same, and so all we need to do here is run this and if you wanted to test different values here, you could just replace any of these values and we can run that. And then we can similarly plot this using the same functions. Basically and what we can see here is that the power to detect C depends much less on our value of A than the power to detect A depended on our value of C. So what we can say here is. We really have to have some expectation of both A and C included in our power analysis.\nOK. So up to this point, we’ve looked at the power to detect A and C when the sample sizes were equal. In this next section, what we’re going to do is we’re going to look at two very different power, the two different sample sizes, and how they affect power. So we’re going to say we’ve got a 5:1 ratio, so 5000:1000 ratio of MZ to DZ twins. And then we’ve got an equal ratio, and then we’ve got a 1:5 ratio of MZ to DZ twins. And just for simplicity We’ll keep it at .6 for the A and the C component, but this doesn’t really. This demonstration doesn’t really depend upon the values of A and C. What we’re looking for is the proportions, that, of the various twin types that we have. And so again, all we need to do is quickly run this. These three lines of code and then we can plot. Plot the power. And if we were to quickly plot that again, what we would see is the plot would shape up much nicer.\nOK. So what we can see in the power analysis where we vary the proportion of MZ to DZ twins from 5:1 to equal to 1:5 is that the best power comes or the smallest sample sizes come when we have about equal numbers of MZ and DZ twins a lot of times people think hey, you know, in order to get more power I should have more MZ twins, but that’s actually not the the way that we would see this shaking out to detect additive genetic variance. Of course, to detect common environmental variance It’s actually much more beneficial to have more DZ twins than MZ twins and if we have MZ twins or dramatically underpowered to detect the common environmental components of variance, which is an interesting and important finding that we need to keep in mind when we’re doing our twin analysis.\nOK, the next thing that we have to keep in mind is whether we’re using continuous or categorical and that’s for the current analysis. Look at binary data so case-control data and what we can do here is we can think of how the prevalence of our case-control trait affects the power to detect a significant genetic component. Now, Let’s say that the genetic component is again a path of path coefficient of .6 and the common environment is a path coefficient of .6. So we’ve got equal A and C here, and we’ve got equal proportions of MZ and DZ twins. So all we need to do to start out with is specify all of the various different. Coefficients that we might be interested in running. And then we can plot it again. And as we can see here, as the prevalence goes from about .5 down to .05, the power drops dramatically. So for our common traits we can get away with many fewer twin pairs, say about 1100. For for a prevalence of .5, whereas for a prevalence of .05 we’re looking at over 4000 and we need to keep this in mind when we’re doing any types of twin models.  \nOK. The final set of power analysis that I want to talk about is the power to detect genetic correlations between our variables. So what we’ve done here is we’ve specified an increasing set of A and C. or A for the first trait and A for the second trait. And what we’re looking at here is. Increasing the genetic correlation between these two traits from .1 to .3 to .5 so very small, moderate and and actually quite large genetic  correlations. And we’re keeping the C correlations proportional to to the A correlations and then we’re going to keep the sample sizes approximately equal again. All right. So if we run these nine different scenarios, what we’ve got here Is all of the results that we need and now we can easily plot those. And we can take a look at what comes out. So for both, because both the variable one and variable two were simulated to have the same A, C and E components we can see here that the power to detect those  variance components is equal for both traits. What we can see is that we need about just less than 600. twin pairs to detect an A of .3. About 200 just over 200 an A .4 and just about 100 to detect an A of .5. Importantly, here, the power to detect our genetic correlations varies pretty dramatically with the amount of heritability that we have in our traits. So for modestly heritable traits, we can only really reliably detect. A genetic correlation of about .5. So with an A of .3. We’re looking at about 1200 people, 1200 twin pairs in order to get our correlation power. up to the .8 level. By contrast, if our A is about .4, we need about 500 people to get that significant genetic correlation power to 80%, whereas we need about 1400 to get it for the for the modest genetic correlation of about .3 and then by the time that our a parameter gets to about .5, we have even more power. And of course, when we have a very small genetic correlation, we never really achieve sufficient power with reasonable sample sizes.\nOK, so just to recap. We’ve gone over the elements of statistical power, hypothesis testing and effect sizes, and we’ve gone over a variety of different methods of calculating them, and I’ve shown you a demonstration of how you can use some of the functions that we’ve put together over the years to calculate power for a classical twin design. Thank you very much. My name is Brad Verhulst and you can ask me all of the questions that you need in the workshop practicals. Where we’ll go through a little bit of this information as it relates to estimating twin models. Thank you very much."
  },
  {
    "objectID": "software_cnvs.html",
    "href": "software_cnvs.html",
    "title": "CNVs",
    "section": "",
    "text": "CNV Analysis\nTitle: How to Run Copy Number Variation (CNV) analysis\nPresenter(s): Daniel Howrigan\nLevel: Intermediate\nLength: 20:03"
  },
  {
    "objectID": "chapter10.6_transcript.html",
    "href": "chapter10.6_transcript.html",
    "title": "Chapter 10.6: GDPR for Dummies: A Survival Guide for Genetics Research (Video Transcript)",
    "section": "",
    "text": "Title: GDPR for dummies: A survival guide for genetics research\nPresenter(s): Heidi Beate Bentzen"
  },
  {
    "objectID": "chapter1.html#sec-section1",
    "href": "chapter1.html#sec-section1",
    "title": "Chapter 1: Introduction",
    "section": "1.1 What are psychiatric disorders?",
    "text": "1.1 What are psychiatric disorders?\nTitle: Psychiatric Disorders\nDescription: A very brief overview of Psychiatric disorder genetics.\nPresenter(s): Broad Institute\nLevel: Beginner\nLength: 2:42\n\n\n\n\n\n\n\n\n\nLink to video transcript here."
  },
  {
    "objectID": "chapter1.html#sec-section2",
    "href": "chapter1.html#sec-section2",
    "title": "Chapter 1: Introduction",
    "section": "1.2 Epidemiology",
    "text": "1.2 Epidemiology\nTitle: What is a genome-wide association study?\nDescription: A brief overview of genome-wide association studies.\nPresenter(s): Till Andlauer\nLevel: Beginner\nLength: 3:15\n\n\n\n\n\n\n\n\n\nLink to video transcript here."
  },
  {
    "objectID": "chapter1.html#sec-section3",
    "href": "chapter1.html#sec-section3",
    "title": "Chapter 1: Introduction",
    "section": "1.3 History",
    "text": "1.3 History\nTitle: History of Psychiatric Genetics (Part 1)\nDescription:\nPresenter(s): Ken Kendler\nLength: 21:44\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\nTitle: History of Psychiatric Genetics (Part 2)\nDescription:\nPresenter(s): Ken Kendler\nLength: 43:41\n\n\n\n\n\n\n\n\n\nLink to video transcript here."
  },
  {
    "objectID": "chapter1.html#sec-section4",
    "href": "chapter1.html#sec-section4",
    "title": "Chapter 1: Introduction",
    "section": "1.4 Psychiatric genomics: State-of-the-science",
    "text": "1.4 Psychiatric genomics: State-of-the-science\nBelow are state-of-the-science videos/papers for psychiatric disorders, which were recorded for the PGC by early career researchers, with accompanying papers in Psychological Medicine (Vol 51, issue 13). Alzheimer and Suicidal behaviour videos recorded separately.\n\nThe Psychiatric Genomics Consortium\nTitle: PGC: Future Perspectives\nPresenter(s): Patrick Sullivan, University of North Carolina at Chapel Hill (USA), Karolinska Institutet (Sweden)\nLength: 33:33\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\n\n\nADHD\nTitle: Insights into Attention-Deficit/Hyperactivity Disorder from Genetic Studies\nPresenter(s): Joanna Martin, Christie Burton, Isabell Brikell, Nina Roth Mota\nLength: 14:24\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\n\n\nAlzheimer’s Disease\nSee Section 4.7\n\n\n\nAnxiety Disorders\nTitle: Genetic contributions to anxiety disorders: Where we are and where we are heading\nPresenter(s): Helga Ask, Rosa Cheesman, Daniel Levey, Kristin Purves, Heike Weber\nLength: 11:32\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\n\n\nAutism Spectrum Disorder\nTitle: Genetic contributions to Autism Spectrum Disorder\nPresenter(s): Alexandra Havdahl\nLength: 16:42\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\n\n\nBipolar disorder\nTitle: Genetics of Bipolar Disorder\nPresenter(s): Brandon Coombes, Kevin O’Connell\nLength: 16:35\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\n\n\nDementia\nTitle: Genetic Risk for Dementia\nPresenter(s): Malia Rumbaugh\nLength: 58:02\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\n\n\nEating Disorders\nTitle: Genetics of Eating Disorders\nPresenter(s): Hunna Watson, Alish Palmos, Avina Hunjan, Jessica Baker, Zeynep Yilmaz, Helena Davies\nLength: 21:30\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\n\n\nMajor depression\nTitle: Major Depressive Disorder: Introduction and General Epidemiology\nPresenter(s): Kim Kendall, Evelien Van Assche, Till Andlauer, Karmel Choi, Jurgen Luykx, Eva Schulte, Yi Lu\nLength: 32:13\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\n\n\nObsessive Compulsive Disorder\nTitle: Genetics of Obsessive-Compulsive Disorder: What we know in 2020\nPresenter(s): Christie Burton\nLength: 15:28\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\n\n\nPost-traumatic Stress Disorder\nTitle: Posttraumatic Stress Disorder: From Gene Discovery to Disease Biology\nPresenter(s): Frank Wendt\nLength: 15:33\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\n\n\nSchizophrenia\nTitle: Genetic Architecture of Schizophrenia\nPresenter(s): Kaarina Kowalec, Niran Okewole, Sophie Legge, Marcos Santoro, Sathish Periyasamy\nLength: 29:52\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\n\n\nShared genetic architecture\nTitle: Shared Genetic Architecture across Psychiatric Disorders\nPresenter(s): Andrew Grotzinger\nLength: 14:04\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\n\n\nSubstance Use Disorder\nTitle: The Genetics of Substance Use Disorders: A Review\nPresenter(s): Joseph Deak, Emma Johnson\nLength: 17:38\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\n\n\nSuicidal Thoughts and Behaviours\nTitle: Insight into the Genetics of Suicide\nPresenter(s):\n\nHilary Coon, Professor, University of Utah\nAnna Docherty, Assistant Professor, University of Utah\nDouglas Ruderfer, Assistant Professor, Vanderbilt University\nNiamh Mullins, Assistant Professor, Mount Sinai School of Medicine\n\nLength: 58:54\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\n\n\nTourette Syndrome\nTitle: Genetics of Tourette Syndrome\nPresenter(s): Matt Halvorsen\nLength: 15:23\n\n\n\n\n\n\n\n\n\nLink to video transcript here."
  },
  {
    "objectID": "ch8_gusev_transcript.html",
    "href": "ch8_gusev_transcript.html",
    "title": "Chapter 8: PGC Day TWAS Primer (Video Transcript)",
    "section": "",
    "text": "Author: Sasha Gusev (alexander_gusev@dfci.harvard.edu)\nLength: 16:39\nI am Sasha Gusev this is my first time at cgsi so thanks everybody for having me and giving me the opportunity to give this tutorial as with the other ones please feel free to interrupt or ask questions throughout and I’ll try to sort of break things down in a way that’s accessible.\nI’m going to be talking about genome-wide Association studies and specifically trying to make sense of genomewide association studies as a way to understand human disease and complex traits and so just to sort of start it at the at a very basic level this is the output of a genomewide association study or GWAS.\nThe procedure is very straightforward - you collect a lot of genetic data on individuals with the disease and without the disease or with a quantitative trait and then you test each genetic variant (and that’s what each of these dots is here) for association with the phenotype. The variants that are significantly associated are above a predefined threshold here, and if they replicate we treat those as genetic variants that are causal for the disease and this is sort of a study design that I think initially almost seemed too simple to work but now over time and with very large sample sizes has produced thousands if not hundreds of thousands of associations for nearly every complex trait that it’s been applied to when there were sufficient sample size.\nIn fact the challenge is now that these Association studies are almost producing too many results and what we would rather have than sort of this figure which is a real plot from geological prostate cancer is something more like this which is a systemic or systematic understanding of the disease of which genes are involved in the disease how they interact what contexts they’re relevant in and so forth and so uh whereas initially there is sort of a challenge of just fleshing out this side of the plot getting these associations um I think a key challenge now is in Connect going from this side of the plot over here to an actual understanding of the disease and one of the sort of most basic pieces of getting to that understanding is connecting variants to the genes associated variants to the genes that they likely operate through and then operate on the trait and so we can break it down into this very simple structure we have a variant we want to know its Target Gene and the effect that it has on that disease and so in particular we can break this down even further and first just ask whether we can identify variants that influence the expression of genes in a systematic way and this is something that was observed some time ago is that in fact if you take gene expression and you you basically kind of run a G was but on expression as your outcome gene expression measured in the past through microarrays or now through rna-seq and test variants typically near the gene in CIS with the gene for association with expression across individuals you will find that the expression of many genes is often highly heritable and so there’s an estimate here in 2011 that the CIS Locus for an average Gene contributed to between 37 and 24 of the variance of expression and again once you have a heritable phenotype in a population you can sort of apply the GEOS Paradigm to that phenotype and instead we call that an eqtl analysis and I’m sure you folks have seen work from the gtex Consortium over many years applying ettl studies and identifying thousands of variants associated with the expression of many genes in many tissues and in fact again this is one of those cases where as the sample sizes have grown this study design has actually yielded a very large number of associations that are almost like too difficult too many to fully process and I think the most recent Gtech study showed that if you sort of relax the significance threshold for these associations nearly every Gene has at least one eqtl in some tissue and in fact I think that if you continue as the sample sizes have grown even further we see that genes then start to have secondary eqtls and tertiary eqtls and this sort of curve uh does not is not even hitting diminishing returns so that’s the piece about identifying genetic variants that influence gene expression and then there’s been a lot of work in trying to understand how these eqtls connect to disease and I’ll highlight a couple studies in particular which basically asked in a couple different ways whether an eqtl is more likely to be a GEOS variant or is more likely to be associated with a complex trait and so the results on the left show that eqtls specifically as you get more confident about them being the causal eqtl are more enriched for heritability across many complex traits from gwas and then this figure on the right from gamazon at all showing that if you just sort of try to partition the amount of disease heritability that could be explained by eqtls those estimates are also quite High across a large number of complex traits again ranging from maybe 10 percent up to 35 percent so there’s this sort of incidental evidence that eqtls are enriched for disease heritability and may therefore give us an instrument to understand um the likely causal genes and eventually go back to that sort of big system-wide understanding of the phenotype so that’s the first part of the arrow the other part of this of this network is we want to understand how this genetic mechanism of gene expression actually goes on to influence the trait and for which traits and this is where the approach of a transcriptomide Association study or a tiwas comes in and I’ll just start with a very basic sort of thought experiment of what would we um want to do if we had the ideal data set how would we in sort of with infinite resources try to relate gene expression genetics and disease together and I think one way that we could do this is we could estimate expression in the hundreds of thousands of individuals that we have genetics and case control status in here like this represents case control status and then we could ask what genes are genetically correlated meaning the effect sizes on expression are also shared with the effect sizes on disease we could do this for every single Gene across the genome and that would give us an estimate of the genes that in principle could be linked to this phenotype and the the hurdle here is that we we uh very rarely or pretty much never have data at this scale what we typically have is a relatively small study of genotypes and measure gene expression usually as in the case of the G tax in a sort of healthy relatively healthy population that was convenient to sample and then we also have very large disease studies that also have genotypes but no gene expression measured and so the basic Insight of the transcript and mind Association study or tiwas is kind of thinking about the fact that we have what is shared across these two these two studies is the genetics and we know previously that gene expression is itself a heritable trait and if it’s a heritable trait then in principle it should be a predictable trait and so what we want to do is use the genetics to predict expression into this study over here where we haven’t measured it and then use the predicted expression as a sort of proxy to estimate the relationship between the genetic component or the predicted component of expression um and the uh in the in the phenotype um and again this is all I’m sort of presenting everything in the context of a single Gene but the idea is to use this methodology and scan across every Gene in the genome and identify the set of genes that are significantly uh genetically correlated or uh For Whom the predicted expression is significantly associated with the phenotype uh and so right then we do the test so the first question is can we actually predict gene expression in this way and the fact that we’ve observed significant eqtls or individual variants that affect expression basically tell us that tells us that that we can and in work that we’ve done and others have done we’ve shown using a number of different prediction schemes that I sort of won’t go into but that are various forms of penalized or Bayesian regression that you can in fact predict gene expression with a substantial degree of accuracy and in particular when you use models that incorporate all of the genetic variation around the gene you typically have substantial gains in the predictive accuracy so even though the single topic ETL explains a large fraction of the CIS effect or of sort of the total heritability near the gene um there is a very large number of genes for which additional variants contribute substantially to the predictive accuracy and so simply going from a single snip Paradigm to a sort of locus wide Paradigm increases our predictive accuracy and that’s going to translate into better Association statistics in the in the eventual guas study um now one additional constraint is that we typically don’t really even have this design where there’s individual level data in both studies what we actually have more frequently is this design where we have individual level data for the gene expression study and then we have summary statistics for the gys and the summary statistics are basically for every snip the marginal the marginal sorry I think this turned off dead battery I think I know what that means um is there a does anyone know if there’s a backup or can you guys just hear me like this I don’t know if this can you hear does that does that work okay let me try to decouple one sorry okay all right I’ll try to work with this and then let me know if you can get another one for that one cool okay so what we typically have is these summary statistics oh sorry do you need anything else it’s perfect okay let’s do it great all right that’s easier thank you okay so summary statistics uh this is what we actually have and they are the marginal Association statistics for every variant and what we want to know from this kind of data is what would the gene the predicted Gene trade Association have been if we could get to the individual level data and measure it and so this is really where the the uh the tus methodology comes in again because this is the type of data we have most of the time um and I’ll just sort of sketch out how this uh this parameter is estimated and the basic idea is that we think about what we would want to do with individual level data and then we kind of move terms around and try to identify pieces that can be estimated from the summary level data and so we start with predicted expression over here x are the genotypes that we use for the prediction W are the weights that we’ve trained in the gene expression data that gives us this this term G that’s the predict expression and then what we want to know is the association between y the phenotype and G the predicted expression so specifically we want to know this orange beta T was so we can kind of plug in the terms into a basic ordinarily squares regression and then decompose these terms and you can start to see pieces here that you can actually estimate from summary level data and in particular you’ll see that this covariance between the genotype and the phenotype actually corresponds to this these G was summary statistics that we get the association between each snip and and the phenotype and then this term down here the covariance between the Snips themselves is also something that we in principle can get from reference panels because it doesn’t rely on knowing the phenotype and so these two pieces we can get externally we plug them back in and now this is a summary based estimate of the beta tus that only requires the z-scores the reference LD and then these weights which we have we sort of assume that we have a priority and then I won’t go into the details of how we derive the variance for this Statistics it’s very very similar and the final Association statistic that we get looks like this where again in the numerator you have you can think of this as a weighted sum of the gyc scores that’s weighted by the uh the predictors of expression and then in the denominator we have essentially the variance of that predicted expression that accounts for the correlation across these Snips so Snips that are correlated are going to add to the variance and Snips that are independent or not so this is basically the the score and I think this is also kind of a useful framework to think about how you can go from Individual level data to estimates of quantities we’re interested in with summary level data uh when we apply this technique to summary based data and individual based data it works really well correlation is nearly perfect and again we didn’t really make any assumptions going through it uh going through that previous derivation except for the fact that the LD is well matched to the Target population and also there’s sort of a hidden assumption that the effect sizes can’t be so enormous that we need to account for changes in the environmental variants and those assumptions are very easily satisfied in most studies so now thinking about when does this approach actually uh lead to associations we ran some simulations where we considered three different study designs under the model where there is a causal Gene and we’ve observed the predictors of that causal Gene so you could imagine in that scenario just running your standard G was to try to identify the association um you could imagine testing only the top snipped the top eqtl that’s associated with expression or you can imagine running a full tus test and when we do that we see that in this scenario because we’re testing fewer features we’re only testing each gene instead of each snip then the power of the tuos or the eqtl only approach is higher than the guas approach so this is one case where not only are we getting a parameter that we’re interested in on its own we also have some increase in power because the multiple testing burden is effectively is effectively lower furthermore if we expand the model and say additionally consider genes with multiple causal variants where now the to us approach of applying a penalized model to the entire Locus is giving us more signals more predictive accuracy than the top eqtl we see that the power of these single snip approaches drops but the power of the tiwas locus-wide approach remains effectively the same and so again this is another scenario where when we have many causal variants for expression that all lead to disease then we can substantially boost power uh and where the truth is is is is sort of in between or maybe a little bit off the page there’s going to be some loci where we don’t have the measured expression uh at all so these uh expression based approaches will just fail there’s going to be some loci where there’s only a single variant for the Gene and will be up here and there’s going to be some loci where there are many causal variants for the Gene and the tiwas will then maximize power relative to other approaches uh so this is all in simulations under very specific presumed models we can also ask how well does this approach perform in real data and this this has actually been quite a challenging question to answer because as it stands we have very few well-established causal genes for disease so I showed you that plot at the beginning that had over 200 known associations for prostate cancer but the number of well-established really definitively established causal genes for prostate cancer for that study is extremely small and that’s sort of the case for most complex traits so we don’t actually have kind of working in a regime where we don’t really have a ground truth there was a study that was done in this pre-print by weeks at all from The Phoenician lab which I thought was an interesting attempt to to try to get at a ground truth and the basic idea was that if we look at data in their case they looked at data from the UK biobank where you had associations both with common sort of standard gwas and also a rare variant coding variant based set of tests and you identify a Locus where there’s both common non-coding associations and rare coding variant associations you can assume maybe it’s not a safe assumption but they assume that the rare coding variant is telling you the right causal Gene and so under this model they basically have a kind of ground truth which is what is the rare coding variant tell you the causal Gene is and then they can ask how various other approaches do based on just the blue stuff just the common variant associations for identifying that causal Gene and so now they have a ground truth um they can plot Precision recall curves and they used this approach to evaluate a bunch of different methods listed here and then also to propose their method which is uh conceptually quite different I won’t go into it but it’s sort of like an ensemble that integrates many many different features at the locus to make the predictions but I think the the what’s relevant here is how these other approaches perform and what you can see is that there’s quite a lot of heterogeneity in their performance uh the tiwas is here in blue and as one point it has the highest uh recall in this model relative to the other approaches aside from their sort of Ensemble based approach and then additionally they integrated each of these method methods together with their model and in that scenario the tiwas had the highest Precision together with their approach but again I think an important takeaway here is that this is far from a solved model with a clear optimal method the TOs provides you an estimate of a certain statistical quantity but this is biology in biology is complicated and so lots of different approaches have different trade-offs for what they’re able to identify and at what levels of precision and recall and then you know one thing I should mention that maybe people are noticing is that if you take a very simple model of just what is the nearest gene or what’s the distance or how far away is the potential causal Gene that actually performs really well and in fact it performs about as well as the method that they that they develop developed and also when combined has very good Precision so again I think that there’s many explanations for this one is that in fact it may be that the nearest Gene oftentimes is the correct Gene it may also be the case that this specific model tends to emphasize genes that are close to the association statistics but again I think it’s also important to keep in mind that probably some hybrid of all of these methods to that also consider proximity is going to eventually be this sort of optimal solution okay uh so that’s kind of where we stand with um with with 2s applications um I you know coming back to this figure you can sort of wonder why the Precision uh of tiwas is relatively low compared to these other methods and I think again there’s an important set of caveats which were sort of highlighted in this paper from Mike Weinberg at all a couple years ago nature genetics which essentially come down to the fact that tiwas is an association study it’s not a causal inference technique and as an association study it’s going to be susceptible to tagging and correlation in the same way that genome-wide Association studies are and so in this paper they proposed a number of sort of alternative models which could still identify a significant T was hit one alternative model is that you can have co-regulate Mission at a Locus where the same genetic variant or set of genetic variants Drive the expression of multiple genes both the causal and non-causal genes and this is a real phenomenon that it’s not that uncommon that you will identify loci with multiple genes with very high cyst genetic correlation and high genetic correlation to the trait another case is you can imagine some part of the genetic effect on a non-causal gene is tagged because of LD between variants with the effect on the causal Gene and this would produce a false positive Association or it would sort of induce some effect on both the causal the non-causal Gene and the causal Gene and then likewise you can imagine a scenario where the effect on the causal Gene was missed in the gene expression study because it was not sufficiently powered or it didn’t get the right context and so this would lead to a false negative association and so again I think this is important to keep in mind that this is a test that is expected to tag the causal mechanism when these assumptions are met but in the real world these assumptions should also be sort of interrogated uh the other I think important limitation and one that’s potentially solvable uh is to consider is the fact that as we’ve seen with other genetic predictors the uh uh these expression predicted expression models do not uh generalize well to other um to different populations and really because most of the data has so far been collected in individuals of European ancestry this is particularly a problem for generalizing to data from non-european populations um or in admix populations with low European ancestry and so this paper showed models that were trained in European individuals that had high accuracy predicted into held out European industry individuals and had significant drops in accuracy when predicting into individuals of African ancestry and again I think that they’re potentially interesting ways to address this problem you know probably the the most basic is just to start collecting more data in other populations we should definitely be doing that but also there’s methodological approaches that could potentially leverage all of the training data that we have available or think about the differences between populations to improve the prediction of these of these models okay I’m gonna drink so I also wanted to talk a little bit about methods I think these are all methods that we did not develop and had no hand in but that I think are interesting approaches to moving beyond just that beta tus that I described for the association between expression and disease and I’ll sort of walk through them briefly you know to to give you guys a flavor of methodologically what else can be done in this space there’s a great method called utmost that came out a couple years ago in nature genetics which uh thought about how um gene expression data that’s measured in multiple tissues in the same individuals could potentially be used to improve these predictive models so everything that I’ve been talking about so far sort of assumed that there was a population with some single modality of gene expression but you could imagine and this is exactly what the how the gtex was designed that you’ve measured multiple tissues for every individual this Y is now a matrix instead of a vector for a given Gene and then the approach that utmost proposes is to actually try to learn the expression for each tissue together with all of the other tissues observed and so again you see some similarities here these bees the what they’re learning are sort of the W’s that I talked about earlier now are being learned for all tissues at once and they do that by using again a form of penalized regression where they have a penalty within each tissue where they want the weights to generally be sparse and then they also have a penalty across the tissues where they don’t want to see a lot of differences between tissues they sort of assume that if a snip is important for one tissue it should also be important for another tissue and this approach particularly for tissues that had relatively small sample sizes substantially increased the prediction accuracy basically by borrowing signal from other tissues that were available and that’s sort of shown here in in purple is the increase in prediction accuracy and held out data um another approach thinking about this sort of multi-tissue framework is instead of learning weights using multiple tissues we may be interested in testing multiple tissues where each set of Weights were learned individually and so there was this work a method called multi-skin by barbaridol in 2019 which essentially showed that if what we’re interested in is this relationship here between now we have many G’s for a single Gene we have the predictive model from one tissue a second tissue a third tissue and we want to know if there’s an association for any of these features in a joint model so a multi-degree of Freedom test for Association what we actually have again because we don’t have the individual level data we’ve actually observed is these marginal tiwas individual tus statistics um but if we know the correlation between these statistics then we can actually approximate the relationship or the effect under the three degree of Freedom or end degree of Freedom a test from these marginal effects the multi-scan paper also did some clever stuff where you have many tissues with highly correlated expression and you don’t want to just throw them all into this model by using principal components analysis to First reduce the dimensionality of the expression down then just test the leading components of expression for Association in this P degree of Freedom test and this also in practice showed that it produces a much larger number of significant Gene trait associations again now we’re sort of saying that if there’s a little bit of signal here and a little bit of signal here and here then that can add up to a lot of signal across the the three degrees of freedom um the other I think interesting method or any other interesting method in this space is uh is now thinking about how to integrate together um many tiwas associations across a given Locus and so you’ve probably seen methods for GEOS fine mapping that try to identify the set of causal variants or variants that contain the causal variant with some predefined probability the same kind of methodology or the same sort of concept can be applied to tiwa statistics and so instead this work of Mancuso at all in 2019 reformulated this problem in terms of having multiple tiwas associations at a single Locus and then fine mapping these down to the likely to the set of likely causal genes and this is actually sort of starting to address some of the caveats that I outlined earlier when you have co-regulation of multiple genes or you have some tagging across multiple genes this is now an approach to put probabilities on which genes out of many are likely to be causal whereas which are likely to just be tags and to sort of estimate posterior probabilities of causality for a given Gene so just in the last couple of minutes I wanted to mention a bit about what else can be done with this framework and so everything so far that I’ve been talking about has involved gene expression or transcription but really the idea is that any molecular trait that is heritable and that can be predicted from data that we’ve measured is amenable to this sort of approach and this way of integrating with gwas and in particular we can go back to this model which maybe we’ve we’ve solved now in some sense and and sort of we can observe that this is this is also an over simplification in that most of the time for non-coding variants what we expect is that there’s some regulatory element that sits in between the variant and the express Gene that maybe excuse me is the modifier or is the mediator of this gene expression so really there’s probably an enhancer or a transcription factor or a combination of those features um through which this snip has an effect on the expression of the gene which then goes on to have an effect on the trait and with sufficient data we can actually start modeling these regulatory elements and the genetic predictors of these regulatory elements and we have some recent work to that end which we call a regulome or a system-wide Association study so we’re sort of padding out the the the the letters of the alphabet here but the idea is that instead of learning predictors of a given Gene you can learn predictors of a some biochemical activity including transcription Factor binding chromatin state or chromatin accessibility and and additionally in this regime we can also leverage some allele specific information of variants that are inside these Peaks that we suspect to be modifying their activity and so we can boost power even further because because we can we can sort of Leverage signal within each individual in addition to Across the individuals and again we’ve shown that this approach is fairly robust that you can identify a very large number of predictive models and when we’ve applied this approach specifically to cancer gwas phenotypes so again this is cancer risk we’re just talking about the predisposition to develop cancer we see going back to this plot that I started with we’ve now characterized each of these loci where we see that there were this Inner Circle here is the number of lots that it had a significant tiwas association with a gene but then actually when we incorporate these epigenetic features we see a much larger number of loci that additionally have associations through chromatin accessibility in this case many of which are do not actually exhibit a direct transcriptomic Association and so this is actually sort of interesting and somewhat mysterious in that we’re able to identify loci where there seems to be a genetic regulatory effect that we don’t see have a downstream Cascade on expression we do capture most of the low side that have the the the the tiwas association those were able to characterize but then we have this number of additional loci and we’ve sort of started to think about what those loci could be telling us one observation is that if you look at uh um if you look at the sort of distribution of evolutionary constraint across the genome you will see that as in regions with higher evolutionary constraint we see fewer tiwas models that can be built probably because selection is making it more difficult to detect or is decreasing the observed effect on expression making it harder to pick up the eqtls but we actually see more of these arwas or chromatin was models observed in those loci so this is maybe this Gap could potentially explain that sliver in the previous figure of these are low side that are very difficult to pick up in the expression framework but are not as difficult to pick up when we look directly at the intermediate chromatin phenotype and a sort of related observation that we’ve made is if you look at genes in terms of their tissue specificity so as we move from here to here from the left to the right these genes are more specifically expressed in prostate tissue we’re looking at a prostate cancer GEOS again we see that the tus models there’s fewer of them they’re harder to fit for more tissue specific expression but for the chromatin based models and the transcription Factor based models there’s more of them and they’re easier to fit and so again this could be pointing towards a phenomenon where more tissue specific expression has lower power for the sort of eqtl and transcriptome-based models but higher power for these epigenetic based models and so with that I’ll conclude um I hope I’ve been able to uh convince you at the very least that gene expression is a complex heritable and predictable trait and this predictability is something that we can leverage to integrate that trait into other data sets or we don’t have it measured and specifically we derive this tiwas statistic which is a measure of the cisgenetic correlation between the gene expression and the disease as I noted at the end this is not just limited to transcription other molecular phenotypes can be used within the same framework and and again I want to sort of emphasize the caveats that go along with any kind of Association is that it’s not a a causal inference and in fact causal inference in this space is I think a really interesting and sort of ongoing open problem how do we disentangle all of those different arrows that I was showing earlier and then also just to remind you that all the prediction here has been within the cislocus of the gene that’s where we have power at the current sample size but there’s a whole world of trans effects which we haven’t uh really we sort of just barely scratched the surface in understanding and so that’s something that I think as studies get larger and as we have more experimental data we’ll also be able to fold into this framework and so with that I’ll take your questions thanks thank you"
  },
  {
    "objectID": "chapter5.html#quality-control-introduction",
    "href": "chapter5.html#quality-control-introduction",
    "title": "Chapter 5: GWAS analysis",
    "section": "Quality control: Introduction",
    "text": "Quality control: Introduction\nTitle: Quality control\nDescription: This video provides a brief introduction to the logic of the steps of QC (how genetic data are generated for subsequent QC, briefly what each step of the QC does, and why these steps are useful and necessary for future analyses of cleaned-up data)\nPresenter(s): Katrina Grasby (katrina.grasby@qimrberghofer.edu.au) and Lucía Colodro Conde (Lucia.ColodroConde@qimrberghofer.edu.au), from the 2021 International Statistical Genetics Workshop hosted by the Institute for Behavioral Genetics at the University of Colorado, Boulder.\nLevel: Beginner friendly\nLength: 16:34\n\n\n\n\n\n\n\n\n\nLink to video transcript here."
  },
  {
    "objectID": "chapter5.html#running-quality-control-on-genotype-data",
    "href": "chapter5.html#running-quality-control-on-genotype-data",
    "title": "Chapter 5: GWAS analysis",
    "section": "Running Quality Control on Genotype Data",
    "text": "Running Quality Control on Genotype Data\nTitle: How to run Quality Control on Genome-Wide Genotyping Data\nDescription: Another nice QC guide which provides overviews of dealing with missingness, relatedness, and population stratification.\nPresenter(s): Jonathan Coleman (jonathan.coleman@kcl.ac.uk)\nLevel: Beginner friendly\nLength: 16:19\n\n\n\n\n\n\n\n\n\nLink to video transcript here."
  },
  {
    "objectID": "chapter5.html#considerations-for-genotyping-qc",
    "href": "chapter5.html#considerations-for-genotyping-qc",
    "title": "Chapter 5: GWAS analysis",
    "section": "Considerations for Genotyping QC",
    "text": "Considerations for Genotyping QC\nTitle: Considerations for genotyping, quality control, and imputation in GWAS\nDescription: This video provides key steps of QC for the GWAS data, similarly to videos above, but it additionally discusses steps of quality controlling imputed data and factors affecting imputation (starting at 25:57)\nAuthor: Ayşe Demirkan (a.demirkan@surrey.ac.uk)\nLevel: Beginner friendly\nLength: 33:21\n\n\n\n\n\n\n\n\n\nLink to video transcript here."
  },
  {
    "objectID": "chapter5.html#imputation-introduction",
    "href": "chapter5.html#imputation-introduction",
    "title": "Chapter 5: GWAS analysis",
    "section": "Imputation Introduction",
    "text": "Imputation Introduction\nTitle: Haplotypes and Imputation\nDescription: Introduction to haplotypes and imputation. This video explains the concepts of phasing and imputation in an easily understandable way.\nPresenter(s): Dr. Gábor Mészáros, University of Natural Resources and Life Sciences, Vienna.\nLevel: Beginner friendly\nLength: 19:25\n\n\n\n\n\n\n\n\n\nLink to video transcript here."
  },
  {
    "objectID": "chapter5.html#imputation-steps",
    "href": "chapter5.html#imputation-steps",
    "title": "Chapter 5: GWAS analysis",
    "section": "Imputation Steps",
    "text": "Imputation Steps\nTitle: Imputation\nDescription: This video talks about the steps needed for imputation, including the imputation reference panels, common softwares for phasing and imputation, imputation servers, content in the output files from imputation, and brief explanation of imputation accuracy parameters (r2 or INFO).\n\nPresenter(s): Dr. Sarah Medland, Queensland Institute of Medical Research.\nLevel: Beginner friendly\nLength: 16:21\n\n\n\n\n\n\n\n\n\nLink to video transcript here."
  },
  {
    "objectID": "chapter5.html#imputation-deep-dive",
    "href": "chapter5.html#imputation-deep-dive",
    "title": "Chapter 5: GWAS analysis",
    "section": "Imputation Deep-Dive",
    "text": "Imputation Deep-Dive\nTitle: An Introduction to Genotype Imputation\nDescription: This video talks about the details and math behind imputation, for example, why choosing imputation when you can sequence, as well as the Hidden Markov Model for imputation. Note: The imputation-related content ends at 32:07. After this, the speaker talks about best programming practice.\nPresenter(s): Dr. Brian Browning, University of Washington.\nLevel: Intermediate/Advanced level.\nLength: 44:37\n\n\n\n\n\n\n\n\n\nLink to video transcript here."
  },
  {
    "objectID": "chapter5.html#the-biometrical-model-basic-statistics",
    "href": "chapter5.html#the-biometrical-model-basic-statistics",
    "title": "Chapter 5: GWAS analysis",
    "section": "The Biometrical Model & Basic Statistics",
    "text": "The Biometrical Model & Basic Statistics\nTitle: Biometrical Model and Basic Statistics\nDecription: This video presents an overview of a biometrical model.\nPresenter(s): Benjamin Neale (bneale@partners.org)\nLevel: Beginner friendly\nLength: 34:09\n\n\n\n\n\n\n\n\n\nLink to video transcript here."
  },
  {
    "objectID": "chapter5.html#hypothesis-testing-effect-sizes-and-statistical-power",
    "href": "chapter5.html#hypothesis-testing-effect-sizes-and-statistical-power",
    "title": "Chapter 5: GWAS analysis",
    "section": "Hypothesis Testing, Effect Sizes, and Statistical Power",
    "text": "Hypothesis Testing, Effect Sizes, and Statistical Power\nTitle: Hypothesis Testing, Effect Sizes, and Statistical Power\nDescription: An introduction to association testing, effect sizes, statistical power, and how to calculate power\nPresenter(s): Brad Verhulst: bverhulst@vcu.edu\nLevel: Intermediate\nLength: 35:52\n\n\n\n\n\n\n\n\n\nLink to video transcript here."
  },
  {
    "objectID": "chapter5.2_transcript.html",
    "href": "chapter5.2_transcript.html",
    "title": "Chapter 5.2: Imputation (Video Transcript)",
    "section": "",
    "text": "Imputation Introduction\nTitle: Haplotypes and Imputation\nPresenter(s): Dr. Gábor Mészáros, University of Natural Resources and Life Sciences, Vienna.\nhi everyone welcome back to the introduction to genomics lecture series this time we will be talking about haplotypes and imputation before we move to the new material so here is the quick summary from the previous lectures\nso we talked about snip markers that are widely used there is a number of ways how to express these genotypes but we are always talking about bialyc snips and these biologic snips are being genotyped with the species specific snip chips we talked about how to determine the positions on the genome and we talked about fiscal maps and also we talked about recombination events that are of major biological importance and they introduce variability to the populations\nHaplotypes\nthis graph is also from the last time so we have an individual here and there is a recombination event and the previous capital a capital b and capital c haplotype is changing to capital a lowercase b and lowercase c haplotype because of this recombination event during this lecture we will look a bit more closely to these haplotypes and also show how to use them or what is the use for them in the context of genotype imputation so when we look at the genotypes what we see in reality is patternal and maternal chromosomes together that are joined during the fertilization to the set of alleles so we what we see are certain genotypes at certain loci for the sake of example let’s say that we have these four individuals and at four loci we have these genotypes what we see here are summaries only now of course we can ask the question what are the actual sets of alleles on each chromosome for the first individual it is easy because its genotype consists entirely of homozygotes so basically a a b b and c c so we know that on both chromosomes are actually the very same haplotypes of capital a lowercase b and the capital c in the second individual we have one heterozygote already so while here is also just actually one option how to divide the haplotypes the actual haplotypes on both chromosomes are different from each other because on one chromosomes there is uh capital a capital b on lowercase c and the other chromosome has the lowercase a capital b and lowercase c so basically one of each of these alleles goes into one chromosome and the other to the other chromosome of course it becomes more interesting the more heterozygous we have on our genome because this actually creates options how the haplotypes could be distributed so we have these three loci here and two of them are heterozygous so the locus b and c so if we look at the pairwise combinations of these alleles then we could arrive to actually two solutions either this one or or this one if you look at the alleles in these haplotype pairs then the genotypes will end up always with this summary genotype but also if you look carefully the haplotype pairs so the first two haplotype pairs are different from the second two haplotype pairs of course the more heterozygotes we have the more complicated it gets so for example for the three heterozygotes we have even more combinations so i put question marks here so if you want you can work this one out yourself so just pause the video here and try to work out what are the actual haplotype possibilities in case that we have three heterozygous loci so what are the haplotype combinations that are possible that end up with these summary genotypes after you’ve done it you can unpause the video and see if you were right or just continue watching and get the answer in three two one go so these are the actual possibilities so you see that there are actually four haplotype pairs that are possible based on these three heterozygous genotypes and each of these haplotype pairs is different from one another so basically how you solve this so the first you take the first from each pair so capital i capital b capital c and then remains lowercase a lowercase b lower kc so this would be that the first one then you go with the pairwise combinations you so you flip one locus at the time until you get to all combinations\nPhasing\nof course we have many more than just three heterozygotes on the genome so there is a question how we solve these questions in practice where we have tens of thousands of loci and also thousands of heterozygous genotypes now the answer is of course with computers fortunately there is sophisticated software that solves these questions for us and delivers the hepatitis we could analyze further this computation process is called phasing so the phasing is a task of the process in the computer to assign alleles to the paternal and maternal chromosomes it looks for haplotypes or these so-called phases in large-scale genotype data and solve these complex problems of assigning correct haplotypes of course this is easier if the so-called trios are genotypes so they basically desire them and the offspring or in case of humans the father mother and their child or even if we have multi-generational trios that enter families including grandparent and grand grandparents are genotyped so if everyone is genotyped this process is somewhat easier in reality however we don’t have this ideal situation many times only parts of the populations are genotyped so it is harder to work out the actual haplotypes fortunately this is also possible and haplotypes could be determined also for samples of unrelated individuals for a population unrelated here is in in a quotation marks because there is usually some kind of relationship between the individuals within a population so as i mentioned before there are specific software solutions for all of this which actually divide the genotypes to smaller segments and try to derive these haplotypes from these smaller segments and merge them back properly\nImputation - general definition\nnow when we determine these haplotypes or these phases in a population these are really useful for a number of purposes and one of these purposes is the so-called genotype imputation i mentioned multiple times that the sleep genotyping is fairly reliable but occasionally we see missing genotype so actually with this genotype imputation process we can make an educated guess how to fill in these missing genotypes so we get the full information so the imputation process is nothing else than filling in missing information there are two major ways how we can use this method the first one is the imputation of sporadically missing snips and the other one is imputation between snip chips for example we can extend a lower density snip chip for example a 50k snip chip to a higher density for both of these approaches i will give examples in the following slides\nImputation of sporadically missing genotypes\nout of the two methods the imputation of sporadically missing snips is more straightforward so as we established some of the snips could be missing due to genotyping error and because of these genotyping errors we might be forced to remove individuals from our analysis or for example if we need complete data in a sense that all sniffs should be known then this is also a problem for us but this situation could be fixed by imputing these sporadically missing snips let’s say that we have an established haplotype in a population that looks like this and when we have an another animal or individual that is genotype and there is a genotyping error but the haplotype looks like this so it’s basically totally the same as before so all the other loci for this supple type are exactly matching but these genotypes are missing based on this comparison if every other snip fits we have a very good idea what should be filled in in the place of the question marks so we have a complete data also for this individual\nImputation between different SNP densities\nthe imputation between snip chips works on a similar logic but it’s somewhat more complex so let’s say we have a snip chips of two densities and this is a smaller example so you see that there are 16 columns here so this will be our larger snip chip and the second snip chip would be a smaller one that consists of eight snips so each line here would be an individual each column would be a locus and these loci are either homozygous one so that is a zero heterozygous that is a one and the homozygous other that is a two now the usual arrangement with these smaller and larger snip chips so that contain more or less snips is that one snip chip or the smaller snip chip is a subset of the bigger one so basically all the snips from the smaller snip chip appear on the bigger ones as well but there are other snips that are on the larger snip chip but unknown for the smaller one so this shows the starting situation here when we genotyped nine individuals with the smaller sniff check now let’s say that these individuals are from a population that are fairly unrelated but we also know that even in unrelated individuals there are short stretches of sequences that are identical by descent these local patterns of ibd or identical by descent could be described and also the length of these segments determined which of course varies based on the recombinations if we identify these segments or these haplotypes we can use them to our advantage so for example these would be the haplotypes that occur in our population and also for the sake of this example these are also color coded so if we return to our original example for the nine individuals that are genotyped with the lower density snip chip so we could see that each of these individuals could be described as a combination of certain haplotypes and because these haplotypes are already known so we actually know what we should put into the place of the question marks and this is then also done and the information is being filled in to these gaps that were previously unknown so what we basically do is we take the information from the higher density snip chips make the haplotypes for the population and we use the information from these haplotypes to fill in the information also for the other individuals that were genotyped with a lower density snake chip in case this lower density snip chip is a subset of the higher density snake chain here i would also underline that these half plot types are well do not come from nothing but actually we need a sufficient number of individuals that are actually genotyped with this higher density snip chip in this population so we can determine the actual haplotypes that occur in this population which can be further used for this genomic imputation as described\nImputation accuracy and practical use\nhere now why is this useful well the lower density snip chips tend to cost less so if genotyping costs is an issue or we want to genotype a really a large number of individuals we can use well just this lower density snip chip and go for the imputation process of course for this we need haplotypes that were determined based on individuals genotype by the high density snipchi this imputation is a so-called in silicon so basically it’s done with computers which also means that it is with no additional costs other than the computation cost for the whole process there are different options and possibilities for software for this process and to my knowledge all or most of them are also free or open access based on this software we can do the imputation that will be done with a certain accuracy so actually the whole process is not hundred percent accurate but actually works surprisingly well the imputation accuracy in general depends on the size of the reference set and the data quality what i mean with this is that we need to determine actually the haplotypes that occur in this population or the population of interest so of course we need to have a representative sample genotype with the higher density snip chips in order to determine the haplotypes that occur in the population so we could use these haplotypes further on in the imputation process in general the imputation works really well for the common snips which occur reasonably frequently within a population this also means that unfortunately the imputation works less well for these so-called rare snips so that occur very infrequently because there is just no way for the imputation process to pick it up from the haplotypes that are available for this population so the general advice is that if someone is interested in a very specific rare alleles then the imputation process is perhaps not the best solution in that case genotyping the individuals with the actual higher density steep chip is advisable but overall the imputation works really well so i put there that the imputation accuracy could be more or is more than 95 so i just put their numbers so you have a bit of an idea that we are talking about very high values actually especially in the simulation studies uh well in my experience is the imputation accuracy is lower than 99 percent and the people start to get unhappy so it’s uh actually in the papers especially in simulations the imputation accuracy is much higher than 95 in real data well it could be variable as i mentioned this really depends on the reference and the data quality also there is a range of possibilities how to evaluate the actual imputation accuracy but it is mostly done in with the so-called masking procedure so it’s a very similar process that i described also in this presentation so there are the genotypes obtained from a higher density snip chip and basically within this process some of these genotypes are deleted and then the imputation software is used to fill these missing markers in but of course we know what is the actual genotype for these higher density snip chips so then basically the values that were filled in by the software and those that were obtained from the actual genotyping are compared and this is the basis how actually the imputation software is also being evaluated how good of a job it does but as i mentioned these software do a surprisingly good job and we\nSummary of the lecture\nalready arrived to the end of this segment and we end as always with a short summary we talked about the so-called haplotypes that are a series of snips and these haplotypes clarify which combination of alleles come from which parent of course if we want to do these computations on the large scale or in real genotypes we need to use computers for it and there is a range of specialized software programs that does the job for us and the approach itself is called phasing and these phases or haplotypes could be used in a various ways but one of the uses is the so-called imputation process which is nothing else than filling in the missing snips to our data here we also have options if we want to fill in sporadically missing snips that were not genotyped for some reason so some kind of genotyping errors or missing snips could be filled in or imputed or we have a different option when we can actually extend smaller snip chip to a larger one based on upload types and information from these larger and denser snip chips perhaps even saving some money in the process because this lower density snip chips tend to cost less and if we are not interested in some very specific rare alias and we are fine with the imputed version of these genotypes we can use these for our research so we end here today let me know if you have any questions or comments down in the comment section below also thank you for your time you spent on this video and i wish you a very nice day\n\n\nImputation Steps\nTitle: Imputation\nPresenter(s): Dr. Sarah Medland, Queensland Institute of Medical Research.\nHello, my name is Sarah Medland and I’ll be talking to you today about imputation. So there are three main reasons why we might impute data. The first of these being meta analysis or combining our data with that of another cohort. Secondly, find mapping. And I’ll give an example of that in just a moment, and Thirdly is to combine data from different chips.\nSo imagine a situation where you have a large cohort which is being genotyped half on chip A and half on chip B. If we were to put the data from these two chips together and analyze them, we would end up with a mixture of power distributions. So we would have some SNPs that are on both chips and they would be the most powerful SNPs in the analysis compared to those that are on one chip or the other. If we were to take this forward for analysis and look at our QQ and Manhattan plots, we would have a very hard time interpreting those results because of that mixture of powers. So if we were to find an Association and go in and look at the region, we could expect that the distribution of p values wouldn’t follow what we would expect based on the LD or the autocorrelation structure within that region. So because we have this differential in power, the SNPs that would be on both chips would have the highest power and potentially higher P values than those that are on one chip or the other. So to get around this, what we could do is bring those two data sets together, take them forward for imputation, and end up with a data set that has a fairly constant N and a single power distribution that’s not dependant on whether or not a SNP was present on the chip or not. We can also use imputation to correct for sporadic missingness, and genotyping errors, and also impute in types of variation that we haven’t directly genotyped such as structural variants.\nHere’s an example of fine mapping. In this situation we have run a GWAS, but we’ve only used genotyped SNPs and we have this variant that we’re finding on chromosome 19. When we go in and have a look, it appears to be floating, so it’s not really supported. We have nothing really in this region to back it up particularly well. So looking at that, it’s very hard to work out if that could be a true finding or not, and one of the things we might do is fine mapping, which is to go in and impute other content in that region and see whether there is additional support that we’re not observing in our genotype SNPs. So when we go ahead and do that in this case, we can see this actually is the true effect. It’s well supported by SNPs in the region, it’s just that these variants were not genotyped on this chip.\nSo when we’re talking about imputation, what are we actually talking about here? We usually start with a genotype data set that has missing or untyped genotypes. We have a reference set of haplotypes, so a public reference set, and those references are compared to our genotypes. We try and identify which haplotype best represents each segment of data, and then we infer in the missing content. So to put this another way, we start with the genotype sample, which has some genotypes but is missing others. We have our set of reference haplotypes. What we’re going to do is compare our genotype samples to our reference haplotypes. Try and work out which haplotypes best represent which segments of data, and then infer in the missing genotypes. This is done in a probabilistic way, and we can assess the accuracy of this imputation as we go.\nSteps to Imputation\nOK, so there’s a couple of steps and things we need to think about when we’re setting up for imputation. So firstly we need to have really well QC’ed data and this would be similar to the QC that you were shown in the QC & GWAS session from yesterday. Secondly, we need to decide which of our references we want to use and we are in the situation where we now have quite a lot of references, so it’s worth thinking carefully about why you are using a particular reference and what you’re trying to do with your analysis. The most common references to use at the moment are the 1000 Genome and the HRC references the 1000 Genome reference is a multi ethnic reference, whereas the HRC is a predominantly European reference. The HapMap and 1000 Genome references can be downloaded and used locally. The other references are mainly only available from custom imputation servers. Although there is a big difference in the size of the references, so for example 1000 Genome reference yields around 20,000 markers, whereas. Sorry 20 million markers [actually this should have been ~28 million markers], Whereas the HRC yields around 40 million and TOPMed yields around 300 million. At the end of the day, if you have a cohort of predominantly European individuals, you’ll likely to end up with between 8 and 10 million usable markers for your analysis. So once we have QC’ed and decided on our references, the next step is to phase our data.\nPhasing\nPhasing in this case means we estimate the haplotypes within our data. So we take our genotype data. We try and reconstruct the haplotypes using reference data and so for example. In this situation here we have three genotypes and there are four potential haplotypes that we can arise that from that data. We don’t do this manually. We use software that’s been specially designed to do it, and the most common software packages at the moment, are Eagle and shapeit. They use hidden Markov model and Markov chain Monte Carlo methods to reconstruct the haplotypes and then these are used to provide scaffolds to infer or impute the data.\nImputation Programs\nFor our imputation as well we use customized programs and the most commonly used ones at the moment are minimach or impute. There are others that are available. An important point is to never use Plink for imputation, although Plink has an imputation option, it’s really not very well designed and I wouldn’t recommend using that. So the two most commonly commonly used imputation programs are minimach and impute. So minimach comes from the work of Gonçalo Abecasis, Christian Fuchsberger, and colleagues. And has a number of downstream analysis options, including SAIGE which will use later in the week, BoltLMM and Plink2. Impute is now up to impute version 5. This comes from Jonathan Marchini and colleagues, and it incorporates the Positional Burrows Wheeler Transform, so it’s a fast and efficient way of undertaking imputation. Once again, it has a number of downstream analysis programs that have been written specifically for the output of this program.\nImputation Cookbook\nSo how would you actually go about doing your imputation? If you are in this situation where you have to do imputation locally, I would seriously recommend using what we call a cookbook and there are a number of these available online. So here’s a link for minimach3 imputation cookbook for 1000 Genomes. If you are in the situation where you can use an imputation server, I strongly recommend that you do that and there are a couple of these available so there’s one at University of Michigan, which is probably the most heavily used one. This one at the Sanger in the UK and a new one, the TOPMed imputation server for those wanting to impute TOPMed data. Here’s a little shot of each of those front pages. In the practical, we’re not going to walk through how you impute data, because there’s a really good set of imputation practical sessions available on the Michigan imputation server site, these are from the American Association of Human Genetics meeting in 2020, and you can walk through each of those if you’re interested in learning how to run imputation on the server.\nData QC\nThe main points are that as I said, we need to QC the data well so we exclude SNPs with excessive Missingness, low minor allele frequency, Hardy–Weinberg issues and Mendelian errors. We should also drop strand, ambiguous or palindromic SNPs. And you need to be careful that your data is on the right build and alignment. So depending on which reference you’ve chosen, if you’ve chosen the TOPMed references, you need to have your data on build 38. If you choose the others the build should be on build 37. So you need to output your data in the format expected for the phasing program, and it’s really important that you check the naming convention for the references and the program that you want to use. So do the SNPs, use Rs numbers, or are they in a position reference? If you are using an imputation server, once you have your data QC’ed and ready to go, it really is as simple as uploading your data, picking the options that you want to use, and then submitting the job. After the imputation you have about a week to get your computer data off the server and then it’s all wipped. OK, so once we’ve done our imputation, if we use the Michigan imputation server, then our data is going to be in a format called a VCF format. So in this format, each line in the file represents a variant and each block of data represents an individual. The file contains our imputed data in three different formats. The first of these before the colon is hard-call or best-guess genotype and this refers to the number of copies of the alternate allele that someone has. The second of these is the dosage format, which ranges between zero and two, and once again this is a count of a number of doses of the alternate allele that someone has. The third format, which is not used very often is what we call a genotype probability format, and so it’s the probability that an individual is has an AA AB or BB genotype for each of the steps in our file. To go along with these, we have a series of info files that contain the information about the imputation accuracy and the frequency of the variance in the sample. So we have our SNP identifiers and you can see some of these get quite long. We have our two alleles. Our frequencies; our r-squares or imputation accuracies; a column telling us whether it’s genotyped or not. And for those SNPs that are genotyped, we have a leave one out imputation accuracy. So this gives us an idea of how accurately these genotype SNPs have been imputed or would have been imputed if they weren’t already genotyped.\nRsquared\nSo one thing to keep in mind is there are subtle differences between the way that the different programs calculate their R-squared metrics. In both cases, effectively the R-squared is the ratio of the observed variance to the expected variance. But there are small differences in how they are calculated. There’s also a difference in that the impute info measure is capped at one. Whereas the mach or minimach r-square measure is allowed to go above one as an empirical estimate. The two programs to get fairly good agreement though and they should line up fairly well. If you were to impute the same set of data both ways. So the R-squared or the info is telling us about the level of certainty we have in the data. So if we had an R-squared of one, it’s indicating there’s no uncertainty. R-squared zero means complete uncertainty. For example, an R-squared on .8 on 1000 individuals will give us the same amount of power as if that SNP had been genotyped on 800 individuals? So you can think of it that way. After we’ve done our imputation, it’s a good idea to do some QC and check that it’s worked. So some things that you can do is to look at the minor allele frequency compared to the reference and see how that looks. You can also look at the R-squared across the chromosome and see how that looks. So here’s some toy examples of a relatively good imputation, but you can see are r-squared sort of varies quite markedly across the genome, but generally follows a fairly well defined distribution. Whereas this is particularly and deliberately a bad imputation, and you can see that we’ve got a very different distribution here.\nGWAS\nAfter doing the basic imputation, it’s a good idea to run a GWAS for a trait that is fairly well powered. Ideally something continuous. Have a look at the Lambda and look at your Manhattan QQ plots. And then run the same trait using GWAS using only the observed Genotypes and plot the imputed versus the observed variant results and see what you get.\nConsortiums\nLastly, quite often when we’re running imputation, we’re actually running it for a consortium or a meta analysis and they will give you instructions about which reference panels to use and what to do. They will probably ask you to analyze all variants, regardless of whether they pass QC or not. It’s important to think about this, especially if you are using those TOPMed references which have something like 300,000,000 variants as only around 8 to 10 million of those will typically be useful for GWAS, and so you’ll be running analysis and uploading results for many, many SNPs that won’t be very useful for anyone. [This creates an unnecessarily large carbon footprint for your work]. So if you’ve got any questions, feel free to ask them at the start of the practical session or to post them to the slack. Thanks very much, bye.\n\n\nImputation Deep-Dive\nTitle: An Introduction to Genotype Imputation\nPresenter(s): Dr. Brian Browning, University of Washington.\nthis this talk is gonna look at genotype imputation which is a standard technique we’ll cover just an overview of it and we’ll look at some of the models used which then in the research talk that I’ll have on Thursday we’ll make use of some of the information in the tutorial and I’ll finish up with maybe a little bit discussion of programming because that’s something that’s very relevant to this audience.\nso imputation is just estimating missing data you can use the other data in the data set you have an external data sets and if you have played any word games you’ve you’ve done imputation classic examples hangman start give three characters the last two characters or ATT about a third of the letters in the alphabet can fit in there and hangman gives a good illustration of a general principle of imputation at the more context you have the better you can fill in or estimate that missing data if I give you some additional characters that for the sentence the dog chase the you can do a much better job filling it in instead of a third of the characters in the alphabet there’s one a see probably springs to mind first it could also be an R but your probability distribution becomes much more pointed genotype\nWhat is genotype imputation?\nimputation is the filling in of genotypes so it originally started where you actually were imputing genotypes now for computational reasons we work on the haplotype and imputing alleles level so your reference data consists of reference haplotypes to phase reference haplotypes per reference sample and the sample your imputing has two haplotypes to but it’s missing a lot of data it typically is genotyped on a snip array and you have just a couple of just get back there you know you might have a marker here or on one haplotype there’s a G and a marker here or the other haplotype there’s a C and on the second haplotype for the sample an A and a C and based on these reference samples yours and a probabilistic model you want to make inferences about what all these dots there goes again what all these dots are so\nApplications to GWAS\nimputation has been around for a long time imputation of sporadic missing genotypes has been around for a long time but imputation came into prominence in 2007 where a group from Oxford with the Wellcome Trust case-control consortium and sort of simultaneously a group in Michigan gonzalo efficaciousness group developed methods for imputed ungenial type 2 markers using reference data and the initial application was defining new trait associated lo sigh and in the initial study it actually didn’t produce much although it will produce some power the ideas that you have a snip ray in which you genotype three hundred thousand five hundred thousand a million markers but there’s a lot of other markers in the genome if you can impute them then you can test additional markers so it should give you a little bit increase in power and it in it will the second application was for fine mapping so you your genotype markers showed that you have an association in a region but there may be other markers that weren’t genotype that give you a stronger signal that can be valuable for replication studies so you impute the additional markers in the region you might find a marker that’s more highly correlated with the trait you’re interested in and then that’s the marker that you’d want to take forward definitely including your replication study because it will should have a greater chance of replicating the Association if it’s real these first two applications are nice I don’t think they’re necessary game changers are they’re useful very useful but the real killer application is meta-analysis so there’s lots of different snip raised out there with different numbers of markers by different vendors snip arrays from different vendors tend to not have a lot of overlap and when you want to do meta-analysis it’s very difficult to do meta-analysis when your datasets are Gina typed on different markers it’s like this Gordian knot well imputation just slices through that knot you take a reference panel you impute all your data individual data sets so that they all have the same markers that are in the reference panel and now they’re on the same set of markers and you proceed so that’s been very when you see these studies in nature Nature Genetics where they have several hundred thousand samples and they have scores and scores of associations it’s imputation that made that work work in a straightforward way because there’s a lot of different data sets and they had to use imputation to get them all in the same marker sets to do the meta-analysis so the meta-analysis I think has been very very successful what you get out of\nImputation output\nimputation is not necessarily like hangman where you you you’re guessing the most of what you think is the most likely letter it’s probable a probabilistic output so we think based on the reference data and the observed date in the sample that on this this allele of this haplotype there’s a certain probability that the allele is the a allele or at certain probability that’s the B allele so throughout this talk all these methods extend to multi allelic markers but first just to remove that complexity we’ll assume as dye allelic markers now typically refer to the alleles as a and B my background is with human data so whenever I refer to some physical characteristics of data I’ll be usually thinking I’ll always be thinking of human data so my apologies to people from an animal background it’s just that’s not my background so I my examples are from the human domain so here’s a haplotype at a marker you might have for the a allele probability 98% probability B allele 2% and on the other hapless type you also have a probability distribution that essentially gives you all the information you need for whatever you want to to use the advantage of probabilistic output is you’re capturing the uncertainty in the imputation rather than hard genotype calls where you’ve you’ve erased that uncertainty you can get called genotypes if you want by just taking the genotype that was have the highest probability and and to get posterior probabilities the genotype level rather than the allele level you just you can assume hardy-weinberg equilibrium and it pops out also with probabilistic gene types you can use them in the standard standard frameworks for testing so if you do linear regression analysis often typically the predictor at a markers or the number of copies of the minor allele zero one or two that same framework works with imputed data is just instead of an integer number of copies you have the expected number of copies which is sometimes referred as the expected dose of the allele so like in this example the B allele dose turns out to be zero point eight eight that’s what you’d plug into your your regression analysis so\nMeasuring imputation accuracy\nthere’s more uncertainty and imputation you need a way of measuring it and there’s two sort of ways one I think is really obvious one is a little less so so that the most obvious way is just genotype discordance the little less obvious way is the correlation and allele dosage and this was I think again the first groups that were developing imputation devised these methods the Michigan groove and in the Oxford group devised something similar Michigan devised the correlation metrics I’ll talk about here so R squared turns out even though it’s a little bit more complicated has some advantages a big advantages it’s normalized for allele frequency so for example if I tell you I have a marker that I can impute the alleles at this market with 99.9 percent accuracy well it’s very tough to interpret that without some more information if if the marker allele frequency is 30 40 50 percent 99.9 percent accuracy is really really good if the marker allele frequency is 0.1 percent 99.9 percent accuracy is really really really bad right because you could take and just do that the dumb imputation strategy of always imputing the alleles to be the major allele you’ve destroyed all the information at the marker and you’ve achieved 99.9 percent accuracy so you have to know the allele frequency whereas correlation automatically builds it in if you’ve you know from your introductory statistics class if you computed a correlation there’s variances in the denominator those variances or in terms of capture the allele frequencies so it’s it can be interpreted in a much better way without actually knowing having to know the allele frequency the squared correlation may meant metric where you’re looking at the expected correlation between the imputed allele dose imputed number of copies of alleles in samples and the true number of copies of alleles in samples it also has a couple of other factors that strike me is not very obvious but they’re useful so R squared can be estimated if the true genotypes are unknown you can get an estimate of your accuracy without even knowing what the truth is now it assumes that your posterior genotype provo these are well calibrated so there is that assumption built-in but if they are you can estimate the from from the imputed data itself without knowing the truth you can estimate what that correlation is this was this idea was developed by Michigan and Adair a derivation of something similar that illustrates one way to derive this is given in the reference I’ve cited in the american journal of human genetics this the second surprising feature is that R squared gives information about relative power so there’s an interpretation in terms of power that it’s useful for R squared so it turns out that the allelic tests has similar power if you use imputed genotypes for in samples or the true genotypes for R squared x in samples this there’s been something similar to this has been known for a long long time the best explanation i’ve seen in this is a box in the American Journal article that I’ve cited here if you want to look it up it’s just a small box that goes through the derivation so if you’re looking at a marker that you imputed where the estimated R square which are taking to be the true R square or in it to be is 0.8 and you have a thousand total samples you maybe have cases and half controls if you test that imputed marker the power should be roughly the same as using the true genotypes which are correct for 80% of your samples for 800 samples so it gives a way of if you’re trying to determine what R squared thresholds use for accepting your imputed genotypes to use to carry for that imputed marker into downstream analysis this gives you a way to interpret what that r-squared what it might mean so the general rule with imputation is you can impute high-frequency markers really well and low-frequency markers not very well where’s the cutoff what determines the threshold of what you compute there’s two primary factors one which we can change and one we can’t the one we can’t change is the effective population size we’re stuck with that the more diverse the population has the bigger effective population size the the shorter the shared haplotype segments in the population in the shorter those hats shared haplotype segments are the harder it is to impute can’t change that but what we can change if given money is the reference panel size the more the bigger the reference panel size the lower frequencies we can compute and we often think in terms of minor allele frequency it for many applications that’s more natural but for imputation the thing I find useful to think about is minor allele count oops a second so this is the same data\nImputation accuracy (MAF VS MAC)\nthis is and it also these plots are sort of nice for getting a sense of the potential for what imputation accuracy can be for different frequencies this is simulated data from a Northwest European population reference panel sizes vary from 50 to 200 thousand samples and the targa data was is on a million snip chip on the left hand pot plot we’re plotting the squared correlation this is using the true simulated the truth versus the imputed data for different minor allele frequencies on the right hand plot it’s the same data but broken up by minor allele count on the left hand side you can see that the you really need to know the frequency of what your imputing to understand the accuracy on the right hand side when it expressed in terms of minor they’ll count it’s much more much more stable so you with this simulated data around 10 markers 10 copies of the minor allele in the reference panel you’re getting up squared correlation accuracy around 0.74 around 20 markers in the reference panel 2020 variants in a reference panel for at that level you’re getting around 80% imputation accuracy so this this is simulated data it’s going to be a bit better than current reference panels because current reference panels are predominantly if not totally but or at least predominantly from low coverage sequence data and low coverage sequence data its Achilles heel is estimating low low frequency genotypes so it has a very high error rate for low frequency genotypes but as we move into high coverage reference reference panels obtained from high coverage sequencing this kind of performances should be practical in outbred populations like European populations oh one of the inference from this is that as you double the reference panel size if you were able to impute markers with at least 20 copies of the minor allele with a certain reference of panel size when you double it that should still be true it’ll even get a teeny bit better so every time you double the reference panel size the frequency of variants that you can compute other things being equal cuts in half it’s sort of a linear relationship now if you’re from a sequencing background.\nWhy impute when you can sequence?\na natural question to ask is why impute when you can sequence imputation has error sequencing is more accurate high coverage sequencing why why go to the trouble of imputing and so I this slide just sort of breaks down the pros and or the things that imputation is competitive with high coverage sequencing ad and things that it’s not competitive with so the easiest thing to do is is estimating allele frequencies and that’s what we do and we do Association testing which is where imputation has been used most widely that’s its strong point so with 50,000 Northwest European reference samples if you impute down to 20 copies that’s imputing down to a minor allele frequency of 10 or 2 times 10 to the minus 4th so you can go very relatively low and and do very well with imputation if your goal is to estimate minor allele frequencies if your goal is a little bit harder it’s much harder to estimate a genotype than to estimate a minor allele frequency then it gets more problematic in my simulated data for five percent minor allele frequency and above does it does very good it can you can actually estimate the the genotype with about 99.9% accuracy Oh in that range but it’s not true for less than five percent the accuracy slips and the imputation at a genotype level not a minor allele frequency level but a genotype level is just not as accurate now we could improve that by going to ever larger reference panels but there is there is a break and it’s much higher than the break for for estimating allele frequency there’s a much higher break for estimating genotypes threshold and of course for de novo mutations I don’t care how you know bigger reference panel is you aren’t gonna be able to impute them so there’s it’s true there are things that genotype amputate your genotype sequence II and he does much better than imputation so why would you do it money alright high coverage high coverage sequencing thousand dollars if or at least that’s the last time I checked I think it’s still in that range and that that may even require an order in bulk yeah I’m curious if somebody has data on that I’d like to hear that what the current costs are for genotyping as a service chip typing if you a good negotiator you can get a pretty good deal on chip typing you need you know big data sets you need to play the metrics off against Illumina and you know get them to go you know going against each other but you can get it down to $50 a sample imputation with the current methods you know half a cent a genome for 10,000 reference samples and these are order magnitude figures five cents a genome for a hundred thousand reference samples and if you have a million reference samples which we won’t per few TAF you years probably 50 cents 50 cents a sample so essentially a hundred fold less than the chip typing car and there’s a lot of data out there with chip typing costs or with with G while chip data available so compared to sequencing then that the cost difference is a factor of two thousand you may not have a thousand dollars to sequence a sample that you probably have five cents alright so yeah there’s a trade-off depending on what your application is if you’re poor especially if you’re interested in Association testing imputation gives you a lot of bang for the buck so the next part of the talk is\nHidden Markov model (HMM)\nI’d like to go over the models the standard model the most widely used a model for imputation there’s been some very nice clever work developing other approaches matrix completion approaches summary statistic approaches oh just you know in the interest of summarizing I’m gonna talk about the one I’m most familiar with and also the one that from what I’ve seen has the greatest accuracy so the debate the basic methods are based on hidden Markov models where you have a Markov process and you can’t observe the underlying states the process it’s hidden well what you do have is observed data and I’ll go through the parts of the model and then we’ll use this model in the research talk on Thursday so I’ll describe the Lee and Stevens model once the field of imputation moved into a what’s been called as pre phasing is where imputing onto haplotypes the Lee and Stevens model becomes the model in my view becomes the model of choice because it’s computationally tractable at the haplotype level without having to do a lot of shortcuts you can do the full even Stephens model and it gives you very accurate results the reference for that seminal model is is given on this slide so the hidden Markov model has a number of components I’ll go through those components and all of them will typically be based on a slide like this so I’ll go over that in some detail so the first thing it has is model states and there’s going to be a model state for every hat every pairing of a haplotype and marker so the markers these are the on the reference haplotypes are given as columns of the matrix the haplotypes these are reference haplotypes not the typo types your imputing that the from the reference samples are given here we’ll label these h1 h2 h3 h4 and so on the states of the model then just becomes the elements of the matrix these circles and for reasons that will become apparent in the slide or two we want to label those states with the allele that the reference haplotype carries so well we’ll use two the blue allele will represents the reference allele that yellow coloring will represent the alternative allele so the number of states in your model is just going to be the number of rows times columns so it’s number of haplotypes times the number of markers the next component of the model\nInitial state probabilities\nafter you’ve defined the states for the lead this villian Stephens hidden Markov model is the initial state probabilities so these are the probabilities before you’ve seen any observed data and the way the model the process the Markov process works is you start at the first marker and you work your way through to the last marker so the initial state probabilities are there’s only nonzero probabilities in the first column for the first marker so all those for each haplotype all the states at the first marker they will have equal probability so that the probably sum to one there’s no reason to prefer one state to the other and at every other marker another every other column those states have probability 0 to be at the beginning\nState transitions\nthen there’s state transitions and just to keep the slide from getting too cluttered I’ve only shown one so far the state transitions I’ve been shown is just what you can think is the primary state transition so the primary street straight transition just goes when you go from one marker to the next you stay on the same haplotype with probability close to one that’s what happens but actually it’s it’s a little bit more flexible and complex with a small probability you can jump to a random haplotype and that’s what I’ve tried to show for just sink one single marker right here so with a with a with a probability close to one you stay on the same state and there’s no what we call no recombination with a probability a small probability the remaining probability you jump to a random state and that random state can also include the the state you’re on so what that is modeling is is historical recombination or in the past there’s you know for a while you’ve inherited the same that haplotype your imputing on matches or is has inherited the same sequence of genotyped alleles as on as on a certain reference haplotype and then it because of a historical recombination it switches to another reference haplotype so these the probability of that small probability of of transitioning to a random haplotype is proportional over short distance is approximately proportional to genetic distance so the bigger genetic distance the rate higher recombination rate and so you have a greater probability of transitioning to a random haplotype so I won’t show this anymore but just be aware that the actual state transitions can go from - any marker at the or any state at the next marker but I’m only going to show the primary ones will you stay on the same haplotype all right the next component of the hidden Markov model is you have to have some way of relating your observed data to the to the des Markov process and that comes from what are called emission probabilities and this is where the labeling where we labeled each each reference haplotype at each each state with the allele that the reference haplotype carries and so if you’re in a particular state let’s just take the first state marker at the first haplotype it will with probability near one emit the blue allele and that’s shown in these equations here so if you’re in a blue state the square this I use a square to represent the allele on the observed data it will be epsilon here’s a small value it will be close to one with a small probability it will emit the other state you’ll have a mismatch between your observed data and the state you’re in and the same principle holds for yellow if you’re in the yellow you’ll emit a yellow allele with high probability and with small probability I’ll admit the other allele this works at any state where you have observed data these open squares mean you have missing data like you would have if you were performing genotype imputation then here’s another marker that’s genotyped in your sample your imputing and so for example not knowing anything else you just intuitively expect it’s more likely you’d be in a state on hepa type for one or two because the emission probabilities are higher there because the allele matches then in states H 3 and H 5 at the states where you have missing data there the emission terms they’re constant doesn’t matter what the underlying state is you have no observed data and you can treat it treat it as one in it and it drops out now let me let me back up once once you have the state proper beliefs you can get the imputed legal probabilities so the key the key thing you’re trying to understand is given this imputed data at a particular marker at any imputed marker I’m sorry given the observed haplotype the observed alleles on that haplotype you want to understand what in each of these hidden states what its probability is once you have that you’re essentially done with the imputation problem if you want to know what the probability of the blue allele at the third marker once you have these state probabilities you just add up all the state probabilities for for blue heppa for the blue states and that gives you the probability posterior probability of the blue allele posterior probability of the yellow allele at the third marker probability that that’s yellow you just add up the state probabilities for all the states for the reference haplotypes where the reference haplotype carries the yellow alleles so the key is those state probabilities and that’s the next slide so this standard way of breaking this up\nCalculating HMM state probabilities\nit’s a really beautiful math I love the math that that you use to get these state probabilities is you break it up in what’s called a forward probability in a backward probability so first this is the state probability because little m is a marker H is your haplotype so it’s the probability of being in the state at marker M half a type H and the Oh is your observed alleles Capital m is the total number of markers so given all the observed alleles at the genotype markers you want to be able to compute conditional on that what the state probability is and from what we talked about the last side as soon as you know the state probabilities you just sum them up to get your imputed allele probabilities so you break it up using the end of conditional independence assumptions into two parts that are called a forward and a backward probability so there’s a couple things to - I think to note about this equation one is there’s a forward probability for every state in your model so remember there’s number of states is the number of rows times columns number of haplotypes times the number of markers so there’s a ford probability for each of those states and there’s a backward probability for each of those states for each marker and for each haplotype the ford probability only conclude the observed data up through marker m so if we are imputing a ford probability for a state at marker m the ford probably only includes the observed data up to marker m the backward probability includes the observed data from the next marker all the way to the end so there’s that kind of division going on and then let’s see then the name for probability I’m guessing it comes from the way they’re computed it turns out you’d compute the four probabilities by making a forward pass through your data so the way it works is\nComputing forward probabilities\nyou start with the state probabilities for marker 1 which you get just from the initial state probabilities they’re all equal given your state probabilities for marker 1 there’s an update equation which I’ll just flash on the screen in the in a few minutes there’s an update equation that gives you all the state probabilities at mark or two once you have all the state probabilities of marker 2 there’s an update equation that uses these probabilities to determine all the state markers at all the state probabilities at marker 3 and you just march through your data one marker at a time and at each step you use the state probabilities at the proceeding marker to give you the the the Ford’s the four probabilities at the next marker I said state probabilities but use the I mean the forward probabilities at the proceeding marker and you keep marching as you might guess that backward probabilities work the same way it’s\nComputing backward probabilities\njust in Reverse you start at capital n that the last marker in your data set you start with the backward probabilities there which are which which turn out to be all one initially and then from that you get the backward state probabilities at markers at marker and minus one at the preceding marker and you keep marching backwards so you end up at eventually you get to marker six and given the backward probabilities at marker six you can get them at marker five given all the backward probabilities at marker five the the backward update equation gives you at marker 4 and so on and you march back so you do a forward pass and a backward pass through the data and it’s imaginatively called the algorithm the forward backward algorithm will use this\nForward update\nequation in the research talk and I just wanted to give a high-level look at the equation this is an example of the forward update equation don’t have to memorize it but just to understand the different components of it so remember the up forward update equation gives you the forward probabilities at marker n plus 1 given the four probabilities at marker M so that oops for it probabilities a marker n plus 1 that’s the Ford problems at marker and plus one you’ll notice there’s an N plus 1 there then use sum over all the reference haplotypes all the states at marker M so here is the forward probability at marker M and then you end up multiplying that by a transition probabilities this is state probability of being an H reference the state for reference haplotype H Prime and transitioning to the state at the next marker with reference to haplotype h and then you multiply by an emission probability so given this the state you’re in what’s the probability of the observed allele at that state the backward state update I won’t go over it has the same format it’s a little bit different but it’s the same format you’re summing over a triple product product of the the backward state probability at the at the state you’re coming from a transition probability and an emission probability one thing that’s we’ll use in the research talk on Thursday is we’ll use very strongly the fact that when you’re at an imputed marker this term effectively drops out and that’s it and we’ll use that to to find some faster ways to compute compute imputation so the last part of the talk I wanted to talk about programming this is for people with a CS background this this some of this a lot of this all of this perhaps you’ve seen but there’s probably people in here that are like I was a number of years ago coming into computational genetics with little or no programming background and so there’s certain certain things that I was I thought it might be helpful to just go over that may save you some time save you some make your life a little simpler because a lot of our work is involve writing code so I I find when when\nSimplicity\nwriting software that the chief challenge for me is complexity if and when it gets more complex my mind has a hard time grasping at a more error-prone and as much as if you’re like me you get a buzz from getting writing really fast code it’s it’s pretty exciting to write code that’s really fast that’s really not the first object the first object is to write clean simple code don’t worry about the optimization you’re you’re just trying to write this simple code so that when you look at it it’s easy to understand what it does it it operates in a logical way the structure of it makes sense just easy for the mind to absorb and grasp without having to really study it there is a place for optimization but it’s not the first thing you want to do there’s a famous quote that started donald knuth took this this statement from scripture and then changed it after computer science he wrote the premature optimization is the root of all evil and I think the idea is that if you optimize too soon you can end up optimizing the wrong thing or doing unnecessary optimizations that don’t actually improve the code or if your code isn’t simple to begin with you have a hard time finding the right optimizations see optimization optimization is not free yes it can speed things up but it has a cost if your simple code was fast enough you wouldn’t need to optimize so when you optimize by definition you are introducing complexity and that complexity has has costs it’s you’re gonna have more bugs in it because it’s more complex it’s gonna be harder to find the bugs because it’s more complex it’s gonna be harder to extend your code to add new features to it because it’s more complex it’s gonna be harder to maintain the code because it’s more complex it’s harder for other people to come look at your code and under wonder what you’re doing because it’s more complex so it there’s that cost and you have to weigh that cost against the expected benefit is a big increase in complexity worth getting a 5% reduction runtime in most cases not if your code is fast enough for practical purposes getting a tenfold 100 folding reduction runtime may not may not be worth it for the increased complexity if it’s already fast enough for your purposes if it runs in a second so weigh it up and and understand how much complexity you’re adding and what the trade-off is before you even add it because optimization is not it’s just not free the second general principle that I find useful is modularity which just carries the idea that the you want code units of code where the input is very simple what it does is simple to understand at least at a high level and the outputs very simple a module of code that you can treat conceptually as a unit without having to really think about it very deeply now when you write the code you may not have to think about it deeply but once it’s working and doing what you want you can just treat it as a building block and ideally you want your program to be very loosely interacting modules so that when you’re when you’re working on a particular module because it operates very independently you can give that your whole concentration you don’t have to have the whole program the whole complex program at your fingertips in your memory you can just focus on the individual part you’re working at because it’s loosely coupled the classic example of this would be UNIX utilities at the command line if you’ve used a UNIX system so UNIX utilities there’s utilities first sorting counting lines counting words extracting columns extracting lines that meet a certain criteria changing characters replacing words and your there’s all these UNIX utilities and you can do a lot of your programming without actually or a lot of your data manipulation without actually sitting in the writing code you just take the UNIX utilities and you string them together in a series of filters or in a pipeline write you UNIX utilities our classic example of doing some units of code that do one thing do it fairly well and that you can work with as a unit without understanding how they’re implemented and that kind of approach is really useful for writing complex projects another just general thing when you’re writing is if you can be aware when your classes or your methods or which are called functions and depending on language you’re working in when they do too much one of the pieces of advice I read early on when I was learning program is be aware of of functions that extend more than the length of your computer screen and in my experience that’s that’s good advice when it doesn’t fit on the screen I’m more likely to make errors because I can’t see the whole function without scrolling and the extra link that indicates it’s usually a bit extra come has additional complexity and so you know all rules have exceptions but I generally if the function extends more than the screen I want to see if there’s a way to make the code cleaner simpler easier to understand by breaking it up into parts classes if you work in an object-oriented environment which for complex problems can be very useful complex programs for me you know your threshold may be different but my threshold is once my my file my I work in Java predominantly once it’s more than two or three hundred lines I noticed that I have a harder time really understanding what’s going on in the class and so when it gets long I if I can I try to find ways see if there’s a way I can break things up that would make it simpler to understand there’s not always but I try so to get that simple code and involves what’s called refactoring which is just cleaning up cleaning up your code without changing how it behaves so when do you when do you a factor how do you know what parts of the code you want to spend some effort cleaning up just from experience anything I’ve just written a scan they need cleaned up I never get a a really right design the first time unless it’s over just a trivial piece of code I may not realize it when I raid it that day has problems but there’s an acid test for for discovering you have code that’s hard to understand it’s you just come back to it after a couple months and for some reason you have to go back a new code and and you’re looking at it and it’s very humbling because you can’t understand what you wrote you wonder what you were thinking why did I do this is this a bug isn’t this more convoluted than doing it this other way all those thoughts that when you look at the code for fresh eyes it’s usually then that did I spend time refactoring when I first write the code I’m just a little too close for it I’m a little it’s hard for me to see the blemishes but you come at or with fresh eyes they’re really obvious and it’s often in the part of that painful time you’re spending trying to understand what your codes doing again that also can at the same time is very easy to see ways that I could have done this differently I could have combined things I could have changed the organization to make it easier to structure I could divide long methods into in these two shorter methods I can combine code that’s essentially duplicate code so that instead of having to maintain tuned components of code it’s just one so all those things become obvious when you look at the code again with fresh eyes so we’re factoring if you’ve had the experience like you know I’ve had many many times of just having a not very fun day trying to understand what you wrote in the past refactoring just makes it means the next time you look at the code it won’t be so bad and it’ll end in any poor soul that’s not you if you have a hard time understanding your code just imagine what somebody else coming your code is gonna what difficulties they’ll have\nTesting and debugging\nthen for testing and debugging one tool that I find really useful is regression testing so this is not linear regression from statistics it’s just testing to make sure your code is still working correctly and this can be set up in a very automated way you have some test data sets the test data sets you originally used to convince yourself that your code was was behaving well and doing what it was supposed to do save those test data sets write some scripts and save the output that you had from a previous version and when you continue working on your code may be doing refactoring and then you reach a reach is sort of a stable point the code you can check and see whether your code is still producing the same output you can check it a qualitative level to see if it’s still producing the same accuracy you can also just use the diff tool in UNIX to check whether the output files have changed at all and this is a fantastic way to catch bugs that you’ve introduced you had working code you made some changes and you didn’t realize it but you broke something it makes it very systematic very easy to find those type of bugs another useful tool is version control system I use a tool called git it allows you to go back and see previous states of your previous state of the program you may have started off on a develop development and then found out that this I made a mistake I need a backtrack or maybe there’s a bug that you never caught before and it was introduced in the past it allows you to essentially do a binary search to find just the point where you made the bug and determine exactly what changes that you made at that point so you can vary with a microscope narrate and nail down the place where the bug was introduced if you get there’s a lot of tools online for that there’s a Udacity course on get a short Udacity course there’s also a some manual online if you search for something called git GI teeb book you should find that the online book for that explains get if you want to learn that it’s not easy-to-learn get initially I don’t think these version control systems take a little bit of use but they’re a powerful tool once you learn them it’s like the UNIX the UNIX environment it’s hardly hard to learn but very powerful\nthe last is it pays to remember your previous bugs bugs are not uniformly distributed in your code typically they tend to cluster and the reason for that are you know fairly a fairly natural it could be a complex section of code complex code more likely to harbor bugs they either spatially correlated in your but in your code it could be something in your life you where you weren’t at your best the day you wrote that code you didn’t get enough sleep you got a letter from the IRS whatever you just something that threw you a little bit where you’re not up to up to your full full capabilities so if it pays to remember where your past bugs are when you find a bug if you can remember the type of bugs you had and where they tended to occur in the code in the past that can help you can give a little more preference to those parts of the code when you’re trying to track that bug down and it can save you a lot of time so thank you thank you for your attention that’s the end of my remarks"
  },
  {
    "objectID": "chapter1.3_transcript.html",
    "href": "chapter1.3_transcript.html",
    "title": "Chapter 1.3: History (Video Transcript)",
    "section": "",
    "text": "History of Psychiatric Genetics: Part 1\nKen Kendler:\nIntroduction\nI’m pleased to be able to talk to you about the history of psychiatric genetics. In this short seminar, all I can really expect to do is to provide a brief introduction and a few highlights. I’ve decided to focus on the earlier history of psychiatric genetics, as this is likely less familiar to most of you, and rather than only giving lots of dry facts and dates, to try to illustrate this with particular prominent individuals, to provide when I can photos of them, to let you see what their books or articles look like. I would say that the goal I’ve set myself to show you bits and pieces of the rich history of our discipline of psychiatric genetics so that I might whet your appetite for further self-study.\nRobert Burton\nSo let’s get started. In a really prequel stage the modern discipline of psychiatric genetics as we understand it started very close to the time of psychiatry itself, that is somewhere in the couple of decades between 1780 and 1800. But we do have one giant figure, Robert Burton, who was writing in the early 17th century, in this case at the date of 1621, a little more than a decade after the death of William Shakespeare, and he wrote what was a classic that has been read up until this century on depression. The title is the “Anatomy of Melancholy”, and I want to point to you to a small part where he describes the synopsis of his first partition. As you can see my pointer here, that he describes all the causes of melancholy, dividing them into supernatural or natural, and in secondary as primary he describes about the stars and chiromancy, but here particularly he notes that old age is a risk factor for melancholia temperament, and here we note “parents”, it being an hereditary disease. So we have an observation from decades, and actually more than centuries before modern medicine developed and psychiatry developed, with the understanding as people had observed that one form of psychiatric illness, melancholia, clearly passed from parents to their children.\nThis is a pre-print of an article that is coming out of the American Journal of Psychiatry, and it reflects a rather detailed review of the pre-history of psychiatric genetics from these years of 1780 to 1910. What I next want to do is just highlight this overview. As you can see from the abstract here, I reviewed 48 representative text published during this time period, nearly all of those textbooks had sections on the etiology of mental illness or actually they more frequently called it insanity. In that section they almost always made some comments about the importance of genetics in the etiology of insanity. So here is a review of the main points of that review itself:\nFirst, since the beginning of psychiatry as a medical discipline, roughly around 1780, most expert authors who were textbook writers viewed heredity as among the strongest risk factors for insanity. Second, during this time period most writers concluded that it was a predisposition to illness, rather than the illness itself, which was transmitted in families. Third, and this is related to the concept of predisposition, they were well aware of the apparent probabilistic nature of the transmission of insanity. They noted that it often skipped generations, with an affected parent or aunt a grandparent or aunt or uncle, but the parent as well and the child is affected, and also noted that it seemed to often impact only on a few of many individuals within the same siblings. Fourth, authors discussed at length the question of whether there was what they described as “homogeneity” or “heterogeneity” of the familial transmission of the various forms of insanity. The general conclusion was that heterogeneous transmission was the rule. That is, that in the relatives of the insane patients, almost all who would have been hospitalized in asylums at this time. The clinicians who were writing this material viewed a wide variety of psychiatric, and sometimes neurologic, disorders. Homogeneous transmission, that is the idea of like begets like, was an exception. It’s interesting that two kinds of syndromes are mentioned repeatedly across these textbooks as being homogeneously transmitted, as one is much of what we would call cyclic psychosis or manic depressive illness, and the other was suicide, with a number of authors noting suicides running in families with high levels of homogeneity. Fifth, many writers noted that odd and eccentric personality features were common in the relatives, and especially parents, of their insane patients, and this obviously is a prequel to later developments of schizophrenia spectrum conditions within our own time period. Finally, inheritance as they used it, and as Burton used it, was commonly understood more broadly than we would today. It includes prior environmental parental experiences, that is the inheritance of acquired characteristics was accepted for most of the 19th century and even well into the early decades of the 20th century, but also some authors noted that parent-offspring transmission could arise from psychological effects of the behavior of the parents or intrauterine effects.\nProsper Lucas\nNow, let me turn to viewing a highlight of a series of important authors that we might wish to describe and i want to begin with Prosper Lucas. Prosper Lucas was a French Alienist and geneticist. He lived from 1808 to 1885. He wrote a major two-volume monograph on genetics in 1847, as you can see this “philosophical and roughly physiological traits of natural heredity” would be the translation. These are very long and extensive. It would have one of the great mid 19th century reviews of the field of genetics, and he reviewed them across medical, physical (that would be anthropometric traits), and psychiatric conditions. In addition, Prosper Lucas was in his day what we might call a statistical geneticist and developed a pre-Mendelian theory trying to explain the stochastic nature of inheritance. The importance of Prosper Lucas’ work is best illustrated in one of the great books in all of the history of science, Charles Darwin’s “Origin of Species”, in 1859. He noted Prosper Lucas’ book as quote, “the fullest and best on the subject”, and in his later works, particularly on “Variations in Plants and Animals”, Darwin frequently cited Lucas’ work more than a dozen times in that book itself.\nJenny Koller\nNow let’s skip a number of generations to the figure of Jenny Koller, the most prominent woman who contributed to this literature in the 19th century. A bit of background is in order. Throughout the entire 19th century, virtually all observations in psychiatric genetics were uncontrolled. The vast majority of statistics that were reported is a very simple one: that is the proportion of hospitalized psychiatric patients who were hereditarily burdened, and by which they mean has some well-documented history of psychiatric illness or occasionally neurological illnesses in their relatives. These percentages varied very widely from four or five percent to 80 percent, and that’s partly because what disorders they consider to constitute a hereditary burden was quite variable and which relatives they counted was also quite variable, but they had never used controls. The first study to do that was done by Jenny Koller in 1895. Here’s a picture of what Dr. Koller looked like. She was encouraged by her mother to consider medicine. She met with one of the few physicians in Switzerland at the time, she completed medical school there, but she was unable to get what would be the equivalent of an internship, as they were not offered for women in Switzerland, and so went to Paris, France for her training.\nHere is what her article looks like; this would be a translation. She was at that point working in Zurich. I’ll give a brief biography in a minute. The title is “Contribution of hereditary heredity statistics of the mentally ill in the canton of Zurich: comparison of the mentally ill with a hereditary burden on healthy people due to mental disorders and such like”. We can see that people wrote longer titles in those days for articles than we do today, and this was a relatively prominent journal based in Berlin actually, even though she was Swiss. We published in Neuropsychiatric Genetics earlier this year; Astrid Klee, who’s a professional German translator, and I a detailed review of this article, and in the appendix we present an entire translation into English. I want to just briefly summarize for you here just picking up on the abstract.\nThe first such study in the history of psychiatric genetics that is was published in 1895 that was the doctoral dissertation of a Swiss physician Jenny Koller. She was working under Auguste Forel at the Burghölzi hospital in Zurich, Switzerland, a hospital very famous in the history of psychiatry, and Forel was succeeded by Eugen Bleuler, who we will discuss later. She obtained histories of a range of mental and neurologic disorders in the parents, aunts, uncles, grandparents, and siblings of 370 hospitalized psychiatric patients and 370 controls who were not hospitalized. Interestingly, the rates of hereditary burden were only modestly higher in cases than controls. There was a great deal of surprise at the high level of mental illness occurring in the relatives of unaffected control subjects. However, Koller followed up trying to address two quite important questions: first, she examined which individual syndromes were actually more common in the relatives of the psychiatric patients and the controls, and she found only two categories, which what we roughly translate as major mental illnesses and roughly translate as eccentricities, that is abnormal behaviors, but she also considered syndromes of apoplexy, that is a stroke, nervous disorders, which included such things as headaches, and probably some other neurologic disorders, as well as anxiety and dementia and those three were not more common in the relatives of the psychiatric patients than the controls. Furthermore, the rates of mental illness and eccentricities were much more strongly elevated in the first degree relatives of parents, largely children, largely parents, and siblings in cases versus controls but the elevation for grandparents and aunts and uncles were much less. Again, this was the first systematic observations of the tendency of disorders to aggregate more in what we would now understand to be individuals more closely genetically related. In my view, Koller’s study represents a major methodological advance in psychiatric genetics, helping to define which disorders co-aggregated with major mental illness.\nErnst Rüdin\nLet me now turn to what will be a darker part of the history of psychiatric genetics. This is the figure Ernst Rüdin. I will again give you a brief biography. Those of you who can see will notice that he had a little lapel pin in it with the swastika and that will be part of our story.\nWell, Ernst Rüdin was born in 1874, died in 1952. He was Swiss. He graduated from medical school in Zurich in 1898, and his first job was also at the Burghölzi hospital after Jenny Koller to assistant Eugen Bleuler. He then moved to the psychiatric clinic of the university of munich in 1907, where he became an assistant Emil Kraepelin, under whom he completed his doctoral thesis. He was appointed a senior physician there in 1909. When Kraepelin’s lifelong dream the establishment of the German institute for psychiatric research, the first such multidisciplinary institute in the world for psychiatric illness, was established in Munich in 1917, Kraepelin appointed Rüdin as the head of the Department of Genealogic and Demographic Studies. Most of the leading members of the next generation of psychiatric geneticists, and that includes such figures as Eliot Slater, Eric Stromgren, and others from all across Europe, came to train in the department that that Rüdin had established in the 1920s and early 1930s. Rüdin himself had two major works in psychiatric genetics, although his school, the program that he established had many many more works.\nThe first was published in 1911 when he was 26 and in this journal and the title was “Methods and goals of family research and psychiatry”. It’s an extensive article, and outlines a methodologically sophisticated Mendelian-informed research program for psychiatric genetics, but you will notice in there a number of mentions about “volk” eugenic theories about the “volk” and the wellness of the genes of the “volk” himself. His second major work was this and along with his daughter Edith Zerbin-Rüdin, more than two decades ago, I published in Neuropsychiatric Genetics a summary and abstract of this. The title would be “Studies of the inheritance and origin of mental illness to the problem of the inheritance and primary origin of Dementia Praecox”.\nThis was the first large-scale systematic family study ever done of a psychiatric disorder. It included using these newly developed methods of Weinberg to age-correct the sample and to use a proband-wise correction method to get the correct results. The morbid risk that he calculated for narrowly and broadly defined schizophrenia in the sample was 5.4 and 7.7 percent. Those results are quite similar to modern findings; the study that I did in Russ Coleman early in my career found a rate of six percent, so very much in the range that Ernst Rüdin found. He also found other psychoses were common, with a morbid risk of 5.1 percent, lower rates in half siblings. He noted the risk of schizophrenia was significantly increased in parental diagnosis of alcoholism, a history of schizophrenia in second or third degree relatives, and by parental diagnoses of other psychoses. So overall, Rüdin found a familiar relationship between schizophrenia and other psychoses, a substantially lower risk of schizophrenia in parents versus siblings, and a segregation pattern of schizophrenia that was not did not conform with simple Mendelian disorders. Importantly to note, this was not a case-control family study as we might do now, actually the purpose of this study was to see whether segregation patterns consistent with simple Mendelian transmission could be found. Rüdin did note in this monograph that if there were two recessive genes, those at 0.25 squared would equal a recurrence rate of 0.0625, he said that might be consistent with the model.\nNow we need to talk about another key feature of Rüdin’s career. Rüdin as a teenager developed a strong interest in racial hygiene and was in 1905 among the founders of the German society for racial hygiene. After the rise of the National Socialists, that is the Nazis, in Germany in 1933, as director of the German Institute for Psychiatric Research (he had taken over the position from Kraepelin after his death), Rüdin collaborated extensively with the Nazi government, praised their racial policies, and received substantial funding both from them and later on from the SS. From 1933 onward, he played a major role in the development of the Nazi program of involuntary sterilization of the medically and psychiatrically ill. It is generally agreed that he was not actively involved in the Nazi T4 program, which was the systematic killing of the mentally ill and mentally handicapped, that began in 1939, but the historical documents are quite convincing that he was aware of these efforts and knew of colleagues who were working in them. And he certainly made no attempt to stop or to publicize them. Furthermore, as documented in high quality historical resources that have only been recently available in English, I would particularly recommend this book by Dr. Weiss “The Nazi Symbiosis: Human Genetics and Politics in the Third Reich”. It’s quite clear that Rüdin played an important role in the scientific legitimization of Nazi racial policies, both within Germany and internationally, and we know historically of course that the acceptance of these policies laid the groundwork for the future horrors of the Holocaust. In light of his activities from 1933 onward, there is a range of opinions in our field about how Rüdin’s substantial scientific contributions to the history of psychiatric genetics should be treated. Some have argued that any discussion of his work is inherently immoral, as it inevitably rehabilitates him, and dishonors the memory of those whose suffering he has contributed to. And i respect that tradition personally, although i don’t agree with it. Others have argued that his work can and should be studied for two important and complemented reasons: one, he has played an important role in the history of our psychiatric genetics, but second, as a case study of the potential tragic effects of the misuse of our science.\nEugen Bleuler\nLet me then go on and conclude with this figure Eugen Bleuler. He’s probably the best known of all the people that I’ve spoken about here. This is a a lovely photo of him as a young man. Bleuler is most famous for introducing the term “schizophrenia” to the world in a lecture in Berlin on 24 April 1908. He revised and expanded his schizophrenia concept in his seminal of 1911. This is his great monograph, “Dementia Praecox, or Group of Schizophrenias”, and was unfortunately only translated to English in 1950. But in 1917, that is only a few years after the monograph published by Rüdin, Bleuler wrote a very thoughtful critique of Rüdin’s family study of Dementia Praecox, and this is the German version of this. And this is now been published in Schizophrenia Bulletin, which represents a detailed review of Bleuler’s comment on Rüdin and, like the other works that I’ve been doing, here in the appendix, along with Astrid Klee, we publish a full English translation of Bleuler’s comments.\nI want to review here the four key points made by Bleuler, again working at a time very different from our own. First, Bleuler argued that understanding the transmission patterns of schizophrenian families requires definitive knowledge of the boundaries of the phenotype. And he argues that those remain unknown. Bleuler strongly suggest that Rüdin’s choice, which was Kraepelin’s concept of dementia praecox, was far too narrow. As we know, Bleuler was interested in such broader concepts as latent schizophrenia. What Bleuler argued was that clarifying the genetics of schizophrenia is inextricably bound up with the problem of defining the phenotype: you can’t do one without the other. Second, Bleuler argued for the importance of “erbschizose”, which might be described literally as “inherited schizoidia”. And he wondered whether either his famous “4 As” or other (and i quote here from our translation) “brain-anatomical, chemical, or neurologic characteristics” might underlie the genetic transmission of schizophrenia. This really is very similar to our concept of what might be an endophenotype. Third, Bleuler was quite interested in the nature of the onset of schizophrenia, suggesting that environmental adversities would often provoke quote “latent” illness to become manifest. It was important to argue that to identify such risk factors and actually incorporate them into the genetic models, a relatively advanced concept we’re still struggling with today. Fourth, although not optimistic that current knowledge would permit a resolution of the transmission models for schizophrenia, Bleuler finds single locus models for schizophrenia Dementia Praecox to be implausible, and at several points wonders whether polygenic models (he didn’t give the number it certainly was not conceived as the way Ronald Fisher would be), but a small number of genes might better apply.\nSummary\nSo this concludes the main part and I want to summarize. My goal has been to introduce you to a few of the many important figures and topics in the history of psychiatric genetics. I want to leave you with one ethical and two overarching scientific themes of this history. The ethical part: our history illustrates the misuses to which the science of psychiatric and behavioral genetics can be put, with tragic results, and therefore the need for our vigilance to reduce the chances of such developments in the future. From a more scientific perspective, I would suggest that the history of psychiatric genetics includes many themes, but two of which are overarching: the first is the close interrelationship between psychiatric diagnosis and genetics. How can you do the genetics of a disorder whose diagnostic boundaries are unclear? So inevitably there is an intimate relationship with these two fields, that is, to get the genetics right we need to get the diagnosis right, and the genetic strategy is one of the best ways to help us decide that what a valid diagnosis is. So we are in this large, long, iterative process. And the second key issue that runs through much of the 19th and early 20th century, is do psychiatric disorders run true in families or is there often heterogeneous transmission? And i would suggest to those of you acting working in the field is that both of these issues are very extant and we have solved neither of them. Thank you for your attention.\n\n\nHistory of Psychiatric Genetics: Part 2\nKen Kendler:\nMy name is Kenneth Kendler. Last year, I gave a presentation on the history of psychiatric genetics that focused on specific episodes and individuals important in the history of our field. Today, I’m going to give a deeper dive into a single episode. The title, as you can see, is “The Beginnings of the Debate Between the Mendelians and the Biometricians in Psychiatric Genetics.” I’ll be focusing on four individuals: David Herron, Carl Pearson, Abraham Rosenoff, and Charles Davenport. I will be covering a fairly short time period of the years 1913 to 1914, so think of the year just leading up to the First World War.\nNow, it’s not often that psychiatric genetics work gets into the New York Times, but here, if you were to open up the magazine section of the Sunday New York Times on November 9th, 1913, you would see this big full-page spread titled “English Expert Attacks American Eugenic Work – Teachings Are Pronounced Fallacious and Actually Dangerous to Social Welfare” by Dr. David Herron of the Galton Laboratory, London. Dr. Charles Davenport makes a vigorous reply. Here’s a portrait of Charles Davenport, whom we will get to know somewhat better, and these are two buildings: that was the Eugenics Record Office at the Cold Spring Harbor Laboratory, which has developed into a molecular powerhouse in more recent years.\nI want to introduce you to the four key characters: Aaron Rosenoff, a Jewish Emma Gray trained in medicine and psychiatry in the United States; we will be tracking his career in some detail later in the talk. Charles Davenport was a major American Mendelian in the early 20th century; he was at this point the director of the Eugenics Lab at the Cold Spring Harbor. There’s a great deal of literature on Davenport’s life. David Herron, much less well-known, was a Scottish-born statistician who trained as a Galton research fellow under Pearson at the Galton Research Labs of National Eugenics at the University of London, and he performed the first-ever quantitative genetic study of insanity in 1907. Carl Pearson, I think, does not need much introduction; he obviously was a major figure in the history of statistics and in these years was directing the Galton Labs at London.\nNow, all of the four individuals here were ardent eugenics. There are aspects of eugenics and arguments that played into this. I do not emphasize this in my presentation, but if you read the primary sources, you will hear more about that here. Now, what do these folks look like? This is a portrait of A.J. Rosenoff, not the best, but you can get an image of him. This is a picture of Charles Davenport, quite clear, and you saw a portrait of him in the New York Times as well. Um, here is, I could not find a picture, despite much effort, for David Herron, so I give you his first monograph - historically, a quite important document that I may cover in a future lecture. Our first study of the statistics of insanity and the inheritance of the insane diathesis, which was a quantitative study using Pearson’s tetrachoric correlation of insanity. And finally, a Carl Pearson, uh, here he is with his family, the son Egan, he will hear more of, and his daughter - I think a very nice mother of Pearson.\nSo, let’s give a little bit of historical background to, uh, what is going on at this time period. After the rediscovery of Mendel’s Laws in 1900, an intense and often personal debate ensued about the relevant values of biometrical genetics and Mendelian genetics as applied to plants, animals, and especially humans. As the traditional histories tell us, this debate was largely an English affair centered on two chief protagonists: it was Gregory Bateson on the Mendelian side and Carl Pearson as the lead biometrician. For those of you not familiar with these terms, the biometricians emphasize quantitative traits and thought that you should study relationships among relatives by calculating correlations, especially Pearson’s both product-moment and tetrachoric correlations. On the other hand, the Mendelians emphasized investigation of discontinuous traits and were interested in Mendelian patterns of those traits. Um, in essence, what we are seeing now is a small American skirmish as a part of this much broader controversy between these two fields, and here I will give you a bibliography at the end, but these are probably the two best reports about this overall background argument if you wish to follow them up further.\nNow, we really get started here with this monograph. This monograph sort of lifts the fire that exploded in the controversy that we saw in the New York Times. Notice the dates of publication. This was initially printed in the American Journal of Psychiatry in 1911 and reprinted in the Eugenics Office. The key author there was A.J. Rosenoff. The title: “A Study of Heredity of Insanity in the Light of Mendelian Theory.” This was the first major pedigree study of psychiatric illness ever done, applying Mendelian rules. Uh, it was a couple of years ahead of Ruden’s key publication in 1916.\nA little bit of background: with a supportive Davenport and his team of field workers from the Eugenics Record Office at Cold Spring Harbor, Rosenoff studied 72 pedigrees who were ascertained through psychotic patients admitted to King’s Park State Hospital in New York, which, in fact, is where Rosenhoff was working. They studied, quote, “insanity and Allied neuropathic conditions,” unquote, which he claimed in this publication was inherited as a Mendelian autosomal recessive trait. And of course, by that, we mean fully penetrant models of incomplete penetrants were not being seriously considered at this time.\nNow, Rosenoff did not attempt to uncover the mode of transmission of specific psychiatric disorders, those, as so, this deviated certainly from the Galton School that looked especially at dementia praecox and certain forms of insanity, and many of the other subsequent genetic investigations we are used to. Rather, he focused on a very broad phenotype that he termed the neuropathic constitution.\nAnd what he tested: the Mendelian model by examining segregation rates and relationships from these pedigrees as a function of their mating type. He studied 206 matings with a total of 1097 offspring, and any well parent with a neuropathic offspring was assumed to have a heterozygote genotype. So, just so you’re familiar, with these, D stands for the wild type, and R is the risk allele. So, a Dr is a heterozygote, and he focused on six mating types, and you can read these out, of course. These are going to be given the recessive model that he assumed: affected by affected, affected by a heterozygote, affected by a wild homozygote, heterozygote by heterozygote, wild by heterozygote, and wild by wild. And from simple algebra understanding Mendelian predictions, you have these percentages expected in the offspring of those individual mating types from zero percent to one hundred percent.\nNow, what’s the phenotype that Rosenoff studied, and this is obviously a little bit from the monograph itself. In studying any neuropathic defect, one must bear in mind that its clinical manifestations will vary with the personality of the subject and the conditions of the environment. It is indeed a notorious fact that most of the so-called clinical entities are remarkable for the variety of their manifestations. This fact has necessitated the introduction and clinical practice of the conception of neuropathic equivalence, and that will be a major topic that we will discuss further. Now, this is the key result from that monograph that I’ll go through, but I redo this in a clearer format. But, in effect, these are the mating types with the B and B1 differing about whether you know that the person is a heterozygote because they have an affected offspring. We affect the assumption of their model and one in which you assume that, but you can actually read off the EXP versus The observed, and in the next figure, you’ll be able to see that more clearly. So, if you scan your eyes down these columns, you’ll note that there’s a fairly significant deviation here but for most of the other columns and somewhat there, there’s moderately good agreement between.\nNow, there was no chi-square test, even though that had been invented a number of years ago, no statistics observed, but from these pedigrees, Rosenoff and Reach the following conclusion, as is shown in the table: the correspondence between theoretical expectation and actual findings is in some cases exact and in all cases remarkably close. It would seem, then, that the fact of the hereditary transmission of the neuropathic constitution as a recessive trait in accordance with the Mendelian theory, may be regarded as definitively established.\nLet’s look at some of the pedigrees; they are all presented in the monograph, and I’m only going to give you a selection, but just to get a clue about what the meanings of the symbols are. So, the clear squares and circles are going to be normal progeny. Those with a little circle in them are a normal subject without progeny, and so they can’t, according to their system, no other person is a normal homozygote or a heterozygote. And then these are normal subjects with neuropathic progeny, which, by their definition, have to be heterozygotes. And then the affecteds who need to be homozygotes are the fully shaded ones, and then the ones about dieting child another again, you can study this at your leisure if you so wish.\nSo, I’m only going to show, I think, four pedigrees, but working through those, although maybe a little tedious, will be helpful to get a sense of the data. So we always have the proband who is hospitalized at King’s Hospital in this pedigree. In this case, we have a manic-depressive insanity case hospitalized, then all the affected—let’s go through those first.\nThis woman has a sister who is described as very nervous, which meets their criteria, or theirs, for a neuropathic trait. She has a mother with epilepsy, a maternal grandfather with epilepsy, and a paternal grandmother who is described as “hysterical when a girl had ideas someone was trying to poison her.” The only other people here are the obligatory heterozygotes by Rosenoff’s system, and again, because they had affected offspring, assuming their recessive inheritance, those are obligatory heterozygotes. So that’s the first pedigree.\nSecond pedigree, here again, we number seven as the proband with manic-depressive illness, and this is, again, a heterozygote by ill individuals. So according to Mendelian theories, these should, on average, have 50% affected. We have, with this affected male proband, a sister who is described as “easily excited in a nervous temperament,” which is assumed to be neuropathic. She has an affected father with recurrent melancholia with insomnia, having spent five months in a sanatorium. She has a paternal uncle who is described as a “crank,” and a maternal aunt described as having convulsions, and a maternal uncle described as convulsions. Note that these also both meet criteria for neuropathic trait and are considered affected. And then an affected paternal grandfather with senile deterioration, presumably due to some kind of dementing illness like Alzheimer’s.\nOkay, third pedigree, this is obviously now four generations, so much richer. We have the core sib ship here, and here we have, it looks like a double proband family, so both nine and ten, which is our recurrent melancholia with suicide attempts and manic-depressive insanity. The sister and brother, and they have one affected sister who is described as “attack of depression with suicidal tendencies.” This is a homozygote by homozygous mating, with the father having had fainting spells, and the mother having been diagnosed with recurrent attacks of depression. There’s a paternal grandfather who is described as “money mad, very cruel, very miserly, wealthy, left much of his money to a housekeeper.” And then we have a maternal great-grandfather who’s described as alcoholic. We have a couple of individuals here who are marked as heterozygotes because of their obligatory office, by, because they had offspring.\nSo, number four had a daughter with fainting spells, not drawn on the pedigree, but the authors are willing to assume that she must be a heterozygote because of that. And this uncle had a feeble-minded and “queer” son, and again is assigned as an obligatory heterozygote. And then these parents, because they were intervening between these, must have been heteros to transmit the recessive trait.\nAnd this is the last and obviously a much broader pedigree. So here we have a very rich, dense pedigree in which number 13 is a woman who is the proband with dementia praecox. She has two affected older brothers, who are alcoholic, actually both of them affected. Younger brother with a highly nervous temperament, worries, and also with alcoholism, and number 11 worries over things. His sister with blue spells. The father is described as neuropathic with a highly nervous temperament and irritable. One of the uncles is a nervous temperament, alcoholic. Another uncle highly nervous. And then a sister who has a daughter who’s nervous, so she is considered to be an obligatory heterozygote. And then we have a very dense pedigree here in the metro line: fainting spells, shiftless, alcoholic, spending sprees, religious crank, alcoholic, and inferior make, a possibly epileptic, and then an affected maternal grandfather: alcoholic, cranky story.\nSo those are examples of the pedigrees that Rosenoff uses to reach the conclusions that this neuropathic trait, which they consider to be indicative of insanity or vulnerability to insanity, is indeed inherited as an autosomal recessive. So the next step is that in 1913, so about two years later, but there must have been some publication lag, David Herron and Carl Pearson write a three-part series of monographs entitled “Mentalism in the Problem of Mental Defect.” They didn’t mean this to include both what they would have called mental handicap and psychiatric illness, but Herron is the lead one, and the title of this is a “Criticism of Recent American Horror.”\nAnd this is the monograph that directly prompted the New York Times article. And we will, um, I’m going to just read this briefly, and again, if you want to stop the video and read it in more detail, but this is now from that front-page article. I’ll just give you a quote of what I think was written by the reporters in the New York Times: “A spirited attack has been made by Dr. David Herron of the Galton Laboratory, University College London, upon the entire body of American eugenics in general and the work of the Eugenics Record Office at Cold Spring Harbor, under the direction of Dr. Charles B. Davenport, in particular. The criticism, which finds the work of the American investigator biased, faulty, and exceedingly slip-shod, is issued in the form of a monograph, the essential content of which is published below, together with a reply by that cada Davenport himself”. So, Dr. Herron got most of his monograph published in the New York Times, not bad, and you can read the rest as you will.\nSo let me try to summarize a fairly lengthy, and I’m going to be simplifying a number of issues and not dealing into everything, but I’m trying to touch the high points here. So, what did Heron say about the problems with Davenport’s work? And this is both his mental handicap work, which I’m not going through, but on virtually all of it applies to the study of the insane diathesis as well. Quote, “As a defender of the young science of human genetics, which was at this historical stage nearly inseparable from that of eugenics, Heron describes a critical problem of the defenders, which he sees himself and, of course, his mentor, Carl Pearson, of the young field.” And now this is a direct quote: “They see, unfortunately, dogma outstripping knowledge. They see fallacious methods of reasoning applied to problems which are essentially statistical by numerous writers who lack the necessary training.” So, we will see that we’re going to be into some sort of heavy, rather personal descriptions in the ensuing monograph.\nLet me try to run through substantively what was Heron critiquing in Rosenoff’s work. He begins by focusing on the fieldwork itself, and notice here, many of these also apply and uses examples of Davenport’s earlier study of feeble-mindedness. First, Davenport published lots and lots of monographs, and one of them was a field manual for interviewers. Again, he hired this team of mostly young women that would go out, identify pedigrees. We don’t know anything about the specifics of the assessments, but they did contact them based on personal contact, and that was an important methodological advance. So, I don’t want to play down some of these important advances, and Davenport did track down Huntington’s career pedigrees, as well as his work in these other areas. But there are some rather concerning quotations from this field manual for interviews, and this is again a quotation with a highlight from Heron’s monograph itself, and again I will read this. “Some defects that the field worker will study,” I am quoting Heron’s quote of Davenport’s monograph from field workers.\nSo, this is his instructions: “Some field workers will study such defects as albinism and feeble-mindedness, are known as recessive defects.” So he’s assuming that feeble-mindedness, a pretty broad phenotype, is recessive. For example, by hypothesis, feeble-mindedness is, for the most part, a recessive trait, and the hypothesis must be tested as follows. So he’s telling the field workers how do you test for a recessive disorder. “You must find a person suffering from feeble-mindedness who is a descendant of two normal parents (hypothesis: both of the parents are simplex, meaning heterozygous). The field worker must understand that each parent will probably have somewhere in his or her ancestry a feeble-minded person, and it is the business of the field worker to make a special search for such person or persons in the pedigree.”\nNow, you can see why Heron would cry foul. Heron, now quoting from his monograph, says, quote, “It is difficult to understand how the field workers would fail to be prejudiced by these instructions, which appear to be telling the field workers the pedigree structures they are supposed to find.” Heron concludes, quote, “What faith are we to place in data collected in this way when we find that the field workers are instructed to, quote, make a special search, unquote, for those individuals who are necessary for the support of the Mendelian theory, and when the data are, quote, indexed in such a way that no exceptions to Mendelian rules can appear,” he is quoting from another version that I didn’t want to burden you with, explaining how the field workers had to find pedigrees that don’t appear to deviate from the expected Mendelian expectations.\nSo, pretty hard and well-supported concerns about field workers. Next, he addresses the vagueness of the phenotypes that Davenport and colleagues used, and again, Davenport publishes, quote, “The Trait Book of the Eugenics Record Office,” which you can also look up, a 52-page listing of their various phenotypes, and these will test my vocabulary here, but this is just from the eyes and the L’s, most of them here. This is quoting from Heron’s quote, “These are the phenotypes that they were interested in having their interviewers in fact. Impracticalness, adventurousness, disheartedness, unconversationalness, unacquaintanceness, ludicrousness versus absence of sense of humor, sublimity versus solidity, sweetness versus bitterness, coolness in emergency versus loss of head, cooperativeness versus aloofness,” and it goes on for page after page after page. Again, quote, “The use of the term ‘insanity’ in the titles of these two papers is very misleading.”\nParents says only a comparatively small proportion of the affected individuals are actually insane. These papers deal not with the inheritance of insanity but with the inheritance of what the authors call a neuropathic condition which is so comprehensive that it is a matter of surprise that there are any normal individuals at all. You notice a touch of sarcasm in his tone. “It is indeed a fortunate circumstance that the Mendelian theory requires the presence of some normal individuals,” end quote.\nAnd here again is a quote, bear with me, but I think this is important. “The neuropathic condition is based upon appearances and conditions manifest from infancy to old age, and the following are some of the conditions with the author’s opinion justify classification as neuropathic,” and they (what Heron does) and the saying that we did is we go through pedigrees, but he went through many more of them, and he obviously is picking out the ones that seem to be particularly unusual. God in infancy of convulsions, senile deterioration, dot of miasmas, Sister of Mercy in Australia said to have died of homesickness, quick-tempered, very in very queer lives alone, bores out cats, restless, fidgety, nervous, cold headache, sick headache, worry or Rambler, neurality, insomnia, dictatorial, selfish, not very bright, high-strung, odd, very quiet disposition, lost interest in life, etc.\nNow, I did my own research here. I picked by random 10 of Rosenoff’s pedigrees, and they had 73 affected individuals in those 10 pedigrees. And I classify them using their own description: 21 - their main definition was nervous, 13 were described as eccentric or clear, seven alcoholic, six high-strung, four cases of dementia praecox (which the law were probands), four cases of manic-depressive insanity (all probands), four imbecile or feeble-minded (so those were probands), and one is microcephalic. So, I think Karen is right that in the description, the vast majority of individuals in these pedigrees are not severely mentally ill individuals, so terming this the genetics of insanity can be questioned.\nThen Heron turns, and this is the hardcore statistical part of the argument, to two ascertainment problems. Davenport and Rosenoff never correct for the proband through which they ascertain the pedigrees, thus biasing upward rates of illness in that siblingship. Those of you that are not familiar should be aware that the early efforts to try to see especially for recessive patterns from familial traits that you find families with potential affected individuals, and this was mostly in the five to eight years right after Mendel’s work was discovered, kept getting ratios above 25. And it was Weinberg, a pediatrician, a German physician, who noted that you cannot count the affected individual. He developed what is his so-called program correction method, which, by the way, Luden adopts in his methods. But it does appear that neither Rosenoff nor Davenport were aware of this and the fact that counting the affected person, as they always do in the ratios, is going to bias upward in a substantial way. So that’s problem one.\nSecond, they don’t correct for the fact that larger siblingships have many more affected and are likely to be ascertained. And that, we, in fact, one of the pedigrees we saw was doubly ascertained, and they didn’t correct those. They have to be counted independently, which again Weinberg points out to get the correct findings.\nNow, interestingly, Heron doesn’t raise the problem that they don’t correct for age so that an individual who is unaffected and 25 years old, unaffected and 50 years old, probably should be kind of different. By any criteria, you will find that if you’re trying to get Mendelian segregation ratios for age-dependent penetrant phenomena, you’re not doing correct for age, you’re going to distort things. But Heron was not apparently aware of that, although Weinberg was.\nSo here, in summary, we believe that those who dispassionately consider the papers discussed in this criticism must conclude, with the present writer, that the material has been collected in an unsatisfactory manner, that the data have been tabled in a most slipshod fashion, and that the Mendelian conclusions drawn have no justification whatsoever. And when we find such teachings based on the flimsiest of theories and on the most superficial of inquiries, and noticing the rather ad hominem attacks here proclaimed in the name of eugenics and spoken of as entirely splendid work, we feel it is not possible to use criticism too harsh, in our words too strong, in repudiation of advice which, if adopted, must mean the death of eugenics, think also human genetics as a science. Okay, so that is Heron’s response.\nNow, I’m going to go through much more briefly the comments of Carl Pearson, who wrote the other two monographs. Pearson was far more diplomatic in his comments. Um, I’m only going to point out two things. One, he notes that especially the mental defect sections that really treating this as a dichotomy is probably not reasonable and that it’s better understood as a continuous trait. In fact, Pearson and the Galton lab did a number of correlational-based studies of estimates of intelligence and mental handicap. And then, he also expresses concern about the phenotypical assessments and, as we saw, the assumption which got counted in there of heterozygote status without evidence. That is the circular reasoning of counting the heterozygous implicatorially, which assumes the transmission pattern you’re trying to test.\nNow, let’s get into Davenport’s response. This is, in fact, from The New York Times directly, and then we’ll get into the monograph, but I think we can begin to get the tone of how Davenport responded here, quote, “A sweeping condemnation of American work in the science of eugenics is issued by the Galton Laboratory of University College under the direction of Dr. Carl Pearson.”\nThe specific criticism of American methods and American findings being presented by Dr. David Herron, note the nationalism implied here, this condemnation which finds the contribution of America based on worthless, slipshod research and filled with unwarranted conclusions is based chiefly on the Eugenics Record Office bulletins. Being myself responsible for the work of the Eugenics Record Office, which includes not only the actual laboratory work done at Cold Spring Harbor during the past three years, but the active collaboration of institutions and individual scientists throughout the country, think Rosenov, I feel like it is my task to explode the false and injurious impressions sure to be created in the popular mind by the broadcast criticism issued from the Galton Laboratories. So, again, pretty directly attacked.\nNow, let’s switch to a couple of months later. So Davenport didn’t really rest on his hunches here. He published a brief one-page response in Science magazine, which again you can read either here’s the reference here, but I am going to jump to some quotes from this.\nYou can, again, see the general spirit of the vocabulary, and this is how he begins it: “The all-too-familiar blessings of Professor Carl Pearson about Mendelians have recently been continued by this understudy, Dr. David Herron, and directed toward American work in general and that of the undersigned in particular. Like my colleagues in this country, I should have remained silent under the attacks, knowing that discriminating men of science in this country, as well as in England, recognize their true animus that they lie outside the pale of science.”\nAnd then a couple of other quotes from that short Science article: “The numerous errors to which he calls attention fall, for the most part, into three categories based on misunderstandings so gross of the critics, on the critics’ part, as to render it difficult to believe they are not intentional. A critic who is guilty of such extensive, stupid, captious, and misleading criticism can hardly expect a scientific consideration of other points he raises.” So pretty strong language. It will be futile, as a biologist, to attempt to show to the applied statistician his errors.\nSo, noting the contrast, biology, good, positive. We’re going to get to the biological root. Statisticians just deal with numbers, not real biological facts. Quote, “Genuine scientific criticism has always been useful in the advancement of science, but friends of Galton must regard it as a tragedy that the fortune of one of the largest-minded and most fertile men of science,” (small digression: Davenport liked Galton a lot, and in fact, Galton was surprisingly ambivalent in this Mendelian-Biometrician controversy, kind of switching from one side to the other because he liked some of the concepts of Mendelians) “so he, Davenport, is praising Galton and derogatory toward his main pupil and mentee, Carl Pearson. So Galton should be supporting a laboratory one of whose leading members (think Pearson) spends much time making elaborate researchers into his delusions concerning the blunder of others, instead of making positive discoveries in a field where so little is known and where the need for utilizable knowledge is so great.”\nOkay, so now let’s go to the last article I’m going to consider in detail, which is Rosenov’s response. Rosenov wrote a longer article in 1914, so again, moving pretty quickly given publication lag, and the American Journal of Insanity that in 1921 changed its name to the American Journal of Psychiatry that many of us know well. So here are some selective quotes:\n“In reading Dr. Herron’s pamphlet,” this is now Rosenov speaking, “what is struck, first of all, by the unusual temper of the attacks, apparent on almost every page. The work of Davenport, Toomey, Goddard, who worked mostly on mental handicap, and some others are analyzed in a fashion are referred to without a single feature being found in them to be worthy of anything but unreserved condemnation from the critic.” He quickly, however, and I think this is the most interesting part, gets to see what he sees as the heart of the dispute.\nNow, I find it intriguing that it is Rosenoff, the psychiatrist, and not Davenport, the real-deal geneticist, who makes these points. The unfortunate position in relation to the scientific world of the English biometrical school, to which Dr. Herron belongs, may account in some measure for the temper of the attack. They have, by reason of a pride in their own tradition, that think correlations refuse the guidance of the light of mentalism, continuing to devote their time and labor to the investigation of the heredity of various traits in man, as well as in plants and animals, by purely statistical methods (note that’s not used in a praiseworthy manner). While biologists, note the contrast, all over the world, were piling up evidence of observation, experiment, continuously adding to the support of Mendel’s theory, eventually, it came about that the work of the Galton Laboratory has been valued by the scientific world for the development of refined statistical methods and not as a biological contribution to the subject of heredity. So, Rosenoff really kind of calls it as he sees it, that is, the statistics versus biology controversy that I think, in many ways, fed this scientific and eventually personal controversy.\nHe goes on. Rosenoff then quotes Bateson, who, up until now, we haven’t heard his name, but in the English world, he is the leading member of the Mendelian group, very vigorously against Pearson but not about psychiatric illness. By March, and we’re now quoting from Bateson, “Of the so-called investigations of heredity by extension pursued by extensions of Galton’s non-analytic method and promoted by Professor Pearson and the English biometrical school, it is now scarcely necessary to speak. That such work may ultimately contribute to the development of statistical theory cannot be denied. But, as applied to the problems of heredity, the efforts have resulted only in the concealment of that order which it was ostensibly undertaken to reveal.” In other words, statistical methods will never get to the true genetic factors; only the Mendelian approaches will be successful.\nFinal criticism, which focused on Heron’s lack of clinical training, suggesting that is a universal agreement. We’ll see. So this is it. It was the last. Rosenoff: “The burden of proof is upon the critic who, though a layman, assumes an attitude in relation to his psychiatric issue diagnosis which is in opposition to a view universally held by psychiatrists.” That’s simply factually incorrect. In fact, most workers in the field at this point would have focused more on the genetic transmission of specific disorders, although some others did emphasize these very broad kinds of diathesis. So coming back to the quote, “And if he furthermore attempts to disqualify on the basis of his attitude work which, in his opinion, contains too large a margin of error, he must, in addition, take the burden of furnishing an acceptable measure of the error before his criticisms can be rendered valid. This our critic has not done.” She was basically saying he’s got no business criticizing my psychiatric judgment about the breadth of these phenotypes that he studied.\nAll right, so let me now summarize what I’ve taken you through. I’ve left out a lot of detail, but I think I’ve touched the high points. So first, it’s pretty obvious there’s a high level of acrimony, harsh, rather derogatory language on both parties. We’ve got a mixture of some substantive scientific critiques, but quite a lot of ad hominem attacks, not the best way to do science, I have to say. This is certainly started by Heron, but Davenport and to some extent Bateson give it back in measure, and Rosenoff, you know, still has, uh, takes his gloves off to some extent.\nThere are a couple of articles in the reference, one particular contemporary commentator who praises Heron’s science but castigates Heron for this derogatory approach. But it’s very interesting, I found an obituary of Heron written by Egon Pearson, that was the little curly-headed kid at the left of the Pearson picture, and he wrote, quote: “Recently when I asked Karen how he felt looking back about these years, and I think in his academic career, this controversy was pretty substantial, he replied to the effect that as a young man he enjoyed a good fight.” So I think we’d get the response from Heron as an older person that he obviously enjoyed some of the more pugilistic intellectual combat that he engaged in, and yeah, it got started with his monograph.\nI think, having studied this a fair bit, that there’s quite substantial evidence that Davenport used biased field methods, and this is true across a number of the monographs. So, I think there is circular reasoning in the way he trained field workers, training field workers to go out, especially, and find the pedigrees that validated their mendelian assumptions.\nI think that the phenotypes used and the associated assessment methods simply strain credulity. I don’t have time to present you, and this is a trend of Davenport. Davenport wrote his most notorious monograph, which is called “Nomadism or the Wandering Impulse,” in which he determined was a sex-linked recessive with affected individuals. And this is again, I read from this pedigree table, having phenotypes including Western Desperado, sailor, traveling salesman, itinerate Tinker, canal boat captain, as manifesting this sex-linked recessive trait called nomadism.\nAnd again, although this is not pointed out in this debate, several other psychiatrists, including Abraham Myerson, an important figure in psychiatric genetics during this time period in America, also strongly disagreed with Rosenoff’s very extended diagnostic pattern, saying these are completely impossible. So, Heron was not the only one challenging this.\nThere are methodologic errors for Davenport and Rosenoff that are really quite clear-cut, that is, their simple failure to correct for ascertainment biases. This has been published in German by Weinberg a few years before. What can explain that given that Davenport, certainly his reputation, was based on studying human pedigrees, so he was not at that point on the cutting edge of the algebraic problems of how you get mendelian ratios, and one has to fall to them, and again, that no age correction was performed. And here we see the much greater methodologic sophistication of the Louden School compared to what Davenport was doing. So if I had to judge (but you can all reach your independent opinions), Heron is considerably more right than wrong in his critique of Davenport and Rosenoff, and history, by and large, supports this judgment about the validity of the scientific work and particularly the claim of a clear demonstration of autosomal recessive inheritance of this so-called diathus that they study.\nNow, I don’t think this forgives Heron’s tone, but it is a hard for modern readers not to share some skepticism of these methods. To take a step back, what led David Port and Rosenoff to these misjudgments? My own view is that this was a case of what you might call mendelian zealotry and over enthusiasm for mendelian models that can distort scientific judgment. But again, you should reach your own conclusion. Does this have implications for the subsequent history of psychiatric genetics? Well, I think it does. For example, in the late 1980s, this all followed the 1983 demonstration of Huntington’s chorea by Jim Guzella using RFLPs. There was a rush of interest and enthusiasm of linkage jobs for psychiatric disorders, and these, of course, depend on mendelian, or, you know, we call single major locus models of large effect mendelian-like transmission.\nSome of us in the field (a lot’s participated, myself included) talked about the methodologic difficulties, the fact that we don’t have evidence from Indian-like segregating pedigrees for these, that we ought to be worried about false positives, and we certainly shouldn’t be studying very small numbers of pedigrees. Those of you old enough to know, there were three very high-profile false report results of mendelian segregation using linkage analysis for manic depressive illness in 1987 and one for schizophrenia in 1988, and these are the three of them: Baron et al., Sherrington et al., and Janice Egeland, all very high-profile and very disturbing to the field. Gradually, the non-replications came in.\nSo, in my view, it is hard to underestimate the strength of the lure of mendelian models for Psychiatry Genetics. Psychiatrists are craving widespread scientific credibility; they want to demonstrate that genetics is appropriate to find a form and a mendelian model is so attractive to them. And this was a time again, in the 1980s and early 1990s, where twin research was also blossoming.\nWe had the first interview assess population-based studies, both with David Folker at IBG and Linda Neves at VCU. We had wonderful quantitative models fitting these liability threshold models, and I personally experienced tensions between the twin and the molecular researchers, including what you would see at earlier meetings of the World Congress of psychiatric genetics. And if I had to characterize it, if you were listening to each group critical of the other, you had the two following stereotypes: the mendelians, mostly people doing linkage analysis, when they want to put down the Twin researchers, used this phrase, “you’re not doing real genetics, you’re just looking at statistical models,” which of course completely echoes what we saw 60-70 years earlier in the Dilemma that we’ve seen here between the biometricians and the mendelian. So really, what goes around comes around.\nWhat were the Twin researchers saying of these people doing linkages, sometimes on three, five, or eight pedigrees? Quote, “you’re just a star-struck gene jock fitting models that we don’t know don’t apply.” So we have the same kind of ad hominem attacks between these two fields that have visited us again. So the tension between mendelian and biometrical genetics has been on the root of psychiatric genetics for several Generations now. And you would give me a minute to say, I think there’s a deep irony in the evidence that has emerged from Gua studies in recent years, that is the Pearson fissure model of a normal distribution of liability due to many genes of small effect has actually turned out to be right.\nSo, the mendelians were wrong in all their claims that “you’re just studying statistics” and the real biological understanding is only going to be through classic mendelian models. And it’s very pleasing to see, in some important ways through PRS scores through samgy was, that we are now seeing a bit of a merger of these fields. Have we finally calmed down from this kind of internessing argument that we’ve had with one another across the generations?\nSo, before I conclude, what happened to the main players in this little story? Well, Charles Davenport gradually lost influence. He was an ardent eugenicist and was influential in immigration restrictions from Southern and Eastern Europe during the 1920s and 1930s. He retired in 1934. The Carnegie Foundation, which was really supporting all of this, withdrew critical support for the Cold Spring Harbor laboratory in 1940. In part, because the enthusiasm of the Eugenics movement in the United States diminished substantially with the rise of the Nazis in Germany from the mid-1930s on. Davenport did not reduce his enthusiasm for that.\nDavid Herron, on the other hand, left academia in 1915 and worked for an insurance company in England. He went on to be the president of the Royal Statistical Society and had a distinguished career, but he was not doing this kind of work.\nAaron Rosenov went on to a distinguished academic and administrative career in psychiatry. He led the first U.S. twin studies of schizophrenia and expressive illness, so he reverted to more traditional diagnostic-based approaches, and he was sufficiently well-known to be appointed the state Commissioner of Lunacy in California in 1933. He also wrote a textbook, so he did relatively well for himself. Pearson remained the Galton share of eugenics at University College London until his retirement in 1933. He was at that point, although perhaps a little bit eclipsed by Ronald Fisher, one of the giants of statistical work. In June of 2020, UCL announced that it was renaming two buildings which had been named after Pearson because of connection with Eugenics (and for all of these authors you will read parts of here where they are very enthusiastic about eugenic pressures, and that’s a whole other talk). Last word in the New York Times, Heron got his last point. So this is now January 4th, so it’s like a month later, and you can see Heron accused the head of the American Eugenics record office of callousness, inconsistency, and misinformation, and says that the following of his advice would mean the death of eugenics as a science.\nSo here is I am writing a paper on this, so this is an extensive bibliography on this page, and last, so again you can freeze your frame if you want to look at these in detail, and this is a rich area for those of you that have historical interests. I appreciate your attention, thank you."
  },
  {
    "objectID": "chapter4.html#sec-section1",
    "href": "chapter4.html#sec-section1",
    "title": "Chapter 4: Study designs",
    "section": "4.1 Epidemiological study design",
    "text": "4.1 Epidemiological study design\nTitle: Epidemiological Studies: A Beginners guide\nDescription: This video gives a simple overview of the most common types of epidemiological studies, their advantages and disadvantages. These include ecological, case-series, case control, cohort and interventional studies. It also looks at systematic reviews and meta-analysis.\nPresenter(s): Ranil Appuhamy, James Clark, Let’s Learn Public Health\nLength: 9:42\n\n\n\n\n\n\n\n\n\nLink to video transcript here."
  },
  {
    "objectID": "chapter4.html#sec-section2",
    "href": "chapter4.html#sec-section2",
    "title": "Chapter 4: Study designs",
    "section": "4.2 Confounding, Chance, and Bias",
    "text": "4.2 Confounding, Chance, and Bias\nTitle: Confounding, chance, and bias\nDescription: This video provides an overview over three types of error that can lead to misleading or incorrect results in medical studies: confounding, chance, and bias.\nPresenter(s): Cochrane Austria, Department for Evidence-based Medicine and Evaluation, Danube University Krems\nLength: 9:50\n\n\n\n\n\n\n\n\n\nLink to video transcript here."
  },
  {
    "objectID": "chapter4.html#sec-section3",
    "href": "chapter4.html#sec-section3",
    "title": "Chapter 4: Study designs",
    "section": "4.3 Genetic study designs",
    "text": "4.3 Genetic study designs\n\nTwin studies\nTitle: What are “Twin studies”?\nDescription: In this animation, we explore how studying many pairs of twins allows us to estimate how much a trait comes from our genes and how much is influenced by the environment.\nPresenter(s): OpenLearn from the Open University\nLength: 2:13\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\n\n\nChoosing the Right Study Design in Genetic Epidemiology\nTitle: Choosing the Right Study Design in Genetic Epidemiology\nDescription:\nPresenter(s): Duncan Thomas, University of Southern California\nLength: 34:31\nLink to video here.\nLink to video transcript here."
  },
  {
    "objectID": "chapter4.1_transcript.html",
    "href": "chapter4.1_transcript.html",
    "title": "Chapter 4.1: Epidemiological study design (Video Transcript)",
    "section": "",
    "text": "Title: Epidemiological Studies: A Beginners guide\nPresenter(s): Ranil Appuhamy, James Clark, Let’s Learn Public Health\n[Music]\nhello and welcome to this video on epidemiological studies we’re going to have a quick look at epidemiological studies what they are a few different types of studies their advantages and disadvantages\nWhat is a study?\nfirst of all what is a study well simply put a study is a scientific process of answering a question using data from a population it can be any question for example does smoking caused cancer is there more disease in an area compared to another or what food is responsible for causing an outbreak so the first thing to do in any study is to have a study question what are we trying to find out then we need to figure out what the best type of study is that would help us answer the question once we’ve decided on a study type we need to do the study collect and analyze the data using a suitable statistical method then we need to interpret the results to make sense of it all and finally we need to report the results of our study importantly all studies need to be done in an ethical way now let’s have a look at a few different study types\nECOLOGICAL STUDY\nan ecological study is the type of study where measurements like disease rates and information about exposures are made on a group of people the groups can be as small as people in a house or as large as people in an entire country it’s important to remember that results and conclusions from ecological studies apply to a group and not to individuals ecological studies are useful for comparing the health of populations in different places such as measles rates in Australia and New Zealand or at different times they’re also useful for generating questions and highlighting issues that can lend themselves to future investigations or studies\nCASE SERIES\na case series describes the characteristics of a group of people who have the same disease or the same exposure the aim of this is to understand the demographics clinical presentation prognosis or other characteristics of people who have a particular disease or describe something on you for example in the early 1980s the occurrence of an unusual pneumonia in men led to the recognition and identification of HIV a cross-sectional study takes a selected population and measures health information at a given point in time giving a snapshot of their health it usually involved asking participants a series of questions using a questionnaire health surveys that collect health information about people in a population is an example of a cross-sectional study because these studies commonly measure how many people have a disease at a particular point of time they’re also called prevalence studies it’s important to make sure that the selected population is representative of the total population\nCROSS SECTIONAL STUDY- prevalence studies\ncross-sectional studies are relatively inexpensive and easy to conduct compared to other studies they can provide information on multiple exposures and outcomes and are a good way of assessing the health needs of a population however because the information is collected at a single point in time it cannot be used to determine whether a particular exposure caused the disease or not\nCASE CONTROL STUDY\na case control study starts off with cases these are people with a disease it uses a comparison group called controls who are similar to cases but do not have the disease then both groups are asked about their previous exposures to different risk factors now for each of the risk factors the odds of being exposed if they were a case is compared to the odds of being exposed if they were a control this is called an odds ratio an odds ratio of more than one means that people with the disease are more likely to have been exposed to that risk factor than people without the disease this suggests that it could be a possible cause of the disease an odds ratio of less than one suggests that it’s a protective factor and one suggests no association case control studies are commonly used in foodborne outbreak investigations for example we can compare the odds of eating different kinds of food between people who fell ill after consuming a meal and those who didn’t if the odds ratio is greater than 1 for a particular type of food then it’s possibly the cause of the illness a major advantage of a case control study is that they’re often quick and cheap to do also because they start off with cases it can be used to study uncommon diseases however because these studies involve small numbers they’re not good to study rare exposures one of the challenges in a case control study is to find suitably matched controls also because studies ask about exposures in the past and people might not be able to recall their exposures accurately\nCOHORT STUDY\nlet’s have a look at cohort studies now in a cohort study a group of people has followed over a period of time to see what happens to them and information about risk factors is collected we can then compare the risk factors occurrence of an outcome like disease in those who are exposed to a particular risk factor to those who are not exposed to that risk factor the main measurement used in cohort studies is called the relative risk a relative risk is the ratio between the risk of disease in the exposed group compared to the risk of disease in the unexposed group a relative risk of greater than 1 means that the exposure is associated with an increased risk of the disease if it is 1 it indicates that the risk is the same and if it’s less than 1 it indicates that the risk is lower a well-known cohort study is the British doctor study done in the 1950s where a group of doctors were followed up for many years this study provided valuable scientific evidence of the harmful effects of smoking especially the link between smoking and lung cancer one of the main advantages of cohort studies is advantages that the time sequence of events can be determined this is useful when trying to determine what caused a disease another advantage is that information about several different outcomes and risk factors can be collected at the same time this allows for some analysis to be conducted on the data a disadvantage of cohort studies is the high cost and they can involve a large number of people being followed over a long period of time they’re generally not suitable to study rare diseases a challenge in cohort studies especially ones that are conducted over a long period of time is ensuring that people who started the study stay until the end of the study if many people drop out it will affect the results of the study\nINTERVENTIONAL STUDY\nnow let’s look at another type of study an interventional study in an interventional study an intervention is done on a group of people and the outcome studied examples of interventions can be giving a medicine a vaccine or health advice the outcome can be things like a change in disease status or change in behavior a randomized controlled study is the best study design for an interventional study say we want to study the effects of a new drug we start off with a study population people are then randomly allocated to be in the intervention group where they receive the drug or in the control group where they don’t receive the drug then after a suitable time period the outcomes in the two groups are assessed and compared to see what effect the drug had outcomes can be things like a change in disease symptoms or death rates ideally neither the participants nor the investigators should know which group received the intervention this is called double blinding and ensures that neither of them can influence the outcome of the study the main advantage of a randomized control study is that it can provide good evidence that the intervention led to an outcome randomization ensures that both groups have an equal chance of receiving the intervention and that they have similar characteristics this way the effect of the intervention can be determined without other factors influencing the outcome the disadvantages are that these studies are generally quite expensive to do it may also require a large number of participants it may not be suitable in some situations we’re not giving an intervention may be inappropriate or unethical\nSUMMARIES\nfinally sometimes it’s hard to keep up with all the studies that have been done on a particular topic but thankfully there are studies that summarize other studies there are two main ways that this is done a systematic review systematically identifies all the relevant studies on a particular topic assesses the quality of each study synthesizes and interprets the findings and presents an impartial unbiased and balanced summary of the evidence a meta-analysis uses data from all the studies that have addressed the same question and have a similar study design it then uses the data from these studies to do a combined statistical analysis and produces a single summary result and that’s an overview of some of the commonly used epidemiological study types we’ve had a look at what a study is different study types their advantages and disadvantages"
  },
  {
    "objectID": "chapter6.html#polygenic-risk-scores-introduction",
    "href": "chapter6.html#polygenic-risk-scores-introduction",
    "title": "Chapter 6: Polygenic Scores",
    "section": "Polygenic Risk Scores: Introduction",
    "text": "Polygenic Risk Scores: Introduction\nTitle: Polygenic Risk Scores\nDescription: Polygenic risk scores in a nutshell. This video is designed to explain PRS to a lay audience and answers the question why we care about PRS and what we could do with the knowledge of PRS to improve human health.\nPresenter(s): Center for Personalized Medicine, Oxford University\nLevel: Beginner friendly\nLength: 4:03\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\nTitle: Polygenic Risk Scores\nDescription: Introduction to polygenic risk scores. This video explains how to calculate PRS and how to interpret PRS in a clear and easily understandable way.\nPresenter(s): Till Andlauer\nLevel: Beginner friendly\nLength: 4:36\n\n\n\n\n\n\n\n\n\nLink to video transcript here."
  },
  {
    "objectID": "chapter6.html#polygenic-risk-scores-in-detail",
    "href": "chapter6.html#polygenic-risk-scores-in-detail",
    "title": "Chapter 6: Polygenic Scores",
    "section": "Polygenic Risk Scores: In detail",
    "text": "Polygenic Risk Scores: In detail\nTitle: Polygenic risk scores\nDescription: A more detailed introduction to polygenic risk scores\nPresenter(s): Adrian Campos, Adrian.Campos@qimrberghofer.edu.au\nLevel: Intermediate\nLength: 22:21\n\n\n\n\n\n\n\n\n\nLink to video transcript here."
  },
  {
    "objectID": "chapter6.html#polygenic-scoring-methods",
    "href": "chapter6.html#polygenic-scoring-methods",
    "title": "Chapter 6: Polygenic Scores",
    "section": "Polygenic Scoring Methods",
    "text": "Polygenic Scoring Methods\nTitle: Polygenic Scoring Methods: Comparison and Implementation\nDescription: Comparison of different PRS methods I. This video compares different available PRS methods on their performance and required computational time (ends at 12:30) in the UK Biobank. Towards the end, Dr. Pain introduces their new software to calculate standardized individual polygenic risk score based on genetic ancestry.\nPresenter(s): Oliver Pain\nLevel: Intermediate\nLength: 21:14\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\nTitle: Polygenic risk scores: PGS comparison\nDescription: This is another video comparing different PRS methods on their performance in the cohorts in the Psychiatric Genomics Consortium (i.e., schizophrenia cohorts). Dr. Ni gives toy examples to explain the different parameters they used to evaluate how well each PRS method performed.\nPresenter(s): Guiyan Ni\nLevel: Intermediate\nLength: 13:52\n\n\n\n\n\n\n\n\n\nLink to video transcript here."
  },
  {
    "objectID": "software_gwas_transcript.html",
    "href": "software_gwas_transcript.html",
    "title": "Software Tutorials: GWAS (Video Transcript)",
    "section": "",
    "text": "Genome-wide Association Studies\nTitle: Genome-wide Association Studies\nPresenter(s): Hailiang Huang\nMy name is Hailiang Huang. I’m a statistical geneticist at the Broad Institute. I’m very happy today to do this she was tutorial with you so the first thing uh let’s clone the GitHub I have created for you the GitHub repository that I have created for you with the data and tutorial materials please type this in your comment line in your command line in the terminals repeat clone and the location of the repository under my first and last name and G1 tutorial it will take a while but it should be very fast all right this is done let’s get into the location\nuh the folder you just cloned and have a look at all the files I would recommend you to open the gos on the squad tutorial and you know text editor so that you can just simply copy and paste everything from the text editor to your terminal all right I have opened this in one of my text editors and just notice uh the few information here the information here uh so first where you can download this GitHub repository in case you have any questions feel free to contact me and this for this tutorial you have to install the p-link 2 which can be retrieved here and R which can be downloaded from the r project website and also we use the qqman package in R so please have it installed in your R program\nall right let’s get started so first let’s take a look at the data so this is the one of the two data fires it has individual level genotype if you use the head command you will see the first few the first few rows in that file uh the first two columns are the individual ID and the family ID the next two columns are the IDS for the parents of that individual and the sex and the diagnosis after the first six columns the rest of the columns have the genotype of that individual which you can easily tell and in case that genotype is missing we have zero to indicate the missing genotype and the other file is called a map file it basically has a list of snips and each row here corresponds to the you know every two columns here uh the map one-on-one so that uh peeling will know which snip this data set is\nall right so the first thing we typically do after we see this data set is to convert this to Binary what you saw is very convenient for you to you know take a look at what the data is but there is a format called binary format that is more efficient in terms of the storage and Analysis all you need to do is to indicate your input file which can be done by the dash dash file and you can use the command dash dash makebet to convert this text file to Binary format and then indicates you know where you want this binary format to be written to in the dash dash out command all right this is done and if you compare the size of the fire before and after uh the conversion to the binary file here is how you can do that so you can see that before conversion you have 77 megabyte for the fire with the genotypes and after the conversion you only have 4.7 megabytes so the conversion feels still more than uh you know 10 times space\nall right so let’s start some QC the first thing we want to do is to remove the individuals that has a high missing rate you can do that by using the appealing uh dash dash missing command the dash dash B file simply it has p-link where you know the input files should be the dash dash out Health Link where you know peeling has to write the output bias to so you use the dash dash missing command to calculate the business and here is what the output file looks like it basically has a list of individuals with their missing rate and here uh you know the auto command filtering on the mixing rate in the fixed column uh you can tell we have two individuals with more than two percent missing rates missing rate typically a high mixing rate typically indicates problems in the sample quality so it’s not a bad idea to remove the samples with high missing rate\nall right so uh the next we want to do is to remove samples with high heterozygosity rate and this can be done by calculating the hydrode using the dash dash head command and if you take a look this is the output from that calculation it’s basically a list of individuals with their heterozygosity rate I have prepared a r script for you to put together individuals that fail either the missing weight test or the heterozygosity test and for considering the time I’m not going to go into the details of that R script but you can just call that our script by doing this and that script where we will write a list of individuals with extremely high missing rate or with heterozygosity rate that are either too high or too low to this fire and all you need to do is to use a dash dash remove command to remove that list of individuals and tell pilling that you want to write the new data file to this file\nall right so we have removed uh individuals that have a high missing rate but we also have to do that for the Snips for to remove the Snips that have a high missing uh genotype rate uh we can do that by leveraging the other output file from the dash dash missing EP link and that is a high missing bio which has a list of the Snips with their missing weight and we decide to use five percent as a threshold here we write all these things we want to exclude in this fire and we use the p-link dash dash exclude to remove such sleeves so pilling remove removes individuals and the p-link dash dash exclude exclude the snips all right this is done we now have the second iteration of QC with the Snips with high missing weight removed the next we want to do is we want to find Snips with high with differential missing rate uh the missing rate is different in cases versus in controls we do so by using the dash dash test Dash missing here and take a look at what is inside you have a list of the Snips with their missing weight in cases as affected and controlled as unaffected in this case we don’t want to use the p-value because the sample size is too small for the p-value to be helpful we simply want to remove the Snips with five percent difference between cases and controls and this can be done by subtracting the difference of subtracting for the difference between the missing weight in cases versus controls and using five percent as a threshold we write everything we want to exclude here and we will Ex we will uh you know remove that lift up the Snips here\nall right the last thing we want to do on the Smiths is the Hardy Weinberg equilibrium test and this can be done by using the dash dash Hardy command this is a list of snips with their Hardy Weinberg marker P value and we want to only use the Hardy Weinberg test for the unaffected samples and we want to use the p-value of 10 to the minus six as a cutoff we filter for such Snips and we use peeling to remove those snips\nall right now you have a data set that are relatively clean for individuals and for sleeps what’s next the next is to remove individuals that are related to each other but to do that we need to create a data set that is uh independent in terms of genetic variance we can do so by using the dash dash independent pairwise and this is a bunch of current parameters go into the LG pruning process including the window size the step of the window that is moving and also the threshold you want to use for configuring two variants that are in LD with each other\nso after you run this command you will have a list of snips that peeling would recommend you to keep as independent Snips this is in the fire called Dash prune Dash in all you need to do is to generate a new data set with only these variants by using the dash dash extract command that will give you a independent you know set of markers and in a in a new p-link binary format and then you want to use the dash dash genome file to calculate the relativeness between samples and take a look at what is in that output file which is just a pair list of pairs of individuals with their relatedness typically a related means of hi-hat greater than 0.2 indicates something concerning we want to capture that pairs using this command write them to this file and we use the peeling dash dash remove as we did before to remove such individuals\nall right so let’s take a look at how many variants and how many individuals we have removed it thinks that with the data is pretty good before QC we had over 100k 100 individual Snips and 200 subjects after QC we got rid of less than 1000 variants and eight variants\nall right so now let’s take a look how well we did in the QC uh we use the word count Dash L to count how many variants we have before an after QC and how many individuals we have before and after QC so it looks like the data was pretty good um his accusing removed roughly 1000 variants which is very normal uh we started with over a little over a hundred thousand and we ended with uh 99.7000 which is not bad we removed eight individuals from the 200 to 192 which is not bad as well all right so far so good so what next the\nPrincipal Components Analysis (PCA)\nnext is to generate uh the principal components so that we can include them as covariates to control the population structure so this is how you do that you need to create a new independent data set independent in terms of the variance and you want to generate that based on the latest file you have created remove removing the related individuals you do that similarly as you did before using the dash dash independent pairwise command then you create a new file uh the file that only has the independent variance like this and then you print you create the principal component plot sorry you created the print you perform the principal component analysis using the dash dash PCA command all right uh this is output from that command the uh dot angle value and Dot eigenvector the dot eigenvector has what you need for the downstream analysis so let’s take a look it has a list of individuals with principle components a lot of them\nRunning GWAS association test\nso what you do next is to perform your G was with these principal components and this can be done using the dash dash logistic because this is a case control study you want to use the logistic regression the height cover basically asks appealing not to write the coefficient for the covariance because you for this study you only hear about the variance uh the coefficient for the variance not the covariance um this is uh this is to help the peeling where your covariance is which file has a covariance and this is to calculating how many components how many principal components you should include as the covariance and here we do one to ten all right let’s do it\nall right it’s done so the dot logistic file has a list of all the variants with their auth ratio standard error the confidence interval and the p-value this is exactly what you need from a givas and we could use this little script I wrote in R to do some visualization that script also calculates the genomic genomic inflation factor which is only 1.06 looks like the genome has been very well controlled and let’s look at the two figures it has generated the Manhattan plot let’s open it and the qqplot all right so this is a Manhattan plot this is the QQ plot you can tell that the genome The genome has been well controlled you don’t really have no inflation but unfortunately for this particular given study we don’t have a genome-wide significant signal uh for now we do have you know one signal uh surpassing the suggestive significance threshold uh suggesting that we may need more samples to uh do to to to for this device to identify the same significant Locus all right that is a very quick tutorial and I hope it helps for free our research thank you for your attention\n\n\n\nGenotype QC\nhello I’m Johnny Cohen and in this brief presentation I’m going to discuss some key points concerned with running quality control on genome-wide genotype data which is a common first step in running a g was I’m going to provide a theoretical overview addressing the overarching reasons why we need to do QC highlighting some common steps and discussing a few pitfalls the data might throw up I’m not going to talk about conducting imputation or G was analyzes or secondary analyzes nor am I going to talk a great length about the process of genotyping and ensuring the quality of genotyping calls I’ll similarly not go into any deep code or maths however if you are starting to run your own QC and analyzes I recommend the pgc’s rickapili automated pipeline as the starting point there are also some simple scripts on my group’s GitHub that may be useful as well they follow a step-by-step process with codes and explanations we’re currently updating this repository so look out for some video tutorials there as well so here is our starting point I’ll be using this graph on the top right several times through this talk and this is a genotype calling graph with common homozygotes in blue heterozygous in green and rare homozygotes in red hopefully your data will already have been put through an automated genotype calling Pipeline and if you’re really lucky and overworked and underappreciated bioinformatician may have done some manual recalling to ensure the quality of the data is as high as possible but in point of fact the data you’ll be using won’t be in this visual form but rather as a numeric Matrix like the one below that Snips and individuals this might be in the form of a blink genotype file or its binary equivalent or it’s in some similar form that can be converted to the blink format and where we want to go is clean data with variants that are called in the majority of participants in your study and won’t cause biases in Downstream analyzes that should give a nice clean Manhattan pot from G was like the one below rather than the Starry Night effect of this poorly Q Siege Manhattan blunt above however something I’d like to emphasize across this talk is that QC is a data informed process and what works for one cohort won’t necessarily be exactly right for another good QC requires the analyst to investigate and understand the data often the first step is to remove rare variants and this is because we cannot be certain of variant calls consider the variance in the circle on the right are these outlying common homozygates or are they heterozygates we cannot really tell because there aren’t enough of them to form a recognizable cluster typically we might want to exclude variants with a low minor allele count for example five there are many excellent automated calling methods to increase the amount of certainty you have in these variants but it’s also worth noting that many analytical methods don’t deal well with rare variance anyway again the demands of your data determine your QC choices it may be more useful for you to call rare variants even if you’re uncertain of them or you may wish to remove them and be absolutely certain of the variance that you retain next we need to think about missing data genotyping is a biochemical process and like all such processes it goes wrong in some cases and a call cannot be made this can be a failure of the genotyping probe or poor quality of DNA or a host of other reasons but such calls are unreliable and they need to be removed missingness is best dealt with iteratively to convince you of that let’s examine this example data we want to keep only the participants which are the rows in this example with complete or near complete data on the eight variants we’re examining which here are shown in the columns so we could remove everyone with fewer than seven snips but when we do that oh dear we’ve obliterated our sample size so instead let’s do things iteratively so we’ll remove the worst snip again variant 7 goes and then we remove the worst participant bye bye Dave then we remove the next word snip so that’s snip two and now everyone has near complete data and we’ve retained nearly all of our cohort so this was obviously a simple example how does this look with real data so here we have some real data and it’s it’s pretty good data most variants are only missing in a small percentage of the cohort but there are some that are missing in as much as 10 of the cohort so let’s do that iterative thing removing variants missing in 10 of the individuals and then individuals who have more than 10 missing variants and then nine percent and so on down to one percent when we do this the data looks good nearly all of the variants are zero percent messiness and those that aren’t are present in at least 578 to the 582 possible participants and we’ve lost around 25 participants for about 22 and a half thousand snips but what if we didn’t do the iterative thing and we just went straight for 99 complete data so when we do that the distribution of variance looks good again arguably it looks even better and we’ve retained an additional 16 000 variants but we’ve lost another 40 participants which is about six percent more of the original Total than we lost with the iterative method typically participants are more valuable than variants which can be regained through imputation anyway but this again is a data-driven decision if coverage is more important than cohort size in your case you might want to prioritize well genotyped variants over individuals so we’ve addressed rare variants where genotyping is uncertain and missingness where the data is unreliable but sometimes calling is simply wrong and again there are many reasons that could be we can identify some of these implausible genotype calls by using some simple population genetic theory so from our observed genotypes we can calculate the allele frequency at any bioelix nip we’ve called so here the frequency of the a allele is twice the frequency of the AAA calls those are our common homozygots in blue plus the frequency of a b calls are heterozygous in green and we can do the equivalent as you see on the slide for the frequency of the blade knowing the frequency of the AE and the B allele we can use Hardy and weinberg’s calculation for how we expect alleles at a given frequency to be distributed into genotypes to generate an expectation for the genotype to be expect to observe at any given allele frequency we can then compare how our observed genotypes I.E the blue green and red clusters fit to that expectation and we can test that using a chi-squared test now Harley Weinberg equilibrium is an idealized mathematical abstraction so there are lots of plausible ways it can be broken most notably by evolutionary pressure as a result in case Control Data it’s typically best to assess it just in controls or to be less strict with defining violations of Highly Weinberg in cases that said in my experience genotyping errors can produce very large violations of Harvey Weinberg so if you exclude the strongest violations you tend to be removing the biggest genotyping errors the previous steps are mostly focused on problematic variants but samples can also be erroneous one example is the potential for sample swaps either through sample mislabeling in the lab or correctly entered data in phenotypic data these are often quite hard to detect but one way to detect at least some of these is to compare self-reported sex with X chromosome psygosity which is expected to differ between males and females in particular males have One X chromosome they’re what’s known as hemizygous so when you genotype then they appear to be homozygous on all Snips on the X chromosome females on the other hand have two X chromosomes they are holozygous and they have a normal X distribution centered around zero which is the sample mean in this case you could also look at chromosome why Snips for the same reason however Y chromosome genotyping tends to be a bit sparse and is often not of fantastic quality so there are benefits to using both of these methods it’s also worth noting that potential errors here are just that potential where possible it’s useful to confirm these with further information for example if there isn’t a distinction between self-reported sex and self-reported gender in your phenotype data then known transgender individuals may be being removed unnecessarily the aim here is to determine places where the phenotypic and genotypic data is discordant as these May indicate a sample Swap and this might indicate the genotype to phenotype relationship has been broken and that data is no longer useful to you average variant homozygosity can also be applied across the genome where this metric is sometimes referred to as the breeding coefficient it’s called that because High values of it can be caused by consanguinity related individuals having children together which increases the average homozygosity of the genome there can also be other violations of expected homozygosity so it’s worth examining the distribution of values and investigating or excluding any outliers that you see examining genetic data also gives us the opportunity to assess the degree of relatedness between samples for example identical sets of variants imply duplicates or identical twins 50 sharing implies a parent Offspring relationship or siblings and those two things can be separated by examining how often both alleles of a variants are shared specifically we would expect parents and Offspring to always share one allele at each variant whereas whereas siblings may share No alleles they may share one allele or they measure to it lower amounts of sharing imply uncles and aunts and then cousins and grandparents and so on down to more and more distant relationships in some approaches to analysis individuals are assumed to be unrelated so the advice used to be to remove one member of each pair of related individuals however as mixed linear models have become more popular in G was and mixed linear models are able to retain and include related individuals in analyzes related individuals therefore should be retained if the exact analysis method isn’t known again it’s worth having some phenotypic knowledge here unexpected relatives are a potential sign of sample switches and need to be examined confirmed and potentially removed if they are truly unexpected and once again it’s important to know your sample the data shown in this graph does not despite what the graph appears to suggest come from a sample with a vast amount of cousins instead it comes from one in which a minority of individuals were from a different ancestry and that biases this metric I’ll talk a little more about that in just a moment relatingness can also be useful for detecting sample contamination contamination will result in a mixture of different dnas being treated as a single sample and this results in an overabundance of heterozygote calls this in turn creates a signature pattern of low level relatedness between the contaminated sample and many other members of the cohort these samples should be queried with the gene typing lab to confirm whether or not a contamination event has occurred and potentially be removed if an alternative explanation for this odd pattern of inter-sample relatedness can’t be found finally a word on genetic ancestry because of the way in which we have migrated across our history there is a correlation between the geography of human populations and their genetics this can be detected by running principal component analyzes on genotype data pruned for linkage to equilibrium for example this is the UK biobank data you can see subsets of individuals who cluster together and who share European ethnicities other subsets who share African ethnicities and subsets who share different Asian ethnicities and in a more diverse cohort you would be able to see other groupings as well this kind of 2D plot isn’t the best way of visualizing this for example here it isn’t really possible to distinguish the South Asian and add mixed American groupings and you don’t get the full sense of the dominance of European ancestry data in this cohort Europeans in this case account for around 95 of the full cohort but because of over plotting I.E the same value as being plotted on top of each other in this 2D plot you don’t really appreciate that looking across multiple principal components helps for that ancestry is important to QC many of the processes I’ve talked about rely on the groups being assessed fairly being fairly homogeneous as such if your data is multi ancestry it’s best to separate those ancestries out a rerun Key C in each group separately so that was a brief run through of some of the key things to think about when running QC I hope I’ve got across the need to treat this as a data informed process and to be willing to rerun steps and adjust approaches to fit cohorts although we’ve got something resembling standard practice in genotype QC I think there are still some unresolved questions so get hold of some data look online for guides and automated pipelines and enjoy your QC thank you very much for listening I’m doing a q a at 9 30 EST otherwise please feel free to throw questions at me on Twitter where I live or at the email address on screen which I occasionally check thank you very much\n\n\n\nTractor: GWAS with admixed individuals\nTitle: Tractor: Enabling GWAS in admixed cohorts\nPresenter(s): Elizabeth Atkinson\nhi thanks so much for your interest in using tractor I hope this tutorial helps get you started in doing ancestry where g-was admix cohorts so here’s just a brief outline I’ll start with some motivation go into the overview of our statistical model and then talk about implementation of the actual tractor code\nso to start off with our motivation I want to reiterate a point that is now thankfully becoming common knowledge in the gwas community that is that the vast majority of our association studies are actually conducted on European cohorts and if we look further at this breakdown of the small kind of wedge of the pie of who’s not European we can see that there’s actually only a few percent from Recently admixed populations such as African-American and Hispanic Latino individuals groups that collectively make up more than a third of the US populace and just to make sure we’re all on the same page and add mixed individual refers to somebody whose ancestry is not homogeneous but rather has components from several different ancestral groups there are actually many more advanced individuals out there whose samples have already been collected and genotyped or sequenced alongside phenotypes but they’re not making it into this figure due to being kind of intentionally excluded for being admixed so there’s really a pressing need for novel methods to allow for the easy incorporation of admix people into Association studies so admix people are generally removed due to the challenges of accurately accounting for their complex ancestry such that population substructure and stratification can seep in and bias your results and in the context of GEOS this means the potential for false positive hits that are due to ancestry rather than a real relationship to the phenotype\nso in a global collection such as this example from the PGC even controlling for PCs which is the standard way that admixture is attempted to be kind of controlled for there’s still so much variability in the data that there’s a lot of concern over false positives it’s because of this researchers will often sort of draw a circle around people who are deemed homogeneous enough to included in study and everyone else is excluded so this nearly always ends up resulting in European individuals being included as they generally represent the bulk of samples that have been collected it’s the main motivation of the project I’m talking about today was to try to rectify this issue and develop a framework to allow for the easy incorporation admits people into Association studies\nso with tractor we are handling this concern by directly incorporating local ancestry local ancestry tells us the answers for origin of each particular haplotype tract in an advics individual at each spot in their genome so in this three-way admix individual the y-axis are the autosomes the position along them is on the x-axis and because humans are diploid the top and bottom half of each chromosome is painted differently and you can see that each tract in this individual is colored according to the ancestry from which it derived\nso the intuition behind tractor is that to correct from population structure we effectively scoop out the tracks from each component ancestry to be analyzed along alongside other ancestrally similar segments so we do this by tracking the ancestry context of each of the alleles for each person at each spot in their genome so note that here I’m only showing this red ancestry but the same thing is going to be happening for the blue and green ancestry the statistical model built into tractor tests each submit for an association with the phenotype by splitting up the ancestry context of the minor alleles so in the two-way advics example we have our intercept of course and then we include terms for how many copies of the index ancestry there are for this person at that spot in the genome so say if you have zero one or two for example African alleles at this location as well as how many minor alleles fall on your ancestry a versus ancestry B backbone so those are the first three x’s in these terms this is what correct for the fine scale population structure if there were differing allele frequencies for the ancestries at this spot in the genome as we expect there to kind of routinely be to the different demographic histories modern day human populations then we’re no longer going to be confounded by this as we’ve sort of deconvolved them um so the mind really you’ll see it on each ancestry backbone are kind of properly scaled to their background expectations note that while I’m showing two-way admixed example here this model can also scale easily to an arbitrary number of ancestries by addition of terms as well as allowing for the ready inclusion of all of your necessary covariants\nso I already went over how our model corrects for the fine scale population structure at the genotype or haplotype level which is what’s allowing the inclusion of atomix people in a well-calibrated manner in geost but it also has some other nice benefits I’d like to briefly mention we’ve built in an optional step to recover long-range tracks that we find to be disrupted by statistical phasing by using our local ancestry information and with respect to gwas our novel local ancestry aware model can improve your results in a variety of ways most notably through boosting GEOS power generating ancestry-specific summary statistics and helping to localize your G1 signals be closer to the causal snips so we really hope that this framework should Advance existing methodologies for studying advics individuals and allow for better calibrated understanding of the genics of complex disorders in these underrepresented populations\nso I’ll now shift to giving a brief walkthrough of the steps involved in implementing the tractor pipeline so there are three different steps the first one being optional for efforts that require complete haplotypes I’m going to go through each of them kind of individually but the first three steps are implemented as Python scripts and step three the gbos model is sort of the recommended implementation is in the cloud native language scale there’s a lot more documentation and description of these steps on our GitHub page so please check out the wiki there for more information as well as a Jupiter notebook to kind of walk through the steps of the pipeline\nso before running local ancestry aware GEOS you need to call your local ancestry we recommend the program RF mix for this which is described in their paper maples at all 2013 as well as their GitHub repo which I’m linking to here so the success of tractor really relies on good local ancestry calls so it’s really important to ensure that your local ancestry inference performance is highly accurate before you try to run a tractor-gos\num just to make sure you know we’re kind of all ready to go I wanted to show an example command for launching an RF mix run on one chromosome using some publicly available data um so this is sort of the you know basic parameter settings that we’ve come up with as being kind of Ideal for our tractor pipeline again more details on the Wiki page\nbut what I’d like to spend more time on is the actual tractor scripts so this first step um in our pipeline is what’s going to detect and correct for switch errors from phasing by you using our local ancestry calls you can see our supplementary information and extended data figures one and two in our manuscript for some additional context around what we mean by this um so this step uses RF mix ancestry calls as input and is implemented with the script unkink MSP files.pi which tracks those the switch locations and corrects them\nso the output will consist of two files a text file that documents the switch locations for each individual so that’ll be sort of your input MSP file name suffixed with switches and a corrected local ancestry file again the same input name but this time suffixed with unkinked as in unkinking a garden hose we’re kind of straightening out those switch errors here’s your kind of example usage you should just need to point to the MSP file name without this msp.tsv suffix I’m going to call this script so this is one of the default output um files from RF mix\nnext we also need to correct switch errors in our phase genotype file and we’re expecting VCF format for this so this is what’s going to actually recover the Fuller haplotypes and improve our long-range track distribution this step is implemented with the script unkink genophile.pi and expects his input the phased VCF file that went into RF mix as well as that switches file that we generated in the previous step so the switches file is basically used to determine the positions that need to be flipped in your VCI file tractor also expects all of your VCF files to be phased and it’s recommended to strip their info and format annotations prior to running just to ensure good parsing and again Step One is optional and won’t affect your GEOS results but can be useful for efforts that require complete haplotypes\nall right so in step two we extract the tracks that relate to each component ancestry into their own VCF files and calculate the ancestry haplotype and Alternate wheel counts for each component answer for your admixed cohort um so this step is implemented with the script extract tracts.pi and expects as input the stem name of your VCF you can have it just be unzipped or gzipped is also allowed as well as your again your RF mix MSP files so you can input those unkinned files from the previous step if you chose to correct switch errors or you can go straight from your original data if you did not this script now accepts uh an arbitrary number of ancestry calls which is very exciting so you can specify the number of component ancestries with the dash dash num inks flag um the default is two you can change it to however you know multi-way your population was if your VCF file is g-zipped also include the flag dash dash zipped if not leave that flag off\num so there are multiple files output from this step including the counts for each individual at each position for these three different pieces of information I’m listing here so firstly you’ll get the number of copies of each ancestry at that spot so if you have zero one or two copies of you know ancestry a b c however many ancestors are in your data um secondly you’ll get the number of alternate allele counts on each of those ancestral backgrounds and thirdly you’ll get VCF files for each of those ancestries containing only genotype information for that ancestry’s tracks so these will be sort of um BCF files that will contain a bunch of missing data will kind of blank out the other ancestories and it’ll also contain some half calls for instances where there’s you know sort of heterozygous ancestry at a spot\nall right so our ancestry dosage output can now be used in our local ancestry aware g-was model the recommended implementation of the tractor joint analysis model uses the free scalable software framework for kale hail can be run locally or on the cloud I’m going to be showing a cloud implementation here and it can be useful to build and test your you know your pipeline in a jupyter notebook when applying it to your own cohort data so for this reason we Supply an example Jupiter notebook written in hail on our GitHub which can be adapted for user data the specific commands to load in and run linear linear regression on our ancestry D consult file will be as follows so here is the specific sort of Suite of commands in hail that will read in and format our tracker dosage files just to make it a little nicer running things Downstream this is a little bit more involved in terms of loading files in than a standard VCF or Matrix table this is because we need to run g-was on entries rather than rows so for each ancestry and dosage Capital type file will be kind of annotating that information in rather than just sort of one piece of information per variant and this will be come a little bit clearer in the following slides but that’s why there’s like you know a couple extra steps in this reading in dosage files um proportion\nso next we’ll join our ancestry and haplotype dosage files together onto a hail Matrix table so we do this by basically annotating the haplotype counts and any of the other ancestry information we have onto our first ancestry ancestry zero and here we’re creating a new combined hail Matrix table called Mt which will have all of this information in it basically we’ll have kind of our original ancestry zero dosage of information and then structs for each additional term that we’re annotating on so here I’m showing examples for two-way and three-way administ example and the two way we’re annotating structs for the ancestry one dosages as well as our hap counts for our ancestry zero so our index ancestry haplotek counts these are both um these all three of these are default outputs from our earlier tractor steps so if you want to have more multi-way admixed populations included you just need to add additional terms for each new ancestry so one new dosage term for each additional ancestry and N minus one terms for the haplotype counts\nso now we can load in and annotate our Matrix table with our phenotype and covariate information and from here on we are now utilizing standard hail commands the only thing to be you know careful about with loading in your phenotypes is to make sure that you’re keying by the same ID field that will match the names in your genotypes file and again note that I’m doing this in the kind of Google Cloud example so I’m pointing to the location of this file in my Google bucket and finally we can run the linear regression so for a single phenotype for example total cholesterol or TC as shown here along with our relevant covariance so here in addition to our intercept of one we have our half counts term and our two terms for our two-way admix population so our ancestry zero and one dosages and then we also are putting in all of our covariates that we want here so in this case I’m using sex age blood dilution level which is kind of a phenotype specific covariate and then an estimate of the Global African ancestry fraction that we got prior to this from using the program and mixture so we recommend putting in an estimate of your Global ancestry in addition to these sort of local ancestry specific terms just to account for if there’s sort of a relationship to the phenotype with their overall ancestry proportions and we find that the direct measure of global ancestry is a bit more accurate than PCS so this is sort of the recommended strategy so doing that we are basically annotating each of the variants in our Matrix table with the results of the G wasps the results are again saved as a struct here named TC our phenotype um and this will contain pieces of information for each of the summary statistics that we listed um you know here including the beta effect size the standard error and the p-value within each of these there’s going to be an array of indexable values that’s in the order of your terms so the index of zero is going to be your Intercept in the example I listed you know prior to this one is going to be your haplotype dosage two is your ansys dosage three would be ancestry one and then covariance would be four until however many covariates that you have so you can pull out summary statistics for every term in your model I’m going to allow for kind of easy comparison they’re all going to be annotated into kind of one big struct that’s just indexable by what position they were in your model to run multiple phenotypes in batch you can make a list of phenotypes first and then cycle through them annotating The Matrix table with a multiple structs that will each be named um you know the names of your genotypes so again all the information can be stored just in that one Matrix table in our Jupiter notebook we also provide some code to generate Manhattan and QQ plots you can select the term you would like to plot by its index as we talked about in the last slide and you can use the hail commands plot dot Manhattan and plot dot QQ to visualize the results so here we’re pulling in our TC struct from the p-value we want the um the third term or the index of two and here is what those plotting commands will produce with those you know the the settings by launching those commands you’ll end up with figures that look exactly like this um great so with that I hope that this was helpful thanks so much again for your interest in tractor and feel free to leave questions on our GitHub repo or email me with any thoughts I also have many people to thank of course for this project including my ko1 funding from NIMH and I’ll also direct you to our our paper published several months ago on nature genetics which has a much more thorough description of our method and some other considerations again our code is freely available up on GitHub alongside a Wiki tutorial to help you get started with tractor there as well so thanks very much and I hope that you enjoy using tractor\n\n\n\nGenotype Calling and Imputation with MoChA\nTitle: Genotype calling/imputation on the cloud: MoChA pipleine\nPresenter(s): Giulio Genovese\nhello everybody today I’m going to talk about the bunker Pipeline and how you can use it to perform phasing and imputation of a DNA microwave data in the cloud so as an introduction MoChA pipeline what is phasing and imputation well um for every genomine Association study where we work with DNA Micro Data um we always have to go or process the data through three main steps the first step is to call genotypes from microarray intensity data then we have to retrieve the Apple type structure of the samples by phasing and then we have to impute the missing variants that were not present in the immigrate by imputing by comparing the Apple types that we face with apple types of another reference panel of samples for which we have more information and the phasing and imputations in\nparticular are two steps that require a fair amount of computations and that parallelization of computations um do they have the high computation requirements and there have been many pipelines out there to perform these two steps but today I’m going to talk about the mocha pipeline which consists of two different uh scripts one is the mocha riddle and one is the Widow the first one we performs uh calling genotypes and basically in the second one performs imputation these clips are written in Widow which is a language for workflows and allows you to Define very complex uh segments of tasks um and we know in particular corresponds to a set of tasks and a description of the order in which tasks have to be performed and in the writing the task we have to Define which input file we need which commands we’re going to run on those files and which output file so you expect and the nice thing about the writing pipelines in the the video language is that they can be run on any computational environment um in particular uh you can do this thanks to the Cromwell workflow mg which is a a an engine developer the broad Institute and the idea is that you can input your Widow pipeline into the Cromwell server and the Chromebook server will take care of dispatching all the required jobs to different nodes in a highly parallelized fashion uh and this could be nodes on an HPC cluster like an sglsx.com cluster that are common in academic environments or maybe in cloud computing cluster like Google Cloud Amazon web services or Microsoft and this is all transparent to the user that doesn’t have to worry about the technicalities of the cluster implementation when riding a window pipeline um and so what features does the mocha pipeline for phasing reputation have that might be not available in other similar pipelines well the first feature is that it follows a minimalistic approach so the user always has to provide a minimal amount of necessary inputs to run the pipeline it can take a very different input files but in particular it can process a raw Illumina input files also known as either files without requiring uh to be a processor in alternative ways it uses a very state-of-the-art software for phase in a relationship between five it allows you to use the reference Genome of your choice so you can just request the grch38 as the reference that you want to use and once you have set up a Chromebook server or you can actually even run the pipeline in Terra if you if you want it’s very easy to use and it scale so very nicely and seamlessly to a course of a biobank size and so in particular you can run it on the UK biobank and the phasing part will run for less than 100 dollars and facing imputation overall will run in less than 40 hours if you run it in Google Cloud um and the reason I actually developed this phase English program was a support analysis related to music chromosome alterations so in particular when you run this pipeline you’re also want to get a as an output a list of constitutional mosaicanism alterations in the samples new process\nRunning the pipeline\nso how complicated is it to run the mocha pipeline hopefully I can convince you that it’s not that complicated um so the running a pipeline mostly means that you need to define a set of input files let’s say that we’re going to run the pipeline in either mode and we’re going to provide a set of either files um which are again illuminate intensifies we’re gonna have to provide that and you’re also going to have to provide a table with a list of samples and a table with a list of batches and then in particular I’m going to have to provide a configuration file in Json format indicating in which way the pipeline needs to be run and once we input that into the Chrome wall server together with the mocha Widow escape the output we expect is going to be a set of VCF phase files and an output table with the list of these files so this is an example of what the input for a run is going to look like here we have a sample table each row is a sample here we have a batch table on the bottom left where each row is a batch and for each sample we have to Define which batch it belongs to and which green and red eye dot file uh it corresponds to and for each batch we have to define the three different manifest files from Illumina which are the BPM CSV and EGT which inform the the software about which variants have been tested and about what the Clusters original type look like and the configuration file which is in Json format is going to define a just a minimum set of parameters like the name of the cohort the mode which in this case is the ielt mode because we are inputting either files um in this case because it manifest files are in grch37 uh um coordinates we will ask the pipeline to realign the Manifest files this comes very handy when Illumina does not provide a manifest file for glch 38 and then we can provide information about how much parallelization we want to achieve in phasing so we can Define the window size the maximum window size for phasing which in this case would be 50 centimorgan but we can decrease that if we want to parallelize the computation even more and then after that we need to provide a\nfile uh URL addresses for where to find all the files for the dimension of reference for the Illumina manifest files for the islands and also for the two tables and and that’s it once we have these three files so we can just submit the mocha with all together with the configuration file into the Chromebook server with a command like that and then if everything goes well we would obtain as an output a table with a list of VCF files and the VCF files that will be generated will be uh divided across batches and so for each batch we’re going to get a VCF file with the phase genotypes and probe intensity and then also gives you a file just with phase genotypes useful for imputation then we’ll also get some summary plots that will visualize the call rate across samples and maybe some statistics that will indicate the possible contaminations and then also other summary plots indicating probe intensities for the sex chromosome and and other that I’m not sure enough the cost that I’ve observed if you run it in Google cloud is of about one dollar or five thousand samples for GSA like Andries um similarly when I’m in the position pipeline is not that different especially after you’ve run the mocha riddle pipeline uh we’re gonna have to input a set of phase UCF files one per batch we have to gonna input a table listing the batches and then again a configuration file listing some important option and again this will be input into the Chromebook server together with the impute Widow script and we’re going to expect a bunch of inputed VCF files out if everything runs correctly uh this is an example of what an input would look like and again a batch table file in WS format where each row contains information for one batch and then a configuration file here on the right where again we can Define uh parameters like the name of the cohort which chromos we wanna impute if we don’t want to talk chromosome if you omit these all 20 chromosomes will be included and the information about for example which kind of information we want in the output to included BCF files do we want the dosages and then we can ask for dosages or do we want you know type probabilities or maybe we don’t want either and we just want the genotypes by default the limitation pipeline will use the pathogenomic coverage reference panel but you can also input a reference pen of your choice if you want the only requirement is that you have access to the to the reference panel and that it’s the structure in VCF format to split across the 23 different chromosomes and again a simple command like this will submit the job to a running chroma server where we indicate that we’re gonna run the middle script together with the Json file with the configuration up here if everything goes well we’re gonna get as an output again a table with a list of VCF and it’s gonna we’re gonna obtain one VCF for each batch and for each chromosome that we have asked to compute um the pipeline like as I said it uses input5 to run optional you use bigger five it should need to run it in a commercial settings and you know don’t have a commercial license for compute file fine but input file is free for academic research and the cost if you use the pathogens project reference panel is about one dollar for a thousand samples when you run it in Google Cloud\nApplications\nnow I’m going to show you very quickly some applications of the mocha pipeline uh and so first of all uh this is a list of biobanks across the world where we were able to run the scripts um thanks to collaborators that have access to the data on some of these biobanks uh I personally have access to the UK biobank the Massachusetts General brigant biobank data um and then something you might notice that because of different restrictions with different biobanks we actually have to run the pipeline on very different array of computational environments from lsf and slurm clusters to Google Cloud European clusters um but this is a testament the fact that the pipeline that we structure is actually can run uh almost any biobank\num moving on there will be more pipelines in development there is actually a collision score pipeline that can be run right on the output of the application pipeline if you’re interested in computer scores uh and there is a working progress Association pipeline that will run regioning in a very distributed fashion and will take again as an input the output of the more computation pipeline part of the reason why I’ve been developing this set of pipelines is uh that about five years ago I started to work with Polo on a project um in the UK biobank and to our surprise we found that there was a lot of interest in biology to be discovered by doing so and I’ve niched Furniture paper after that I started to realize that there was actually quite some interest in running this kind of analysis with machine clubs and alterations on biobands but the technical skills to actually do that was a really prohibitive for many users and so I decided to try to package the analysis that people were done into a easy to reproduce uh framework and that led to development of the mocha pipeline however although it is designed to scale to biobank size data set it can be applied also to a smaller cohort and this is an example of a work from Metro Sherman that applied the framework on autism problems and he was able to show that Jose comes alterations are more common in autism programs compared to their unaffected siblings even if this explains all a very small fraction of autism problems\nand um to conclude the mocha with the pipeline supports virtual real computational environments it follows a minimalistic design and it can input either files it can be used on grch38 without further corridors and they can scale to biobanks of any size and if you’re interested in CMV analysis it actually will provide germline and somatic outputs for further starting that I would like just to acknowledge the many people that have helped in developing this framework to start I want to thank my supervisor Stephen McCarroll that has supported me for many years uh for a law that has run many of the initial analysis in the UK by Bank showing that this framework actually is important and then the many many many collaborators and users of the pipeline that have provided the incredibly important feedback with that I’ll conclude and understand that will be going to be a q a session after this so I’ll be available for questions if you have any thank you for listening and I hope this pipeline might be useful to you\n\n\n\nSAIGE: Family-based GWAS\nTitle: Genetic association studies in large-scale biobanks and cohorts\nPresenter(s): Wei Zhou\nHi, I’m Wei Zhou. In this presentation I’mgoing to give a quick tutorial on a program called sage which is for scalable and accurate implementation of generalized mixed models it was developed for conducting genetic Association studies in large-scale biobanks and cohorts we will firstly go through several challenges of g-was in large-scale cohorts and biobanks mostly for binary phenotypes followed by our Solutions we have implemented Sage for single variant Association tests because the single variant solution tests are usually underpowered for rare variants more recently we have implemented the region or Gene based Association tests called sage Gene for rare variants in the same Sage r package lasting this talk we’re going to use some examples to demonstrate how to run the sage r package here three main challenges of G was in large-scale biobanks and cohorts are listed including sample relatedness large-scale data sizes and unbalanced case control ratio of binary phenotypes\nfirst let’s take a look at the simple relating needs simple relatingness is an important confounding factor in a genetic Association tests that needs to be accounted for for example in the UK power bank data almost one-third of individuals have a third degree or closer relative in a cohort however linear and logistic regression models assume individuals are unrelated therefore instead we use mixed models for g-was with relative samples mixed models use the random effect denoted as B here to account for sample relativeness B follows the multivariant normal distribution whose variance contains the genetic relationship Matrix called grm each of diagonal elements in grm is corresponding to the relativeness coefficient of a simple pair which can be estimated using the genetic data in a linear and logistic mixed models B which is the random effect is included to account for the simple relating needs through the grm\nin 2015 look at all has developed a linear mix model method called both omm both IMM uses several asymptotic approaches to achieve the scalability of the mixed models and it is the first linear mix model method feasible for G was in biobank scale data so can we use the linear mix model for binary phenotypes in this figure the variance against the mean of residuals are plotted for the linear model and logistic model the linear model assumes constant variance as the Orange Line shows while the true mean variance relationship for binary trades is represented by the green line therefore for binary phenotypes instead of using linear mix model we would like to use the logistic mix model in 2016 Chen at all developed an r package called GMAT in which the logistic mix model has been implemented for jiwas on binary phenotypes while accounting for simple relating needs\nso we use logistic mixed models to account for simple relating needs in jiwas binary phenotypes when we apply the logistic mix model as implemented in GMAT to one example phenotype colorectal cancer in the UK biobank we found that it would take more than 600 gigabytes of memory and 184 CPU years to finish 1G was in the UK biobank this brings out our next challenge large-scale data to be able to run logistic mixed models in biobank skill data for binary phenotypes we need to optimize implementation to make logistic mix models computationally practical for large-scale data sets we use similar approaches that have been previously used in both lmm which is the first linear mixed model method that are scalable for large-scale biobanks several approaches have been used to reduce the computation memory and time we have successfully reduced the computation memory from more than 600 gigabytes to less than 10 gigabytes and the computation time becomes nearly linear as the sample size increases the program optimization allows us to apply logistic mix model for binary phenotypes in the UK power bank here again we have the same example for colorectal cancer after we apply logistimix model to this phenotype we still see many Superior signals as we can tell from the Manhattan plot so what is missing here\nwe later find out the reason is the unbalanced case control ratio of binary phenotypes unbalanced kiss control ratio is widely observed in biobanks this plot is showing a distribution of case control ratio of 1600 binary phenotypes in the UK biobank and around 85 percent of them have case control ratio lower than 1 to 100 which means there are 100 times more cases than controls previous Studies have shown that unbalanced case control ratios can cause inflated type 1 errors of the score test which is represented by the Blue Line in the plot the y-axis is for type 1 error rates and the x-axis is for the minor Leo frequency of the testing genetic markers the score test is widely used in chiwas because of its relatively low competition burden as we can see from the left to the right as the study becomes more unbalanced the inflation becomes more severe also as\nthe myelofrequency becomes lower and the inflation becomes more severe the reason is that when the case control ratio is unbalanced the score test the statistic does not follow the normal distribution in a plot the dotted line is for the normal normal distribution and the black solid line is for the score test statistic so when we try to get a p-value based on the normal approximation we’ll see inflation to solve this problem day at all in 2017 proposed to use the saddle point approximation to approximate the empirical distribution of the score test statistic instead of using the normal approximation and they have implemented this as the in the spa test art package so for our third challenge we’re going to use the spa test to account for the unbalanced case control ratio so in summary Sage can work for GEOS with related samples based on the mixed models and it can work for large-scale data with those optimization strategies and it can account for unbalanced case control ratio of binary phenotypes through the saddle point approximation again back to our example phenotype character cancer and we see Sage successfully corrects the inflated type 1 error\nwe have applied Sage to three other phenotypes in the UK power bank on white British temples from coronary artery disease which is relatively common with the case control ratio 1 to 12 to the less prevalent disease thyroid cancer with kiss control ratio 1 to 1000 as we can tell from the third column of the Manhattan plots and sage has well corrected type 1 error rates Sage contains two steps in the first step the now logistic mix model was fit with phenotype covariates and genotypes that were used to construct the grm on the fly as input in a step 2 score tests were conducted for each genetic marker with Saddle Point approximation applied to account for the unbalanced case control ratio of the binary phenotypes sage has been implemented as an open source r package and has been used to\nrun gwas for UK power bank data and for other biobanks and large Consortium V web has been created for some of the projects for browsing the Phenom white g-was results as more and more sequencing data are being generated more rare variants are being detected however single variant Association tests by g-was are usually underpowered for rare variants and set-based tests can be used to increase the power in which rare variants are grouped and tested together based on some functional units such as genes and Regulatory regions we then extended Sage for region or Gene based tests for rare variants we implemented different set-based Association tasks including burden scat and scad all and searching also allows for incorporation of multiple minor Leo frequency cutoffs and functional annotations here are some useful papers that you can read if you want to know more details about different methods and the first three are Sage Sage ching and the sage Gene plus which is an improved version of the sage Gene that we have recently implemented and all the three methods have been implemented in a sage r package and the fourth one is the GMAT which is an r package implemented the logistic mix model and the bottom map is the first linear mixed model method that is scalable for biobank skill data and regini is a new method based on some machine learning approaches it is very computationally efficient method for biobank scale G was I would like to thank Dr John Chen Zhao who co-developed Sage Gene with me and my PhD advisors Dr Sean Lee and Dr Christy Weller my postdoc advisors Dr Mark Deli and Dr Benjamin nail and now my colleagues from the University of Michigan now fantastic collaborators from the hunt study in Norway next we have prepared some sage and staging examples as Wiki pages in a GitHub and we will go through some of them together if you click on the first link it will bring you to this Sage Hands-On practice to get ready you can Store install the sage r package following the instructions that you may find link here on the stage GitHub and all the example data can be found in the EXT data\nso sage has two steps the first one is to fit a now logistic or linear mix model and the second one is to perform Association tasks for each genetic marker that you want we want to test and while applying the spa to score test to account for unbalanced case control ratios here’s the for step one you can use the trade type option to specify whether it is a binary or quantitative trait for binary trade Sage will automatically fit a non-legistic mix model and for quantitative trade it’ll fit a non-linear mix model and the step one takes two input files\nthe first input file is the in the link format containing genotypes of genetic markers that we want to use to estimate the genetic relationship Matrix and which contains the relatingness coefficient of simple pairs and the second file is the called phenotype file which contains non-genetic covariates if there is any and phenotype and simple IDs here’s a snapshot for the phenotype file and you can see there are multiple columns corresponding to phenotype you want to test covariates included in the model and simple IDs so here’s an example command line to fit the now logistic mix model for binary phenyl types and as we can see here we have the pilling file and we have to specify the final tab file and which column is for phenotype and which columns are for covariates and which comes for simple ID\nso there are way more options available for step one and for more details and you can use call this R script and dash dash help and to see all of them and then if you run the example step one comment in a previous slide and you expect to see the screen output ends with the following text if the job above has been run successfully and step one will generate three output files as you can see model file ending with the dot RDA and the variance file which contains a number for the various ratio and then Association result file which is an intermediate file and so the first two will be used as input for step two\nin Step 2 we perform single variant association test for each genetic marker so step two requires four input files and first of all is the dosage or genotype file containing dosage or genotypes of markers we want to test Sage supports four formats for dosages or genotypes is VCF BCF bgm file or the sap file so we’ll use speech and file in the example today but you can click on the links to see more details about those different file formats and the second one is the simple file so\nthis file contains one column for sample IDs corresponding to the simple order in the dosage file and no header is included in a simple white simple file and this file is only required for BJ input as for some Vision files there is no simple ID information included in a region and but for VCF and the VCF and the staff simple IDs are already included in those files so this simple file is not required for those file format and the third and fourth input files are output from Step One here’s the example code to run step two as we can see we specify bgm file Vision file index and Sample file and we specify the chromosome number because only one chromosome can be tested in each job and we use minimum value of frequency or minimum value account to specify some cutoffs for markers to be tested and GMAT model file and variance ratio file both of them are output by step one and then we use a um line output so we use 1000 here to tell Sage\nto Output the results of every 1000 markers to the output file we don’t want to use a very low number here because it will generate heavier overhead writing to the output file and then the last one is is output AF in case on control you we specify this one to be true and Stage will output the allele frequencies in cases and controls if we’re testing a binary Trace to the output so here’s an um Header information in the output file by step 2 which contains the association results for the genetic markers we’re testing so Association results are all with regard to allele 2. as we see in the output file and I want to point out the column called P dot value.na so these p-values are obtained under the normal approximation so if we generate a QQ plot for p dot value Dot N A we’re very um it’s very likely for us to see the inflation for binary phenotypes with unbalanced case control ratios so um the P dot value are the p-values that we want to use next if you are interested in conducting Qing or region based tests for rare variants you can click on the second link here which will take you to the Wiki page we created for examples of searching jobs so here we can see this page contains the similar formats as Sage that we have went through for the Hands-On practice searching also contains the similar Step 1 and step 2 as Sage does and it it contains the extra step called Step Zero in which we construct the sparse grm based on the provided link file and this tab only needs once for each data set and it does not change according to different phenotypes\nso in Step 0 we’re creating a sparse grm based on the genotypes stored in the p-link file as we use for step one for the full grm the difference is that we create this sparse grm and store it in a file and so it can be reused in step one for different phenotypes so the input file will be the link file same as the plink file for step one and then the output file will be a file storing the sparse grm and a file storing IDs of samples in the sparse grm so if you rather example code and you expect the screen output ends with the following text if the job has been run successfully then we’re running Step 1 again it is to fit a now logistic or linear mix model so step one asks for four input files instead of two as we see in Sage so the first two are the same as step one for single parent association test in stage it’s the appealing file storing\nThe genome types and phenotype file containing a phenotype covariates and Sample ID information and the last two the third and the fourth ones are output by step 0 which are file storing the sparse CRM and the file storing the simple IDs in sparse grm so here’s the example code for step one job and as we can see we can use the two options that highlighted here to specify the sparse grm the sparse grm simple IDs that we have obtained from Step Zero again this test help can be used to see more detailed parameter list and if the step one job has been run successfully and the screen output will end with the following text as we see there are multiple values corresponding to multiple variance ratios and there are actually four different my little count categories that we are using so for the specific cutoffs for different milio count categories and you can use Dash help to have a look\nso this step one will generate four output files and the first three are the same as the step one output by stage and when we try to run single variance Association tests their model file and the various ratio file and the intermediate file for the association tests of some randomly selected markers and the fourth file here is is specific to Sage Gene which is a sparse Sigma file so this file will be used as input for step two along with the first and the second output file which are the model file and the variance ratio file in Step 2 we’re performing The set-based Association tasks it asks for five input files and the first three are similar to Sage and there are dosage or genotype file in different formats and model file and variance ratio file generated by step one note that if you’re using Vision to for dosages or genotypes and you need to use the simple file to specify the simple IDs in the beginning file and then let’s look at the fourth and fifth file they are specific to Sage Gene and the fourth file is the group file we can take a closer look at this one\nso each line in a group file is for one gene or one set of variants that you want to test them together as a set and the first element is for the Jing or set name and the rest of the line is for variant IDs included in this jingle set for VCF or save input the genetic marker IDs should be in the in the format Chrome Dome position the ref alternate allele and for bgm file the genetic marker IDs should be matched should match the IDS in a pigeon file and each element in the line is separated by tab and the fifth file is called sparse Sigma file it is also um output by the step one with all those input files and we can run Step 2 job to perform set-based association test and I want to point out this option called Max MAF for group test and this can be used to specify the maximum value frequency of genetic markers that you want to include in a group test so by default this value is one percent but if you want to test rare variance only and you can lower this number again uses dash dash help to see more detailed parameter list and information and if your step 2 job runs successfully and you see this screen output ends with the text as we show here then we let’s take a look at the output files by step two Step 2 will provide a file with region or gene-based Association has the results for each Shing or region we’re testing will provide p-value of scan o test and also p-value from burden and scat test and there are a couple of columns there showing the number of markers following each monthly account category and the category information can be found in the step 1 script and if we specify that we want to Output the single variance association test result for genetic markers in the genes or regions we’re testing and the second file will be generated to provide those information\nlast if we want to use multiple value frequency cutoffs and multiple functional annotations to test each gene for example and we can use different group files and to run Step 2 for thatching for multiple times so to combine those different p-values for the same gene and we can use the gauchy distribution to combine them so here’s the code that you can use to combine multiple p-values for Jing or testing region\n\n\n\nCC-GWAS\nTitle: Tutorial to use CC-GWAS to identify loci with different allele frequency among cases of different disorders.\nPresenter(s): Wouter Peyrot\nhi thank you very much for your interest toapply ccgwas to identify loci with different allele frequency among cases of different disorders my name is wautre piro in the next 15 minutes or so i’ll show you how to run the ccgwa software as we also conduct this analysis in the cross-disorder working group of the pgc for the next wave of analysis elcas price and i developed the software and if you’re interested in any further reading please see our paper in naturegenetics2021 or visit my github page and if you have any questions or suggestions please do not hesitate to contact me i’m most welcome to explain further or to receive any suggestions you may have and so just at the end first here i’ll show a couple of slides to give a big picture overview of the method and then i’ll get on my hands on some real data to show you how the analysis are done with the our package software so ccg was is intended to compare cases of two different disorders and when you like to extend for example to compare 10 disorders you just need to repeat the ccgwos analysis accordingly to compare two disorders at a time and so ccgwas compares the cases of two disorders based on the respective case control geos results and the case control geos results are of course widely publicly available most of the time and ccg was works with taking a weighted difference of these case control geos results and ccg was combines two components which have a distinct function so the first component is what we refer to as the ccg was ols component and this component optimizes the power and controls for type 1 error at no null snips and with null null snips we refer to snip snips that have no impact on either this order so there is no case case difference and if you’re interested in any more details about the ccg was os component i refer to our paper and then there is a second component which we refer to as the ccg was exact component and the ccg was exact component controls the type 1 error at stress test snips and stress test snips are a very specific set of snips that impact both disorders but have no case case difference so that the impact on both disorders is exactly the same so there’s no case case difference and this set of stress test snips is a very tricky set of snips because they can get type 1 error quickly that’s why we have this additional set of weights\nto control for type 1 error at stress test snipped and so ccg was says that a snip is significantly associated with case case status when the p-value of the ols component is smaller than 5 times 10 to the minus 8 and when the p-value of the exact component is smaller than 10 to the minus 4. and ccg also has an additional filter to protect against false positives and that is a very specific set of false positive that may may result from differential taking of a causal stress test snip so again the stress test snip is a snip that impacts both disorders but there’s no difference in allele frequency between the cases and here in the right column you can see the causal stressed snip so it has an impact on this order a it has an impact on this order b but there is no case case difference so there’s no case case effect but now suppose that you have a tagging snip which stacks in population a in which you stutter study disorder a with a r of 0.6 and this order b it takes a snip with an r of 0.3 then when you look at the difference of the of the this order a and this or the b effects you do find the case case difference so this is a very specific set of snips for which you can find uh false positive and uh to protect against this snip ccg was has an additional built-in step to filter evidence to filter snips that show evidence of differential tagging once again when you’re interested in any more details of this step plea i refer to our paper and its supplements\nso now we get to how to run ccgwas um so i first suggest to visit the github page ccg was github and at this github page i will download all the results and in particular i’m now going to already download the files in the test folder there are the two dummy input files for the for the case control uh input geos results so in return to the previous page and here you can see an explanation of how to run so running ccg was how to get started there’s a detailed description of all the input parameters where you can with that you can look up for reference and there’s a description of the output files and there’s here this there is this example which i’ll go through with you\nso let’s first get to running ccgwas so first here i have a terminal opened and i can show you i’m in a folder where i have already downloaded these two files so the testcase control g was for bib 10 snips and testcase control g was for schizophrenia 10 steps so we’ll look at the example of comparing bipolar to schizophrenia so back to the github page so first we open r and i’m working on my macbook but i know this is exactly the same as working on a linux based server and it also works on windows here and you first need to load a couple of r libraries and install the ccgwas package i’ll just copy paste it from here paste it here and wait for r to run it and now ccg was just loaded and you can see that ccgwash is now a functioning rso yeah you can see that ccgos is now loaded into r okay and i’ve already loaded these two files so now i go to this example\nhere at the top bottom of the github page i’ve loaded this ccg was example and you can see if it all works and i’ll copy paste it and you can see that all is loaded but before looking to the results i’ll first get to you to see the input parameters so probably best do it on this github page so here you can ccg was first you set the name of the output file and then you set the name of the first disorder you set the name of the second disorder which is bipolar disorder you say where where the sum stats fell of the first disorder and the some stats value of the second disorder case control of the case control subsets results and then note the six columns of these results are very specific so they need to be exactly as i’ve described it here\nso let me show it’s at the github page so here it says yeah so the columns should be the snip name the chromosome number the base pair position the effective allele the non-effective allele the allele frequency of the effective allele the odds ratio for the risk of the disorder for the effect per effective allele the standard error of the log alt of the the log of the alternate show the p value for this snip and n effective and when you don’t have an effective you can impute imputed as i also described here on the github page so four divided by one over n case plus one over n control and i note that the results for the for the case control uh disorder and disorder b will be based merged on the snip name so before you run ccg was make sure that the snip names between those two sets are well aligned now a return to the example that we just ran\nso that’s where we left off i just showed you what the columns should look like of these two gz files and then there are more input parameters so k a1 a0 means it’s the it’s the population prevalence of disorder a so for schizophrenia that’s approximately 0.4 percent and then because of course it’s very hard to know exactly the population prevalence of a disorder you can also put boundaries to it and so for schizophrenia we set it at one percent and the low at the point four percent and it’s important to add these ranges because there’s also again protects against type one error at stress test snips and then for this order b you set the exact same three values so the population preference of this or the b and the high estimate and the low estimate and then you specify results that typically come from ldl discord regression but of course you can use different methods for it that’s the liability skill heritability for this order bay the liability scale heritability for this order b and how to get to these estimates i refer to the software package that provide these estimates then you want to know the genetic correlation between this and this order b and also the the intercept that you get from biferit or discord regression and this is very important to set this value because it helps you to increase the power of cctvs and you also set an estimate of the approximated number of causal snips and so this m value you need to specify it and i also show here at the input files here i give some extra explanation around about it and also some papers where you can see if this m is already estimated or how you can estimate it or if you want to make an approximation how to do it so details on setting m is here on the github page under the input parameters\nso we were left here with specifying the number of snips and then at last you need to give the number of cases for this order bay this order this order a the number of cases for this order b the number of controls for this order a and the number of controls for this order b and then you also need to specify the overlap of controls and once again when you set this overlap of controls it really helps you to increase the power of ccgwas and so it has kind of the same function as the intercept so these are two ways to to to increase the power of ccg was and also to double check that you don’t risk any type one error result so when you know these values it’s important to set it to increase the power of gcg os so i hope this clarifies the input parameters of ccgos and now our return to the analysis that we just did by running this command line um yeah so we just ran it and you can see that the that the r function gives kind of an output file which is also saved in the log file so it says well\nwhen the analysis was started and that you run ccg was and not ccg was plus and for ccg was plus i refer to my get it to the github page and this is when you also have a direct case case comparison available and then it shows some summary about the data that were read from the gzip file and it does some very rough qc on the odds ratios and then how it merges the data and so this kind of the just the log file that’s that’s plotted and you also get provided in an fst plot for this and let me see if i can quickly yeah so this is what the fst plot looks like it’s an output of ccg was and here you can see the fst again for the details i refer to our paper but it gives intuition of the genetic distance between cases and controls so here you can see the controls for for bipolar disorder the cases for bipolar disorder the controls for schizophrenia and the cases for schizophrenia and i don’t know what just happened and these numbers gives the relative difference between them so you can see that even though schizophrenia and bipolar disorder have a very large genetic correlation of 0.7 the genetic distance is still considerable with a relative value of 0.49 compared to 0.66 or 0.6 for the case control difference\nokay um and then when we look at the output files i always like this system where from r you can also use the command line in the terminal yeah you can see that by running ccg ones these these are the input summer statistics for these tens these 10 snips this is the output valve this is the fst plot that i just showed this is the log file which is also printed here on screen so i’m not going to open it but it’s good for a later reference if you’re interested and i note this log file also gives you the ols and the exact weights that i just discussed so here in the in the in the log file you can see so for this example the ols weights were 0.54 for schizophrenia minus 0.3 for bipolar and the exact weights are also listed in the in the log file and may be good to report is in your in the description of your ccg was analysis and here there’s also the the results that are [Music] displayed here\nso now let’s have a look at these results i’ll just copy paste this also from the github page and for now i’ll remove the all and i’ll explain to you later what all means so so i need to give it the correct name test out and then let’s have a look at d so once again i only have 10 snips now normally of course these are maybe 10 million snips or something and here you can see the columns so there’s the snip name the chromosome number the base pair position the effective allele the non-effective allele and then you have three columns ols beta or standard error and ols p value and these represent the the effects from the from the oles component of ccgwas and then there are three more columns exact beta exact standard error and exact p value and these three three columns give the ccgs results of the exact component and now as we said and here there’s also a column which labels ccg was significant so when ccg was significant this column is labeled as one right so and you can see for this specific snip in the ninth row so when we look here you can see that our lsp value is 9 times 10 to the minus 12 so it’s smaller than 5 times 10 to the minus 8 and you can see that the exact p-value is 4.7 times 10 to the minus 11. so it’s smaller than 10 to the minus four so this snip ccg was significant and so if you’re interested\ni think it’s good to to note that you can also have a more detailed output file and to do this you need to add to this to this r script yeah so you need to add here an additional input parameter which was named if i remember correctly yeah so then you add save all is true here save always true and then when we look again at the files that are now in the folder here you can see that an additional output file popped up and when we when we have a look at this file so i open this file changing the file name a little bit at the all here so and now this file takes much more space of course for 10 snips it’s not really a problem but when you have many snips it can be tedious\nso and there are many many columns here so the first uh columns of d uh uh represent again the columns that i just disc that so the first columns are the snip chromosome base pair position effectively or non-effectively and then you have the case control effects and these are these are skilled to the standardized observed scale based on a 50 50 case control ascertainment again when you’re interested in the details please see our paper and it supplements and then you have the case control results for for this for this order b then you have the oles results as we just discussed and you have the exact results but then there are additional columns so there is the potential tagging for the stress test snip potential differential taking of the stress test snip and it’s not necessary to understand all the details if you’re interested please have a look at the at our paper or at supplements but just know that behind the scenes we test for it and it’s being excluded from the from the previous output file that i showed\nand then here we also have this exact approach which which again so the exact component of ccg was protects against type 1 error of stress test snips and this is also based so that you don’t know exactly the population preference of both these orders so here you can see the exact results when you take the low low values of both population prevalence disorders low high high low and the high high population prevalence bound and here’s the ccg was significant folder so as you can see this d this output fell all if you’re interested to look into more details of your results you can have a look at it and if you have any questions please don’t hesitate to contact me and ask me for the details but in general advice not to look at these results because also in the in the trimmed results which are the default output and now i’m going to load it again we already removed the snips that may have a problem with differential tagging or some other issues right so if you use this file you’re you’re safe in that aspect and then finally and i see i extended the 15 minutes but i hope you don’t mind so now we showed how to run ccgos in practice and for uh\nyeah for follow-up analysis as i just said use ccg was resolved based on the save all is false results and i know this is a default so when you don’t set save or this is what’s being done and it’s a trimmed set of results and then we advise to use the ccg was ols components so these are the columns that are labeled oles beta ols standard error and osp value for clumping and for example polygenic risk or analysis so to look at which loci have the different allele frequency and when you’re interested in genetic correlation analysis or any other sort of ld score regression based analysis or methods that are alike we advise to use the ccgo’s exact component so the exact beta exact standard error and the exact p-value so that brings me to the end of this tutorial once again please don’t hesitate to contact me if you have any questions and i hope this tutorial was helpful"
  },
  {
    "objectID": "chapter7.html",
    "href": "chapter7.html",
    "title": "Chapter 7: Ancestry-Specific Analyses and Considerations",
    "section": "",
    "text": "Chapter 7.1: Cross-ancestry analysis\nTitle: Cross-ancestry PTSD and Polygenic Findings, and the Cross-Population SIG of the PGC\nDescription:\nPresenter(s): Laramie Duncan\nLength: Video starts at 00:00 and goes until 13:39 (13:39 total)\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\n\n\nChapter 7.2: Ancestry-specific PRS\nTitle: Clinical use of current polygenic risk scores may exacerbate health disparities\nDescription:\nPresenter(s): Alicia Martin\nLength: Video starts at 32:39 and goes to 47:06 (15:45 total)\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\n\n\nChapter 7.3: Local Ancestry and Admixed Populations\nTitle: Tractor: A framework enabling the well-calibrated genomic analysis of psychiatric traits across admixed populations\nDescription:\nPresenter(s): Elizabeth Atkinson\nLength: Video starts at 14:50 and ends at 32:38 (total 19:28)\n\n\n\n\n\n\n\n\n\nLink to video transcript here."
  },
  {
    "objectID": "chapter1.4_transcript.html#alzheimers-disease",
    "href": "chapter1.4_transcript.html#alzheimers-disease",
    "title": "Chapter 1.4 Psychiatric genomics: State-of-the-science (Video Transcript)",
    "section": "Alzheimer’s Disease",
    "text": "Alzheimer’s Disease\nlet’s move on let’s talk about the genetics of Alzheimer’s disease Alzheimer’s disease when you look at it as a whole um about 75 percent is going to be late onset non-familial so symptoms begin after the age of 60 65 depending on your definition and these individuals do not have a strong family history of the disease about 20 percent will be laid on set familial onset after 6065 and there is a strong history of um the disease in the family depending on what study you look at your definitions about five to ten percent of uh Alzheimer’s diseases early onset and about one percent is genetic onset and by genetic I mean full one of those causative genes um for Alzheimer’s disease those genes are PSA and one psan2 and app um they are all involved in uh the production of amyloid and amyloid being one of the Hallmarks of Alzheimer’s disease in this cartoon here the vertical structure you can see here is the cell membrane cytosol is the inside of the cell here’s the outside of the cell the amyloid precursor protein which is the protein coded for by app Falls across the cell membrane and it is cleaved or cut by two different enzymes beta secretase and gamma secretase and psen1 and psn2 interact with gamma secretase and if we have a mutation in one of these genes that can affect where gamma secretase Cleaves this amyloid precursor protein and that results in different forms of amyloid and some forms of amyloid are more likely to go and form these amyloid plaques which are the Hallmark of Alzheimer’s disease um they tend to have different ages of onset uh uh pscn1 being the earliest most sources will say 30s at the earliest I have to say I knew somebody who had a PSE and one pathogenic variants and had onset in his 20s so we can see that thankfully very rare and you can see onset into the 70s with PSE and two or sometimes um I know one case of somebody who had a pathogenic variant psen II and did not develop the clinical signs of Alzheimer’s disease that is very rare unfortunately this is a typical family tree that we would see in one of these families let’s say this is our patient a man 50 years old onset of cognitive decline at 48. he his father had onset at 42 um his paternal aunt had onset at 51. and his grandmother on his father’s side had onset at 45. so when we talk about these purely genetic early onset Alzheimer’s families this is the sort of thing that we see now what about the other 99 of Alzheimer’s disease that is not purely genetic here we enter the realm of risk factor genes and the best studied and most common of these is uh apoe it’s the major lipid and cholesterol carrier in the central nervous system there are three polymorphism polymorphisms or forms flavors if you will of apoe E2 is associated with a lower risk of Alzheimer’s disease E3 we consider average risk of Alzheimer’s disease and that’s the most common form an E4 is associated with a higher risk of Alzheimer’s disease and E4 has been linked to all different aspects of uh Alzheimer’s pathology so it has been linked to neuroinflammation synaptic dysfunction mitochondrial or metabolism dysfunction Tau aggregation which forms those Tangles also a Hallmark of Alzheimer’s an amyloid aggregation and reduce clearance so this is fairly well studied Gene has been linked to a number of different things how does apoe affect our risk of dementia well\num the specific risk for Alzheimer’s related to apoe varies between studies and you can see different numbers quoted in different sources but the numbers I’m showing here are compiled by one of the most careful epidemiologists I know uh Deborah Deborah blacker at Harvard and it shows the lifetime risk or age through age 85 for people who might carry a two copies of the E4 apoe genotype have a lifetime risk of between 30 and 55 percent so it’s definitely higher than the general population but not everybody who carries any four or four will develop Alzheimer’s disease if you have just one copy of E4 you also have an increased risk about 25 20 to 25 percent and three three which we considered normal is about 10 to 15 percent now there are other genes related to Alzheimer’s disease and I apologize this is a very busy and complex graph but I think it captures the complexity of what we know so far about the genetics of Alzheimer’s really well so here on the vertical y-axis start at the bottom genes that are associated with relatively low risk of Alzheimer’s disease and moving up to a near certain chance of developing Alzheimer’s and then here population frequency we go from very very rare at zero percent or close to zero percents to relatively more common and up here we can see our old friends appsen one psen two being very rare in the population but having a near certain chance of developing Alzheimer’s disease equally four four relatively rare and a higher chance of getting Alzheimer’s and then we move into some of the more common genes that have a smaller effect on our chance of getting Alzheimer’s disease and there are many many more genes than this actually and here the colors relate to some of the cell functions that are related to these genes whether it be amyloid metabolism so app metabolism cholesterol immune response endocytosis cytoskeleton there are researchers working to develop\nwhat they call polygenic risk scores where we can combine all the data from testing these multiple and interacting genes and give somebody more of a accurate hopefully closer to Accurate assessments of their chance of getting Alzheimer’s so not there yet but we may get there um and before I move on to the genetics of frontotemporal dementia Barb any questions at this point no questions have come across do any of you have questions at this point and it’s fine if you don’t we can go ahead everyone to just put your questions as they occur to you for this lecture in the chat box and I’ll track them to the next time um we pause for questions and I really\ndon’t mean to put you on the spot it’s okay if you don’t genetics a frontal temporal dementia now this disorder is less likely to be sporadic than Alzheimer’s disease we saw about 75 percent of people with Alzheimer’s didn’t have a strong family history of the disease frontal temporal dementia we’re looking at about 40 percent also about 40 percent have a positive family history of dementia so that includes also psychiatric disease and motor symptoms and it’s thought perhaps in this 40 percent we’re looking at a similar combination of risk factor genes as we saw in Alzheimer’s disease this these risk factors for risk factor genes for uh frontal temporal dementia are not as well understood as Alzheimer’s disease um and then 20 of people who have frontal temporal dementia have a pathogenic variant in a single Gene and those genes are C9 or 72 granulin or grn and map t now each of these are more common in the behavioral variant of FTD versus the semantic variant of FTD they each have their own flavor C9 or 72 is the most common cause of genetic frontotemporal dementia it’s also the most common cause of ALS or Lou Gehrig’s Disease and we do see sometimes in families that have a pathogenic mutation or variance in C9 or 72 you can see people who have both frontal dementia and ALS you can see people in the same family who have one or the other um C9 or 72 can also look like Alzheimer’s disease and so we do see that as well where someone will have a family history of Alzheimer’s disease we do the testing and find out it was likely also uh it was seen on Earth 72. granulin and map T are uh can cause parkinsonian features um also Progressive superclare nuclear palsy cortical basal degeneration so we also see that in these families there are also rare genes for frontotemporal dementia listed here I will say a lot of these are extremely rare but they are also fully penetrant for frontal temporal dementia and you might be wondering um with all of these genes how do you pick how do you know which Gene to test and the uh quick answer is we don’t um with current technology it’s often the same price to test multiple genes and so if you know it’s front to temporal dementia versus Alzheimer’s disease maybe you will just test for the front to temporal genes but um often if a patient hasn’t had an amyloid scan or any sort of positive verification that it is one disorder versus another the clinician might just order a dementia panel and include all of the causative genes for dementia um"
  },
  {
    "objectID": "chapter1.4_transcript.html#lewy-body-dementiaparkinsons-disease",
    "href": "chapter1.4_transcript.html#lewy-body-dementiaparkinsons-disease",
    "title": "Chapter 1.4 Psychiatric genomics: State-of-the-science (Video Transcript)",
    "section": "Lewy Body Dementia/Parkinson’s Disease",
    "text": "Lewy Body Dementia/Parkinson’s Disease\nso I will move on to genetics of Lewy Body dementia say this one under this one I’m including both Parkinson’s disease dementia which as you might know is the onset of dementia more than one year after the start of motor symptoms or parkinsonian symptoms and also dementia with Lewy bodies which is the onset of dementia before or within one year of motor symptoms and the genetics of Lewy Body dementia are not well understood I have to say but it seems to be midway between Alzheimer’s and Parkinson’s disease\nSo genes recently identified for a Lewy Body dementia include here um some of the uh Parkinson’s um jeans that you see in black especially GBA which is one of our more common risk factors for Parkinson’s Disease has also a higher chance of dementia associated with it and so sometimes you can see somebody with Lewy Body um dementia who has a pathogenic variant in GBA snca which codes for synuclein which is the protein that is characteristic for Parkinson’s Disease that’s the protein that builds up in Lewy bodies um pathogenic mutations in snca are more rare but they are more likely to also involve cognitive issues and dementia we also see some of the dementia genes um in Lewy Body dementia apoe we’ve already talked about granulin and map tea are actually both tied to frontal temporal dementia but they also come up in Lewy Body dementia um it’s a complex system and I think the best quote I saw in the papers that I’ve been reading about Lewy Body dementia is that the genetic basis of it is not well understood so I’m not able to give you a nice schematic on how many people have familial versus non-familial and How likely it is to be inherited in the family I don’t think we’re quite there yet with Lewy Body dementia\num I will say that if you know a patient who has a Parkinson’s Dementia or as a family history or has Parkinson’s disease I do happen to know a study that’s doing um genetic testing for Parkinson’s Disease so that’s PD Gene from the Parkinson’s Foundation you can just Google PD Gene Parkinson’s Foundation as no cost genetic testing for Parkinson’s which includes Parkinson’s dementia\nin case that’s helpful um and Barbara any other questions have come in before we move on yeah there was a question about whether or not insurance pays for any of these tests you just mentioned study that covers the cost what about insurance covering the cost of any of these it varies um for early onset dementia where doing genetic testing can be diagnostic um for a person’s symptoms I’ve seen uh insurance cover the testing for apoe\nbecause we have these new anti-amyloid therapies and in a person’s and here I’m talking about people who have been diagnosed with dementia and are considering test or treatment for one of the anti-amyloid therapies um having an apoe4 allele or variance increases the chance of side effects from those medications and so I would expect insurance to cover those costs in other cases they don’t um and it is it is highly variable so I yeah I’ve seen it go both ways\nany other questions Barbara there was a question about typing out\nthe PDG contact info and I’m trying to find it right now um I’m seeing it under Michael J fox organization would that be right oh somebody else got it before I did never mind yep sorry to put you on the spot with that um it’s a full disclosure I work on that study um and we have had a fantastic uh response to it which means there might be some wait lists now um but it is you know no cost genetic testing for the seven most common genes\nfor Parkinson’s so it is a great service that they Parkinson’s Foundation is doing and\npeople can do it remote they can sign up through the website and be sent a kit or they can sign up through a site a local\nUniversity Medical Center yeah the link that\nso great thank you Lisa yeah it’s a great program thank you\nanything else Barb\nwe’ll say that’s a no not yet sorry no not at all let’s move on to common questions\num so one of the most common questions that I get maybe you get\nis Alzheimer’s genetic and this is what I say um that there are rare cases of\nAlzheimer’s disease that are purely genetic people in those families get Alzheimer’s early like in their 30s or\n50s and there are several people with early onset Alzheimer’s disease in the same family so yes it does happen it is\nrare um most cases of Alzheimer’s disease are caused by a combination of genetic and\nenvironmental factors and I should say speaking as someone who works in genetics I consider everything\nthat’s not genetic environmental um so here we’re talking about uh\nright hypertension uh hyperlipidemia diabetes sedentary lifestyle what have\nyou one way to think about this is that\neveryone has an Alzheimer’s jar and this same analogy could be used for\njust about any common disease cardiovascular disease what have you\neveryone has an Alzheimer’s jar and this jar can be filled with environmental risk factors that I just described it\ncan also be filled with genetic risk factors and some of these you know might be apoe might be trim two might be been\none any of those genes that we already looked at\nwe start with a certain number of genetic risk factors and over time\nwe accumulate environmental risk factors and if our jar fills to the top we\ndevelop Alzheimer’s disease different people have different\ncombinations of genetic and environmental risk factors so some\npeople are going to start with a relatively large number of genetic risk\nfactors and they will accumulate environmental risk factors just like the\nrest of us but they don’t need to accumulate quite so many before they develop Alzheimer’s disease\nother people start with a relatively low number of genetic risk factors for\nAlzheimer’s but if they accumulate enough environmental risk factors they will still develop Alzheimer’s disease\nthere are protective factors that can make the jar taller\nand those can be social activities mental activities physical activity\nhealthy diets good sleep habits all of the things we know that decrease our chance of getting Alzheimer’s Disease by\ndoing these we can sort of add rings to the top of the jar make the jar taller\nand give us more time before we develop Alzheimer’s disease and I find when\ntalking to people it’s helpful to steer the conversation and end on things that they can do in\nsteps that they can take to lower their chance of getting Alzheimer’s disease\nhere’s another question that I get um my parent had Alzheimer’s does that\nmean I’ll get it too and I think it’s helpful to put that in context we all\nhave a chance of develop of developing Alzheimer’s it’s unfortunately not a rare disease\nbut it depends on what study you look at but people who have a parent with Alzheimer’s have about a 20 chance of\ngetting it themselves that’s compared to about 20 10 in the\ngeneral population another way to look at it 20 chance of getting Alzheimer’s is an\n80 chance of not getting Alzheimer’s there’s not a Surefire way to prevent\nAlzheimer’s disease but you can improve these odds through Lifestyle Changes\nbar maybe I will pause their thank you Barb for monitoring the chat because I\nam awful at reading and talking at the same time um but it looks like maybe a couple\nquestions have come in yeah well um Joanne just mentioned that these visuals\nare really great for um helping to translate some of this information to\npeople Lorraine is wondering if by environmental factors are you referring to Lifestyles or perhaps collusion and\nwork related exposures or maybe all of the above all of the above I have a\ncomplete bias in this area um working in genetics in in my world\nview there’s genetics and there’s everything else so um we should probably find another name for\nit but yes um could be medical conditions like diabetes\ncardiovascular disease could be air pollution I think has been linked to a higher chance slightly higher chance of\ngetting Alzheimer’s um what have you anything that’s not genetic I would call Environmental\nanything else cool\nhere’s a great one um should I get tested for aprily uh so there are clinical guidelines recommending against apoe testing where people without memory complaints due to its low predictive value many people who carry the bad copy of apoe which is E4 do not develop Alzheimer’s disease and about half of those with Alzheimer’s disease don’t have an E4 allele so the catchphrase is apoe is neither necessary nor sufficient to develop Alzheimer’s disease you can get Alzheimer’s without having an apoe4 the bad copy um and if you have an A4 uh that doesn’t mean you’re going to get Alzheimer’s disease um and so a bunch of us got together and recommended against testing for apoe um in people who do not have memory complaints that said in the real world um things happen differently and you might have seen this in the news last November uh the actor Chris Hemsworth learned he carries two copies of April E4 as a part of a National Geographic docu-series um and it’s not clear uh that he was fully informed before he was tested or that the results were adequately explained um probably goes without saying but uh don’t get tested as part of a television show um or without carefully considering what the tests can and cannot tell you and whether you really want that information for people who are thinking about getting tested and do not have memory concerns um there is a really helpful website Gene test or not DOT org um and I’ll leave this up for a few minutes so you can write it down if you want it’s an online decision Tool uh to help people decide whether or not they want to get tested and it asks people to consider\num do you have a family history of Alzheimer’s disease will this genetic testing give you useful information um is this the right time to get tested or maybe you should wait um on this and whether the advantages outweigh the disadvantages among the important issues discussed on this website is the genetic information non-discrimination act or Gina which protects against discrimination based on genetic information for health insurance or employment important to note that this protection does not cover long-term care insurance or life insurance and so you once you know something you can’t unknow it and it’s better to think things through uh before getting tested another good question somebody walks into your clinic and hands you their 23andMe apoe test results or maybe they got tested through Cardiology or Nephrology which also do apoe testing and now they want to know okay what tell me what does this mean um and it’s a really complex question it depends on a person’s family history it depends on their age depends on whether they’re currently having memory concerns um and in order to address or begin to address all of these questions we wrote a paper um and it’s a series of vignettes of different situations of when patients come in asking about their apoe test results and here are some ideas and some guidance on what to tell them and I should also say that that table that I included earlier that had the various apoe genotypes three three four four what have you and the risk of Alzheimer’s disease is included in this paper so this is a publicly available article through the general family practice if you have any problems um finding it you’re welcome to contact me and I can just shoot you a copy of the paper and that is actually all I have unless there are other questions or comments\nwhat um questions might you have or um issues that have come up for you if you as you’ve connected with um your patient populations or people in the community one of the main questions or sort of concerns I get is when we hear the word genetic we think predestination right we think absolutely positive if it’s in your genes it’s in your genes and you are going to get whatever it is and the idea of this complex interplay between our genes and our environments and our lifestyle is a little harder for people to get their heads around um that’s one of the big issues\num among the people I see for genetic testing one of the big concerns is especially if they have been diagnosed with Alzheimer’s disease front to temporal dementia Parkinson’s what are the chances that their kids are going to get it and almost all of these genes are autosomal dominant which means that if you carry the gene the pathogenic variant in this Gene there’s a 50 50 chance that each each of your children will also carry this same pathogenic variants now that doesn’t necessarily mean they’re going to get Alzheimer’s or Parkinson’s or frontal temporal dementia depending on whether this is a causative Gene if it is a causative Gene then that means if that individual lives long enough they will have a very high likelihood of getting the disease risk variance it just means if they have inherited pathogenic variants in that Gene they have a higher chance of also developing the disease um so those are the most common issues that that I hear from about from patients and research participants um what does it mean if I carry a mutation pathogenic variance in one of these genes and what does it mean from my kids so a question what lifestyle changes or environmental changes do you see that are most effective at prolonging onset of symptoms of Alzheimer’s or dementia I the Alzheimer’s Association is a fantastic source for that um and I would definitely go to that website um the main thing I talked to people about is the link between heart health and head health or heart health and brain health so all of the things we know we’re supposed to be doing to uh protect our heart we should also has implications for our brain health so managing any uh high blood pressure managing your blood cholesterol levels managing your blood glucose levels um exercise is very important eating a balanced diet you know the standard Mediterranean diet is a very good idea in addition to all of those things um staying socially active and staying mentally active are great things to do for for your brain health so yeah what I tell people is it’s all the boring stuff that we know we’re supposed to be doing anyway and it’s hard to do but it really does have an impact on our health\nMalia you have an earlier slide something about mild cognitive impairment I’m wondering if you can talk a little bit about um the relationship between mild cognitive impairment and genetics and development of Alzheimer’s um do you deal with that at all where you’re seeing people come with mild cognitive impairment and then wanting genetic testing done for example yeah I so personally I don’t often see people with mild cognitive impairment because they are not eligible for our Alzheimer’s studies since it um I do sometimes see people with Parkinson’s disease who are beginning to have cognitive issues um in terms of genetic testing for let’s say let’s say somebody is concerned about their memory and they want to get apoe testing to see if um it’s Alzheimer’s or not that is an option but it gives you limited information um it’s since again apoe testing is not absolutely predictive and half of people who develop Alzheimer’s disease don’t have an apoe3 allele for allele um\nwhat I’ll sometimes talk to them about is what might be just as helpful or more helpful is doing Baseline cognitive testing and then repeating that a year later to see if there has been decline and also doing some of those lifestyle interventions in the meantime so it’s a different story I’ll have to say if we are if it’s a family with uh early onset Alzheimer’s disease or early onset dementia um in that case you may want to think about doing testing for those genes and in one of those families it’s most helpful to do genetic testing on an individual who has been diagnosed with early onset dementia um to know that you’re testing the right person in the family because if you say have you know somebody whose parents and uncle and aunts and grandparents died with early onset dementia and you do genetic testing on that individual in the Next Generation who does not currently have a cognitive complaints\nlet’s say that testing comes back negative which is great um but\nyou don’t know if you’ve tested for the right thing I mean maybe it’s come back negative because there’s a gene we\nhaven’t found yet in the family and so we are in our testing for the currently\nknown genes we’re not going to pick that up and we may be giving false reassurance to that individual so\nGeneral Concept in genetics start with somebody in the family who has the disorder make sure it’s a gene that you\ncan identify in the family and then you can offer predictive testing to other\nindividuals as appropriate if they want the information\nso I also I hope you all\nhope you don’t it seems like a lot of the genes are focusing on the amyloid hypothesis and yet there’s still some\ncontroversy about amyloid and Alzheimer’s for example and I’m\nwondering if you have some information about\num how that’s playing into the research that’s going on right now and what we might see in the future yeah let’s uh\nflip back sorry for all the changes here let’s flip back to that table um that graph of\nall of these um so there was I mean there’s been a huge\ndebate for years right um is it Tau or is it amyloid it used to be called the Baptist versus the taoists\num in Alzheimer’s research and\na lot of the genetics is pointing toward amyloid um\nbut there are genes like we can see here bin one which has the red circle around\nit is linked with Tau um and it’s entirely\npossible there’s other genes linked with Tau as well that um in just concentrating on amyloid and\nnot Tau at all we are missing part of the picture we don’t know yet but I think as we have\nseen many of these anti-amyloid Therapies in the early stages fail and\nfail and fail and now we have some anti-amyloid therapies which are\nsomewhat uh effective um they are they are not a cure they\nhopefully slow down the disease [Music] um yeah you do wonder if\nif it’s an oversimplification to only go after amyloid um so say we don’t know at this point um but it’s it certainly seems that amyloid is very important in Alzheimer’s disease is it the only pathogenic mechanism no and is it the only thing we should be targeting in our therapies potentially not but we will see um so Lorraine mentioned that um she thinks that since we don’t have treatment or specific Improvement it would be a big conundrum to decide to test or not she said I can see this if you’re interested in research and helping for the future you might test so that makes me think about other studies I’ve been a involved with where you really puzzle about what kind of meth recruitment message you want to convey and I’m wondering how you do that for the studies that you’re involved with yeah it is really difficult um I would say in our early onset Families Our early onset genetic families where our testing is predictive more than half of people decide they don’t want to be tested because it’s not information they can act on um and it’s not something they think will make their life better and one of the exercises we go through in genetic counseling often is have somebody imagine getting their test results and finding out they do have the family variants that they do have a strong genetic predisposition to Alzheimer’s or friends of temporal dementia and um how that sits with them and whether this is something that they are going to regret learning or not some people or some people the uncertainty is worse than knowing and they just want to know that if they forget their car keys if they lose their car keys you know is does that mean it’s the start of something um or not and of course individuals in these families have even if they don’t carry pathogenic mutation in the gene they have the same risk for laid Onsted Alzheimer’s disease as the rest of us many use the information those who said who decide to get tested many use the information to join studies and there’s a study called Diane dominantly inherited Alzheimer’s Network um which has done great work in understanding the pathology of Alzheimer’s disease understanding the very early signs of Alzheimer’s disease because in these individuals who carry one of these variants um you can look at the family and make a fairly good prediction of the age at which they will develop symptoms and so those individuals are carefully followed in the five to ten years before average symptom onset to see what are the changes in you know on brain scans on lumbar puncturism blood tests what are the very early signs we can pick up um as Alzheimer’s starts to develop in the brain but the person isn’t showing symptoms yet and once we do have very effective Therapies it’s going to be really helpful I think to be able to identify people in those very early pre-clinical stages and start the therapies at that point so that’s all the causative genes apoe um some people again they just want to know whether or not they have it um frankly a lot of people I think get you know 23andMe testing um and people I’ve talked to may not think through it that thoroughly and then they find out that they do have an. apoe4 and then they’re really wondering what it means preparation is involved in that any they do so April uh 23andMe tests for Parkinson’s and that’s part of their standard um panel for your your health uh health screening for apoe they do ask you to opt in they do ask you to check a box and say Yes I want this information and they tell you a bit about what the information uh involves so they do ask people to read through a description about what the test can tell you and what it can’t um and I don’t think I’ve seen any numbers from 23 on me on how many they’ve tested but it’s at least in the thousands um so that is a common way that people find out their apoe results dominantly inherited Alzheimer’s Network expected in case say they want life insurance or long-term care insurance so if they join a study and there’s testing is that protected information it is so well depends on the site depends on the hospital um the Mayo Clinic uh requires that any test done in a research study be added to the clinical record and so that is something that for our sites at Mayo both you know Rochester and Jacksonville and they have many sites um that those individuals need to understand because their research results are not protected and from their clinical record that’s the only site I know of that does that for everybody else it is not included in their clinical record [Music] um often clinicians really want it in the clinical record because that’s useful information for a person’s Medical Care and so participants are given the choice whether or not they want it in there um now if somebody you know decides that they don’t want it in their clinical record and they are asked by long-term care insurance you know have you ever gotten genetic testing for this or that for Alzheimer’s disease um they could say no I don’t think there would be a way for that insurance company to find out but it is insurance fraud so it is something not to be taken lightly um when we are especially talking about some of these more predictive testing one causative genes um for especially Alzheimer’s disease from the temporal dementia for someone who has not developed symptoms of that disorder we say you know are are you comfortable with your long-term care insurance um are you comfortable with your life insurance that might be something you want to review before getting this testing because it is not protected by federal regulations and other than 23andMe is it likely that anyone getting genetic testing would see a genetic counselor before the testing occurs um it depends so if you uh are seen in a genetics Clinic you’re certainly likely to see somebody in a genetic counselor for something like Huntington’s disease or some of the genetic forms of other neurological diseases charcoal Mary tooth or what have you um especially for predictive testing again if someone doesn’t currently have symptoms they’ll probably talk with a genetic counselor before other like for our Parkinson’s studies uh people don’t meet with a genetic counselor prior to testing we have a video that goes over the most important issues in genetic testing it’s an issue in health care because frankly there aren’t that many genetic counselors and um genetic testing is getting more and more common and so we’re trying to find ways to make sure that people are fully informed about the ramifications of this testing um while taking into consideration the availability of people to really sit down one-on-one and go over those issues so so it depends sometimes yes sometimes no it’s helpful any other questions folks well I for one have learned an incredible amount from this I really appreciate your expertise Malia and all the information that you’ve shared um uh we’re getting comments about it being a wonderful presentation and just the information the way you’ve presented it makes it easier for us to convey to people out in the community so I appreciate that absolutely and again if you have any questions in the future if you’d like a copy of that um uh paper that’s my email address shoot me a note and all right oh goodness all right thank you so much and I everyone again this is the last day of the last session for this series the next one will start up March 28th you should be getting an email with all of the information for registering but what a great session to finish up with because um it’s there’s not a lot of people with your expertise Malia and so I really appreciate it yeah absolutely see everyone in the spring hopefully we’ll be done with the rain and the Snow by then bye everyone bye Barb bye-bye"
  },
  {
    "objectID": "chapter10.2_transcript.html",
    "href": "chapter10.2_transcript.html",
    "title": "Chapter 10.2: Caution in Genetic Prediction (Video Transcript)",
    "section": "",
    "text": "Title: Predicting the likelihood of future psychiatric disorders: a closer look, and some cautions.\nPresenter(s): Howard Edenberg\nPredicting the likelihood of future psychiatric disorders sounds very appealing, particularly if there’s the possibility of an effective early intervention.\nBut let’s take a closer look and look and look at some cautions before jumping ahead too quickly.\nHI. I'm Howard Edenberg from Indiana University. [These are my personal views.]\nPsychiatric and substance use disorders are complex genetic disorders. The risk is affected by both the genes and the environment. Neither works alone and they interact, sometimes in complex ways. For many disorders, including the substance use disorders, the total genetic contribution to risk is about equal to the environmental contribution, and for substance use disorders there’s a required environmental component: consumption or use of the substance. Thus genetics will never fully predict future psychiatric disorders, particularly the substance use disorders.\nEven the best powered genetic studies to date are far from being able to make confident predictions. We’re only beginning to identify the genetic variations that affect risk. For the substance use disorders, for example alcohol use disorders, at this moment, the common variants measured across the entire genome predict only a small fraction of the risk, about 10 percent, so current attempts at prediction are at best premature. What makes such attempts even more problematic is the risk that people may experience stigma associated with being labeled as having a substance use disorder or psychiatric disorder.\nThere are, however, some claims of very high accuracy in predicting risk for a complex disorder such as opioid dependence. We learned of a company claiming that they have a test to advise physicians whether someone is likely to become opioid-dependent before that physician prescribes opioids for pain relief. The company’s prediction is based on training a machine learning model on a small set of cases and controls using a handful of genetic variants, only 15, that are purportedly from brain reward pathways, and they claim an extraordinarily high accuracy. We examined their claims and found serious problems. The chosen brain reward system SNPs differ in allele frequencies in different populations, and machine learning models based on those chosen SNPs turned out to predict genetic ancestry much better than they predicted risk. In fact, we found that random SNPs matched to the same minor allele frequencies performed the same way as these brain reward system SNPs. Thus there are serious problems with the purported accuracy of this test. The first problem, of course, is that prediction is not accurate and can therefore lead to poor medical decisions. In particular, this may lead to the undertreatment of pain in those individuals who are flagged as being at increased risk for opioid dependence. Because the test picks up differences in ancestry, it may lead to increased medical discrimination against those of some ancestries and it may stigmatize those predicted to be at risk. Having that marked in one’s medical records could affect treatment down the line, and of course stigmatization can go beyond just medical treatment into the wider society. We’ve posted our analyses of these particular tests on medRxiv for those who’d like to know how we came to some of the conclusions that I just relayed to you. [Now published: PMID: 34710714; PMCID: PMC9358969]\nSo this has been a cautionary tale. Although our study focused on one example of a predictive test for opioid use disorder, similar issues are likely to arise in tests for other substance use disorders and psychiatric disorders in general. Particularly because these kinds of disorders are often accompanied by stigma, we have to be especially careful that our tests are accurate, that our tests don’t discriminate against people from different ancestries, and that the good that they do outweighs the potential for the stigmatization.\nThanks for listening to my comments."
  },
  {
    "objectID": "software_geneset.html",
    "href": "software_geneset.html",
    "title": "Gene Set Identification",
    "section": "",
    "text": "MAGMA\nTitle: Gene- and gene-set analysis in MAGMA\nPresenter(s): Christiaan de Leeuw\nLevel: Intermediate\nLength: 12:09\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\n\n\nH-MAGMA\nTitle: Annotating Genetic Variants to Target Genes Using H-MAGMA\nPresenter(s): Nancy Y.A. Sey, University of North Carolina at Chapel Hill\nLevel: Intermediate\nLength: 10:09\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\nLink to GitHub tutorial and datasets.\n\n\n\nE-MAGMA\nTitle: E-MAGMA: an eQTL-informed method to identify risk genes using genome-wide association study summary statistics\nPresenter(s): Zac Gerring, Eske Derks\nLevel: Intermediate\nLength: 7:16\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\nLink to GitHub tutorial and datasets.\n\n\n\nPRSet\nTitle: How to run pathway specific Polygenic Risk Scores\nPresenter(s): Judit García-González\nLevel: Intermediate\nLength: 15:28\n\n\n\n\n\n\n\n\n\nLink to video transcript here."
  },
  {
    "objectID": "software_genomicSEM_transcript.html",
    "href": "software_genomicSEM_transcript.html",
    "title": "Software Tutorials: Genomic SEM (Video Transcript)",
    "section": "",
    "text": "Title: Genomic SEM Tutorial\nPresenter(s): Andrew Grotzinger\nI’m Andrew Grotzinger and in this video we’re going over how to run genomic SEM using psychiatric traits as examples genomics stem is a general framework for modeling genetic covariance matrices produced by methods like LD score regression to then estimate any number of structural equation models that can be used to test hypotheses about the processes that gave rise to the data that we observe it only requires you want summary statistics and those summary statistics can come from samples with unknown and varying degrees of sample overlap what that means is that you can now estimate models for really rare traits that you would not otherwise observe in the same sample it’s going to be split into two parts for the Practical\nthe first is how to estimate a user-specified model using genome-wide estimates the second part is how to incorporate the effects of individual Snips to do things like estimated multivariate g-was all the Practical materials are available at this box link here and this includes an R script and all the files needed to run that on screen the top of that our script is going to include some code to actually download genomics Sam if you haven’t done so already throughout the presentation we’re going to be using gy summary statistics for schizophrenia bipolar major depressive disorder and I’m moving on to how to actually estimate user specified model this takes three primary steps the first is munging the summary statistics the\nsecond is running Elder Scroll regression and the third is using that output from LD score regression to then run the model that you specify munge is a general procedure to format the summary stats in the way the LD score regression is expecting for the sake of this practical we’re using a subset of 10 000 steps for schizophrenia bipolar major depression in general you can download and use a full set of summary stats on a personal laptop for the purpose of the Practical and restricting file size to produce using a subset here it takes four arguments the first is the name of the files the second is a reference file that you to a lot to the same reference allele across trades third is the name of the traits and the force is the total sample size and putting it all together in this last line here where I’m specifying all these arguments and running munch\nagain to really highlight that we are only using the restricted subset for the purpose of the Practical when munge runs it’s going to produce a DOT log file that you should inspect to make sure things like column headers are interpreted correctly and in particular I’ve highlighted this section here where it prints how the effect column was interpreted for schizophrenia it’s an odds ratio and it’s interpreted correctly but you’ll want to make sure to read the corresponding readme files for the summary statistics and then look at this log file to cross-reference whether or not that effect column is being interpreted right in this second\nstep we run LD score regression within the context of genomics M the first argument is traits and that’s the name of these now munge summary stats which will all enter that.sumstats.gc ending then for binary traits we do the liability threshold correction that requires inputting the sample prevalence which reflects the cases over the total sample size and then the population prevalence which you can get from either epidemiological papers or from looking at the paper from the corresponding unit variate G was the fourth and fifth argument are the holder of LD scores and the LD score weights in almost all cases this is going to be the same folder and here we’re using the European LD scores because we’re using the European only summary stats and then finally we specify the names of the traits and this is how the traits are going to be named in your actual model and on this last line of code we are at ldscore aggression in The Next Step we’re actually going to specify the model but before we do that I want to switch over to R and actually run through the codes you can see it first\nI’m going to load in this package and I’m going to set the working directory to where I downloaded those Workshop materials and then for munge I’m going to set those files the Hat map 3 list that reference file the trade names the sample size and then run Munch and the Second Step I’m going to take those month summary stats and put the sample on population prevalence for the liability correction the folder of LD scores and all the score weights which are the same and then the trade names and finally actually run reality score aggression this is going to produce results that are actually interpretable because we use that subset of 10 000 Snips so when we now go on to step three of running the model I’ve created an LD score regression object that uses the full set of summary statistics that we’re now going to load in so you can actually produce integral results in the context of the model we’re gonna load that in and now switch back over to the PowerPoint to talk about how you specify a model in genomic stem we use the little Bond Syntax for running the model and in this case we would specify a regression relationship of a using a tilde b or for those of you that think of it in the format of Y till DX or outcome till the predictor for covariances you would specify two and of course for a variance of a variable it would be the covariance with itself so a totally tilde a for a factor you specify the factored name here followed by equal sign utility and then the factor indicators to fix a parameter you would put a number followed by an asterisk on the right hand side of the parameter\nthat you’re estimating so this would fix the covariance between A and B to 1 and to name a parameter you would write the parameter label using some set of letters here I’m naming the covariance between A and B Cove a b and what this does is it allows you to use models transfer this parameter let’s say the covariance is estimating as negative but you have some really sense that it should be positive so you put them as parameter constraint to keep it above zero we loaded in that pre-made ldsc data which again is using the full set of summary statistics this is not simulated data because we’re using summary stats this is often something you can readily download online than to actually run the model this takes two necessary arguments and two optional arguments the first is Cove struck which is that output from LD score regression the second is the model so we’re running a common factor model here and we’re telling Lavon using this n a star that we want to freely estimate the first loading and then we want to fix the variance of the factor to one so we’re using what’s known as unit variance identification for this model an optional third argument is what estimation method you want to use we offer DWS at maximum likelihood but the default is DWS and then another optional argument is std.lb and that’s whether or not you want to automatically specify the variances of variable sort of one and you would run the model so this is going to produce this set of results and\nI’ll show that in R here in a second but to walk you through what those results mean this first three columns or the parameters being estimated the fourth and fifth column here the unstandardized estimate and standard error for those parameters so it’s the senator applied to the genetic covariance Matrix and the standardized estimate in standard error is the estimate standard error for the model applied to the genetic correlation Matrix where the heritabilities are scaled to one the model fit here is all going to print as an a because when you’re estimating a common factor divided by three indicators uses it up all of your degrees of freedom so it perfectly fits the model but I want to walk through how you would interpret these model fit statistics more generally so chi-square is the model chi-square that reflects the exact index of fit I’m in the degrees of freedom and p-value for the model chi-square in the next two columns note that this will almost always be highly significant for your model because chi-square is sensitive to sample size which by definition is massive in G wasp space the next piece is AIC which can be used to compare models regardless of whether they are nested with lower values indicating better fit CFI is the comparative fit index which has these General heuristic cutoffs with higher being better and then finally SMR is the standardized rootme Square residual or lower is better going back over to R we’re going to specify that argument specify the model the estimation method and std.lb we’re going to run the model and you’ll see that we produce the same results that we produce over here not in the slides but in the code I’ve included some alternative ways that you could specify this model so for example if you wrote scd.lb equals true you don’t have to write that n a star F1 star F1 or you could use what’s the common factor function to estimate the same model and produce the same results moving on to part two I want to talk about how you would ride multivariate gwas and genomics n and to be clear you don’t have to do both of these parts you can certainly run a genome-wide model in part one and publish that alone you don’t have to bring in these individual simple facts but I did want to show that because oftentimes for people the multivariate G wasp space is what’s of\nmost interesting this includes four primary steps the first two of which mirror what we already did which is to write a monginelli score regression and you don’t need to redo that we’re only going to go over the last two steps of running the sunsets function and the multivariate gwas functions common factor g w acid user G was some stats takes a number of different arguments it can be a little bit confusing and I’ll note that we have a flow chart on our GitHub to help you figure out how the argument should be specified again we’re using the drastically subset Snips for the purpose of the presentation and we’re also using a subset of reference file which is used to align the alleles and to pull out the snip minor allele frequency the third argument is the trait names and then the fourth argument here we’re letting the subsets function know the standard errors for these traits are on the logistics scale and you’ll want to be really careful with this because oftentimes for binary traits they might be effect column on an odds ratio scale but then the standard error will be on a logistic scale and that’s something that you can determine oftentimes from the readme file if you have continuous traits or binary traits analyzed using a continuous model it’s going to be a different set of arguments for the sake of time I’m not going to go over all of that here but we do have that flowchart on the GitHub and of course you can reach out to us if you’re confused or getting strange results so putting that all together you’d run the subsets function here and much like the Munch function this will produce a log file that you want to make sure you go over to make sure everything’s interpreted correctly and then once that’s done you can use that output in combination with the LD score regression output to run models that include the effects of individual Snips I’m going to go over both the common factor G was at user gwas functions starting with the common factor G bus function this takes two necessary arguments the first is the output from LD score regression and the second is the output from the substance function and then finally once again you can specify dwbls or ml for the estimation method and then you can also specify whether you want to run in parallel so in practice if you’re running a g-was bat you cannot run on a laptop unlike the models that we were showing in part one so for this you really will want to be running in parallel on a Computing cluster but for the purposes of this because we’re using a small subset of the Snips we’re going to run not in parallel so you get a sense of how these functions work I’m going to switch now over to R to go through that substance function where again we read the files the reference file the trait names whether the standard errors on a logistics scale we’re writing t for true for all three running some stats and now taking that output and specifying the LD score regression output the output from some sets that we created and actually running the common factor gwas function\nso this is going to produce output that looks like this there’s a lot of columns it’s got the sent chromosome base pair monolithal frequency A1 and A2 the parameter being estimated which is the effect of the snip on the factor and the estimate sandwich corrected Standard air Z estimate p-value and then this other metric that we call qsnip which indexes whether or not that snip really does not fit the model so this is the chi-square distributed test assist with degrees of freedom they’re going to depend on the number of indicators in your model so that indexes the extent to which a common factor model is insufficient for accounting for the pathways from an independent Pathways model so if this common pathway of the snip on the factor is really a poor representation of the independent effects of the Snips on these individual indicators then Q is going to be significant so that’s going to happen when you have things like a simp that has directionally opposing effects on the indicators or let’s say the Sim has a really strong effect on one of the indicators but not the other if we go now over to these results you’ll see the estimates here one thing you want to inspect is these last two columns of fail and warning zero means it’s good to go but if you look at the warning messages you’ll see that a handful of the variances are estimated as negative\nI want to use this as an opportunity to talk about how you might troubleshoot a warning like this and then also go over how you would use user G was in this context for user gwas it takes those same first two arguments of the output from multi-score regression and the subsets output but then you’re also going to specify the actual model that you want to run that’s now going to include the effect of this individual snip so we’re running that same model of the common factor model the common factor G was is automatically specifying behind the scenes where the stand predicts that common factor but then we’re additionally adding in these model constraints we’re renaming these parameter labels at the residual variances your schizophrenia bipolar and MDD and we’re constraining them to be above zero because of that warning that we got when running common factor G bus\nif we now go over and run that I want to mentioned that another optional argument for user G wasp is this sub-argument this is whether or not you want to save a particular piece of the model output the common factor guas is automatically only saving the effect of the sniff on the common factor what the sub argument does is it allows you to tell user gwas that look for each of these Snips I don’t want to save all of the model output including the factor loadings and the residual variances are really for the sake of memory I want you to save the effect of this snip on the common factor so sub does not change at all how the model is estimated it changes how large the output file is and I would highly recommend setting this argument so now we’re going to run this and while that’s running I’d want to make some final notes that parallel processing are available for both user gwas and common factor g-was parallels executing the exact same code serial processing except that it takes advantage of additional cores an ideal runtime scenario you would split your jobs across Computing nodes on a cluster and run in parallel there is also MPI functionality available so again all runs are completely independent of one another I’ve listed a number of resources here including the GitHub and I’m not going to go through this but I’ve included some slides about some things to keep in mind so the final\nthing I’d want to go back over here and show you that this produces the same set of results common factor Gus are very nearly so now that we’ve added those residual variance constraints but now if we look at the warnings we see that those warnings are now gone so with that I’ll add it you can reach out to us with questions and I hope this was helpful to you"
  },
  {
    "objectID": "software_crossdisorder.html",
    "href": "software_crossdisorder.html",
    "title": "Cross-disorder Analysis",
    "section": "",
    "text": "Multi-trait Analysis of GWAS (MTAG)\nTitle: MTAG: Multi-trait Analysis of GWAS\nPresenter(s): Patrick Turley\nLevel:\nLength: 1:32:04"
  },
  {
    "objectID": "chapter4.3_transcript.html",
    "href": "chapter4.3_transcript.html",
    "title": "Chapter 4.3: Genetic Study Designs (Video Transcript)",
    "section": "",
    "text": "Twin studies {sec-video1}\n\n\nChoosing the Right Design {sec-video2}"
  },
  {
    "objectID": "chapter1.1_transcript.html",
    "href": "chapter1.1_transcript.html",
    "title": "Chapter 1.1: What are psychiatric disorders? (Video Transcript)",
    "section": "",
    "text": "Psychiatric disorders can be thought of as the severe chronic disabling disorders of the young. When you add it up they cause a disproportionate amount of suffering for individuals, for families. Unfortunately, we haven’t made as much progress as we would have imagined in the last 60 years. We need a revolution in both efficacy, but also in the breadth and number of symptoms that we’re able to treat medically. It’s easy to study cancer. It’s easy to study inflammatory bowel disease because you just go in and you take some of it out and you look at it under the microscope. You just can’t do that with the brain. The most important clue that we have though about human traits in particular and about human psychiatric disorders, comes from genetics.\nFor example, we know the adolescent brain is a period of vulnerability in a lot of neuropsychiatric disorders including schizophrenia, but we don’t know why. So there were all kinds of theories of schizophrenia which may or may not be true. The largest single risk factor genetically, at least in European populations, turns out to be a gene that encodes complement factor four or C4. In the brain, the role of complement proteins is to mark individual synapses that are weak or inefficient. One possible thought about what causes schizophrenia: Something having to do with these complement proteins inappropriately signals to these brain immune cells to remove synapses that actually you needed for your cognition, for thinking, for your processing of information from the world. The actual risk gene itself is very rarely a good target, but it gives you insight into the pathway or the mechanism by which that disease is manifest. So if you march up the pathway, or down the pathway, or sideways into other pathways that kind of impinge on this particular pathway, you can usually identify more tractable targets for drugging.\nIt’s still early days but the whole point of doing the genetics and the difficult follow on biology ultimately is therapeutics, to make a diagnosis ideally very early, to intervene ideally to prevent the disease from getting going. Certainly that is my hope and actually that’s my mission, to improve the lives of people with serious psychiatric illness. After all my own family is affected with that and I have seen firsthand the suffering from it. So, I’m fully committed to it and actually very optimistic."
  },
  {
    "objectID": "chapter5.1_transcript.html",
    "href": "chapter5.1_transcript.html",
    "title": "5.1 Quality Control (Video Transcript)",
    "section": "",
    "text": "Quality control: Introduction\nTitle: Quality control\nPresenter(s): Katrina Grasby (katrina.grasby@qimrberghofer.edu.au) and Lucía Colodro Conde (Lucia.ColodroConde@qimrberghofer.edu.au), from the 2021 International Statistical Genetics Workshop hosted by the Institute for Behavioral Genetics at the University of Colorado, Boulder.\nThanks for joining me for this session on Quality Control. In this recording I’m going to be talking about the quality control or QC steps that we apply to genetic data. So this is in the very early stages of a study. We’ve collected our DNA, it’s being transformed into data. We’re going to clean that data up and then we will impute and then we can do our statistical analyses. So there’s many points in a study that will be applying QC, but these steps that we’ll be discussing here and in the tutorial, are the quality control steps that we apply to our genetic data.\nWhy do quality control?\nSo why do quality control? Essentially, poor quality data is going to contribute to false positives and false negatives in our results. So we want robust results. We’re going to need to clean our data up. So we’ll be removing essentially genotyping errors. These can be errors in the calling of genotypes, or the translation of DNA into data. They can be due to lots of different factors. One of the pictures that I like to bring to my own mind was a story given to me by a woman that I work with who was involved in a project where they posted out two spit kits to a couple who were participating in a project, and somewhere in that delivery one of the kits went missing or was damaged. And the couple thought or were trying to be helpful and both of them spat into the same kit and posted that back to us --to her. In doing so they also included a letter to say what they had done, but it was a classic example of DNA contamination. It’s an example of human error. After all, we ended up with no usable data from two people instead of having usable data from one person. There is no way that we can disentangle that DNA in that spit kit and say this belongs to that person and this belongs to that person. It’s also an example of contaminated DNA, and even if they had not included a letter to say what they had done, the steps that we will go through in the tutorial would be able to identify a problem like this. So we can actually go OK, this isn’t a clear indication of data from a person, a specific person. We can remove that it doesn’t interfere with our analyses.\nSo one of the other things that will be doing in the tutorial is, after we’ve cleaned up our data, we’re going to have a look at the relationship structure within our data, and whilst that’s not necessarily a quality control step, it is a necessary aspect of coming to understand our data so that we can apply appropriate analyses and that is going to be important for minimizing our false positives and false negatives. So how do we go from DNA to data?\nDNA to data\nI’m a behavior geneticist. I use statistics to analyze data. I have no experience working in a laboratory, actually processing the DNA into data. But it is still useful for me to have an idea of these many different steps that are involved and an appreciation of what are the possible sources of error and what exactly does my data represent. So we are able to post out spit kits to participants who can spit into that kit at home and post it back. The sample is then processed so that the DNA is fragmented, it’s chopped up into little pieces. And then it’s amplified, so we’ve got more of it. And then DNA is extracted. We can store some and then we can plate some on to SNP chips or genotyping arrays. For it to be then further analyzed. So this down the bottom here. These images come from the Illumina website. This is an example here at A of a SNP chip or a genotyping array. So there are many different forms of SNP chips. The technology has improved overtime and I’m sure it will continue to improve. This here is an example of the bead technology. On this particular chip, there is space for information, DNA, from 12 different individuals. These horizontal bars here are each [for] a different individual. Now, if you’re thinking then, looking at this SNP chip, then if you got information from multiple individuals, and you’ll have many chips and they might be sent off to a DNA a genotyping company for processing in different batches. If you are thinking from an experimental point of view, and when you’ve got cases and controls, you want to have your cases and controls randomly allotted to both the chips and the batch runs that they’re being processed under. In a similar way, if you have males and females you want to randomized them across your chips and also your batch runs. That way we can ensure that we can actually pick up any particular batch effects in our data once we’ve got our data at the end.\nSo back to the chip. For each of these individuals, there will be hundreds of thousands of probes in order to test the alleles at hundreds of thousands of points in the genome. Many many many loci. So each of these wells have has a bead. This here is a schematic of a bead. So this bead is targeting an allele at a locus. So it has a particular sequence here, an address, so this is the order of bases. And then this here is the locus of interest. So once you’ve got your fragmented DNA, it’s going to come along, if it’s the right location in the genome, it will bind to this bead and then depending on if this allele here bonds to this C, so G will bond to C, this bead will fluoresce green. A different bead it might bond to, if there’s an A at that location and a T here, then it will bond this way and it will fluoresce red. So this is how we’re establishing at that locus. You might have a G or you might have a T and if it’s a G it’s going to bond to the C and fluoresce green. If it’s a T it will bond to the A and fluoresce red. So this is translating the DNA into a color, it is called an intensity. So if you’ve got you’ve got DNA coming from your biological father from your biological mother. You’ve got two alleles at that locus. If your two alleles are the same, you have two G alleles. They’re both going to be fluorescing green. Nice solid green color. If they both, if you’ve got two T alleles, they’re both going to be bonding to these A beads. They’re both going to be a nice solid red color. If you’ve got a G coming from one parent and a T coming from the other, then some of the beads that are C you’re going to fluoresce green. Some of the beads that are A are going to fluoresce red and that person is going to be heterozygous and they’ll have this yellow color. So these colors are then representing the three possible genotypes at that locus in the genome. And then these here are for hundreds of thousands of different loci in the genome. What I’ve got in this particular slide are examples of genotyping intensities. genotyping intensities So this is how we’re going to look at the color clusters representing the different genotypes. And see whether or not there are any problems. Now, this will likely, this is typically done by a genotyping company, you will probably not be doing this. But they will give you information about these first steps of quality control at this stage so you know what’s going on with your data. It’ll be there in a report from the company.\nThis top left-hand corner is a really good example of what we’re looking for. We have three nice, separated clusters. This is a homozygous A allele, This is a heterozygous group of individuals, and this is homozygous for the other allele. And in these two examples, with their little black Xs they are representing missing data. So missing data may not be terribly problematic if there’s just a little bit of missingness and it’s across all the different genotypes. However, if it is biased to one allele or one genotype, then that’s going to interfere with our allele frequencies in our sample, and that is going to mean that it’s not going to be representative of the population, it’s not representative in terms of how we can actually test for this genotype against this phenotype. We don’t want to have biased information about allele calling or genotype calling. Down here in the bottom left hand corner of it we have an example of a very rare allele. Sorry, a rare genotype, or it is a rare allele as well as a rare genotype. So there’s only one individual here who’s homozygous for the A allele. Very few heterozygous. In the middle down the bottom, this would be an example of a monomorphic group at this locus, so it really isn’t a useful locus for us to have genotyped. Or it could be that just this population is, there’s no variation in this population at this locus. And in the right hand bottom corner we have an example where there’s really been a failure to call the genotypes correctly. There is no indication of any red color, which is representing the heterozygous group of people. We’ve got these two kind of green clusters and the missingness is all off on this cluster it’s a complete fail.\nChecking the data\nSo the steps that we’re going to be going through with our quality control tutorial is we’re going to start off by checking the data. We’re going to have a look at the file format. How is data coded? How is missingness coded? We’re going to look at the build, so that we know what assembly our data is on. The genotyping company would have provided us with that, but you might not always have access to that information, so there are ways that we can check that out ourselves. This is a very useful resource, which we will use in the tutorial to do that. And... Knowing what build your data is on is very important, particularly for meta-analysis, but also if you’re going to do any follow-up analyses with, or follow-up work with, your results. We’ll be doing a sex check, which is to check that the sex that we can infer from the genetic sex check information is matching the sex reported by the individuals. So this check is looking at the heterozygosity of the X chromosome. And we have different expectations depending on whether an individual has one or two X chromosomes. So if the individual is reporting their sex and the genetic information comes back and it doesn’t match, and that happens for a lot of your sample, then you might have a problem with the information that is, matching your genetic information that has been returned after genotyping to your participant IDs. Bear in mind this is about biological sex and not about gender.\nGenotyping call rate\nWe will be checking for missingness. So there’s two types of missingness that will check for. One is this one, the genotyping call rate. This is where SNPs are missing information on individuals. So for each SNP we want to have information coming from most of our individuals. If there is too much missing data for that SNP, so too many individuals did not have information that was called correctly for that SNP, then that SNP might not be a good SNP for us to be using in our analyses.\nHardy Weinberg equilibrium\nWe will have a look at the Hardy-Weinberg equilibrium, to see whether or not our allele frequencies are matching what we expect. So this can highlight whether we’ve got some bias in terms of the frequency of alleles, or perhaps in our terms of calling genotypes appropriately, thinking back to those genotype Gwise intensities. Will be checking the minor allele frequency. So this is to make sure that we have enough information to do statistical analyses. If it’s too rare, then our GWAS is not the appropriate tool to use perhaps for this particular locus.\nSample Call Rate\nWe’ll be having a look at sample call rate. So this is another form of missingness. This is to say, do all of our individuals have information across almost all of their SNPs. So we don’t want individuals to be missing too much information across many SNPs.\nHeterozygosity\nWe’ll be looking at the proportion of heterozygosity. So this is a way of checking-- Think back to that sample where we had two people spitting into the same kit. That’s going to give us too much variation. There will be way too much variation in that DNA sample. So heterozygosity would be excessive. Inversely, reduced heterozygosity could be an example of inbreeding, but it could also just be that we had lots of missing data.\nReduced Heterozygosity\nSo that’s one of the reasons we’re going to check out our missingness first before we do our heterozygosity check. Because we don’t want to be making, or we don’t want to be setting ourselves up, to potentially making inferences that have social consequences that are negative. So if you’ve got missing data and that’s the reason you have reduced heterozygosity, you don’t want to end up looking at your sample going “oh, there’s lots of inbreeding here”.\nRelationship structure\nTowards the end of the tutorial, after we cleaned it will then have a look at the relationship structure in our data. So we might have lots of families or we might have extended families. We want to know whether or not our individuals are related so that we can apply the right type of statistical analyses.\nPopulation structure\nAnd finally, we’ll be having look at population structure or stratification. So that will be talked about more in another one of the sessions, but this is when we have a look at a little frequencies. There is differences in allele frequencies across different groups or different populations and that is an important thing for us to be aware of and to be including appropriately in our analysis. Elsewise, we’re going to get false positives and false negatives. If your population structure is also correlated in some way with your outcome of interest, that’s where we’re going to get a problem. And that’s when we’re going to talk about it in terms of population stratification.\nSo these are going to be out checklist for our key steps in QC that will be running through the tutorial.\n\n\n\nRunning Quality Control on Genotype Data\nTitle: How to run Quality Control on Genome-Wide Genotyping Data\nPresenter(s): Jonathan Coleman (jonathan.coleman@kcl.ac.uk)\nhello, i’m johnny coleman and in this brief presentation i’m going to discuss some key points concerned with running quality control on genome-wide genotype data which is a common first step in running a g was i’m going to provide a theoretical overview addressing the overarching reasons why we need to do qc highlighting some common steps and discussing a few pitfalls the data might throw up\ni’m not going to talk about conducting imputation or g-was analyses or secondary analyses nor am i going to talk at great length about the process of genotyping and ensuring the quality of genotyping calls i’ll simply not go into any deep code or maths however if you are starting to run your own qc and analyses i recommend the pgc’s rikipili automated pipeline as a starting point there are also some simple scripts on my group’s github that may be useful as well they follow a step-by-step process with codes and explanations we’re currently updating this repository so look out for some video tutorials there as well\nso here is our starting point i’ll be using this graph on the top right several times through this talk and this is a genotype calling graph with common homozygotes in blue heterozygotes in green and rare homozygotes in red hopefully your data will already have been put through an automated genotype calling pipeline and if you’re really lucky and overworked and under-appreciated bioinformatician might have done some manual recalling to ensure the quality of the data is as high as possible but in point of fact the data you will be using won’t be in this visual form but rather as a numeric matrix like the one below with snips and individuals this might be in the form of a blink genotype file or it’s binary equivalent or it’s in some similar form that can be converted to the plink format and where we want to go is clean data with variants that are called in the majority of participants in your study and won’t cause biases in downstream\nanalyses that should give a nice clean manhattan pot from g was like the one below rather than the starry night effect of this poorly qc’d manhattan plot above however something i’d like to emphasize across this talk is that qc is a data informed process and what works for one cohort won’t necessarily be exactly right for another good qc requires the analyst to investigate and understand the data often the first step is to remove rare variants and this is because we cannot be certain of variant calls consider the variance in the circle on the right are these outlying common homozygotes or are they heterozygotes we cannot really tell because there aren’t enough of them to form a recognizable cluster typically we might want to exclude variants with a low minor allele count for example five there are many excellent automated calling methods to increase the amount of certainty you have in these variants but it’s also worth noting that many analytical methods don’t deal well with rare variants anyway again the demands of your data determine your qc choices it may be more useful for you to call rare variants even if you’re uncertain of them or you may wish to remove them and be absolutely certain of the variants that you retain\nnext we need to think about missing data genotyping is a biochemical process and like all such processes it goes wrong in some cases and a call cannot be made this can be a failure of the genotyping probe or poor quality of dna or a host of other reasons but such calls are unreliable and they need to be removed missingness is best dealt with iteratively\nto convince you of that let’s examine this example data we want to keep only the participants which are the rows in this example with complete or near-complete data on the eight variants we’re examining which here are shown in the columns so we could remove everyone with fewer than seven snips but when we do that oh dear we’ve obliterated our sample size so instead let’s do things iteratively so we’ll remove the worst snip again variant seven goes and then we remove the worst participant bye bye dave then we remove the next first snip so that’s snip two and now everyone has near complete data and we’ve retained nearly all of our cohort so this was obviously a simple example how does this look with real data\nso here we have some real data and it’s it’s pretty good data most variants are only missing in a small percentage of the cohort but there are some that are missing in as much as 10 of the cohort so let’s do that initiative thing removing variants missing in 10 of the individuals and then individuals who have more than 10 missing variants and then 9 and so on down to one percent when we do this the data looks good nearly all of the variants are zero percent missingness and those that aren’t are present in at least 578 to the 582 possible participants and we’ve lost around 25 participants for about 22 and a half thousand snips but what if we didn’t do the iterative thing and we just went straight for 99 complete data so when we do that the distribution of variance looks good again arguably it looks even better and we’ve retained an additional 16 000 variants but we’ve lost another 40 participants which is about six percent more of the original total than we lost with the iterative method typically participants are more valuable than variants which can be regained through imputation anyway but this again is a data-driven decision if coverage is more important than cohort size in your case you might want to prioritize well-genotyped variants over individuals\nso we’ve addressed rare variants where genotyping is uncertain and missingness where the data is unreliable but sometimes calling is simply wrong and again there are many reasons that could be we can identify some of these implausible genotype calls by using some simple population genetic theory so from our observed genotypes we can calculate the allele frequency at any bioluelic snip we’ve called so here the frequency of the a allele is twice the frequency of the aa cools those are our common homozygotes in blue plus the frequency of av cores are heterozygotes in green and we can do the equivalent as you see on the slide for the frequency of the blade knowing the frequency of the ae and the b allele we can use hardy and weinberg’s calculation for how we expect alleles at a given frequency to be distributed into genotypes to generate an expectation for the genotypes we expect to observe at any given allele frequency we can then compare how our observed genotypes i.e the blue green and red clusters fit to that expectation and we can test that using a chi-squared test now harley-weinberg equilibrium is an idealized mathematical abstraction so there are lots of plausible ways it can be broken most notably by evolutionary pressure as a result in case control data it’s typically best to assess it just in controls or to be less strict with defining violations of harley-weinberg cases that said in my experience genotyping errors can produce very large violations of hardly weinberg so if you exclude the strongest violations you tend to be removing the biggest phenotyping errors the previous steps are mostly focused on problematic variants but samples can also be erroneous one example is the potential for sample swaps either through sample mislabeling in the lab or correctly entered data in phenotypic data\nthese are often quite hard to detect but one way to detect at least some of these is to compare self-reported sex with x chromosome homozygosity which is expected to differ between males and females in particular males have one x chromosome they’re what’s known as hemizygous so when you genotype them they appear to be homozygous on all snips on the x chromosome females on the other hand have two x chromosomes they are holozygous and they have a normal x distribution centered around zero which is the sample mean in this case you could also look at chromosome y snips for the same reason however y-chromosome genotyping tends to be a bit sparse and is often not of fantastic quality so there are benefits to using both of these methods it’s also worth noting that potential errors here are just that potential where possible it’s useful to confirm these with further information for example if there isn’t a distinction between self-reported sex and self-reported gender in your phenotype data then known transgender individuals may be being removed unnecessarily the aim here is to determine places where the phenotypic and genotypic data is discordant as these may indicate a sample swap and this might indicate the genotype to phenotype relationship has been broken and that data is no longer useful to you\naverage variant homozygosity can also be applied across the genome where this metric is sometimes referred to as the breeding coefficient it’s called that because high values of it can be caused by consanguinity related individuals having children together which increases the average homozygosity of the genome there can also be other violations of expected homozygosity so it’s worth examining the distribution of values and investigating or excluding any outliers that you see\nexamining genetic data also gives us the opportunity to assess the degree of relatedness between samples for example identical sets of variants implied duplicates or identical twins 50 sharing implies a parent offspring relationship or siblings and those two things can be separated by examining how often both alleles of a variant are shared specifically we would expect parents and offspring to always share one allele at each variant whereas whereas siblings may share no alleles they may share one allele or they may share to it lower amounts of sharing imply uncles and aunts and their cousins and grandparents and so on down to more and more distant relationships in some approaches to analysis individuals are assumed to be unrelated so the advice used to be to remove one member of each pair of related individuals\nhowever as mixed linear models have become more popular in gbos and mixed linear models are able to retain and include related individuals in analyses related individuals therefore should be retained if the exact analysis method isn’t known again it’s worth having some phenotypic knowledge here unexpected relatives are a potential sign of sample switches and need to be examined confirmed and potentially removed if they are truly unexpected and once again it’s important to know your sample the data shown in this graph does not despite what the graph appears to suggest come from a sample with a vast amount of cousins instead it comes from one in which a minority of individuals were from a different ancestry and that biases this metric i’ll talk a little more about that in just a moment\nrelatedness can also be useful for detecting sample contamination contamination will result in a mixture of different dnas being treated as a single sample and this results in an over abundance of heterozygote calls this in turn creates a signature pattern of low level relatedness between the contaminated sample and many other members of the cohort these samples should be queried with the genotyping lab to confirm whether or not a contamination event has occurred and potentially be removed if an alternative explanation for this odd pattern of intersample relatedness can’t be found\nfinally a word on genetic ancestry because of the way in which we have migrated across our history there is a correlation between the geography of human populations and their genetics this can be detected by running principal component analyses on genotype data pruned for linkage to equilibrium for example this is the uk biobank data you can see subsets of individuals who cluster together and who share european ethnicities other subsets who share african ethnicities and subsets who share different asian ethnicities and in a more diverse cohort you will be able to see other groupings as well this kind of 2d plot isn’t the best way of visualizing this for example here it isn’t really possible to distinguish these south asian and admixed american groupings and you don’t get the full sense of the dominance of european ancestry data in this cohort the europeans in this case account for around 95 of the full cohort but because of over plotting i.e the same values being plotted on top of each other in this 2d plot you don’t really appreciate that looking across multiple principal components helps for that ancestry is important to qc many of the processes i’ve talked about rely on the groups being assessed fairly of being fairly homogeneous as such if your data is multi-ancestry it’s best to separate those ancestries out and re-run qc in each group separately so that was a brief run-through of some of the key things to think about when running qc\ni hope i’ve got across the need to treat this as a data informed process and to be willing to re-run steps and adjust approaches to fit cohorts although we’ve got something resembling standard practice in genotype qc i think there are still some unresolved questions so get hold of some data look online for guides and automated pipelines and enjoy your qc\nthank you very much for listening i’m doing a q a at 9 30 est otherwise please feel free to throw questions at me on twitter where i live or at the email address on screen which i occasionally check thank you very much\n\n\n\nConsiderations for Genotyping QC\nTitle: Considerations for genotyping, quality control, and imputation in GWAS\nAuthor: Ayşe Demirkan (a.demirkan@surrey.ac.uk)\nhello everyone my name is aisha demerka i’m affiliated at the university of\nroningam from the netherlands and university of surrey from the uk this is a pre-recorded lecture\nin the second lecture of on-demand sessions introduction to the statistical analysis of genome-wide association\nstudies i will be talking about considerations for genotyping quality control and\nimputation in genomic association studies jivas\nso here you see an overview of the lecture we will shortly go over genotyping platforms\nLecture outline\nand options quality control then i will talk about definition and purpose of imputation and how it is done\nand this is going to include reference data tools analysis of imputed data\nimputation accuracy and accusing\nGenotyping and platforms Genotyping is the process of determining differences in the genetic make- up (genotype) of an individual by examining the individual’s DNA Sequence\nwhat we call is genotyping is the process of determining differences in the genetic makeup\nhence the genotype of an individual by examining the individual’s dna sequence\nof course the technology used for genotyping depends on the structural properties of the genetic variation\nwhether it is a single nucleotide polymorphism or a copy number variation or other structural variations\nit also depends on the project rationale or scientific question and your budget\nmainly and related to that of course how many snips\nyou want to genotype if it is a genomic association study and number of individuals you would like to include\ndepending on your study design you will also be limited with your dna\nsample quality and quantity\nso here on this slide you see the most common approach used for genotyping\nCommon approaches\nsynips and depending on your study you will be most likely using one of these what are those illuminati matrix arrays\nso on the y-axis you see the number of snips that are easily captured by the arrays and on the x-axis you see the\nnumber of individuals and then what do we have we have pcr rflp sequence\npyrosequencing and fluidicum platforms and tacman\num for instance one of the best examples are the illuminae arrays for whole\ngenome scans um whole genome genotyping by these arrays provide an overview of the entire\ngenome and enable you know white discoveries and associations so you using a high throughput\nnext generation sequencing and microarray technologies you can obtain a deeper understanding of the genome\nbecause you are covering a very wide proportion of the genome so you can use\none of their selection of this illumina or f metrics arrays which you think may be suitable for your study\nthere are many options and so for s4 illumina there are genome-wide genotyping is for 18 species\nat the moment so number of markers on each array it changed by products for human up to four minion markers per\nsample are possible now and then there is an infinium low cost screening\narray so for this one for example includes 600 000 markers on it\nyou can use start from 200 nanogram genomic dna and what you can also do you\ncan add some custom marker panels there is an add-on capacity up to\n50 50k markers\nand then there is this omni family of illumina arrays\nOmni family of Illumina arrays\nhere you see a simple description of their coverage and the inclusion of genetic markers in relation to their\nminor alleged frequencies so these expressed chips on the left include only common variation with minor\nlife frequencies higher than five percent some include cmes and some include snips with lower minor allied\nfrequencies so which one to choose among those will depend on your question research\nquestion and population you want to screen for instance are you looking for a rare or common variation in terms of\nsnips are you looking for cmes are you looking are you working with a rare or common disease and what is your sample\nsize and your budget now\ni listed some websites here please take 10 20 minutes to check on\nthe technologies mentioned in the first section using these websites\nQuality control (QC) of genotyping From machine to dataset: genotype calling\nnow let’s talk about genotyping quality control qc you designed your study you chose a\nproper array platform service you used or you used a service from your institute\nso one critical initial step from chemically induced intensity signals and data analysis is a transfer and\nqc of genotypes determined towards your computer this critical step is called genotype\ncalling so genotype calling algorithms are always implemented in the probability\nsoftware accompanying the genotyping platform you choose so you don’t need to invent them yourself\nso it’s typical calling software uses a sort of mathematical clustering\nalgorithm to inter to analyze the row intensity data and it estimates the probability that\ntheir genotype is one of the a a a b or bb for a given individual for a given b\nallelic marker locus so one method of checking initial synopquality is visually inspecting the\nintensity clustering of a particular snip in the overall population and depending on this one can decide\nwhether a snip is characterized by a clear signal or not so\nhere on the left of these figures you see a clear intensity clustering of a\nscene in the population so you see that the common variant is\ndepicted by red on the left there is some hetero heterozygous in the middle\ndepicted by purple and the homozygous um people for the less common allele are\ndepicted as blue and the table on the right it shows the row values that this plot\nis figured from so here the plot shows a tight\nclustering of genotypes and there is not moist much noise in the measurement of\nthis cinema following that there are imputed the\nimportant data qc steps one of them is\nto work on uh replicates so for inspecting plating issues and by\nlooking at you know type concordance this would be a a good thing to do to include the same\ndna sample on different batches of experiments and then there are mendelian areas to\ncontrol for for instance transmission and the inconsistencies for example snips with more than 10 percent manual\narea rate can be excluded this would be based on the number of trios that you would include in your\nexperiment unfortunately this option obviously is only available for family\nbased and trio designs only another thing another qc measure we use\nis snip call rate this is basically the missing genotype rate\n1 minus the missing genotype rate per snip so this can depend on the quality of tsa\nand this is generally between 95 percent and 99 is this is a very standard thing\nto include in your qc another thing is the hardy weinberg\nequilibrium uh deviance of your snip so this is another method for checking the quality and\nexclusion of this of your snips this will be explained in the next slide another one is the sample call right so\nthis is a sample based uh qc method this is a good indication of sample\nsuccess so different platforms have different thresholds but this will this is will be mainly\ndetermined by your initial dna quality and uh it will somehow will be in\nrelation to with a snip color so once you do snip call rate you could do sample call rate and you may want to\nrepeat snip call rate depending on that and another thing to do is sample gender\ncheck for this quality measure you need x chromosome information to calculate this\nand you may want to add this as an additional sanity check in your data to make sure that there is\na perfect overlap with your phenotype files in terms of sex and\nanother important one is sample heterozygosity this is to check for example outliers\nfor example samples with more heterozygosity than expected um can be an indication of contamination\nin your samples and you also want to do something in on top of all of that you\nyou need to check samples cryptic relatedness and unexpected uh twinning\nand whether there is actually a relatedness and structure in the data\nbut this will be more covered in the lecture of redik magi\nso let’s talk about hard wineback equilibrium shortly so as occurrence of uh two allies of a\nHardy-Weinberg equilibrium\nsnip in the same individual are two independent events the distribution of the genotypes across\nindividuals should be more or less in equilibrium with the frequencies of the alas of a b allelic snip\nso this is only possible in ideal conditions of course which would be random mating\nno selection equal survival no migration no mutation and selection based on\nmutation no inbreeding and large population size\nso under these conditions above deviations from high divine back equilibrium is an indication of\ngenotyping calling problems and a commonly used threshold for\ngenotypes variance is a p value of hardy weinberg equilibrium uh that is less than ten to the minus five\nis an indication of a deviation from hardy weinberg equilibrium and you may\nwant to take a look at these snips or you you may want to exclude them from your\ndata set another important thing to always\nGenome builds and alignments\nconsider is genome bills and alignments so the characterization of the human\ngenome is an ongoing effort and a genome build tells us the positions of the snips in the genome on\nthe genome so the latest build is called build 38 but the most commonly used one at the\nmoment is still built 37 for instance the head map was released on build 35\nand bill 36 so you need to be aware of issues relating to merging and meta-analyzing\ndata from different genome builds also for when preparing your data for\nimputation this is very important because you need to make sure that your data is coded according to the same\ngenome build between the target set and the reference data set\nso there are tools uh for that one they are called liftover tools for instance there is one from oxford that we use for\npurpose and i provide the link to that here\nCommonly used software for QC plink...\nso all of these qc steps i shortly went over here are pretty standard and there\nare a couple of widely used tools one very commonly used tool that we also\nuse for data storage analysis and qc is called plink\nhere on this slide i made snapshot of some of the blink\noptions that i also covered during the lecture and these functions are implemented in\nthe plink software and you can use it for the qc of\nyour genetic data so first thing to be able to use bling\nto obviously install plink and you will need to read your genotype call data in plink in the form of a map or pad files\nand then you can perform qc at the snip level remove or extract snips and you\ncan perform qc at the sample level you can remove or extract individuals and under the summary statistics option\nhere there are functions listed to check for call rate missingness hardy-weinberg\nequilibrium highlight frequencies and mandel errors you can also perform\nsex checks what billing can also do is to extract genetic principle components and\nidentify cryptically related individuals or twinnings in the data\nand and the genetic structure of the data and uh can be which you can then use to determine ethnic outliers in your\ndata sets i will not talk about this because this is a part of the lecture of redick muggy\nof the next session now\nIntermezzo\nhere i put two websites here one of them is for\nblink and how to use blink for qc and the other one is\nfor a beat studio which i mentioned is one of the algorithms that you could\nuse for a genotype calling so now take 10-20 minutes to have a look at\nthis website and try to grasp what you can do with them\nGenetic data missingness\nnow let’s talk about imputation why do we need imputation we need imputation to\naddress missingness in the genetic data this is all about missing values in the genetic data where do the missing values\ncome from so during the qc we already set some values to missing right and also during\ngenotype calling you could set some data points to missing but actually most of\nthe missing values come from the initial targeted coverage of the genotyping\nchips and platforms we used so remember that there are many types of arrays some of more dense less dense\nthere are arrays made specifically for oncological studies like onco arrays there is a metabolic\nchip that is designed for metabolic disease especially and there are areas focused on focusing\non mainly snips with higher minor area frequency or their whereas focusing on cnvs\nbut even the dense snip areas do not cover all of the genetic variation they\ncover much less than you would imagine and in addition to that snips included in one array may not be included in the\nother one and for many variable positions on the genome we do not have\nmatching information across genotype set of individuals for instance\nlook what i try to depict here i think of three individuals\nfirst two are typed on the array x and the third one is typed on array y\nso hence they have different missing data points and when you try to\npull their data for a pooled analysis or to be using meta-analysis you’re going\nto have even more missingness in this data because of the non-overlapping positions\nand you will not be able to replicate findings from one of data set and in the\nother one so additionally we will be analyzing only half of the\ngenetic variation and we may miss causal variance in the analysis this is this is\nbecause of all these reasons we use a genetic data imputation\nImputation principle\nso what do we do in principle in principle it means estimating the most likely genotypes in\nan individual at the missing positions by looking at the correlated snip values\nfrom a more complete data set and based on that writing the\nwriting over the missing values in the target data set so how does it work\nfirst of all we need a data set where dense genotypes are directly measured this can be a density array or it could\nbe a set of sequence individuals this we call a reference panel\nthen we use an imputation software or service and by looking at the correlation structure of the density\ntypes or sequence snips we estimate them in the target data set\nso at the end these are probabilities and we end up with dosage information for alleles or\ngenotypes rather than hard genotypes calls and this dosage information\nwhich accounts for the immunity in the estimation is then included in the\ngenomic association study analysis so to sum up the purpose of imputation\nPurpose of imputation\nis to increase power because obviously the reference panel is more likely to contain the causal\nvariance than a less dense geos array to improve fine mapping because\nimputation provides a higher resolution overview of an association signal across\na locus and then to enable meta-analysis because imputation is going to allow viewers\ntyped with different arrays to be combined up to variance in the reference panel\nHistorical milestones 2010-2018\ngoing over the historical milestones in terms of imputation also summarizes the\ntheoretical and technological advancements in human data immune genetic data imputation\nso one important advance in all of these was the generation of reference panels so the first reference panel was hepmap\nand the headmap2 was the most commonly used release of hapmap it consists of a limited sample of individuals from\ndiverse genetic backgrounds 60 yoruba indians 90 hein chinese and japanese and six\nindividuals that were utah residents descending of european ethnic origin\nnow it sounds funny to think that we imputed thousands of people based on the genetic material of 60 euro residents\nonly talking about the europeans but actually this yielded a lot of success and actually this is what we had um only\nfor a long time so we could only impute up to 3 million\nsnips with headmap at the time and then came the 1000 genome reference panel which included at the\nend 2500 individuals from multiple ethnic groups\nand later on and currently the most widely used reference panel is the panel\nof haplotype reference consortium hrc recall shortly this is a combined set of\nwhole genome and exome sequence data for more than 30 000 individuals and use 39 million\nsnips after imputation of course this this is going to depend uh this on the\nscaffold that you use for imputation as well many of these snips will not be include imputed with the good quality\nbut in the ideal conditions you can go up to 39 million and finally we now have a reference\npanel from the transomics for precision medicine topmed program and this consists of almost 100k deeply sequenced\nhuman genomes and it can yield up to 308 genetic variants\nto be identified one technical milestone is mentioning\nwas prefacing of haplotypes so genetic imputation is a highly\ncomputationally intensive process because of the probabilistic framework and high rate of missing data that we\nare trying to deal with one of the major milestones is to reduce the computational\nburden was introduction of prephasing so this idea involves a two-step\nimputation process so there is one initial step of previousing which is actually haplotype\nestimation of the geos genotypes and a subsequent step of imputation into the estimated steady haplotypes\nso this reduces complexity of the imputation process and speeds it up the current version of all imputation\nsoftware can deal with the prefacing approach\nand what is very important is a choice of reference panel\nso it is shown that making use of the all ancestry’s reference panels rather than\nethnic specific reference panel improves imputation accuracy for rare variants in\nany population and formatted reference panels for impude and minimax can be\ndownloaded from the software websites and it’s very important to make sure\nthat genotype scaffold and reference panels are aligned to the same build of the human genome i will get back to that\nlater as well\nso another and very important and current technological advancement that makes our\nlives easier is the imputation services these are freely available services such\nas the michigan and sanger imputation services you can simply format and upload your data in a secure way to this\nserver and get the data imputed and face genotypes back in a few days\nand this depends on the speed and how busy the server is and depending on the\nsample size you are trying to impute of course\nHistorical milestones -Sanger\nso in parallel to michigan imputation server uh there is also a sanger institute uh has\na similar service in this service also you can upload your data in a vca format and optionally\nperform pre-phasing using beagle or shaping software and current reference panels\nin the sanger imputation server includes hrc uk 10k and 1000 genomes\nas i said there is also a server dedicated to the\ntopmat this is all very self\nexplanatory uh this is how uh the sanger imputation server would like\nyou to prepare the data so there is a whole a bunch of instructions there that you would like to use\num so the use of these services comes with instruction and manuals so feel free to make an account there and run\nsome test data says in there you will need to qcn format the data as required\nin the instructions you will need to match the coordinates and reference level of the genome bills and prepare one file for each chromosome this is for\nsanger imputation server and another important thing in terms of imputation is of course the speed\nSpeed-Impute 5 PLOS GENETICS\nso increasing reference panel size improves accuracy of markers with low minor allele frequencies but\nthis positive every increase in computational challenges for imputation methods so recently a new imputation software\ninput 5 was introduced from the same group so it does memory efficient imputation by selecting haplotypes using\nthe positional borrows wheeler transform so using hrc reference panel\nthe developers of the software uh showed that input 5 is up to 30 times faster\nthan minimax 4 and up to 3 times faster than a beagle\n5.1 and uses less memory than both of these methods\nExample framework\nso using all the mentioned considerations up until now you can\nbuild an insico framework similar to this one so you can use for instance blink functions for the first two steps\nof genetic data qc then you can check cheap information and\nstrength issues using rhino tools and if needed you can update your genome\nbuild by using leftover tool and you can then preface by using shape it and\nfinally imputed in-house or using one of the servers are mentioned so two links\nto this software are given here\nnow take time\nprobably hours to explore these three imputation services uh of\nsanger michigan and topmatz uh you will be asked to make an account\nand perhaps it will be need to be improved so take your time to do so\nImputation QC\nand the next topic is imputation related qc so there are two qc steps um\naround imputation one is pre-imputation qc um so we have already discussed standard\nqc after genotyping and on top of that you you may want to exclude snips with less than one percent minor allyl\nfrequency and a post imputation quality is assessed\nby information measures which is in some value in the range of 0\nto 1 and it is typical to filter snips by this\nvalue less than 0.8 for a strict filtering or less than 0.4\nand in the impute software this is called infoscore and in the minimax\nimputation software this is called r square per snip so it’s important to check quality of\ntype snips in the scaffold in the region also by visual inspection of cluster\nplots and you may also want to produce quality plots per chromosome\nvarying by minor allele frequency strata and a position\non the chromosomes for instance is an example figure this\nImputation quality vs MAF\nshows a typical relationship between minor allyl frequency and imputation quality\nso on the y-axis you see the imputation accuracy as a determined by imputation\nquality as by r square on infoscore from different softwares and on the\nx-axis you see the minor allele frequency you see that the accuracy is top when\nthe minor alarm frequency is high when the allyl is common and more common and then the accuracy\ngoes lower where the minority frequency goes lower as well you still have some\nsnips which you still have some well-imputed snips\namong the rare ones as well but most of the low quality snips are going to come from a low minor added frequency\nsnips so keep in mind that when you filter by imputation quality you will be filtering out a lot of rare snips as\nwell so what are the factors affecting imputation quality\nFactors affecting imputation\nso at the genome-wide level the number of individuals imputed has something to\ndo with it for this reason we merge scaffold data sets before imputation if we are going to impute\nmore than one so the the more the merrier\nand the second factor is the reference panel\nthe choice of the reference panel as well as the whole idea is to use the correlation between snips\nacross different populations and this may be different from population to population\nyou want to go for a large multi-ethnic panel if you’re not able to go for a\nlarge ethnic specific panel and uh finally at the snip level uh the\nlower the minor life frequency the lower the quality of the imputation\nis going to be and how to analyze the imputed data\nAnalysis of imputed genotypes\nso for each individual imputation provides probability distribution of\npossible genotypes for each untyped variant these properties can be converted into\nbest guest genotypes but this is not something really generally recommended as it increases false\npositives and it reduces power but also you want to filter your best\nbest guess genotypes you want to put a strict filtering on the best uh guest genotypes and this would result\nmore nas in your data set so it’s better to convert the uh\nprobabilities to expected allele counts and analyze uh by taking the uncertainty\nin the imputation into account that’s really important and to do that you need to match the\ndata formats to the software not all software uses all types of data and you may need to do\ndata conversions um and um software called epex snip test to\nand link to supports the knowledge information and you need to check the lecture from\nmedic magi for analysis of genome-wide data\nMessages\num so this is the last slide of this lecture so the take-home message is that\nis we are dealing with hypothesis free approaches here unfortunately it all comes down to the bittersweet money and\nuh resources we have so you need to think what is the best and most cost effective way of getting\ngenetics done in large sample size and the answer is combining a dense genome scan array with imputation as the reference panels are free at the moment and cost of arrays are going lower as well but you really need to think as the in silico part of doing so also will cause staff and a computational resources to some level and what else should you consider so in comparison you want to know depending on your research question of course whether there is a better array for you or perhaps an array or a metabold chip if you are going to conduct your research in a in a very restricted field and the most importantly what are the future uses of this data because obviously you don’t want to build something that you’re going to use only a couple of for only a couple of years and finalize the research on that you want to you ideally want to invest in big data uh so are you gonna invest in population based cohort or disease-based cohort is it going to be a short-term project or is it going to be a follow-up study that’s going to be likely build up and extended extended throughout the years and by inclusion of new phenotypes so and finally who do you want to collaborate with which consortia which disease\num yeah so i hope this lecture will be useful for your research and future studies and for the people who are interested and to have a better and in-depth understanding of imputation every year two times we have a jivas course organized by university of surrey in collaboration with imperial college and university of tartu from estonia this is a hands-on this includes a hands-on workshop as well as theoretical uh lectures where we teach these concepts and matters in more detail so the last one was in five to ten july 2021 and for information there is an email address you can connect to thank you very much and have a nice conference of the remaining time"
  },
  {
    "objectID": "chapter4.2_transcript.html",
    "href": "chapter4.2_transcript.html",
    "title": "Chapter 4.2: Confounding, Chance, and Bias (Video Transcript)",
    "section": "",
    "text": "Title: Confounding, chance, and bias\nPresenter(s): Cochrane Austria, Department for Evidence-based Medicine and Evaluation, Danube University Krems\nthe goal of clinical studies is to investigate the efficacy and safety of treatments they leave room for three important sources of error that could lead to false study results confounding chance and bias confounders confuse us in interpreting study results and may lead us to infer incorrect conclusions about cause and effect relationships for example in one study we observed that those who drink a lot of coffee are at an increased risk for coronary heart disease is coffee dangerous for heart health or could another factor be responsible for this relationship in our example smoking is responsible for the observed relationship smoking is a risk factor for coronary heart disease people who smoke tend to be people who drink a lot of coffee so taking the confounders smoking not into account one would draw the wrong conclusion that drinking coffee is a risk factor for heart disease we are dealing with the confounder\nProperties of confounders (= confounding factors)\nif the following properties are present the confounder must be related to the outcome regardless of the exposure in our coffee example this means that smoking is an independent risk factor for coronary heart disease regardless of whether someone drinks coffee or not the confounder must be related to the exposure in our example this means that people who smoke often often drink coffee the confounder should not be on the causal pathway between the exposure and outcome this condition would also be fulfilled in our example drinking coffee does not automatically lead to smoking how do you deal with confounders\nHow do you deal with confounders?\nyou can get a grip on confounders that you know in advance in the planning phase for example the inclusion criteria of a study are defined so narrowly that the influence of confounders is eliminated this procedure is called restriction one can also consider confounding in the analysis for example with stratification in our coffee study we would stratify all individuals into groups that is divide them into smokers and non-smokers and examine in both groups if there is a connection between coffee drinking and heart disease if only people who smoke are at an increased risk for heart disease then we would see that drinking coffee is not the cause of heart disease we are mostly dealing with several confounders here multivariate analyses can be calculated which adjusts for several confounders but what do you do with confounders that are unknown to us\nWhat do you do when a confounder is unknown?\nthe only way to deal with unknown confounders is randomization the study participants are randomly divided into study arms the randomization leads to an equal distribution of known and unknown confounders in the study groups if one observes a difference between the groups after an intervention we can assume that this was caused by the intervention and not because influencing factors were already distributed differently between the groups at the beginning randomization is the only effective remedy against unknown confounders\nChance\nanother source of error in studies is chance also called random error the result of a study may coincidentally differ from the true effect in the population just by chance this random deviation has no definite direction if you were to do many of the same studies some would overestimate the effect others underestimate and others would appreciate it correctly one can minimize the influence of chance by large sample size imagine you have a bowl of 500 green and 500 red gummy bears you draw two samples one with ten gummy bears and one with a hundred in the small sample it can be easy for you to draw eight green and two red gummy bears with 100 gummy bears you are probably already much closer to the 50 50 distribution it’s similar with studies in very small studies it may happen despite randomization that the study groups are not similar a non-evidence-based rule of thumb states that studies with fewer than 300 study participants are susceptible to random variability additionally a low event rate is prone to random error a large study size can minimize random errors\nbias is another source of error in studies bias is a systematic deviation from the true effect that can be caused through the design conduction or analysis of a study in contrast to random error bias would always distort the results in the same direction if one would perform the study multiple times there is a variety of bias subcategories here are four types of bias that play an important role in interventional studies selection bias performance bias measurement bias and attrition bias\nSelection bias\nselection bias describes the problem that there are systematic differences in the allocation of study participants imagine a study examines the effects of a physical exercise program versus no intervention and study participants choose which group they go into it is very likely that more sports enthusiasts and health-conscious people opt for the sports group thus two groups would form that would not be comparable from the start in some aspects if at the end of the study it emerges that the sports group had better results this could not be automatically assigned to the intervention since the group already started with better starting conditions selection bias can be avoided by randomization and secrecy of the randomization sequence also called allocation concealment\nPerformance bias\nperformance bias occurs when apart from the intervention under investigation there are systematic differences in the treatment and care of patients for example the type of care and attention changes with the intravenous administration of a medication so if you compare two drugs that are administered differently it remains unclear whether a different effect is due only to the drug or to the other care and attention given in order to prevent performance bias it helps to have a standardized treatment concept for all study groups and to blind all people involved in a study blinding means that the participants in a study are not aware of which people receive which treatment\nMeasurement bias\nmeasurement bias exists when there are systematic differences in measuring outcomes for example if a person evaluates their knee pain knowing if they were in the intervention or comparison group can influence the perception of the pain to prevent measurement bias those who measure the outcomes should not know which individuals receive the intervention and which the control intervention this allows them to assess the outcomes unaffected and objectively with attrition bias\nAttrition bias\npremature exit from the study results in systematic differences between study groups in general it is normal for some people to leave studies early however it becomes problematic if this happens systematically and not accidentally suppose those who feel particularly unpleasant or have many side effects leave the study if at the end of the study we only look at those in the analysis that remained in the study until the very end we would ironically believe that there were very few side effects one can minimize the influence of attrition bias in the data evaluation by providing an attention to treat analysis that includes all individuals originally randomized to a study\nRisk of bias\nwhen we critically evaluate studies we try to estimate the risk of bias bias cannot be measured directly the risk of bias can only be assessed indirectly through evaluation of the study design and the execution of studies in addition the risk of bias between outcomes may vary for example while subjective outcomes such as pain can be strongly influenced by lack of blinding this has no effect on hard outcomes such as mortality\nComponents of a study result\nthe aim of studies is to map the true effect of an intervention as well as possible that is the distorting influence of confounding chance and bias should be contained as much as possible a large sample can minimize the influence of random air the study design the good execution of the study and the analysis can minimize the influence of confounding and bias"
  },
  {
    "objectID": "chapter4.2_transcript.html#confounding-chance-and-bias",
    "href": "chapter4.2_transcript.html#confounding-chance-and-bias",
    "title": "Chapter 4.2: Confounding, Chance, and Bias (Video Transcript)",
    "section": "",
    "text": "Title: Confounding, chance, and bias\nPresenter(s): Cochrane Austria, Department for Evidence-based Medicine and Evaluation, Danube University Krems\nthe goal of clinical studies is to investigate the efficacy and safety of treatments they leave room for three important sources of error that could lead to false study results confounding chance and bias confounders confuse us in interpreting study results and may lead us to infer incorrect conclusions about cause and effect relationships for example in one study we observed that those who drink a lot of coffee are at an increased risk for coronary heart disease is coffee dangerous for heart health or could another factor be responsible for this relationship in our example smoking is responsible for the observed relationship smoking is a risk factor for coronary heart disease people who smoke tend to be people who drink a lot of coffee so taking the confounders smoking not into account one would draw the wrong conclusion that drinking coffee is a risk factor for heart disease we are dealing with the confounder\nProperties of confounders (= confounding factors)\nif the following properties are present the confounder must be related to the outcome regardless of the exposure in our coffee example this means that smoking is an independent risk factor for coronary heart disease regardless of whether someone drinks coffee or not the confounder must be related to the exposure in our example this means that people who smoke often often drink coffee the confounder should not be on the causal pathway between the exposure and outcome this condition would also be fulfilled in our example drinking coffee does not automatically lead to smoking how do you deal with confounders\nHow do you deal with confounders?\nyou can get a grip on confounders that you know in advance in the planning phase for example the inclusion criteria of a study are defined so narrowly that the influence of confounders is eliminated this procedure is called restriction one can also consider confounding in the analysis for example with stratification in our coffee study we would stratify all individuals into groups that is divide them into smokers and non-smokers and examine in both groups if there is a connection between coffee drinking and heart disease if only people who smoke are at an increased risk for heart disease then we would see that drinking coffee is not the cause of heart disease we are mostly dealing with several confounders here multivariate analyses can be calculated which adjusts for several confounders but what do you do with confounders that are unknown to us\nWhat do you do when a confounder is unknown?\nthe only way to deal with unknown confounders is randomization the study participants are randomly divided into study arms the randomization leads to an equal distribution of known and unknown confounders in the study groups if one observes a difference between the groups after an intervention we can assume that this was caused by the intervention and not because influencing factors were already distributed differently between the groups at the beginning randomization is the only effective remedy against unknown confounders\nChance\nanother source of error in studies is chance also called random error the result of a study may coincidentally differ from the true effect in the population just by chance this random deviation has no definite direction if you were to do many of the same studies some would overestimate the effect others underestimate and others would appreciate it correctly one can minimize the influence of chance by large sample size imagine you have a bowl of 500 green and 500 red gummy bears you draw two samples one with ten gummy bears and one with a hundred in the small sample it can be easy for you to draw eight green and two red gummy bears with 100 gummy bears you are probably already much closer to the 50 50 distribution it’s similar with studies in very small studies it may happen despite randomization that the study groups are not similar a non-evidence-based rule of thumb states that studies with fewer than 300 study participants are susceptible to random variability additionally a low event rate is prone to random error a large study size can minimize random errors\nbias is another source of error in studies bias is a systematic deviation from the true effect that can be caused through the design conduction or analysis of a study in contrast to random error bias would always distort the results in the same direction if one would perform the study multiple times there is a variety of bias subcategories here are four types of bias that play an important role in interventional studies selection bias performance bias measurement bias and attrition bias\nSelection bias\nselection bias describes the problem that there are systematic differences in the allocation of study participants imagine a study examines the effects of a physical exercise program versus no intervention and study participants choose which group they go into it is very likely that more sports enthusiasts and health-conscious people opt for the sports group thus two groups would form that would not be comparable from the start in some aspects if at the end of the study it emerges that the sports group had better results this could not be automatically assigned to the intervention since the group already started with better starting conditions selection bias can be avoided by randomization and secrecy of the randomization sequence also called allocation concealment\nPerformance bias\nperformance bias occurs when apart from the intervention under investigation there are systematic differences in the treatment and care of patients for example the type of care and attention changes with the intravenous administration of a medication so if you compare two drugs that are administered differently it remains unclear whether a different effect is due only to the drug or to the other care and attention given in order to prevent performance bias it helps to have a standardized treatment concept for all study groups and to blind all people involved in a study blinding means that the participants in a study are not aware of which people receive which treatment\nMeasurement bias\nmeasurement bias exists when there are systematic differences in measuring outcomes for example if a person evaluates their knee pain knowing if they were in the intervention or comparison group can influence the perception of the pain to prevent measurement bias those who measure the outcomes should not know which individuals receive the intervention and which the control intervention this allows them to assess the outcomes unaffected and objectively with attrition bias\nAttrition bias\npremature exit from the study results in systematic differences between study groups in general it is normal for some people to leave studies early however it becomes problematic if this happens systematically and not accidentally suppose those who feel particularly unpleasant or have many side effects leave the study if at the end of the study we only look at those in the analysis that remained in the study until the very end we would ironically believe that there were very few side effects one can minimize the influence of attrition bias in the data evaluation by providing an attention to treat analysis that includes all individuals originally randomized to a study\nRisk of bias\nwhen we critically evaluate studies we try to estimate the risk of bias bias cannot be measured directly the risk of bias can only be assessed indirectly through evaluation of the study design and the execution of studies in addition the risk of bias between outcomes may vary for example while subjective outcomes such as pain can be strongly influenced by lack of blinding this has no effect on hard outcomes such as mortality\nComponents of a study result\nthe aim of studies is to map the true effect of an intervention as well as possible that is the distorting influence of confounding chance and bias should be contained as much as possible a large sample can minimize the influence of random air the study design the good execution of the study and the analysis can minimize the influence of confounding and bias"
  },
  {
    "objectID": "software_genomicSEM.html",
    "href": "software_genomicSEM.html",
    "title": "Genomic SEM",
    "section": "",
    "text": "Title: Genomic SEM Tutorial\nPresenter(s): Andrew Grotzinger\nLevel: Intermediate\nLength: 14:59\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\nLink to tutorial scripts and datasets."
  },
  {
    "objectID": "software_genomicSEM.html#genomic-sem-tutorial",
    "href": "software_genomicSEM.html#genomic-sem-tutorial",
    "title": "Genomic SEM",
    "section": "",
    "text": "Title: Genomic SEM Tutorial\nPresenter(s): Andrew Grotzinger\nLevel: Intermediate\nLength: 14:59\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\nLink to tutorial scripts and datasets."
  },
  {
    "objectID": "software_conditional.html",
    "href": "software_conditional.html",
    "title": "Conditional Analysis",
    "section": "",
    "text": "mtCOJO\nTitle: How to perform mtCOJO\nDescription:\nPresenter(s): Zhihong Zhu\nLevel: Intermediate\nLength: 14:51"
  },
  {
    "objectID": "software_ewas_transcript.html",
    "href": "software_ewas_transcript.html",
    "title": "Software Tutorials: EWAS (Video Transcript)",
    "section": "",
    "text": "Title: How to Perform Epigenome Wide Association Studies\nPresenter(s): Adam Maihofer"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PGC Video Textbook",
    "section": "",
    "text": "Welcome to the PGC Video Textbook!\nHere, we’ve pulled videos from our PGC experts to give a comprehensive overview of Psychiatric Genetics: From its history, current state of research, and resources for conducting analyses in Psychiatric genetics.\nEach video is embedded into the textbook webpage, and a link to the video transcript has been provided below the video.\n\nTable of Contents:\n\nWelcome and Introduction\n\n\nChapter 1: Introduction\n\n1.1: What are psychiatric disorders?\n1.2: Epidemiology\n1.3 History\n1.4 Psychiatric Genomics: State-of-the-Science\n\n\n\nChapter 2: The Genome\n\n2.1: Organization of the genome\n2.2: Types of genetic variation\n2.3: Evolutionary signatures\n2.4: Linkage disequilibrium\n\n\n\nChapter 3: Technologies\n\n3.1: SNP array genotyping\n3.2: Next Generation Sequencing\n\n\n\nChapter 4: Study designs\n\n4.1: Epidemiological study design\n4.2: Confounding, Chance, and Bias\n4.3: Genetic study designs\n\n\n\nChapter 5: GWAS analysis\n\n5.1: Genotyping Quality Control\n5.2: Imputation\n5.3: Association testing\n5.4: Meta-analysis\n\n\n\nChapter 6: Polygenic Scores\n\n6.1: Polygenic Risk Scores\n\n\n\nChapter 7: Ancestry-specific analysis\n\n7.1: Cross-ancestry analysis\n7.2: Ancestry-specific PRS\n7.3: Local ancestry and Admixed populations\n\n\n\nChapter 8: Post-GWAS Bioinformatics\n\n8.1: SNP Heritability\n8.2: Genetic correlations and partitioned LDSC\n8.3: Gene-association analysis\n8.4: Gene-set analysis\n8.5: Fine-mapping\n8.6: Quantitative Trait Loci (QTLs)\n8.7: TWAS/PWAS/MWAS\n8.8: pheWAS\n\n\n\nChapter 9: Advanced Topics\n\n9.1: Copy Number Variation\n9.2: Mendelian Randomization\n9.3: Genomic Structural Equation Modeling\n9.4: Interactions with Environmental Factors\n9.5: Family-based analysis\n9.6: Therapeutic Implications\n\n\n\nChapter 10: Other considerations\n\n10.1: A Career in Psychiatric Genetics\n10.2: Caution in Genetic Prediction\n10.3 Small Effect Sizes\n10.6 GDPR for Dummies\n\n\n\nSoftware tutorials\n\nCNVs\nConditional Analysis\nCross-Disorder Analysis\nDatasets\nEpigenome-Wide Association Studies\nGene Set Identification\nGenome-Wide Association Studies\nGenomic SEM\nMendelian Randomization\nPolygenic Risk Score\nSNP Heritability and Genetic Correlation\n\n\n\nGlossary\n\n\nSoftware Resources\n\n\nAdditional Reading"
  },
  {
    "objectID": "chapter10.3_transcript.html",
    "href": "chapter10.3_transcript.html",
    "title": "Chapter 10.3: Small Effect Sizes (Video Transcript)",
    "section": "",
    "text": "Title: Small effect sizes\nPresenter(s): Howard Edenberg\nA brief note about small effect sizes.\nHi. I’m Howard Edenberg from Indiana University. [These are my personal views.]\nPsychiatric and substance use disorders are complex genetic disorders. The risk is affected by both genes and the environment. Neither works alone, and they interact, sometimes in very complex ways. No single genetic variant causes a complex trait such as a psychiatric or substance use disorder. Variations affecting many genes contribute to variations in physiology and these in turn contribute to variations in risk for the disorder, the course of the disorder, and the response to treatments.\nGenome wide association studies (GWAS) are currently the best approach to identifying individual genetic variations that contribute to the risk of these and many other disorders. GWAS are unbiased analyses testing common variations across the genome to determine if they affect a trait. But because only about half of the total risk is contributed by genetic variation and that part is distributed among hundreds to thousands of genes, the effects of any one common variant are typically very small. So very large studies are needed to find these variants.\nSince the effect of any one variant is very small, why bother to find them? Some have argued that discovering variants of such small effects is not helpful.  I'll argue here that they’re wrong. What we want to do is to reliably identify the hundreds of variants that contribute to risk even though each individual variant contributes only a tiny amount. There’s much we can learn from this. There are some immediate uses of GWAS findings. The overall patterns of association can help predict part of the risk for the disorder through polygenic risk scores. They can show if traits are genetically related. They can examine potential causality by Mendelian Randomization. And they can help explain heterogeneity in many cases, if there are different patterns of variants among different people with the same overall disorder.\nWhat else can we do once we’ve discovered many variants of small effect? The genetic variants and the pattern of them can lead us into biology. They can show us which genes are involved and which pathways are involved. These lead us into the physiology of the disorders and that in turn can lead us to better diagnosis, to rational drug discovery, and to personalized medicine, all of which I think will be very valuable.\nThe effect of a drug on a pathway is not limited by the effect size of the variant that led us to discover the pathway that’s important in the disorder. Even if the variant is rare or the effect size of the variant is very small, if it leads us to a biological pathway, that pathway can be targeted by drugs that have a very large effect. The impact of a drug upon a pathway is in no way limited by the effect size of the variants that led us to understand the role of the pathway.\nSo I, for one, continue to think that using large GWAS to identify variants and genes that contribute to the risk for these and other disorders will prove of great value. Perhaps not immediately, but in the long run. Without such knowledge we’re really flying blind when we look for new treatments.\nSome thoughts about translating genome-wide association findings into new therapeutics for psychiatry have been discussed in a review article published a few years ago that’s worth looking into. [PMID: 27786187. PMCID: PMC5676453]\nThanks for paying attention. Goodbye."
  },
  {
    "objectID": "chapter7_transcript.html",
    "href": "chapter7_transcript.html",
    "title": "Chapter 7.1: Trans-ancestry analysis (Video Transcript)",
    "section": "",
    "text": "Cross-ancestry analysis\nTitle: Cross-ancestry PTSD and Polygenic Findings, and the Cross-Population SIG of the PGC\nPresenter(s): Laramie Duncan\nCaroline Nievergelt:\nto the PTC worldwide late meeting I see that people are still calling in so let’s give them a minute okay looks like it’s stable now so today’s topic is analysis across ends history we have three speakers today we have Laramie Duncan Elizabeth Atkinson and Alicia Martin all three talks are centered around Shiva’s analysis across industries which is a really timely topic today we’re gonna do something a little bit different than usual we are actually gonna wait after to take questions until all three talks are over and then we will allow you to unmute yourself so if you have a question for any of the presenters then you can just mute yourself and ask a question directly and if that’s not gonna work then we can still take questions we had a chat function okay so let’s get started with Laramie Duncan who is currently assistant professor of psychiatry at Stanford University Larry Moe Laramie received a joint PhD in neuroscience and clinical psychology from the University of Colorado and then did a postdoc in statistical genetics in the labs of Jordan smaller and Mark Daly at MGH in Harvard med school today she will be talking about call centers to analysis in PTSD intercourse population especially interest cope of the PGC so take it away Laramie\nLaramie Duncan:\nokay great um thanks so much Caroline I really appreciate this opportunity to present so thank you to you and pat like you mentioned I’m going to talk about three topics today related across ancestry and analyses first the pgc PTSD analyses second telegenic or scoring analyses and third the cross population special interest group within the PGC so starting with pgc PTSD the original wave of the pgc PTSD analysis was led by dr. Kirsten Conan and my postdoctoral mentor as Caroline mentioned with dr. mark Bailey they provided incredible leadership and mentorship for this I wanted to mention them upfront um and the reason that we’re talking about pgc PTSD is that actually compared to most large-scale Jewess our samples were relatively diverse so as you can see here we actually had a minority of European ancestry participants and we had a rather large sample of african-americans as well as some Latino Hispanic individuals this is again for the first wave of PTSD analyses within the PTC at the time when I made this figure I looked at the other large psychiatric to us and like the rest of medical genetics we can see that most samples are of European ancestry the samples the art of European ancestry across medical genetics tend to be from East Asian populations so despite the fact that pgc PTSD does have greater representation of more populations it’s important to note that it’s still not reflective of world ancestry but nevertheless in conducting these analyses the fact that we had these more diverse samples necessitated modifications to our analysis and so when we approach this problem we did what what anyone does we thought about what might be appropriate talk to experts and we tried a number of different analysis approaches and there were a lot of modifications that were necessary just showing one here we decided ultimately to conduct a trans ancestry meta analysis and so in order to do that we needed to assign ancestry for each of the participants in our study such that within each cohort we could individually cut conduct ancestry specific Jewesses prior to meta-analysis within each ancestry and then transits ancestry meta-analysis so actually though this work has continued in terms of making further analytical improvements to the pipeline and I just want to point everyone here - Caroline’s work on this and Adam a hopper and her group they’ve continued to improve the pipeline with some really nice additions and changes to quality control imputation steps and the analysis and this second round of the pgc PTSD analysis is now available on bio archive the link is here so for questions about the most up-to-date improvements I would suggest that people check out this paper or talk to - Caroline going back to PTSD the first wave that the PTSD analyses though I’d like to point out something that was that was interesting and informative from these analyses so purely by chance alone it happened to be the case that we had about 10,000 individuals of African American ancestry and European ancestry with about 25% cases each despite this comparable these comparable sample sizes it turned out that we had no significant results and the African American samples whereas as expected we had significant snip heritability estimates in the European ancestry individuals as well as apologetic predictions for example using schizophrenia external schizophrenia GYC results to predict PTSD in the European ancestry samples and based on power calculations we thought that we would have enough power in these European ancestry samples but due to expectations of lower transferability and African American samples we didn’t know if we would have enough power and and in this in these samples it turned out that we did not the reasons for this are are at least somewhat well understood number one we have there was worse coverage of African ancestry variants in our samples this is both because African ancestry individuals have greater genetic diversity than other populations and so for any given number of variants you can’t cover as much as of the genome but also as as many people know there’s a bias towards European ancestry variants on typing trip the chips so both of these problems probably contributed also the lack of external data resources from non-european populations is really a problem particularly for these African ancestry samples but this is true for other populations as well and then finally there are also methodological and that inadequacies currently for example differences in linkage disequilibrium and allele frequencies are still not handled as well as they could be in many different analyses so the good news is that in the second wave of the PTSD analyses this paper that I mentioned before from Caroline we had larger sample sizes for all populations and with these larger sample sizes we could actually estimate heritability and have polygenic predictions in the african-american samples so it does look like this was an issue of power and so certainly as expected it helps to have larger sample sizes I’m going to turn it out to a couple of quick points about polygenic risk scores so I don’t know if it’s partly just being in Silicon Valley and being near 23andme but everyone is talking about polygenic risk score scare not you know not within my psychiatry building but just people are very interested in these and a couple of years ago when for example 23andme is releasing results without any apologetic results without any mention that these scores might or would certainly would have differential performance the cross ancestry group so we thought that this was a really important topic to spend some time exploring so to give a bit of background as early as 2009 the ISC paper Purcell at all demonstrated that polygenic risk scores that are derived from European ancestry populations have poor performance and African American samples and this flora performance and non-european ancestry samples has been shown many times since and lots of different polygenic risk scoring publications so what you know kind of the new question that we wanted to look at what we wanted to quantify the decrement of performance between European ancestry performance and other ancestor ease and we looked at a couple of ways of doing\nthis I’ll just show one bit of information here we looked at 10 years worth of polygenic scoring studies first to just see who was included in these studies as expected these studies also have primarily European ancestry but then getting to the performance apologetics scores across different ancestry groups we made comparisons and for example one of the results is that we found that both apologetic scores performed about three times better in European ancestry individuals and in african-american samples I don’t know if I think Alicia may or may not mention an additional data that she has here that has I think even better analyses addressing this question so in summary regarding polygenic scores the performance it’s worse from non-european ancestry populations this is not surprising this was you know known for quite some time but efforts are underway to quantify how much words the performances in different populations I think it’s important to know what to expect both for scientific research or we want to have accurate power calculations and then to the extent that anyone is using these scores in clinical practice ultimately or just you know sort of in their own lives getting 23andme reports it’s important to know how well they might work efforts are also underway to improve prediction across populations but getting to a bigger picture question I think that they questioned a lot of us have on our minds is what do we do about the substantial under-representation of most population of most populations and genetic studies so this gets me to about my last topic and at first or as a point of background information there are many potential solutions to this to this problem and I know that many researchers within the pgc have been working on this in this area for considerable time and so we’re just mentioning one additional approach we you know if I had more time I would mention other efforts that have been ongoing so something that we do it we did is that with high laying mom who is now junior faculty and Mark Bailey’s group I’m at the Broad we started a cross population special interest group within the PDC with the goals of improving applicability of genetic results conducting analyses and supporting the involvement of researchers from different parts of the world in one of our very first meetings\nRoseanne Peterson had a really great idea to write a best practices paper describing the ways in which genetic analyses ought to be modified for different samples of diverse ancestry and in particular admin samples and just mixtures of different different samples and so some of the topics that are addressed in this paper include specific recommendations for how to modest and to modify quality control steps which parameters should be modified and how and why as well there are recommendations for imputation and for how to analyze samples that issue us or mixed model context and this is absolutely a huge team effort um I want to just say thank you to this group I think that the reason that such a best practices paper doesn’t already exist is that they’re actually you know really isn’t any one person in the world that could have written a best practices paper and it’s really thanks to the analysts in this group especially that have expertise in particular areas that we were able to cover these different components of analysis and really write about the different approaches and what the pros and cons are for taking one approach versus another so that’s what’s in this best practices paper I’m highly eggs spoke to an editor at sell and they were very interested in this paper and so they suggested that we write it up as a primer so that’s the current format of the paper we’ll be submitting in on Monday and so that’s our first work product and the special interest group is open to anyone it’s technically within the stats group but if you’re interested we have meetings on the first Wednesday of the month at 1:00 p.m. currently and you can get in touch with us and just thanks to the group all these groups for these incredible collaborative efforts so\n\n\nLocal Ancestry and Admixed Populations\nCaroline Nievergelt:\nthank you thank you very much Naomi and thanks for being on time this was a really nice presentation and I’m sure that there are many comments and questions but let’s keep them until the end we are now moving on to our next speaker and it’s Elizabeth Atkinson who is currently a postdoc at MGH in that broad with Mark Daly and peniel Elizabeth did her PhD at Washington University in st. Louis and is currently developing resources that allow for improved genetic analysis in admixed populations Clemmy she’s a member of the PTC PTSD group and she has developed local ancestry analysis for us today Elizabeth will be talking about her framework enabling well-calibrated genomic analysis of psychiatric traits across admixed populations then the slides are up let’s see okay I’ll see you then so take it away Lisa B\nElizabeth Atkinson:\nall right thank you so much again for inviting me to talk about this project I’m really excited to hear the feedback so yeah this Caroline said today I’m going to tell you about a project I’ve been working on hoping to improve the ability to do sophistical genomic\nanalyses on admixed populations so to start off there we go a whole reiterate\na point that you know we’re hoping is now becoming common knowledge in the GOS community and that laramie just spoke deeply about the vast majority of our\nassociation studies are conducted on European cohorts and if we look more closely at this sort of wedge of the pie\nthat is non-european we’ll notice that only a few handful of percent are populations that are admixed and just to\nmake sure that we’re all on the same page starting off when I say admix I’m talking about an individual whose ancestry is not homogeneous but rather\nis comprised of several ancestral populations so importantly there’s actually many more samples out there who\nhave been genotyped or sequenced alongside phenotypes but they’re not actually making it into this figure they’re being excluded from analysis due\nto being too admixed and there’s also several significant large-scale efforts\nto collect psychiatric phenotypes alongside genomic data in diverse populations some of them led right here\nat the Stanley Center so there really is a pressing need for the development of novel methods that will allow the easy\ninclusion of admix people into association studies we really can’t afford to kind of leave large swaths of\nour data on the table anymore and you know let alone the other problematic aspects of leaving entire ethnic groups\nunderstudied so at mixed individuals are generally\nremoved into the challenges of accounting for their complex ancestry so that there are kind of concerns about\npopulation structure infiltrating analysis and biasing results which can lead to false positive associations\nstudies that do include admixed individuals tend to generally only correct for PCs so pcs actually I think\nI can make a laser pointer here let me do that all right so pcs generally only collect correct\nfor kind of average ancestry so if somebody is say you know 75% African 25%\nEuropean or vice versa and they actually you know can’t really consider a lot of the more fine scale population structure\nthat can still be present on the data and this is important which you know this this sort of fine scale pattern\nmight be different for example in one case in control cohorts and still leave the door open for false positives so here I’m going to present a novel\nanalytical framework to hopefully rectify this issue and allow for the easy incorporation of admix people into\nassociation studies we specifically do this by accounting for local ancestry\nwhich takes into account this finer scale of population structure so to get\na little bit more into the actual kind of method we’re calling this method\ntractor for the moment the core feature of the proposed framework relies on a\ncertain population structure as I said informed by local ancestry so the first step is this automated pipeline to call\nlocal ancestry tracts in your sample so just in case you know you haven’t seen\nany painted carry grim figure before here’s an example of a Latino individual the autosomes are along the x axis\nposition along the chromosomes on the Y and then the two strands of each chromosome are painted according to the\nancestral origin of that tract so we use this information after we collect it to\nkind of improve long-range phasing and haplotype tract recovery which I’ll go\ninto in a lot more detail in coming slides and then the end goal is basically to be able to extract out the\ncomponent ancestries of interest so for example if you have a large European cohort and you have this admixed\npopulation you could use tractor to scoop out the European bits of your admixed cohort to include alongside your\nEuropean individuals and you know same goes for the African and Native American components in this example so in this\nway you can kind of leverage the information and add mix people you don’t have to exclude them from analysis anymore so since the first step in this\npipeline involves calling local ancestry I wanted to validate that it was performing well and kind of the target populations that we had in mind so the\nuse case I’ll be talking about for the rest of this talk is the african-american demographic context and\nwe modeled this after PGCE PTSD african-american cohorts so to test\nlocal ancestry inference using one of the very worthy existing methods to call local ancestry our F mix I simulated a\ntruth dataset that resembles these realistic pgz PTSD populations using\nreal haplotype data so this will retain the LD patterns and you know other genomic features present and real data\nthat are sometimes hard to simulate and using this truth dataset with known\nphase and local ancestry I then ran chapter step one calling you know local\nancestry and quantified how often we got it correct and hardening ly about 98% of\nthe time no matter how you slice it we were getting correct slope lines which recall so this seems to perform very\nwell next I wanted to see how the pipeline would work in kind of a more\nreal looking data set so usually we don’t have perfect phase we have statistically phase data so I took our\ntruth data set and ran it through a standard phasing pipeline shape at two\nusing a thousand genomes as a reference panel I mean noticed something that we kind of hadn’t initially expected to see which\nis that a lot of the originally long trucks were really broken up by these\nswitch errors and phasing so even though local industry inference is performing really well you know if it’s calling\nwell if we have zero one or two African tracks at a given position these long range\nhaplotypes are being disrupted due to switch errors so we ended up building in an additional step in tractor to find\nand unkink is the way we’re using as if you know I’m kicking this garden hose to recover these long range chaplet types\nso here’s our karyogram again now after this I’m kinking step has been\nimplemented we noticed that there were still sort of stretches where due to the local ancestry you know we painted the\nchromosomes on they statistically phase data so there’s some overhang areas that were still not able to be recovered so\nwe decided to implement one more iteration of our F mix on this sort of\nironed out data set to see if this could help recover these full Apple types and\nindeed this did dramatically improve the situation so just to convince you that not only\ndid this you know make our carry grams look prettier but this actually made the data look more realistic I modeled the\ntrack distributions using a plus sign waiting time centered at nine which was the number of generations ago that the\npulse of admixture occurred so given this this is what we would expect the European tracts in our data set in this\nyou know the European tracks of these African American individuals to look like you know given given all things\nbeing correct and we can use this to put a p-value on how likely it is to get the\ndistributions in the various treatments of the data so in our truth data set\nagain focus on these red tracks which is the European segments we do see pretty\nclose to expectations distribution here almost around nine after statistical\nphasing however the tracks get extremely short they’re actually P times 10 to the\nnegative 28 likely and to observe a distribution of this many tiny tracks so very very unlikely after on kinking it’s\nabout twice as improved and with one iteration of the pipeline we have gotten\nthings almost back up to where they should be and just sort of to zoom out and put this all in into perspective\nanother way you can see you know how we really dramatically improved things from this original\npurple phased situation back down to this yellow line which is as close to\nthe truth dataset in black here as we’ve gotten it so far so we are indeed making\nthe situation look statistically significantly more realistic by recovering these long-range haplotypes\nwhich which can be important too if you’re concerned with things like LD for example in your data set we’ve also\ntested this and a bunch of other demographic scenarios and it performs you know similarly well so with\ndifferent admixture fractions putting a different you know demographic models in\nso pulse times at different points in in the past and also between different\nancestries with varying kind of divergence times so in the last couple\nminutes here I’d like to kind of take a step back and look at one of the main applications of this framework which is\nreally to show that it improves performance in Ag wast context so not only should we better correct for\npopulation structure by using this local ancestry information but we can actually identify a new low sy through an\nincrease in power from local ancestry as well so the statistical model that’s\nbuilt into tractor basically tests each snip for an association with the phenotype using the logistic regression\nmodel that I’ve show here where X is the number of copies of the risk allele from the you know the first of your ants this\ntrees x2 is the number of copies from your second ancestry and then you can put in whatever other code various\nimportantly including global pcs you know as for the rest of your parameters\nyou also notice that this is currently written as a two-way admix context but it could be scaled up to an arbitrary\nnumber of ancestries and you can further test if the risk allele is ancestry\nspecific by evaluating the difference between beta 1 and beta 2 using AZ test\nso we wanted to see if there were any kind of power gains using this model and\nI want to acknowledge Adam here who heroically ran a whole lot of these simulations with me\nso to do this we we simulated again a realistic african-american demographic\nscenario we drew a by allelic disease variant with the probability of each\ngenotype copy drawn from the minor allele frequency we then simulated a disease phenotype\nassuming a 10% disease prevalence and an individual’s risk of developing the phenotype was modified basically by\ntheir percent admixture and the presence of the minor allele on the African genetic background so this is basically\nassuming no effect in a European genetic context but an effect on an African genetic context which would be you know\nequivalent to modifying effect sizes due to a tagged snip being present in African background and not in Europeans\nfor a shared causative mutation so we ran a whole lot of simulations varying\nall of these kind of parameters to sort of characterize the landscape of how tractor would improve your power this is\nthe simplest context just showing the allele frequency held constant at a twenty percent minor low frequency and\nin all of these plots the dotted line or the dash line is tractor and the solid line is sort of the traditional G wast\nmethod which does incorporate PCs and you’ll notice that both the same sample\nsize is shown in blue and black we do see a significant power gain using local\nancestry and this can get very large and you know this game can get really big in\ndifferent genomic contexts so for example when you introduce a minor allele frequency difference between the\nancestry is now shown in this red line this sort of gap between the methods\ngrows very dramatic and I kind of want to dwell in this for a moment just to show how much you know how significant\nthis power improvement is so if you pick your favorite power let’s say 80% you’d be able to detect variants that are of\nan odds ratio of about point one lower using our local ancestry incorporating method compared to your traditional G\nwasps or if you pick your favorite odds ratio let’s say 1.2 you would basically have you know very little power to find\nthis variant in a normal context that we’d be able detected at very high power from using\nour our method so we’ve also done a lot\nof sort of tests characterizing other aspects of the genomic landscape since I’m running a little low on time I’ll\njust sort of briefly mention the most significant one which is the effect size difference across ancestries so now\nwe’ve introduced an effect in both backgrounds not only in the African genetic context and the sort of major\ntake-home point is that you know if you have the effects so on the right hand\nside here we have the effect now swapped only in the European background rather than the African background and you’ll\nnotice that we would have effectively zero power to detect it using a traditional model but can detect it at\nreasonable power with Traktor this is you know because European ancestry only\nmakes up about 20% of the sample so the signal would really have gotten swamped\nout if we hadn’t deke involved the local ancestry tracts and importantly in a\ncontext where we would actually expect no improvement from local ancestry the effect is the same everything is the\nsame in both ancestries we don’t see much of a power loss a very minimal\npower loss from using tractor so there doesn’t seem to be you know a major hit from including this in your analyses so\nyou know if you’re uncertain of the effect of ancestry it seems like it’s not a massive problem to incorporate\nthis and importantly we’re talking about you know the perceived effect of this tag snip not the real causative mutation\nand so you know even if there’s the shared causative mutation is the same across ancestries you know what you\nactually would detect in your Jie wasps could differ depending on the amount little frequency or LD patterns or environmental interactions or many other\nfactors that could be different across these ancestries alright to sum up today\nI’ve talked about this readily implemented pipeline that should allow you to be able to include admixed\nindividuals in your jaw studies in a well calibrated manner I am currently optimizing it with\nmembers of the Hale team from the Broad Institute so it should be a very quick\nand work in all systems once it’s finalized there but it’s already written up in\nPython and I can you know distribute it to whoever’s interested we have also\nbuilt in these extra features to improve long range phasing and I’ve shown you\ntoday that we not only you know boost your sample size by allowing inclusion of admixed individuals but we can\nactually further boost your power by leveraging this information from admix populations to find novel variants we’re\nalso hoping to take this in a few kind of directions I’m starting testing on an\nempirical data set that has a very well-established you know ancestry specific hit so blood lipids and\nafrican-american individuals to make sure this really does work not only in simulation but in real miracle data as\nwell we’re hoping that since recently admixed individuals have disrupted LD\nblocks we should be able to leverage this to more kind of fine scale pinpoint\ncausal variants so we’re sort of diving into this now too and developing a\nheterogeneity test to try to automatically suggest and eliminate candidate sites based on your admixed\npopulations and i’m also working with alicia martin who’s speaking immediately next and some other collaborators here\nat the road and mph to try to build a polygenic risk or framework that would actually produce reliable estimates for\nadmixed populations which remains another kind of notable gap in current methods and I’ll just close with saying\nnot only is this procedure useful in the\nG wast context but could really be applied to any situation where you need to control for population structure at a\nvery fine scale manner so for example even things such as you know evolutionary study is doing genome-wide\nscans of selection so with that I’ll acknowledge my collaborators and\nadvisers and I guess I’ll take questions at the very end but thank you for your attention\n\n\nAncestry-specific PRS\nCaroline Nievergelt:\nthank you so much Elizabeth that was really interesting Thanks our third speaker is Alesha Martin a militia is main structure at mth and affiliate researcher at the board in Harvard she received her PhD in genetics from Stanford University and her current research focus is on developing novel statistical methods to improve the generalizability of genetic risk prediction from your centric genetic studies so her talk today is entitled clinical use of current Polytechnic whiskers may May X activate health disparities go ahead Alicia\nAlicia Martin:\nthanks Caroline um sorry try to move this other way well hopefully this goes away so I’m excited to talk to you guys today and thanks for having me here can you guys all see this part of the\nzoom screen on the screen you can see your slides you can see my slides okay\nperfect so I think an important point to start\nat is in thinking about human population history so all of our genetic\ndifferences across populations are sort of shaped by how we originated and how\nwe migrated and mixed as we moved out of Africa so of course humans originated in Africa this has been shown through\ngenetic evidence as well as archaeological and linguistic evidence so many sources I’m showing this and\nthen humans migrated Out of Africa and as they did so they took a subset of genetic diversity with them as they\npopulated Europe Asia Australia and into\nthe Americas another point that I think is worth talking about sorry here is the\nfact that gos are becoming super increasingly powerful and so that’s very very exciting because we’re making so\nmany more bio medical discoveries these days some drugs are even being developed out of this and this has been growing it\nsuch an exponential rate that it’s been really you know impressive and somewhat challenging even to keep track of all of this progress so that’s\nbeen really really awesome to watch unfortunately however as laramie\nmentioned earlier and Elizabeth did as well genetics has this diversity problem\nso now shading the same growth and progress in genetics by the populations\nthat are represented in these genetic studies at the individual level you can see of course that the vast majority of\nparticipants in genetic studies about 80% these days are of European descent and this is far out of step with the\nglobal population where about 16% of the world is of European ancestry and\nperhaps even more troubling Lee if we look at the fraction of individuals that have been in these jiwa studies as an\noverall proportion the progress in diversifying genomics has somewhat stalled or perhaps slid a little bit\nsince about 2014 and so this is a really big issue if we’re trying to generalize studies to everybody and so one of the\nnuances I want to mention here is that it’s not that the studies have been getting smaller studies have generally\nbeen either staying the same or have been growing in different populations but one thing that we’ve seen is that\nstudies have been growing much much much more rapidly in European descent populations than elsewhere and so one\nquestion that I really focus on is in understanding how to do ancestry study\nbiases in genetics impact the generalizability of the knowledge that we can learn from these studies and so I\nbreak that down in a few ways in different aspects of our work but in general I want to highlight a few key\npoints that I think are worth keeping in mind throughout all of these questions so one of these is that the fundamental\nbiology is really shared across different populations so one person from\none population will probably have if they’re gonna have a heart attack it’s probably for the same underlying reasons\nfor example as an individual with another ancestry and that’s true in genetics face as well so when we’ve\nlooked across many different biomedical domains and tried to understand what causal variant effects are in\ndifferent populations these really tend to be mostly shared so it’s not that there’s anything special about genetics\nthat’s different from other biology in general the causal genetic variants seem to be the same and shared across\npopulations but there are some complications and interpreting genetics across populations for several different\nreasons so of course there’s it’s also worth keeping in mind that there’s more genetic variation within than between\npopulations so populations are not substantially genetically differentiated\nto the point where we’re finding completely distinct genetic populations in fact most of the genetic variation is\nshared that’s common across populations and there’s more genetic variation within than between another point that I\nthink is really worth keeping in mind is the LD structure this correlation structure of the genome is one driving\nfactor that’s created a lot of challenges because this really differs across population as a function of human history going back to the first map of\nslaw that I showed you so to try to address this question of how\ngeneralizable generalizable our genetic studies are I started with computing\npolygenic risk scores so laramie talked about this earlier this great enthusiasm in polygenic risk score space and how\nit’s been really impressive to see the growth in this area in the past few\nyears just so we’re all on the same page I think most of you are familiar with a polygenic score but in general this is\njust predicting an individual’s phenotype from genotype so we’re basically taking genotypes from some\ntarget individual some effect size estimates from aji wasps that exists multiplying these together summing them\nup across the genome and that’s basically our phenotypic prediction that’s a very simplistic method there\nare other methods but some considerations that sort of cut across all of these different methods for computing polygenic scores are which\nsnips we should include what weights we should use and one thing that we always\nneed to address in terms of the utility of our polygenic scores is how accurate\nis the score and this is really going to vary a lot with sample size the heritability the genetic architecture of\nthe trait and a number of different factors so definitely worth keeping in mind all of\nthese complexities and interpreting polygenic scores across the literature so a study that we had done previously\nshowed that population history really impacts genetic risk prediction across\ndiverse populations and so a few points here are the genetic prediction accuracy\ndecays with increasing genetic divergence between the discovery and target populations so and this figure on\nthe right I’m showing you a polygenic score distribution computed in the thousand genomes project so you can see\nthat we predicted European populations for example to be taller than American and South Asian populations and we\npredicted East Asian and African populations to be the shortest globally but these differences really don’t line\nup with anthropometric studies and they’re rather misleading these distributional shifts are really really\nmassive and so this is clearly not necessarily reflecting reality we also\nset up some coalescence simulations alongside some statistical genetics\nsimulations and showed that these polygenic scores can differ across populations arbitrarily and these are not necessarily meaningful and we also\nshowed that neutral human evolution alone can be sufficient to explain these differences we don’t necessarily rule\nout selection but we do say that neutral evolution and drift in particular may be driving some of these differences\nthere’s a really interesting couple of papers that I want to point out by Michelle Sohail Jeremy Berg and colleagues that were\npublished in Elife yesterday on this topic so we wanted to look at this in a\nlarge-scale setting and so to do that we looked at the UK biobank data the UK\nbiobank is of course mostly consisting of European descent individuals and so\nwe used those individuals to conduct G wasps for several different traits 17\ntraits that were all quantitative so things like height and BMI and then a number of blood panel traits and then we\nused the diversity in the UK biobank in this non European subset to try to\nunderstand how generalizable prediction accuracy is and so in general what we saw is that if we normalize prediction\naccuracy across all of these 17 traits to how accurately we predicted in europeans we\nsaw a pretty substantial drop-off and prediction accuracy across these different populations so for example on\nthe rightmost part of this graph we saw a four and a half fold improvement in\nprediction accuracy in Europeans with respect to how well we’re doing in African descent populations we’re also\ndoing about twice as well predicting in European populations as in East Asians and you can see how well we’re doing in\nthese other populations so these are really really quite large disparities here so why is this happening a lot of\npeople have written about this before and it’s been well covered in the literature but in general there’s a\npretty predictable basis of polygenic risk for disparities and it obviously relates to who we’re studying so why\nthough well as you know G wasps are best powered to discover variants that are common in the population so if we’re\nstudying European populations over and over again in general we’re detecting variants that are most common in\nEuropean populations which are then able to explain more of the phenotypic variation in European populations than\nthose variants that are less common in other populations additionally the LD differences across populations mean that we are probably\ngetting better tagged snips in the G wasps in European populations than in\nother populations and then there are other really much more complex topics\nthat are influencing this generalizability so difference is an environment selection and other more complicated differences but I want to\nemphasize that there’s a lot of consistent promise from diversifying efforts so far so for example the pgc\nschizophrenia working group led by hi Liang Wong and with a first author max\nLam have been working to really increase the sample size of East Asian individuals and schizophrenia studies so\nright now the European case control studies in schizophrenia are still about\nthree fold larger than the East Asian Studies but the progress in the East Asians have been really really rapid and\nmassive lately and so when we looked at Paulo were able to predict east-asian\nschizophrenia risk in these case control cohorts in general what we saw was that\nwe were much better able to predict east-asian schizophrenia risk using the\nancestry matched east-asian data despite the fact that the European training data was about three fold\nlarger so that indicates that there’s really a lot of promise and value in doing ancestry specific studies hearing\nso lastly I want to turn to this effort that we did to try to compare some bio Bank scale analyses so generally what\nwe’re really interested in was doing equal size G wasps in the UK biobank and in the bio Bank Japan where we have a\nlot of overlapping traits that have been deeply studied then we’re interested in predicting in the European populations\nand in the Japanese population to try to understand what prediction accuracy looks like to see if this was symmetric\nand comparable across populations we also tested prediction accuracy in the UK biobank African descent populations\nso in general what we saw was the ancestry match disease prediction was most accurate so on the Left I’m showing\nyou along the x-axis the disease and on the y-axis I’m showing you the prediction accuracy and the ancestry\nmatched results are indicated by matching colors so on the y axis is the target of prediction and the summary\nstatistics that we’ve generated that we’ve used to generate the predictors are shown in blue for the biobank Japan\nindividuals and in red for the UK biobank individuals and so in general we\ndid better in both scenarios with the ancestry matched Disney use prediction one thing that’s sort of interesting\nthat we learned was the overall disease prediction was more accurate in Japan and that was sort of a consequence of\nhow these cohorts were constructed so the biobank Japan cohort is a more Hospital based disease ascertained\ncohort whereas the UK biobank is a healthier and wealthier than average population-based cohort a similar\nfinding emerges from the quantitative traits so for these general health\nmeasures anthropometric and blood panel traits in general the ancestry match quantitative trait prediction is also\nmost accurate taking a look at these y axes you can again see notable differences and help predict how\naccurate prediction is in each of these populations and the quantitative traits are predicted most accurately overall in\ngeneral in the UK European samples and that’s again because of this ascertain\nmontano the-- populations of individuals\nokay so I want to stop here and wrap up and think about next steps so in general\nI think polygenic scores are really exciting and interesting and may have some power to improve clinical models\nbut right now I’m a little concerned or kind of a lot concerned that they’re likely to increase health disparities\ndue to these vast Eurocentric gee wasp IOC’s so I see this as a call for a few\npushes that we really need to make concerted efforts on one is we need much more diverse jiwa studies and another is\nthat we need new statistical methods to address these major issues and on this topic since it can be a bit sensitive i\njust want to urge everyone that’s working on these cross-cultural you know widespread topics to communicate their\nresearch responsibly and widely and anticipate the implications of your research thinking about the potential\nnegative consequences that will happen I also wanted to point out that there are large efforts for example going on in\nAfrica to scale up jiya studies there especially in psychiatric space for for\nexample developmental disorders and for schizophrenia and I think this is really exciting and it’s taught us a lot about\nhow to do cross-cultural research and an ethically responsible way that a company\nis some capacity-building efforts so I want to acknowledge that this is a huge\neffort involving a ton of really awesome people including my advisor and mentor\nMarc Daly and with that I’m happy to stop and have everyone take questions I\nsuppose excellent presentation Alicia very\ninteresting thank you so much all right so Tammy can you make sure\nthat people can unmute themselves yes it\nlooks like they can and they can also ask questions in the chat box or the Q&A box at the bottom of their screens okay\nlet’s see are there any questions\nso I see people raising their hand let’s see um según would you like to go ahead\nokay so thank you very much my name is chef at sumo so today I’m I’m just\ncalling from the South Floyd on that Monica I have many questions but I would\njust ask them the most important one to me today so I have one question each for each presenter but my but what my fall\nspecial goes to Elizabeth so I want to answer is that the method design which\nis called structure and she has implemented this method by evaluating\nits battle with the common way of checking for for population structure\nthat’s the principle principle component I wanted to parks Elizabeth are you also\na compare method with other other model that is used to control for population\nstructures such as the linear linear mixed model which was implemented in\nmany software so I would like to know how if your model is much more better\nthan endodermis model which is commonly used now in Africa to do Cheevers sure yeah\nthanks so much for your question so yeah we started off kind of doing the simulations with a case control\nframework and I guess it’s also important to note that in all the comparisons that I showed you today for\ntractor versus the traditional GUI smart we do include principal components in\nboth so that is you know improvements even beyond the sort of incorporation of\nglobal ancestry this really is like a local ancestry specific game but yeah\nwe’re also hoping to kind of think about other you know phenotype contexts beyond\nthe case control framework so yeah some some things were to be done but I think\nwe should expect similar power gains across contexts that answers your\nquestion all right\nnext sem Leah Davies hey can you hear me\nyes okay great thanks so much these were great presentations and I guess so one\nquestion you know we’ve been struggling with this in the context of the biobank\nin terms of performance of polygenic risk scores and trying to think about\nways to potentially squeeze out more information from the european central GA\nus that would be translatable to non-european populations and one thing\nthat we’ve thought about but haven’t yet tried and i’m curious if any of the panelists have tried this is instead of\ntaking a clumping approach that’s based on p-values which will be in part driven\nby the allele frequency in the discovery sample using a comb perch that’s\nbasically based on the effect size so that you’re retaining the snip was the\nlargest effect but also the highest allele frequency in the target\npopulation I’m wondering if that could possibly coming of course it’s not going\nto solve the problem certainly but could it possibly improve the predictions a little bit anybody looked at that we’ve\nlooked at it a little bit yeah um so I guess in terms of two different thing\nboth in terms of prediction accuracy and then also in terms of Alicia showed the\nslide where the different thousand genomes populations have apparently different distributions apologetic\nwrists courts mm-hmm so when we clump on all the ancestry is instead of clumping\non European ancestry and when we use a smaller set of snips that accounts for a\ngreater LD across all the populations we actually see pretty much normalized distributions across all ancestors so\nthat was kind of interesting we we tried to look at how well the predictions\nworked across ancestry groups doing a you know clumping ideas like you mentioned and I think unfortunately in\nwhat we looked at we didn’t have enough power to really determine if it was making a difference so I can’t say for\nsure but it seems like a great idea to look into an adequately powered datasets yeah adding to what Laramie said there\nare a bunch of different approaches with how you would do clumping and how you would potentially evaluate even in you\nknow sort of other methods and they can you know various methods can improve on\nprediction accuracy across populations so for example LD pred does a little bit better than printing a threshold\napproach but not as much as we would like and so one issue I think is that\nthere’s no method currently that I’m aware of that looks at multiple LD\npanels across populations to try to translate sort of the effect size the\nmarginal effect sizes from one population to a marginal effect size to another population and so that’s an area\nthat we’re actively working on developing new methods in and so stay tuned and hopefully we’ll have some new\nnew method that you can try out there yeah awesome well if you if you are looking for a biobank size test case\nthat was a lot of diversity let us know that’s great thanks created all right we\ncan take a few more questions I see you David correct yes\nyeah hi so I guess people know me so two things really one is where I’m coming from is that I think\nquite likely that polygenic risk scores will never ever be committee useful so with that as the context my question is\nreally it is don’t don’t we think there’s a risk that in African populations because of the structure\nthat diverse the diversity and lack of linkage disequilibrium that the kind of phenomena that we see with gee wisest in\npopulation have been through the Out of Africa bottleneck in East Asia in Europe the that that stuff just won’t work in\nAfrican populations the polygenic risk scores not work in African populations all would require at least sample sizes\nof ten times a hundred times what we’re using to generate progenitor scores now\nand I think the question is if polishing it with scores are gonna be clinically useful are we gonna massively invest in\ngetting these huge African samples and seeing whether they work or not or are we at this point can I look at it carefully and think maybe there won’t be\ncrimpy useful and therefore will deal with other things in Africa but not that\nyeah David that’s an interesting question and we have also been kind of thinking along the same lines like how\nwould you improve things for everybody the fastest and so clearly the way that\nthe you know way that you would improve things for everybody is by starting to look at the most genetically diverged\ngroup from what’s already been studied the most so looking at the most genetically diverged group from\nEuropeans means looking in Africa and so the way that we would generally help African populations have higher\nprediction accuracy East Asian South Asians you know admixed populations everybody would be to start in Africa to build up\nthese very very large cohorts and so I think that’s part of the reason we’ve had such a concerted effort with trying\nto push much larger studies in Africa because it’s been so vastly underrepresented in genetic studies to\nkind of your first point will they ever be useful there has so I mentioned super\nbriefly that there is some promise from diversifying efforts so the ancestry matched results do a whole lot better in\nnon-european populations than just using a larger European population even when there’s several fold the sample size and\nsort of a meta-analysis approach might improve things even more so I think in general having a\nvery very large African cohort is probably like step one and I’m diversifying and meda analyzing a lot of\ndifferent groups together is probably step two and yeah we’re you’re right we’re a long ways off I think that’s\nkind of the direction we need to move in thank you Alicia that kind of and also\nfit into the question that Geeta sent ill and posted what kind of sample size\nis do we need across various populations to compute and just a specific PRS so do\nyou have any idea of it’s probably very difficult to say right yeah that’ll\ndepend a lot on genetic architecture of the traits you’re looking at how shared the genetic like what the genetic\ncorrelation across ancestries is yeah it’s a hard question to answer without\nwithout some more specifics unfortunately for sure some more funding would help right alright so I see we\nhave time for one more question Monica do you wanna sure excuse me can\neveryone hear me yeah great well first of all I’d like to compliment our three presenters today for doing a fabulous\njob and in particular bringing to a larger audience our attention on this\nvery important problem that you know it sounds like you individually and collectively been working on for some\ntime but sometimes I don’t think the larger scientific community is as plugged into this as they need to be so\nyou’re doing a great service today by presenting what you’ve been working on so so well my question is I think really\nfor Elizabeth and in particular in relation to something that Laramie\nmentions so Larry mentioned that our abilities to sort of detect these\nancestry differences was informed a little bit by the PGC ptsds more diverse\ncohorts that included a larger proportion of individuals from African ancestry in particular and so these\nmethods have been developed using those data if I understand correctly that Elizabeth presented well those methods\nbe available soon for other pgc cohorts to use or even for other cohort\nthat focus specifically on psychiatric outcomes sure so thanks so much for the\ncomment to us all and for the question yeah certainly so we’re using a you know\nPTSD in an african-american demographic context as kind of our our initial use\ncase just to test the performance of this method but it certainly could be applied to any complex you know type you\nknow it’s built around psychiatric phenotype so certainly psychiatric you ties phenotypes but even kind of more\nbroadly and medical genetics complex traits in general should be all served well by by attractor it’s also it should\nalso perform well provided local ancestry performs well its perform well in other populations so I’m starting to\ndo some work looking at more complex demographic scenarios so for example and you know three-way add mixed Hispanic\nLatino individuals are a little bit of a trickier situation when a two-way admixed population so I’m scaling it up\nnow and making sure that it’ll still perform well in those contexts as well but yeah basically any any time that you\ncan accurately call local ancestry which depends mostly on divergence between the component populations doctors should be\nshould be able to to handle it and so do you have a timeline for making the code\navailable I know you said we could email you but to deploy it on the broad website or something like that\nyeah I’m working with some developers for this you know Hale software which is a hosted by the\nBroad Institute and is uh you know going to make it a lot fancier and run faster\nand work in every computational environment so I think the timeline for that is certainly within six months it’s\nuh it’s currently a set of Python scripts that runs but probably will run nicer once I get the Hale developers to\npretty it up so yeah within six months there should be a bi archive paper with\nwith the software attached to it I would say thanks so much everybody I think we\nare past the hour now and I would like to thank all the speakers and all the\nmany participants this was a really world late meeting thanks so much thanks\nCaroline guy thanks everyone yeah thanks so much"
  },
  {
    "objectID": "glossary.html",
    "href": "glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "Below are common terms and acronyms used within this book and across the field of Psychiatric Genetics.\n\n\nA\nAdmixture:\nAllele:\nArea under the ROC curve:\nAUC: see “Area under the ROC curve”\nAutosome:\n\n\n\nB\nBiallelic:\nBonferroni adjustment:\n\n\n\nC\nChromosome:\nCommon variant:\nCopy Number Variant (CNV):\n\n\n\nD\n\n\n\nE\nEffect estimate:\nElastic Net Regression:\nEWAS:\n\n\n\nF\nFalse Discovery Rate (FDR):\n\n\n\nG\nGenetic Correlation:\nGenomic Inflation factor (lamda):\nGenotype:\nGenome-Wide Association Study (GWAS): Chapter X (What is a GWAS?):\n\n\n\nH\nHaplotype:\nHardy-Weinberg Equilibrium\nHeterogeneity\nHorizontal pleiotropy:\nHyperparameters\n\n\n\nI\nIdentity-by-decent (IBD):\nImputation:\nIndel:\n\n\n\nJ\n\n\n\nK\nKinship:\n\n\n\nL\nLasso regression:\nLDSC\nLinkage disequilibrium\n\n\n\nM\nManhattan plot:\nMendelian Randomization\nMinor allele frequency\n\n\n\nN\nNull hypothesis:\n\n\n\nO\nOdds ratio:\n\n\n\nP\nPhenome\nPhenome-wide Association Study (pheWAS)\nPhenotype:\nPleiotropy:\nPolygenic Risk Score (PRS):\nPopulation stratification:\nPower analysis:\nPrincipal Component Analysis (PCA):\n\n\n\nQ\nqqplot:\n\n\n\nR\nR2 (Genetic Correlation):\nRare Variant:\nREF/ALT:\n\n\n\nS\nSensitivity\nSingle Nucleotide Polymorphism (SNP):\nSingle Nucleotide Variant (SNV):\nSNP Heritability (H2SNP)\nSpecificity\nStructural variant:\n\n\n\nT\nTranscriptome:\nTranscriptomic Imputation:\nType I error:\nType II error:\n\n\n\nU\n\n\n\nV\nVariant\nVariance explained\nVertical pleiotropy:\n\n\n\nW\nWES:\nWGS:\n\n\n\nXYZ\nX-linked:"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Welcome to the PGC Video Training Textbook!",
    "section": "",
    "text": "Welcome to the PGC Video Training Textbook!\n\n\n\n\n\n\n\n\n\nHello! I’m Cathryn Lewis from King’s College London, and I’m the Education and Training lead for the Psychiatric Genomics Consortium. I’m very pleased to welcome you to the PGC’s Video Training Textbook.\nThe aim of the textbook is to provide comprehensive training materials in psychiatric genetics. We start with an introduction to mental health disorders, give a background to genetics and the technologies used to generate genetic data, and then we step through the methods that are used to analyze that genetic data, from quality control, to genome-wide association studies, to polygenic scores, and pathway analysis.\nThese videos have been collated from online resources, including those produced for the Psychiatric Genomics Consortium, and others from external groups. They range in length from a few minutes, to an hour, and cover both teaching lectures, and methods tutorials. Some videos are marked as “Basic”, “Intermediate”, or “Advanced”, so you can plan your learning program accordingly.\nYou can use the textbook in any way you choose, working systematically through each section, or picking specific topics that you want training in. If you identify areas that we don’t cover, or that you want to suggest additional videos for, please let us know! The textbook is a flexible resource that we hope to update regularly.\nFinally, a huge thank you to the team, who have given their time, their energy, and their enthusiasm to create the textbook. This is an international team from the US, Europe, and New Zealand, and it’s been a great pleasure to work together.\nWe hope that this Psychiatric Genomics Consortium Video Training Textbook fills a need in building capacity in the psychiatric genetics community, and so, enables us to train a new generation of researchers worldwide. With the rapid expansion of available genetic data, the need for skilled analysts has never been greater! And that is an essential step, if we are to realize the potential of using genetics to improve the prevention, diagnosis, and treatment of mental health disorders. And I hope the Video Training Textbook helps you achieve that aim.\n\n\nThe Video Textbook Team\n\n\nAbout the Psychiatric Genomics Consortium"
  },
  {
    "objectID": "chapter8.5_transcript.html",
    "href": "chapter8.5_transcript.html",
    "title": "Chapter 8.5: Fine-mapping (Video Transcript)",
    "section": "",
    "text": "Title: Introduction to fine-mapping methods\nPresenter(s): Hilary Finucane\ngood morning everyone and welcome to the MGP primer for today it’s 8:30 so we’ll go ahead and get\nstarted with the introductions so this is our penultimate primer for the season\nand we are very happy today to have dr. Hillary Hillary then who can today to\nspeak to us about fine math dating methods her background includes bachelor’s in Harvard in math from\nHarvard she then followed that up with a master’s in theoretical computer science\nhim went on to then complete a PhD in applied math at MIT she was selected for\na very prestigious and IH director’s early independence award and has been\ndoing wonderful work here at the broad she’s now co-director of the program in\nmedical and population genetics and she’s also a assistant investigation at\nthe analytic and translational genetics unit at MGH and is about to being the\nassistant professor at HMS and we are so thankful for her today for sharing this presentation with us she’s happy to take\nquestions and has natural pauses built in her talk but I will also keep an eye on any raised hands and Q&A and so we\nwelcome your participation thank you very much thanks very much Sarah for\nthat lovely introduction and hi everyone I’m happy to be talking today about\nBayesian fiying mapping methods and this isn’t going to be a comprehensive review I’m going to try to give an overview to\nsome of the main ideas in the field but as Sarah said I’m very happy to take questions as I go and answer I’ll be\nmoderating those questions so let me start by talking about the context for\naffine mapping so in a genome-wide Association study we see often these\ndays many genome-wide associated regions so here’s an example of a Manhattan plot from the 2014 schizophrenia to us where\nevery green diamond is now Janome its locus that’s past genome-wide significance and that naturally invites\nthe question was actually going in the locust and there’s a lot of\nquestions that that we can ask about a particular locus that can mean a lot of things and what I’m gonna focus on now\nis what are the actual variants that are driving the Association at the locus and\nso typically when we zoom in on a locus we might see something like this so here\nwe’ve got your moment coordinates on the x-axis and then level of significance on the y-axis and this is an example from\nHighland Kwan’s IBD analysis and what we imagine is going on is that there’s\nactually a simple underlying causal structure or maybe there’s only two causal variants in the locus and it’s\nonly because of patterns of LD and then the noise due to finite sample size that we see all of these many variants coming\nup as associated in this in this way and so the goal of statistical affine\nmapping is to take the Jewess data that shows this complex Association at the\nlocus and to try to you know detangle it and figure out what’s the actual simple\nstory that’s underlying it what are the causal variants that are underlying this association and so why might we want to\ndo something like this well one reason is if we’re interested in genes if we\ncan identify the causal variants and these variants sometimes implicate genes for example the variants may be coding\nvariants that directly implicate a gene or they may be regulatory variants that\nthat we can then tie to a gene and so so\nso if I’m mapping can often help us with this goal of finding causal genes and another reason might be even once we’ve\ngot the name of the gene we want the variant to gene mechanism and that for example might enable us to do an\nexperiment that more realistically recapitulates the disease relevant\nbiology then knocking out the gene altogether and then there’s a another\nset of reasons having to do with a genetic architecture and so for example by looking at many fine mapping results\nacross many low sigh or by building models that are based on online mapping models you might be able to do enrichment analyses which\ntypes of variants tend to be associated or causal for disease moving from\nassociation to find mapping can also enable cross population and cross trait comparisons and has the potential to be\nparticularly useful in prediction and so there’s a lot of a lot of things that\nwe’re trying to do that become easier once we have some model that lets us get\nnot just Association but rather to make some inference about causal structure\nand so today I’m gonna focus mostly on\ndifferent aspects of statistical methods for fine map and this is the outline I’ll start by talking about posterior\ninclusion probabilities incredible sets and then I’ll go through a few different methods points and then I’ll close with a some thoughts on evaluate and find\nmapping methods and so and I’ll pause after each section here and so maybe\nI’ll just start by pausing after that brief introduction if there are any questions so far\ngreat so then let me continue with PII\nkeys incredible sets what what are these kind of basic concepts so our goal in\nfine mapping is to recover the causal variants but of course we can’t always with precise accuracy and perfect\ncompetence recover exactly what the fine mapping with it with the causal variants are and so what does the output of\naffine mapping algorithm typically look like well there’s two aspects that I’ll focus on here we’ll take each variant in\nthe locus and then we can plot it now with the y axis being the posterior inclusion probability so each variant\ngets a P IP and then we can also identify sense of variance called credible set so here one credible set is\nred and one credible set is blue so what are P IPS and what are credible sets the\nposterior inclusion probability for a variant is the posterior probability that the variant is causal and this\ncourses according to the model so once you’ve bought into all of the\nassumptions of your model then the p IP reflects the causality there the causal probably the probability that the\nvariant is causal and so a p IP of one would be the most confident you can\npossibly get and then as the gets lower that means you’re less and less you think it’s less and less likely that\nthis is actually a causal variance driving the signal and this has a couple of different names posterior inclusion\nprobability is the most standard one that i’ve seen but some people call this posterior probability of causality or\nyou may see other acronyms in the literature and then a credible set\ntypically we talk about 95% credible sets is a set of variants that contains\na causal variant with at least 95 percent probability and so and this has\nalso been defined in some alternative ways and in some places in the literature but this is now to my\nunderstanding the most standard use and so if we go back and look at this\nparticular locus you can see that the blue credible set is a set of variants that contains exactly one variant and\nthat variant has a very high p IP and so that means that there’s one signal\nthat’s been really resolved very well so the blue credible set says i think that one of the causal variants is here and\ni’m pretty confident about it and then there’s a red credible set and so that means there’s a second causal variant i\nthink it’s one of these five red variants i’m not quite sure which of the five and my posterior inclusion\nprobability is going to quantify exactly what do i think is the probability that\neach one of these variants is is the causal variant for this second signal\nand so you can think of each credible set as corresponding to one putative causal variant and it’s reflecting the\nuncertainty around which variant is that actual putative causal variant so so\ntypically when we think about fine mapping methods what we’re interested in is getting a p IP per variant and then a\ncredible perd you know variant in the locus and then a credible set each one of which\nflex one causal variant and the uncertainty around where that causal variant might be so let me again ask if\nthere are questions so far on P ip’s and credible sets\nokay so then with that I’ll dig into some of how do we actually try to compute these P IPs and credible sets\nand I’ll start with the case of single causal variant fine mapping so so you\ncan imagine you’ve done a gos you’ve got a particular locus you’re interested in you’ve got the data on the locus and\nI’ll discuss later on whether by that I mean summary statistics and LD or genotypes and phenotypes and now what\nyou’d like to get are some p IP s and some credible sets and you have a choice\nnow which is are you going to figure there’s probably only one causal variant in the locus or there may be multiple\ncausal variants in the locus and the there’s increasingly good evidence in\nthe field that many loci Harbor multiple causal variants and so that’s going to be an important point but single causal\nvariant fine mapping is very robust and\nand statistically straightforward and it’s also a building block for a couple of the different multiple causal variant\nmessage so first I’m going to talk about single causal variant find mapping so here we have our locus we’d like to know\nis what’s the p IP for each variant and then there’s only going to be up one\ncredible set here because we’re assuming one causal variant and so which variants\nshould we put into our credible set and the p IP now these p IPS will sum to one\nwe’re saying there’s actually one causal variant we just don’t know which one it is and so now I’m gonna talk in a little\nbit of technical detail for a few slides on how we actually go about doing this\nso what is the p IP at snip j let’s\nstart by computing the p IP it’s NF j and we can write this as the probability under our models it sniffing j is causal\ngiven the data that we have and we’re being bayesian and so let’s say that we\nhave a flat prior on which variant is causal and so then Bayes rule allows us to say\nto rewrite this probability as the probability of the data given J is causal divided by the sum over all\nvariants in the locus of this probability of the data given with the variant is causal and this is a pretty\nstraightforward application of Bayes rule and then a trick comes in saying that well in order to make the\ncomputation easier let’s just divide everything into both the numerator and the denominator by this null probability\nthe likelihood of the data under a null model in which none of the variants is causal and now we can call this new\nquantity that we’ve got a Bayes factor so the Bayes factor is the likelihood of\nthe data given the variant K is causal divided by this null probability and\nthis just allows us to rewrite our p IP is the base factor person FJ divided by the sum over all variants of the Bayes\nfactors and the reason that this is a nice thing is because this Bayes factor\nturns out to be pretty simple to compute and so Tamala at all showed that the\nBayes factor you don’t actually need to model all of the data at the locus to compute the Bayes factor for a single\nvariant you only care about what the genotypes are at that particular variant and then Wakefield and others showed\nthat this base factor can in fact be computed or approximated depending on the model that you’re fitting from\nsummary statistics and so it can computing this Bayes factor you can just\ngo one variant at a time and compute a pretty straightforward transformation of\nwhat the of the summary statistics that you’ve seen so in particular this\ndoesn’t depend on LD at all and is a linear time computation so this is how\nfor simple causal variant fine mapping you might compute P IPs and so how about\ncredible sets well let’s first remember how we defined a credible set s is a set\nof variants that we’ll call a 95% credible set if the probability that it\nharbours the causal variant because we’re in a single causal variant land is at least 95 percent so we now have a\nprobability for each one of our very that it’s the causal variant and we want to know which causal variant should be\nput together so that we are covering at least 95% of the probability space and\nbecause we only are assuming the single causal variant assumption the probability that the causal variance in\ns is just the sum of the p IPS of the variance in s and so to construct the\nsmallest 95% incredible set we can just add the variant that has the highest P\nIP and then add the variant that has the second-highest VIP and just keep on going until our p IP son to 95 to 95%\nand typically I shouldn’t I should know there’s a lot of different ways to construct credible sets you could always\njust throw all of the variants in and that’ll some to more than 95 percent and so usually the goal is to construct the\nsmallest possible credible set because what you’d like is to have as much\nresolution as possible and to be able to say we really narrowed down our signal\nto as few as possible variance then the question yes asking what values are part\nof the flat prior and what assumptions are made in order to calculate that flat prior absolutely absolutely so so when I\nsay flat prior yeah your I should have clarified this better what I mean is a flat prior over which variant is causal\nwhich is also something that I’ll come back to so the flat priors here is saying a priori before I seen the G\nWasps data in my locus at all I’m gonna say that every bearing is equally likely to be causal there’s another prior that\nhas to be defined that has to do with what’s the effect size of each variant in the locus and there you do have to\nspecify it and that there’s different ways that different folks do that and it\nturns out that that might actually be pretty important but for the sake of time I’m leaving that out of this\nparticular presentation and so here in order for what I said on this slide to hold all what you need is for the prior\non which variant is causal to be uniform across the different variants does that\nanswer the question yeah there is a follow-up one whether a\nsingle causal variant is a prior that there is only one or no causal variant\nor a constraint in this case it’s a constraint so in this case when I say\nsingle causal variant fine mapping what I mean is the model that you write down says there is exactly one variant and\nit’s gonna be one of these so under the prior you know if you have M variants your probability is 1 over m that your\nfirst variant is causal and it’s 1 over m that your second variant is causal and that sums to 1 across the whole locus\nwhen in in subsequent work that I’ll talk about in the next section we put\npriors on the number of causal variants and those my top waiter down weight you\nknow well--there’s tend to up wait sparse sparse solutions like single causal variant solutions but in this\ncase there’s been hard constraint there is only one causal variant at the locus\nand does LD structure affects the p IP there’s a lot of questions coming into\nbed great no so that’s kind of the magical thing about single causal variant fine mapping\nso this was first shown in 2012 and the smaller a tall paper but for one single\ncausal variant there’s a couple different ways to see it and if I had a white board then I would show some of\nthem but the fact that these Bayes factors that you can actually compute\nthe probability of all of the data given that a snip is causal divided by the probability of the data under the null\nmodel that that no longer depends on all of the other variants in the locus you\ncan see this for example if you’re looking at like a linear model a\nspanning of the standard model for quantitative traits them usually write down you can actually write down the normal likelihoods and watch the you\nknow watch things cancel and then a bunch of stuff disappears and you wind up with something pretty simple but there’s also probabilistic arguments in\nboth mama at all and wing at all that show that whether you’re conditioning on X or consider X to be part of your data\nI it actually again you get this this\ncancelling and so it doesn’t your Bayes factors don’t depend on any\nvariant except the variant that you’re computing the Bayes factor for and I think that’s part of why people like\nsingle causal variant fine mapping so much is that means there’s no way to miss specify your LD it’s it’s super\nsimple and straightforward I think a related question just to finish up is\njust whether there are any other methods that then prefer proximity so whether if you have a you know a clustering of\nvariants instead of a single variant i’ma that adjacency is considered in any alternative models interesting so the\nquestion there is now you’re modeling multiple causal variants and you want to put a prior that your causal variants\nare likely to be close together but you don’t want to up wait or down with any particular variant is that right well so\nthat was my interpretation of the question but I’ll read the I’ll read the question which was does your candidate set select our selection require that\nvariants are adjacent or is there a method that prefers proximity so this is about credible sets now so with credible\nsets it doesn’t there’s nothing explicit about adjacency I think that typically\nif you have a single causal variant then the variants that have the highest P IPS\nare going to tend to be an LD with each other and so typically credible sets tend to consist of variants that are and\nat least a medium amount of LD with each other and it can this is even used as a diagnostic and susi method if you’re\ncredible set contains a bunch of variants that are in very loose LD with each other then there’s a sense in which\nthings didn’t work and you should become suspicious so I would say that if the\nmodel is well specified then you might expect a credible set to consist of variants and I’ll deal with each other but there’s nothing explicit here that\nenforces that thank you so much so to recap how might\nyou do the single causal variant fine mapping well first you can take your summary statistics and compute a\nproximate bayes factors or base vectors transform these into P IPs and then compute credible sets from your p IPS\none nice thing about single causal variant side mapping is it also allows us to build some intuition about some\nbasic concepts and fine mappings so one thing that we might be very interested\nin is what factors affect our ability to find math effectively so we’re happy if\nwe get a few variants with high P IP and other the other variants are with low p IP and that means we’ve really been able\nto you know zoom in on the on the causal variance another way to think about so\nso I’m using power in quotes because it’s a very frequent test idea but intuitively we’re trying to say with\nwhat confidence have we been able to identify these causal variance and you can imagine that if there’s a lot of LD\nin your locus then it’s gonna be harder to identify the causal variant if you\nknow in the extreme if you have two variants in perfect LD then it doesn’t matter what your sample size is or what\nyour algorithm is you’re never going to be able to tease apart which of those variants is causal without bringing in\nsome extra information and then the less LD there is and the locus the easier it becomes to kind of tease apart that\nwhich variant is causal in which variant which variants are non causal and then similarly as with to a sample size and\neffect size are both both very important for being able to confidently zoom in on a small number of most likely causal\nvariants and so in this work by shaded\nall the authors wrote down kind of a\napproximate expected p IP at a causal snip under a simplified model and so\nhere’s an example you can imagine you have a locus with ten snips all snips\nhave equal LD they’re correlated to each other at level R there’s a single causal\nsimple that explains 1% of the variance in your phenotype and so now the authors\nwrote down an analytic expression for roughly under this scenario what would you expect the P IP of the causal\nvariant to be and and they created this figure so here high values are good that\nmeans we’d when we were able to narrow in with a lot of confidence on the causal variant and you can see that on\nthe x-axis as the amount of LD among variants in the locus changes you’re\nless and less confident that the causal variant is actually causal and then the colored lines show how as you increase\nyour sample size you’re more and more confident and so being able to get this\nyou know kind of quantitative sense of what’s the trade-off between LD and\nsample size as you’re trying to zoom in on particular causal variants can be a\nuseful way to build an intuition and one comment I want to make here is that when\nwe think about cross population by mapping one reason that it’s that it can\nbe particularly effective to combine information across multiple populations\nand fine mapping is because it changes the LD structure so the relevant LD is\nrelated to the average LD between the two populations and so even if you’re\nnot so if you compare let’s say the same sample size but you can choose to have\nit either all in one population or all in another population re or\nhalf-and-half two populations then because there are differences in LD structure among the two populations\ncombining across populations can help you move to the left in this plot which\nis as you can see a good way to also move up which means you’re more\nconfident in the causal variant so that’s an overview of single causal\nvariant fine mapping and so now I’ll give kind of a high-level introduction\nto a multiple causal variant evasion fine mapping and maybe I’ll pause one more time for questions we had some in\nthe middle but not just because I’m at the outline slide again are there other questions\ngreat so we we know that there’s often\nnot just a single causal variant in a locus and so that’s usually not an assumption that we’d like to hard-code\nand especially as our sample sizes increase that this becomes more and more relevant and is reflected more and more\nclearly in the G loss data that we see so now if we think about multiple causal variant fine mapping there’s two main\napproaches and the first one is to say okay there’s multiple causal variants let’s split our locus up in some way and\nthen apply single causal variant by mapping because that’s a really robust tool that we can use well so then how\ndoes this typically work what does it mean to split the locus up there’s a lot\nof different ways to do this one standard way is conditional analysis and so here’s a figure describing\nconditional analysis let’s say that this is your locus in the top left here and in conditional analysis you take the top\nsignal and then you include the genotypes at that variant as a covariant\nin your Association and if that variant is in high LD with a causal variant and\nthere’s only one causal variant then that variant explains the that variant explains all of the other associations\nin the locus and so by conditioning on that variant you get this you know you\nkill all the signal and you get this modal pattern here so if there’s a single causal variant and if the top variant is in high LD with that causal\nvariant then conditioning on the top variant will kill all of your signal on the other hand if you’ve got two causal\nvariants then conditioning on the top variant is unlikely to kill all of your signal and in particular if that top\nvariant is in high LD with a causal variant then after you’ve conditioned on it then there’s a sense in which you’ve\nyou know accounted for the effect of that causal variant and now you’ve got a locus that’s got one fewer causal\nvariant than before and so you can iterate this and then get these a set of\nindex snips and so conditional analysis is one\ncommonly used way to break complex locusts into multiple signals and then\nto you know one way you might then use simple causal variant fine mapping would be to find map each each of these\nsignals conditioning on the other so once you’ve got all of your index variants that you got by a conditional\nanalysis and maybe you’ll include all but one as covariance and apply single causal variant fine mapping and then\nrepeat that excluding each signal one at a time so that’s a commonly used type of\napproach conditional analysis and it has some limitations one limitation is that\nyou might why there’s no there’s no guarantee that your top variant is in\nhigh LD with a causal variant so here’s an example from the susi paper where\nthey did a simulation where snips 1 and snip 2 are the causal snips but because\nthe yellow snip tags both of the two red snips it comes out as most associated\neven though it’s not in particularly high LD with either one of these causal snips and so this would be a case where\nif you did conditional analysis you start by conditioning on the yellow snip but that wouldn’t properly kill either\nof your signals because this idea that your top variant is an high LD with a\ncausal variant if is violated in this particular case and so conditional\nanalysis is you know one one approach but examples like this motivate instead\nwriting down a Bayesian model to jointly model the effects of multiple variants\nat the same time and so that’s what I’m calling you know approach number two how\nmight we jointly model multiple causal variants in one Bayesian model for the\nlocus the way there are two questions about that that last approach yes oh not\nthree uh-huh what do you mean by top variance is that defined by the G wife’s score yes yes\nsorry by marginal significance and then when you iterate for variants conditionally there’s an assumption that\nit’s not done manually what’s the process like and sorting out hits so\nthere’s a software to do this and it’s pretty automatic right at each case at\neach step you want to take the so okay so I guess typically you you have to\nkind of manual parts you have to decide when you’re gonna stop and that’s often done by setting a threshold of\nsignificance at what point are you going to say you killed all of the signals and so you know you take the most\nsignificant variant you include it as a covariant if any variant passes whatever\nyour predetermined level of residual significance is then you’ll do that again you’ll take the most significant\nvariant condition on it and then and then iterate and then you consider yourself done when no variant passes\nyour predetermined level of significance set answer the question I think so great\nthank you great so then I’ll move on to how we\nmight jointly model multiple causal variants so here let’s start by analogy\nto single causal variant fine mapping but here instead of one variant we’re\ngonna look at sets of variants so let’s let SJ be a set of variants and we want\nto know what’s the probability that this set of variants is causal given the data and we can again apply Bayes rule and\nand then start to try to compute some likelihoods but we get stuck very quickly and the reason is before we were\nonly summing over variance in in the locus and so we could say like what is\nthe space of all things that could possibly happen well variant one could be caused variant two could be causal variant three could be caused when\nthere’s only number of variants possible choices but now what’s the space of all things that could possibly happen\nwell variant one could be causal or variant 1 and 2 could be causal or variance one 3 and 10 could be causal\nand so now if you want to just naively apply Bayes rule you’re summing over all\ndouble configurations of causal variants and that’s large but you know to to the\nsize of the locus and so that’s for a typical locus way too many terms to be\ntractable and so there are a number of different methods to do joint modeling\nof multiple causal variants and each one of them approaches this challenge differently so caviar which to my\nknowledge was I think the first work to write down this model in this way limits\nthe maximum number of causal variants and is typically applied to a smaller loose I and and once you limit the\nmaximum number of causal variants then that limits the you know the total number of configurations as well in a\npretty you know direct way and then there are methods such as fine map and\nApogee that sum over what their algorithm thinks is the most likely\nconfigurations and then more recently the susi method takes a different\napproach based on variational inference for those of you can know what that is it’s analogous to iterative conditional\nanalysis where instead of just doing conditional analysis once through the locusts they then go back and redo the\nconditional analysis multiple times until convergence and this has some nice\ntheoretical properties as well and so this isn’t you know a comprehensive overview of multiple causal variant fine\nmapping but just to give a sense that when you want to do joint modeling of multiple causal variants there’s kind of\na fundamental challenge to the first way we would think of to do it and then there’s been a series of really nice\nwork making that more and more efficient in these different and in other works so\nI’m not going to go into the details of exactly how these different methods work\nso that’s something that I find very interesting and instead I’m gonna touch on two other methods topics and one of\nthem is we informed by mapping so let me pause again for questions before I move on to functionally informed by mapping there\nis one question do you need to take into account effect size when you when you do this either assume effect size of each\ncausal variance in the same or weight causal variance by effect sense yeah that’s um a really subtle point that the\ndifferent methods deal with differently so you have to put a prior on effect size as the usual\nway to do it and then integrate out the prior and so and then the question is well how do you figure out what the\nprior should be and some methods do this\nby having the prior be you know a mixture of normals or learning the prior\nfrom the data in some cases it’s shared across all variants in some cases it’s different for the different variance and\nso that’s an important point that different methods deal with differently\nyou\nso let me sorry is there another question I might be looking at the wrong\nplace just popped up um mention that there’s evidence that there are multiple causal variants for Jia slow side you’re\ncurious he um and just curious as to which studies have confirmed that yeah\nthere’s a couple of different ways to see that I guess I mean one way to see\nthat is if you look at the applications of multiple causal variant methods that\nthen give you a posterior on how many variants there are then that posterior\nis often concentrated away from one another way to see that is doing conditional analysis if there’s a single\ncausal variant then conditional analysis should kill your signal pretty well and it very often doesn’t another is\ndepending on how you define your locus sometimes you can just look at the you know the locus zoom plot and it’s pretty\nclear that there’s more than one signal for example if you’ve got a variance\nwith a high marginal effect that are in low LD with your top variant that’s not really consistent with more than one\nvariant at the locus there’s been some\nwork from on actually estimating amounts\nof allelic heterogeneity from far farhad and others reskin where they try to you\nknow model this specifically but i’d say that there’s just for the fact that it\noften happens that there are multiple causal variants um that seems to be something when you can see in a lot of\ndifferent ways and then the question for how often and how many causal variants i think is a much the subtler and more\ndifficult thing to get at thank you\nwhen we’re just popped up yes oh so\ncaviar Susie and Daphne each you’ve different models is there a way to judge a priori which method best suits our\nusers data so that’s something that I’ll get into towards the end evaluating fine\nmapping methods and I in my opinion one\nof the things that this field really needs more of is benchmarking in in realistic settings and so I’ll talk a\nlittle bit about about that at the end but you can also base it a bit on\nintuition based on just the assumptions that the methods make\nbut I but I think actually rather than go into that I think that empirical like more empirical evaluation is really\nneeded a common thing is also to apply more than one method and then when they agree to have more confidence so that’s\nsomething that art has done where we apply both sine map and Susie and then\none way of evaluating the methods is to look at functional enrichments of the variants that get prioritized by these\ntwo different methods and if you look at the enrichment when they agree versus the enrichment when they disagree and\nyou go with either method then you can see much stronger functional enrichment at the low site where the two methods\nagreed and when they disagree but in our hands at least they mostly agree which\nis I think Cosford thank you okay so I\ngot a question earlier about flat priors and and what I was saying was the\nmethods that I’ve described so far assume that before you look at the G Weiss data in the locus you think every\nvariant is equally likely to be causal but intuitively of course that’s not the case if you look you haven’t looked at\nyour gos data yet you just know which variants are in the locus but some of them are coding and some of them are\nnon-coding then a coding variant is is more likely to drive disease than a\nnon-coding variant and because we’re doing Bayesian analysis here that can be\nincorporated into a prior so a functionally informed prior is one where\nyou take into account the functional annotations that are variant to up weight and down weight certain variants\naccording to which ones are more or less likely to be causal a priori and then\nthe question is how do you set that prior do you have to just kind of trust\nyour own intuition that I don’t know enhancer variants are five times more likely than then you know they’re not\ncoding variants to be causal and one way to get around this question is to learn\nthe prior from the data so there are so a lot of the methods that I described so far if you want to just say what the\nprior that can actually be done pretty simply and what makes this difficult is learning from the data by looking across\nmany low sigh what prior it would make sense to set and so now what you’d like\nto do is say ok I’ve got several different low say I’m gonna find max I’m simultaneously but I want to learn by\nlooking at these low sigh are they consistent with like what how much enrichment are they consistent with and\nso different methods again have have done this in different ways FQs is a\nfunctionally informed single causal variant find mapping method and then painter allows for a functionally\ninformed fine mapping at multiple causal variants and then caviar BF allows for\nmany annotations in a multiple causal variant framework and most recently poly fun leverages polygenic enrichment in a\nby leveraging stratified LD score regression and so to give um just a\nexample of how this works sometimes I’ve pulled a pig or a figure from the puffin paper and so here if you first focus\nonly on the squares then you can see that so the squares reflect here the P\nIPS that are not functionally informed and if you look only at the squares then\nwhat you can see is the none of the P IPS are bigger than 0.4 and this are s 2\n8 8 3 2 6 the Red Square gets a p IP that’s you know somewhere below point 4\nbut that particular variant turns out to be mountain synonymous and so the\nfunctionally informed fine mapping results which are displayed in circles here up wait that in the prior and so\nthen if you look at the posterior inclusion probability or the posterior causal probability here then you can see\nthat incorporating this functional information has bumped up that nonsynonymous variant to a posterior\nprobability closer to one which might map our match our intuition better from\nthe combination of the data together with our understanding that this is an onsen\nvariant so this is this is you know an example of the kinds of ways that\nfunctional information can be incorporated into fine mapping and this has pretty clear advantages for example\nyou know if your prior reflects true biology then you’ll get a more accurate\nposterior and one disadvantage would be\nif you want to use functional information downstream to for example evaluate your fine mapping method or if\nyou sometimes it can be useful to say my fine mapping results don’t actually have\nwhat I haven’t incorporated the functional information yet and so then I can do for example enrichment analyses\nbut I think that especially as these methods become more efficient and robust\nas they have recently then this is going to be an important direction as well a\nlot of very useful type of information to be incorporating into fine mapping so\nthere any questions on functionally informed fine mapping you\nI’m great so then um sorry was that sure\nit was just a question about variants that might be in trans and how that complicates this analysis yeah for sure\nfor sure so in order to do functionally informed find mapping you need a set of\nannotations so when when you say so so\nwhat you’re taking advantage of is you know how to characterize variants if you don’t know how to characterize the\nvariants then you can’t take advantage of that anymore so typically you first start by writing down a set of\nfunctional annotations here are my coding variants here are my promoter variants and one thing that’s different among the different methods is how many\nof those can write down but if something is regulatory and trans in a way that hasn’t been well characterized or that\nyou can’t work into your model then yeah then that’s not something that you can take advantage of with these types of\nmethods and how specific is polyphen to a particular cell type disease or\nphenotype and can that be customized so actually Omer feel this question but but\nin general if you think about functionally informed find mapping it again depends on which annotations get\nused and so if you only incorporate annotations from a certain cell types\nand it’ll be cell type specific my understanding is the default for poly fun is not cell type specific and that\nit uses annotations that don’t correspond to a particular phenotype which makes it pretty widely applicable\nto polygenic phenotypes where you can only pick enrichment estimates I don’t\nknow if I’ll merge on the call but if he is then he shouldn’t feel free to chime in and then does do those annotations\nand include features like promoters and enhancers yeah yeah coding is just one\nexample but there’s depending on which method you’re looking at typically a\nlarge number of annotations that can be incorporated and then this is testing\nthe limits of my zoom abilities but Lela would like has a hand up\nI will thank you so much great all right\nso then maybe I’ll say a few words about summary statistics so many of the\nmethods that I’ve described I haven’t been differentiating so far but many of them rather than requiring your full\ngenotype matrix and phenotype vector can actually be run given only your LD matrix and summary statistics and this\nis convenient because depending on what your sample size is and how you’re\ndefining your lo site the LD matrix can be a bit bit smaller but it’s particularly convenient if you can\nestimate patterns of LD from a reference panel and so I’ll get into that in the\nnext slide but let me first point out that this isn’t actually a coincidence if we call our genotype matrix X in our\nphenotype vector Y rld matrix is then up to normalization proportional to X transpose X and our summary statistics\nallow us to recover X transpose Y X transpose X and X transpose Y are actually sufficient for me in the linear\nmodel that most of these methods are based on and so what that means is that X transpose X and X transpose Y\nstatistically have the all the information about being that you would want to get from x and y and so the fact\nthat there continues to be summary statistics based methods is based on\nthis very nice fact as long as we’re starting from this y equals x people see model then it’s gonna be possible to do\nit from summary statistics although here the only guarantee is if you have the actual x transpose x from your entire\ngenotype matrix so this is full in sample exact LD and of course it doesn’t\napply to you know logistic regression there are things like that and so so\nwhen do you actually need so the statistical guarantees come from in sample LD and when is it okay to use a\nsubset of your samples or LD that you’ve estimated from a different population and so\nBenner it all have written about this particular question and this is their\nschematic of what is the question that we’re asking here so starting from the right you can do fine mapping from\nsummary statistics and LD information if your LD information comes from your G wass data traits and genotypes then\nthat’s optimal and then the question is if you have a reference panel then can it work to compute LD from the reference\npanel instead and their conclusion is that it depends on the size of the\nreference panel and the size of your gos and so as your gos gets bigger you have\nto have a bigger and bigger reference panel and of course the population has to match as well and so for uh I think\nwhat they say is for a Jewess of over 10,000 variants of 10,000 individuals you need a reference panel of at least\n1,000 individuals or something like that and then I think this question of the population must match as well to my\nunderstanding I haven’t seen much work exploring exactly how well do you have to have chosen a perfectly random subset\nof the individuals you did your gos in or is it okay to get the right continent or is it something in between there and\nand I think that the fact that you know a small perfectly matched subset doesn’t\nsuffice means that as your gos gets bigger and bigger you have to really be getting the LD very close to to perfect\nand so I think that continuing to explore exactly in what situations\nreference panel LD is okay and gives accurate answers is something that it\nwould be helpful to still have more work to understand that set of kind of\nconstraints because then you know know if it did work that would be very good\nso that’s some summary statistics versus\nfull data and now move on to with my last small number of minutes oops to evaluating find method methods\nand it looks like I don’t actually have time to talk about evaluating fine mapping methods so maybe I’ll actually\nconclude there and just say the high level of evaluating fine mapping methods\nis that it’s important to to try to\nbreak them in all of the ways that we think they’re broken I’ll show you just this one slide by mapping methods tend\nto assume that all the causal variants in the locus are modeled there’s no imputation noise you have exactly\nbetween one and five or one and ten causal variants and that your phenotype\nis normally distributed and conditional and your genotype you know things like that and then typically when fine\nmapping methods are evaluated all of these assumptions are satisfied in the evaluation and so one thing that my\ngroup has been working on that we think is very important is trying to find other ways to evaluate fine mapping\nmethods both in simulations that might break some of these assumptions and also\nbuy real data analyses that can give us insight into what’s working and what’s\nnot so with that I will conclude because\nwe’re out of time if there’s any final questions maybe I could take one first was that omer wrote and didn’t\ncompletely agree with you that pali thein can be customized but isn’t by default and then I’ll just take one\nquestion I think this is an interesting one I’ve been actually wondering is summary statistics preserve privacy but\nis there a way to publish the true underlying LD matrices or approximations\nthere AB it will also preserve adequate participant privacy I think that’s a\nsuper interesting thing to look into and I don’t know the answer to that I I’m\npretty sure that you can release approximate LD while preserving privacy because approximate LD should be the\nsame in different samples from the same population but I’m not sure whether you\ncan whether or not it’s possible to publish you know infinite precision exact LD while preserving privacy that’s\nnot something I’ve worked on myself and I don’t know of any work on that in particular if someone\nelse on the call does they should chime in good this was a wonderful session thank you so much Hillary this was our\nmost interactive timer yes clearly a topic of great interest very well presented but thank you all and we’ll\nsee you in just a few minutes for the mpg session"
  },
  {
    "objectID": "chapter9.6_transcript.html",
    "href": "chapter9.6_transcript.html",
    "title": "Chapter 9.6: Therapeutic Implications",
    "section": "",
    "text": "Title: Pharmacogenomics knowledge for personalized medicine\nPresenter(s): Michelle Whirl-Carrillo\nMichelle Whirl-Carrillo:\ndirector of the pharmacogenomics knowledge base from jkb at the Stanford University she leads the farm jkb team and is responsible for develop the development of new content projects and futures for the last 20 years she has led pharmacogenomics research on its application to personalize medicine and personal genomics she has leadership roles in multiple nih-funded pharmacogenomic projects and in addition from Farm jkb including the clinical pharmacogenetics implementation Consortium and the pharmacodynamics clinical annotation tool and has served on multiple national International pharmacogenomic working groups and steering committees so her research interests include the translation of human genome sequencing data to clinical implementation using curated pharmacogenomics knowledge so\nCristina Rodriguez-Antona:\nMichelle thank you thank you thank you so much for the introduction and thank you for the opportunity to speak with you all\ntoday I really appreciate it and I’m well aware that I’m standing between you\nand dinner for most people so I’ll try to move it along\noh just a disclosure really fast I I don’t I don’t really have anything to disclose other than my um Public Funding\nso we had an excellent overview of pharmacogenomics um from the previous speaker munir did a\ngreat job explaining it so um just going to start with uh talking about how\nthe amount of pharmacogenomic information that we’ve had over the past 20 years has really gone up and this is\nalmost a a very similar diagram to what munir showed where I just started in the\nyear 2000 there were only about 387 Publications for pharmacogenomics that\nwere found in PubMed for that year versus in 2021 in that year alone we’ve\ngot over 2 600 Publications so we are accumulating knowledge all the time\nthere’s over 35 000 Publications in total in PubMed right now about pharmacogenomics and genetics so it’s\ngreat to have all this information more and more information means that we are more informed but\njust the information alone can be tricky what do we do with that so this raises some issues about how do we organize all\nthis information and how can we standardize it across all these different Publications people use\ndifferent terminologies or are measuring different things and it’s not standardized currently\nit also leads to questions about how can we use this information for clinical\nactionability so we really need centralized resources\nto help us be able to organize this information standardize it so we can\neasily search across it so I’m going to talk to you today about a few resources\nthat are available that help centralize this information but first i’m going to talk about a few\nsources of pharmacogenomic knowledge that we can accumulate together in order to get a better picture of the field so\none is the peer review published studies I already showed you what’s in PubMed right now thousands and thousands of\nstudies we also know that there are some information and Regulatory agency\napproved drug labels so you heard a little bit about that from munir as well that some labels from the EMA for\nexample from FDA do have information on them about testing for particular\ngenetic variants or having a metabolizer status and how you can choose a drug or\nchange the dosage accordingly but that’s a very few drug labels that have that\ninformation on them currently although it is getting more and more all the time\nwe also have published guidelines so there are different groups that write\nactually clinical guidelines for clinicians so how to go from the\ngenotype to dosing the patient most of those guidelines are derived from the\npeer-reviewed published literature though so they’re kind of a derivative of that data source we also know there’s a lot of\nunpublished data out there right so there’s clinical trials and other experiments that may be proprietary from\npharmaceutical companies that information is more difficult to to use\nand to standardize and also to vet as a as a public resource it’s hard to deal\nwith that information but we know that a lot of that information is used in those regulatory agency approved drug labels\nas well a lot of that submitted to these agencies when they’re the drugs are going for approval\nso one of the sources centralized sources for dealing with organizing this\ntype of information that I just discussed is Farm gkp which is the pharmacogenomics knowledge base\nthis is a website with a database backend it’s publicly available it’s probably the biggest resource that’s\npublicly available in the world right now and we started this around the year\n2000 it’s based at Stanford University but our mission is basically to collect\nencode and disseminate from code genomic knowledge for uses ranging from research\nand Discovery all the way through clinical actionability and implementation\nso what do we do at Farm gkb we take the information that is out there that I\nkind of already went over guidelines drug labels what’s published in the literature and we think of this as our\nknowledge information stream this is all publicly available right now for everyone but at from gkb we have a team\nof scientists that extract information from these sources and using expert\nmanual curation we take the relevant parts of of uh like what\nwhat drug what genotype What gene\nEtc what variance we standardize the terminologies and we aggregate this information together and we’re always\nworking towards clinical implementation and adoption and in the end we present some resources that I’m going to show\nyou in a minute to actually go back out into this knowledge stream and we hope enrich it and make it a a better source\nof of information for people who are trying to implement pharmacogenomics in their clinics\nso one source of information at Pharmacy KB are these guidelines that I mentioned earlier and the two groups that are\nprobably published the most information about clinical implementation of pharmacogenomics would be cpic and the\nRoyal Dutch pharmacists association’s Dutch pharmacogenetics working group out of Netherlands\nso I’m from gkb we this is just an example web page I don’t expect you to\nread it that’s just an example but so for example the Dutch working group has some guidelines about the use of\namitriptyline and how sub 2d6 variants can affect implementation of effect dosage of this\nparticular drug and uh the Dutch working group puts out a PDF that is Tech space that people can\nread and get this information themselves but what we tried to do it from dkb is make this a little bit simpler\npresentation through tables and by using all the information available from the\nDutch group itself we have um on the web page a way for people to\ninput what the particular variants are of a specific patient or an example\nand pull up what the recommendation is for that specific genotype so uh just a\nlittle bit of an aid for people to be able to take the information from the Dutch group and um and uh get right to\nthe source without having to understand the mapping from the actual genotype in the gene to the metabolizer phenotype\nsuch as poor metabolizer Etc to the actual recommendation so we have on our website uh just the\nbroken down information from multiple groups including cpic the Dutch group and there are a few other groups that\nhave some one-off recommendations as well and you can compare across these\num due to Resource issues and so forth not all the same drugs have been\num have guidelines written about them from from both organizations and so sometimes one organization may have\nsomething about a drug that the other one doesn’t most of the time the recommendations agree or at least are\nvery similar but sometimes there’s a little bit of differences and you can compare those at the at Farm gkb to see\nwhat the differences or similarities are and we also highlight where guidelines tell you whether give you testing\nguidance what to test and when to test so another source of information that we\norganize would be drug labels we annotate drug labels from FDA but also\nfrom EMA Canada and we’ve had a couple of collaborations with groups in both\nJapan and Switzerland so we have some labels that have been annotated from those groups as well and we’re always\nKeen to collaborate with others so if anybody is interested in working with us\non that please let me know but yes but our biggest group of drug\nlabels would be from FDA and Then followed by with email so\nwe saw an example of a drug label earlier from munir and so sometimes on\ndrug labels they do highlight that there is information but much of the time the\nlanguage on drug labels can be not precise or somewhat vague and so we\nhad feedback from many of our users that what we they really wanted to know what these drug labels is just tell me which\nlabels say I need to test which one is recommend testing and which ones have any kind of information that I can use\nso we came up with this a labeling system where we can tag these labels\nthat we curate with um with those categories and also we get very specific\nif there is prescribing information based on pharmacogenetics on a drug label such as changing the dose or\nchanging the drug we highlight that as well so this is just an example screenshot I\ndon’t expect you to read it of annotation of the EMA label for abacavir and hlab you heard a little bit about\nthis earlier so yes this is a very kind of famous example of pharmacogenomics\nvery well understood and so yes testing is required according to the label the\nlabel gives you specifics about what variant it is specifically in HLA B that\nyou should be interested in and gives you advice about what to do so this is an example of a nicely laid out drug\nlabel and we were able to capture this information very easily but not all drug labels are quite so simple\nso um again we have a page at farm gkb please go ahead and check it out for\nyourself at some point if you are able and inclined we have drug labels from FDA EMA and a few other groups as well\nand you can use this table to compare across and you can drill down to see\nthat exact annotation like I just showed you for the EMA one in a Bakka beer but what’s interesting is that some\nRegulatory Agencies have different information on their labels depending on either they may have be lacking\ninformation altogether about pharmacogenomics on that label whereas other countries or groups have a very\nhigh level of actionability for pharmacogenomics so it’s kind of interesting to look at this webpage to\nsee overall again where the similarities and differences are and it just has to do with what is submitted to these\nRegulatory Agencies and what is deemed actionable at the time\nso we also heard from you near a little bit about randomized control trials\nagain yes this is considered the gold standard and a lot of um clinicians prefer to have information\nthat comes from rcts and this is where they would like to see any kind of proof\nof clinical actionability but as was pointed out it’s really not feasible a\nlot of times we have small populations in these studies again when you mentioned that you know we don’t have\nhuge uh cohorts like in diabetes Etc so the small populations do often make\nstatistical significance difficult and but we do know that statistical significance significance is not always\njust the same as clinical significance anyway um and there is a lack of standardization again across all of\nthese studies and Publications that are uh that come out but we can use a replication of data\nfrom multiple different sources to help us address some of these issues right so we can take an article like this this\nis from New England Journal of Medicine and is talking about a particular\nassociation between a variant and slcl1v1 and a reaction to Simvastatin\nand if we take this and manually curate it at Farm gkb again we’re standardizing\nacross all the Publications the gene names the gene variants there are many different names for genetic variants as\nI’m sure you’re all aware right people can use rsids they can use cdna changes\num protein amino acid change Etc so we can standardize so that we can see\nacross all Publications which ones are talking about the same variant we can standardize in drug terms Etc and we\ntake a bunch of information out about you know statistics and what type of\nstudy and the study size we can collect the information from each article and\nover time there are more and more articles right about a particular Association that can be published\nmany times these articles replicate the original findings but sometimes they do not so sometimes an association is\npublished and the next paper that comes out might refute actually that Association but if you look at a large\nenough cohort of papers and like I said you’re able to compare across because the standardization we can start seeing\nthe bigger picture and write summaries about the association based on the Corpus of evidence and then we can also\ntake the information from those drug labels and the guidelines that I referenced earlier that are published in\nan annotated at Pharmacy KB as well and we can get an even bigger and better picture of what’s going on with a\nparticular Association once we have all this evidence together we can not only write a summary about\nwhat’s going on but we can assign a level of evidence about how confident we\nare and the strength of this particular Association so this is just a screenshot again for a\nparticular association with the same variant with simvastatin and down here\nwe can see that there are two dosing guideline annotations available in farm gkb one from cpic and one was from the\nDutch group and you know in this case we only have nine Publications but there can actually be many many more\nPublications depending on the phenotype that’s studied so based on that grouping of information\nwe can come up with a different levels of of evidence you can read more\nabout the system that we have come up with for standardizing when we’re trying to collect evidence together or visit\nthe web page I can’t go into a lot of detail here because it would take a while to explain but basically we score\nthe information that we take from every single publication we take that information and we add it together\nacross all the different Publications and taking into account if any\nregulatory labels are available and the information on those labels and also guidelines of their information if\nthey’re available and we come up with a total score for that particular summary annotation then based on that score for\nthe summary annotation we can figure out which cutoff for a level that it meets\nand assign accordingly so for all of these summaries you can go if you’re on a farm gkb page you can see uh get a\nfeeling for how much support there is for a particular Association based on the little labels on the left\nhand side that are color coded and also have the level um written in them\nfor in most cases as a level one or instances where we know of a guideline\nor a label that really confirms basically actionability or clinical\nimplementation can be instituted for a particular Association and level two\nwould be those that are very close but maybe don’t have a guideline or a label\nyet but information on a label but there’s a lot of information published and the association looks very strong\nsome people on the other hand are very interested in low-level associations if they’re not looking to implement clinically but they’re looking at from\nresearch perspective okay so I’m going to switch I’m just\nyears for just a couple minutes to tell you a little bit more about cpic again this is one of the groups that writes\nguidelines for how to implement uh pharmacogenomics in the clinic the Dutch\ngroup also writes guidelines as well I happen to be involved in cfix I’m going\nto give you a little bit of the background of that particular group so the goal of cpic is not to tell people\nwhether or not to test or even what to test per se but if you are a clinician\nand you have genetic results in hand we want you to be able to understand and quickly be able to know what you can do\nwith that information preemptive genotyping is becoming\nmore widespread there are direct to Consumer genotyping companies out there\nand some patients are going to their doctors at least in the states with information about their their genetic\nvariants and they want their doctors to be able to understand that information\nand act on it but most clinicians as we heard they don’t have the time for\npatient to sit and and really research for themselves what a\nparticular genetic variation might mean or might imply for a particular drug prescription\nso they need a facile way to access information that can tell them up front what it is that they should do\nso these guidelines are written by expert groups put together these groups involve a clinician include\nclinicians Gene experts Pharmacists\nand sometimes in some cases just research scientists as well\nand again we start with a PubMed literature review so these guidelines are based on PubMed literature reviews\nbut the information is collated together\nand graded for every single outcome by the the\num the clinical authors the authors of the guideline and so a consensus is reached from that particular group and\nthen a statement about what you could do with a particular genetic um\num test result so as part of this process we have to\nunderstand for every genetic allele what the functional implication is somebody\nhad mentioned that before so yes we do have to care about what the function is for all of these variants and so that’s\npart of the um the process for creating stupid guidelines is to look at the\nbasically do a deep dive literature review of every allele for the particular Gene and that guideline and\nsee if we can come to agreement about what the function of that allele is then once we have a function defined for an\nallele we can put the two alleles together and map to a phenotype such as\na poor metabolizer or an ultra rapid metabolizer that you heard about in the previous talk\nonce we have the metabolizer status for a given genotype for a given patient\nthen we can come up with what the therapeutic recommendation would be for that particular phenotype\nso cpic also makes available clinical decision support flow charts and\num and wording for CDs alerts to help aid with clinical implementation for\ngroups that don’t have the resources to do that as well so this is just example charts and CDs language that people\ncould use if they want to implement in their Institution so there’s a bunch of Civic guidelines\nright now the last count was 26 but that might have changed really recently\nbecause we’re always publishing but please go and check out the website for\nyourself we also have a database and API for people that are interested to access the information computationally and\nimport it into their own databases so another challenge with\nimplementing pharmacogenomics would be our nomenclature you heard a little bit\nabout the star nomenclature already I get people ask me all the time who\naren’t in the farm codes next field but more clinical genomics what what’s\nwhat’s the deal with the star wheels why do you guys have them and what do they mean so just really briefly right we\nhave the the symbol the gene symbol and that star with the number after it is just an allele designation and so for\nexample this uh D6 star 8. this is hgbs representation for it so there are you\nknow three different um uh three different variants across the gene that Define this allele so in\npharmacogenomics with the most of the cytochrome p450s we’re interested in\nwhat the combination of variants are across the entire allele it’s not necessarily one particular variant or\nanother but what’s the combination of of uh variance across the entire Gene it’s\na haplotype that matters and so to define the haplotypes the the star\nallele shorthand was um was born and these variants these are Snips so that’s\nquite simple but in many cases the the variation across the gene can include repeats or indels there can be\nstructural variants and copper number variation as well and the variation occurs not just in the\nexons but also in trons or upstream or Downstream of the genes sometimes and they have all have functional effect and\nare very important to to capture and understand what the variation is across\nand so this shorthand helps the community understand what the variation\nis I’ve been asked many times well the star alleles just can they just go away from\nfrom people who are you know clinicians but deal with a mendelian disease many\ntimes or other kinds of genetics and unfortunately I don’t think that’s going to happen so it’s used throughout the\nliterature lab tests and assays test reports all refer to the start allele nomenclature right now and it’s used by\nmany of the prescribing guidelines and this started in the 1990s and it was\norganized to a great extent in the early 2000s by the human cytochrome p450 allele\nnomenclature database in Sweden and they I think they were the first group to really start tracking these alleles and\ndefining them as what’s in what variation is in each allele and they’re the ones that started naming them with\nthe star alleles that has transitioned in 2017 to a group called Farm bar\nthe farm far group has taken over the reins there and so Farm bar please check\nout this group as well so when we’re talking about standardization Farm fire is critical to standardization for these\npharmacogenetic alleles and it is the central repository people who find new allelic variation can submit that\nvariation to the farm bar group and uh and there’s a a very rigorous process\nrules that must be followed for a new star allele to be assigned but if uh if\nthat’s the case if the submission reaches the level of a new star allele it will be assigned\nso just to show you a little bit about the this is not the most complicated example by any means but just to show\nyou a little bit what I’m talking about so this is again a sip 2d6 illustration\num we’re looking at the star alleles star 10 3637 Etc and then this is a list\nof variants down the side and so what we see here is the the 100 C to T variant\nis um is a part of all of these different alleles so is this variation\ndown here so if you for example we’re just going to test this 100 C to t you\nmight not really know which star allele you’re talking about for simplicity’s\nsake many times people just call it a star 10 because okay that’s the defining variant in that particular star allele\nbut you can see that there’s many these other star alleles also contain that variant and then other variants as well\nso it can get much more complicated than this as well and some genes such as 2d6\nof structural variance and it gets it gets crazy but these star alleles really are important for understanding what the\nvariation is and that all has functional implications so the functionality of\nthese different star alleles can and do differ so that leads to one of the other\nchallenges of uh of pharmacogenomics and implementation is that the type of the\ntest what is tested can affect your results right so there have been studies done where they’ve sent the same samples\nout to different labs and gotten different results back and and that a lot of times has to do with whether\nyou’re talking about a panel or an exome or a whole genome sequencing is someone had brought up earlier and it’s true\nthat if you have a panel and not everything is on that panel that you can see in whole genome sequencing you could\npotentially miss genetic variation that exists in that particular patient and\nthat’s why we have to be really careful with this often in the field if you\ndon’t see any variation it is defaulted to what’s called a star one allele which\nimplies that you don’t have any genetic variation or wild type but of course depending on what’s tested\nthat may or may not be true so just something to keep in mind when people are trying to implement pharmacogenomics\nthat transparency about the tests and the test result can be very important there can be situations where a patient\nis tested for pharmacogenomic alleles and none are found and it goes into\ntheir the EMR or their record that they have\nno genetic variation in a particular Gene but we know that that may not always be 100 True right for example\nthere are some non-existing genetic variations where we\nknow of the genetic change but we don’t know what the function is we don’t know what to do with that change but over\ntime you know research is going to eventually elucidate that for us also there’s new variation out there\nthat may not be covered so the current star allele nomenclature can’t cover everything that’s not known yet so as\nnew discoveries happen they’re submitted and we have new star alleles that may not have been tested for previously so\nat the very least\nand what the results are is can be very key to both clinicians and patients\nespecially going forward in the future so a um another project that I’m\ninvolved in is called the pharmacogenomics clinical annotation tool this is a software where the goal\nis to take the output from a genetic test result report sorry the output from\na genetic test such as a panel test or whole genome or exome sequencing\ncome up with the star allele designations For Those Jeans which can be very complicated as was mentioned\nearlier so this software aims to be able to take whole genome sequencing and\ndetermine that without using those tag alleles that are used on the snip panel and then connect the resulting genotypes\nwith clinical guidance and we’ve started with the clinical guidance from cpic but we’ll be expanding that to other groups\nas well such as the Dutch guidelines and also what’s available on a regulatory\nagency drug labels that’s the goal so basically the way this tool works is\nit takes a VCF file which is just um a way that genetic variation can be output\nfrom testing and then we take those allele definitions like from Farm bar\nwhat that has the definitions of what variance are in each star allele and the information from the guidelines\nthat can be accessed via the API in the database and then we also have some messaging\nfrom the farm cat tool as well with caveats and disclaimers Etc and we take\nall of that together so the what’s the user supplies is the actual genetic\nvariation that are detected and we come up with a genotype summary which tells\nyou what the star alleles what the diplotypes are or genotypes for that particular patient based on that VCR\nVCF file and give you a summary of the output then the recommendations by drugs so you\nmay be a sip2d6 star 4 star 8 for example what does that mean we have\nrecommendations for each drug that we know that can be affected or have guidelines showing that the drug\nresponse can be affected by sip2d6 and then we would tell you exactly what the recommendations there are for that\nparticular genotype and then there’s a section of the report\nthat is uh very key but not necessarily you know first and foremost in any\nclinician’s mind or patient for that matter but very importantly we highlight what variation was given to us in the\nVCF file versus what is known to date so we can easily see in this section of the\nreport if not every known genetic variant that could be part of a star allele Etc was tested so in this way we\ncan see with an assigned a genotype at the end if information was essentially missed if\nthere was a known star alleles that were not tested for in this case you would know then if say a patient is a star one\nstar one as the result whether or not there are potentially\nvariation that that patient is carrying in that Gene that was not covered by\nthat test so um it’s great to have this as a report you\nget a purport out of farm cat and that’s a report that a patient could have and take to their doctor or a clinician\ncould look at but in many times as we know clinicians don’t have the time to sit and read reports either so having\nthis information go automatically into an EMR or EHR system is extremely important and that is a a big challenge\nsince they are not standardized across all hospitals and or Hospital systems\nand so that’s another area that we’re working on for the future\num one of the final challenges I’m going to talk about today would be allele frequency and this again was mentioned a\nlittle bit earlier but the issue is that in many cases a mendelian disease a\nfrequency a low frequency is a is a sign of maybe pathogenicity right so\nfrequency can be used as a way to figure out if something might be very important in a particular Gene but unfortunately\nin pharmacogenomics some of these variations can be quite common up to you\nknow 30 percent or even more in certain populations so\nso you cannot use frequency the same way and pharmacogenomics is a sign for what could be effective function\nand yet um you know knowing the frequency of the alleles is very helpful to know for\nexample what are the if we can only test a handful of alleles which ones should you be testing unfortunately we don’t\nknow the frequency for many of the defined alleles they may be undiscovered in a particular patient submitted to\nfarmvar for example gotten a star allele now we know we can test for them but if people haven’t tested for them\npreviously we don’t really know what the frequency in any given population is so\nmany times we think of in pharmacogenetics in the field we think certain star alleles are rare because\nit’s not published much in the literature we can’t find statistics\nabout it but that’s not necessarily the case so we do have to be careful there also\num you know many populations around the globe are understudied so again we want to make pharmacogenomics available for\nall a diverse populations but we haven’t done a lot of studies yet in many of the\npopulations around the global a lot of them have been you know focused on European populations or perhaps Asian\npopulations as well so research is needed there to understand what the frequency is for\nthese for these variants but we always must remember that when we’re trying to implement this the population frequency\nis never a proxy for what a patient actually has so testing the individual patient is really important so the last\nchallenge I’m going to talk about is just the separation of pharmacogenomics from the rest of clinical genomics so I\ngave you a little bit of an overview of all these four resources farm gkb farm cat Farm bar and C pick these resources\ntalk to each other all the time they trade information back and forth\nin the US we have very big projects for clinical genomics clinvar and Clinton are a huge resources\nthat are funded through the NIH but they’re kind of uh isolated these are\nisolated from pharmacology I should say pharmacogenomics is isolated from them because they are very large and and\ncapture most of the genetic clinical genomics that are documented today but\npharmacogenomics kind of exists as a silo we are trying to address that by\ndepositing information from both cpic and farm gkb into both clinvar and Clinton\nthere are some issues with trying to take a pharmacogenomic information and\nwedge it into a clinical genome database and so it’s a little bit tricky at times\nand the deposition of this information is slow but in addition to depositing it we are\nlooking forward to being able to exchange information with clinvar and Clinton as well so what we really want\nto do is not have pharmacogenomics be isolated and siled from Clinical genomics overall and genomic medicine\nand our vision right would be full integration of pharmacogenomics with\ngenetic genomic medicine and this is just a diagram to try to to illustrate\nthat all those pharmacogenomic resources and more could you know eventually be\ncombined together in some way to have a much easier interface maybe one place that people can go and that that group\ncould interact very closely with both clinvar and Clinton and efforts are\nunderway we we have a panel that we’re starting within the Clinton group to\naddress pharmacogenomics and how to better crosstalk between pharmacogenomics and genomic medicine in\ngeneral so um in many cases we don’t know the um\nthe implications of pharmacogenomic discoveries for clinical implementation\nbut we definitely know that the clinical utility is there for for a couple dozen\nexamples at least and then we know that integrating this together with the risk of genomic mess\nand is going to push implementation forward having to try to address this separately is slow going as meniere\ndescribed it’s not being implemented across as many places as we’d like pharmacogenomics to be yet\num it does differ from genomics disease models the haplotypes and those star\nalleles and the fact that you have to be aware of the diplotypes and map that to metabolizer phenotypes in many cases is\nvery different and we are aware of that so we do still need some specialized resources but integrating would be the\nway to go in the future and as of right now we have a few knowledge centralized\nknowledge sources that people can use in the meantime and with that I’d like to just thank all my colleagues there’s\nmany different projects that I was talking about today and that takes many\npeople in many different institutions to help make all this all these projects a\nreality and so I’d like to thank them and thank you for your attention your time [Applause]\nwe have questions I will start from the chat one question regards\nsclo1b1 and statins and basically what they they ask is are there studies uh\nregarding drug adherence and pharmacogenetic testing or genotyping I\nmean does it influence adherence of the patients adherence yes\num I’m not aware of specific studies about adherence to drugs but we know\ncolloquially just anecdotally that this is true and that patients who have\nadverse effects that are not as severe as like what we saw in the previous talk but just myopathy maybe uh you know pain\nor some other kind of nausea Etc can lead patients to stop taking their\nmedication and so yes it has been shown that because of the myopathy that’s\nassociated with a particular genetic variation um there there have been cases like I\nsaid anecdotally where patients just stop taking their medicine because of it yeah\nhi Michelle over here thank you a great presentation fabulous resources I think\nincredibly useful for the community can I just ask are there plans or thoughts\nabout translating those resources into different languages to make them more\naccessible to do that unfortunately resource wise\nbeing government funded we don’t have uh the resources to pay for that but if\nthere are people that would like to collaborate with us we could see what we could do yes that we get that question a\nlot you know can this be in multiple languages and we’re happy to do it uh just a resource issue right now\nokay another question from the chat is despite the fact that pharmacogenomics data is growing rapidly and importance\nof testing is clear the integration in the clinics is very slowly so maybe you\ncould comment on on the barriers or the reasons for that right um yes I think um a lot of the reasons\nfor the slow uptake we just were mentioned previously just having\num the education I think of clinicians to for them to be familiar with this\ntype of data familiar with how to to use it also there are issues putting this kind\nof information in medical records so getting this into the ahr system I think would be key that’s one barrier we know\nto uptake and then just also you know testing and reimbursement what are you\nsupposed to test for when should you test who’s going to pay for it these kinds of issues definitely in effect\nimplementation as well last question yeah thank you as well for a great\npresentation we in our company started testing the\nfarm cat and I’d like to ask how many drugs are in this moment\nsupported because we have a only a limited number and also if it can be adjusted by the\nusers the parameters and what are you going to expend it yeah absolutely so\nyes so if you um check out farmcat.org so we’ve updated a lot of the\ndocumentation maybe since you have checked it out so all of the drugs that\nare covered by cpic guidelines are currently covered by Farm cat and their end report all the genes and yes you can\nyou can you can alter well first of all it’s freely accessible so if you want to\ndownload the tool and Branch off you can change the code however you like but it\nis also a customizable in terms of if you have jeans that you want to include\nand you have the definition files for those you can add those as you want and same with uh with the recommendations as\nwell and we are going to be expanding those within the next year is the goal to to include\ndrugs and recommendations from the Dutch pharmacogenetic working group and also\nwhat information we can find on the first the FDA labels we’ll probably start with FDA but then ultimately can\nexpand from there as well okay uh those are bioinformatics that I\nwork in in our company on it so I’m not completely familiar but is it possible\nto speak to your its yeah sure sure um you know send um I didn’t have the\nemail up here but if you send email to just feedback at farmgkb.org we will make sure it gets to\nfarm cat I think there’s also Farm cat at Farm gkb.org but I can’t swear to\nthat so if you send it to feedback at farmgkb.org and you say this is a question about Farm cat we can deal with\nthat we’re a very small team and we are the same people working on many projects\nat once but um yeah but yes definitely thank you thanks\nokay thank you so I I want just to remind you that if you continue"
  },
  {
    "objectID": "chapter2.1_transcript.html",
    "href": "chapter2.1_transcript.html",
    "title": "Chapter 2.1 Organization of the genome (Video Transcript)",
    "section": "",
    "text": "[Music]\nHumans around the world have much in common, but also enormous diversity. Some of the differences between each of us come from our environment and life experiences, but our DNA plays an important role in determining our appearance, our traits, and our health. There are thousands of genes in the human genome sequence. Changes in individual genes can determine if we have freckles, can digest lactose, have wet or dry earwax, are red-green color blind, or are likely to have blue eyes or think broccoli tastes better. Individual genes can also determine if we will develop sickle cell anemia, cystic fibrosis, or Huntington’s disease. Multiple genes act together with our environment to determine our hair and skin color, our height, our weight, our blood pressure, and our risk of developing type 2 diabetes, depression, cancer, some autoimmune disorders, and many other conditions.\nIn spite of all these potential differences humans are 99.9% genetically identical. How is it possible that we are all so similar and yet so different? Let’s zoom into the smallest genetic unit: a single nucleotide of deoxyribonucleic acid or DNA.\nA DNA nucleotide is composed of sugar and phosphate groups, and one of four nitrogenous bases: adenine, thymine, guanine, and cytosine, annotated in shorthand as A,T,G, and C. The sugar and phosphate groups form the DNA structural backbone, allowing nucleotides to concatenate into a long single strand of DNA, while the bases determine DNA sequence. The chemical properties of DNA allow bonds to form between the bases in order to create a double strand with two hydrogen bonds pairing A and T and three pairing C and G. Though different types of human cells can be very different in appearance and function, they contain the same genome, which consists of about three Giga bases or three billion base pairs of DNA. All the DNA in the cell would be about two meters in length if it were stretched out and must be condensed down to fit into cells as small as 10 micrometers across.\nThe DNA is first coiled into its canonical helix structure, and then wrapped around histone proteins to form a DNA protein structure called a nucleosome. These nucleosomes can be further wound and coiled together to create a compact structure that fits into the nucleus. During cell division, the DNA is organized into tightly wound chromosomes, 46 in total, with 23 coming from each parent. These chromosomes can be easily and accurately separated during cell division, guaranteeing that each new cell contains an exact copy of DNA. Outside of cell division, the DNA is decondensed in the nucleus, allowing greater accessibility.\nThe transcriptional machinery regulates expression of the approximately 20,000 genes, which, although they correspond to less than 2% of all genomic DNA, encode all the proteins necessary to build and run a human cell. So, to return to the original question, “how is it possible for all of human diversity to exist when we are 99.9% genetically similar?”, it is important to remember that the 0.1% of DNA that varies, on average, between each of us, actually corresponds to about 3 million differences across the genome, with 20,000 of them on average falling into protein coding genes. Although that equals approximately one difference per gene, in reality, these differences are not evenly distributed across the coding regions.\nDifferences in DNA sequence are called variants, and those affecting a single position are called single nucleotide variants or SNVs. Common SNVs that occur in more than 1% of a population are called single nucleotide polymorphisms or SNPs. SNPs, along with larger scale sequence changes like deletions, duplications, and rearrangements, create all the richness of human genetic diversity at the population level.\nThis raises many questions on an individual level as well: How does DNA determine our traits? How can we understand what a gene does, and how variants in that gene might affect our lives? What can our DNA tell us about our risk, and our loved ones risk, of disease? How can information from our genomes improve our medical care? Understanding genetics allows us to apply the concepts of heritability and genetic variation to questions of human health and disease in our world today."
  },
  {
    "objectID": "chapter3.2_transcript.html",
    "href": "chapter3.2_transcript.html",
    "title": "Chapter 3.2: Next Generation Sequencing (Video Transcript)",
    "section": "",
    "text": "How to sequence the human genome\nTitle: How to sequence the human genome\nPresenter(s): Mark J. Kiel\nYou’ve probably heard of the human genome,the huge collection of genes inside each and every one of your cells. You probably also know that we’ve sequenced the human genome, but what does that actually mean? How do you sequence someone’s genome?\nWhat is a genome\nLet’s back up a bit. What is a genome? Well, a genome is all the genes plus some extra that make up an organism. Genes are made up of DNA, and DNA is made up of long, paired strands of A’s, T’s, C’s, and G’s. Your genome is the code that your cells use to know how to behave. Cells interacting together make tissues. Tissues cooperating with each other make organs. Organs cooperating with each other make an organism, you!\nSo, you are who you are in large part because of your genome. The first human genome was sequenced ten years ago and was no easy task. It took two decades to complete, required the effort of hundreds of scientists across dozens of countries, and cost over three billion dollars. But some day very soon, it will be possible to know the sequence of letters that make up your own personal genome all in a matter of minutes and for less than the cost of a pretty nice birthday present. How is that possible? Let’s take a closer look. Knowing the sequence of the billions of letters that make up your genome is the goal of genome sequencing. A genome is both really, really big and very, very small. The individual letters of DNA, the A’s, T’s, G’s, and C’s, are only eight or ten atoms wide, and they’re all packed together into a clump, like a ball of yarn. So, to get all that information out of that tiny space, scientists first have to break the long string of DNA down into smaller pieces.\nDNA binds to DNA\nEach of these pieces is then separated in space and sequenced individually, but how? It’s helpful to remember that DNA binds to other DNA if the sequences are the exact opposite of each other. A’s bind to T’s, and T’s bind to A’s. G’s bind to C’s, and C’s to G’s. If the A-T-G-C sequence of two pieces of DNA are exact opposites, they stick together. Because the genome pieces are so very small, we need some way to increase the signal we can detect from each of the individual letters. In the most common method, scientists use enzymes to make thousands of copies of each genome piece. So, we now have thousands of replicas of each of the genome pieces, all with the same sequence of A’s, T’s, G’s, and C’s.\nReading the genome\nBut we have to read them all somehow. To do this, we need to make a batch of special letters, each with a distinct color. A mixture of these special colored letters and enzymes are then added to the genome we’re trying to read. At each spot on the genome, one of the special letters binds to its opposite letter, so we now have a double-stranded piece of DNA with a colorful spot at each letter. Scientists then take pictures of each snippet of genome. Seeing the order of the colors allows us to read the sequence. The sequences of each of these millions of pieces of DNA are stitched together using computer programs to create a complete sequence of the entire genome. This isn’t the only way to read the letter sequences of pieces of DNA, but it’s one of the most common. Of course, just reading the letters in the genome doesn’t tell us much. It’s kind of like looking through a book written in a language you don’t speak. You can recognize all the letters but still have no idea what’s going on.\nInterpreting the sequence\nSo, the next step is to decipher what the sequence means, how your genome and my genome are different. Interpreting the genes of the genome is the part scientists are still working on. While not every difference is consequential, the sum of these differences is responsible for differences in how we look, what we like, how we act, and even how likely we are to get sick or respond to specific medicines. Better understanding of how disparities between our genomes account for these differences is sure to change the way we think not only about how doctors treat their patients, but also how we treat each other.\n\n\nNext Generation Sequencing: A Step-by-Step Guide to DNA Sequencing {sec-video2}\nTitle: Next Generation Sequencing: A Step-by-Step Guide to DNA Sequencing\nPresenter(s): ClevaLab\nThe Human Genome Project uncovered all 3.2 billion bases of the human genome. This project started in 1990 and took until 2003  to complete 85 percent of the first genome. But, in 2022, the gaps got filled and the sequence  became complete. So in total, sequencing the human genome took 32 years. Now, with Next Generation sequencing or NGS, it takes only a day to sequence a person’s entire genome.\nNGS vs Sanger Sequencing\nOne day is a dramatic speed increase compared to 32 years! The difference is due to the number of DNA strands sequenced at once. Billions of DNA strands get sequenced simultaneously using NGS. However, only Sanger sequencing was available for the Human Genome Project. With Sanger Sequencing, only one strand can get sequenced at a time. However, NGS only works because the Human Genome Project created a human reference DNA sequence.\nThe Basic Principle of NGS\nThe basic principle behind NGS is that DNA can be cut into small pieces and sequenced. The sequences of these small pieces then get assembled based on the reference genome. NGS can be used to sequence both DNA and RNA. First, samples get collected, and the DNA or RNA gets purified.\nDNA and RNA Purification and QC\nNext, the DNA or RNA gets checked to ensure it’s pure and undergraded. RNA first needs to be reversed-transcribed into DNA before it can get sequenced. A library then gets prepared from the DNA.\nLibrary Preparation - The First Step of NGS\nA library is a collection of short DNA fragments from a long stretch of DNA. Libraries get made by cutting the DNA into short pieces of a specified size. This cutting gets done by using high frequency sound waves or enzymes. Then sequences of DNA called adapters get added to each end of a DNA fragment. These adapters contain the information needed for sequencing. They also include an index to identify the sample. Finally, any non-bound adapters get removed, and the library is complete. Depending on the application, there can be a PCR step to  increase the library amount. A successful library will be of the correct size. It will also be of a high enough concentration for sequencing. The main sequencing instruments used in NGS are from Illumina.\nSequencing by Synthesis and The Sequencing Reaction\nThese instruments use a method called sequencing by synthesis. The sequencing occurs on a glass surface of a flow cell. Short pieces of DNA, called oligonucleotides, are bound to the surface of the flow cell. These oligonucleotides match the adapter sequences of the library. First, the library gets denatured to form single DNA strands. Then this Library gets added to the flow cell, which attaches to one of the two aligos. The strand that attaches to the oligo is the forward strand. Next, the reverse strand gets made, and the forward strand gets washed away. The library is now bound to the flow cell. If sequencing started now the fluorescent signal would be too low for detection.\nCluster Generation From the Library Fragment\nSo each unique library fragment needs to get amplified to form clusters. This clonal amplification is by a PCR that happens at a single temperature. Annealing, extension and melting occur by changing the flow cell solution. First, the strands bind to the second oligo on the flow cell to form a bridge. The strands get copied. Then these double-stranded fragments get denatured. This copying and denaturing repeats over and over. Localized clusters get made, and finally, the reverse strands get cut. These strands get washed away, leaving the forward strand ready for sequencing.\nSequencing of the Forward Strand\nThe sequencing primer binds to the forward strands. Next, fluorescent nucleotides G, C, T and A get added to the flow cell along with DNA polymerase. Each nucleotide has a different color fluorescent tag and a terminator. So only one nucleotide can get sequenced at a time. First, the complementary base binds to the sequence. Then the camera reads and records the color of each cluster. Next, a new solution flows in and removes the terminators. The nucleotides and DNA polymerase flowing again, and another nucleotide gets sequenced. These read cycles continue for the number of reads set on the sequencer. Once complete, these read sequences get washed away.\nThe First Index is Read\nThen the first index gets sequenced, and washed away. If only a single read is needed, the sequencing ends here. But, for paired-end sequencing, the second index is sequenced, as well as the reverse strand of the library.  \nThe Second Index is Read\nThere is no primer for the second index read. Instead, a bridge gets created so that the second oligo acts as the primer. The second index is then sequenced. These two index reads use unique dual indices. These allow the use of up to 384 samples in the same flow cell.\nSequencing of the Reverse Strand\nNext, the reverse strand gets made, and the forward strands are cut and washed away. The reverse strands are then sequenced. Once the sequencing is complete, any bad reads get filtered out.\nFiltering and Mapping of the Reads\nThese include the clusters that overlap, lead or lag with sequencing or are of low intensity. The clusters cannot overlap on a patent flow cell, but there can be more than one library fragment per nanowell. These polyclonal wells will also get filtered out. Next, the reads passing the filter get demultiplexed.\nDemultiplexing and Mapping to the Reference\nDemultiplexing uses the attached indexes to identify and sort reads from each sample. Finally, the reads get mapped to the reference genome. The different reads align to the reference genome, overlapping each other. Paired-end sequencing creates two sequencing reads from the same library fragment. During sequence alignment, the alogarithm knows that these reads belong together. Longer stretches of DNA or RNA can get analyzed with greater confidence that the alignment is correct.\nWhat is Read Depth in NGS?\nRead depth is an essential metric in sequencing. Read depth is the number of reads for a nucleotide. Average read depth is the average depth across the region sequenced. For whole genome sequencing, a 30x average read depth is good. A 1500x average read depth is suitable for detecting rare mutation events in cancer. Another essential metric is coverage. The aim is to have no missing areas across the target DNA.\nHow is NGS being used?\nNGS gets used in a wide variety of applications. In diagnosing cancer and rare disease, treatment guidance for cancers, and many research areas from ecology to botany to medical science.\nWhat Types of NGS Applications Are There?\nBoth DNA and RNA can be sequenced. It could be the whole genome or transcriptome, just the coding regions (called exomes) of the DNA, or target genes in the DNA or RNA. All types of RNA can be sequenced including non-coding RNAs such as microRNAs and long non-coding RNA. In addition, cell-free DNA, single cells, as well as methylation or protein binding sites can also get sequenced."
  },
  {
    "objectID": "software_ewas.html",
    "href": "software_ewas.html",
    "title": "EWAS",
    "section": "",
    "text": "Epigenome-Wide Association Studies\nTitle: How to Perform Epigenome Wide Association Studies\nPresenter(s): Adam Maihofer\nLevel:\nLength: 21:17"
  },
  {
    "objectID": "chapter9.1_transcript.html",
    "href": "chapter9.1_transcript.html",
    "title": "Chapter 9.1: Copy Number Variation (Video Transcript)",
    "section": "",
    "text": "Title: How to run Copy Number Variation (CNV) analysis\nPresenter(s): Daniel Howrigan, Broad Institute\njust hello my name is Daniel Howrigan and today I’ll be talking about how to run copy number variation analysis today’s talk I’ll address a couple of questions namely what is a copy number variant and how do we detect it with genotype data what does the CNB file format look like and what is CNB analysis output look like finally how do I use this to run CNB burden and Association tests\nso what is a copy number variant well I Define it here as a subset of structural variation involving a gain or duplication or a loss slash deletion of genomic sequence now I call it a subset of structural variation because structural variation can Encompass basically any change in the length of a genomic sequence that can be as small as a single base pair insertion or deletion now most Snips that we we think of are just substitutions so the actual size of the genome isn’t changed with a single nucleotide polymorphism but a structural variant can Encompass anything from very small all the way to entire chromosomes what we call cmvs well it being kind of a general name falls into the category of roughly at least defined here at minimum one kilobase to sub microscopic with array data we generally say something around 10 to 100 kilobases given our sensitivity to detect these cnvs whereas larger cmvs usually are greater than 500 kilobases up to multiple megabases and then we get into much larger events now I’m not going to get into the details of the mechanisms that cause it I’ve listed a few here you can look them up but there are different hot spots in the genome that are more prone to these copy number variants due to repeat regions or often ways in the Machinery that can make mistakes and lead to these gains or losses of genomic sequence how do we detect it using genotype data so when we run across the genome collecting a bunch of snips I show here kind of the basic mechanism or at least a figure showing how we take light intensity from different experiments looking at capturing either you know allele a I have a a as allele a or the B allele\nbeing TT here and then a heterozygous you can see a mix of both red and green now we can leverage these allele frequencies in what we call the B allele frequency or at least the frequency of of the T in this case now normally when there’s no copy number variant these will be roughly equal and so be around 50 percent when we see something like a large deletion or any sort of deletion we should see a loss of heterozygosity and therefore we will see a gap in these be alleles at least at any site that you would normally be heterozygote at with the duplication these would be a little bit off so it would be more like two-thirds one-third and you would see maybe a movement kind of akin to where these pink lines may be going even though they don’t quite Define that you would see in a duplication that the B allele frequencies move off 50 but not to one and zero now that’s looking at the B alleles the other thing we use to detect it is the log ratio and this is basically measuring the light intensity and when you see a drop here where we would see maybe in in all of these calls you should see a drop of say 50 percent when we have a deletion and you will see subsequently a rise of about 33 percent for a duplication now I’m not going to get into the details of the collars that are used to detect these copy number variants suffice to say this is kind of the foundation that it’s built upon what I’m going to talk about next is given that you’ve run copy number variant callers\nhere are the kind of QC calls that you’re moving into the analysis stage now I use plink to analyze the CNB data so you can get a bunch of different file formats copy number variants from different callers but you would let you know if you want to use plink you would convert this into this particular file format it’s definitely which is the dot CMB file and it’s basically individual identifiers the chromosome the start and end position of this CNB the type one being a deletion three being a duplication and then also there’s a few other fields here the score and sites field in this example I use the score being say the number of copy number variant callers that agreed upon the Skip and call and it could range up to six sites here being the number of snips that are used to call the CMV now I note here that score and sites are not forced into a particular convention you could say replace score with the number of genes overlapping the CNB um or the site being some other variable that you would be interested in measuring now along with this file plink creates a cnb.map file and this is basically\nbreaking down the break points of each cnv into akin to a map file similar to what we have for snip data and you can see here I note that every different position is mapped even the the end position but also maybe a single position after that end because you may want to do a test an additional test after the end of a CNB to see how things have changed note that the CNB file format commands are not available in plink 1.9 but the initial version granted the speed ups that you get using the newer version of plink aren’t all that applicable here because we’re generally dealing with rare variation and so these file sizes generally not too large and the sorts of computing that you use is not too heavy so what does CNB analysis output look like usually whenever you run a command and plink looking at your CND files you get out.cnv.indiv file so this is per sample file where you say the number of CNB segments that this individual has the number of kilobases that these segments cover and the average kilobase is covered per segment you also get a cnv.summary file this is akin to the map file that is summarizing the number of affected and unaffected individuals at any given break point or start and and plus one of a c and b and this is basically you know the the output here well it seems quite simple I’ll show how with these files you can do quite sophisticated analysis by using a lot of different filters so a lot of the Magic in plink is all the flags that you can use to subset your list of cmvs so what I have here is a kind of verbose command just to show the optionality available in plink and for each of these I describe what that function is doing so with plink you have dash dash C file this reads in the CNB and CNB map file I want to select only deletions I want to select cnvs that are at least 100 kilobases in length I want cnbs with a score of four or higher and at least 50 sites\nI want to exclude cnbs that overlap a particular region so I can insert a different text file with a list of chromosome and start and positions here and I want to make sure in this exclusion that the cmvs must overlap by at least 50 percent to be excluded I can also look at frequency where at least 10 cmvs overlap I would like to exclude those because maybe I’m more interested in very rare uh cnbs I can also write out the frequencies of of these of these cnbs as well just so I can guarantee which cnbs are being dropped which cnbs are being kept and then I can run a basic burden test using a permutation model and here I just set the number of permutations to these ten thousand so you can see there’s a lot of different flags here and it’s manipulating a lot of these flags that can give you uh just what you would like in terms of your analysis now granted uh using the burden tests in plink doesn’t handle covariates it basically just looks at something like case control status and so what I recommend is taking the output particularly the CND cnd.indiv file and reading that into say python or R I prefer R to run more sophisticated models and so you can see when you put more filters here if I go back the number of segments will change depending on what filters and obviously subsequently the number of kilobases covered by these segments will change as a function and it’s kind of iteratively reading in these files at different filtering steps that can produce a wide range of tests\nso I’ve shown a couple figures here that we published when looking at CMB burden in the PGC schizophrenia so I took in those dot cnv.indiv files into R I ran a logistic regression predicting schizophrenia status and adding a number of covariates such as principal components genotype platforms and then basically as I iteratively ran different uh commands in plink to look at KB C and B counts lengths frequencies whether or not they’re in or not in a particular regions you can build up a number of tests of overall burden so in this example here this would be all cnvs deletions and duplications stratified by different genotyping platforms and then all together and then B here I’m stratifying by different frequencies and say previously implicated cmvs as a region I want to say in the blue bars we’re looking at enrichment here in the green and blue bars but you can see the big deviation here there’s a lot of enrichment when we talk about cnbs at this size or at least this frequency I mean and they go away because most of these have been implicated so I’ve excluded these regions rerun the burden test and you can see that we’ve captured much of the signal already with previously implicated cnbs so how do we use this to run CNB\nAssociation tests at individual CMB loci or at individual break points of cnbs basically I would run a very similar command I could use all the same filters basically get rid of a number of commands in particular you’re getting rid of this CNB in indiv.perm Step but you still run a permutation test and what you’ll get out is a dot cmd.summary dot and perm file and at each base position you can run an association test here using permutation this would be the pointwise permutation value but there’s also a family-wise permutation p-value that corrects for all the tests here so this Association is run at all possible start and in N plus one positions and one of the things you can do if you like to include covariates in your data at least what I’ve done in the past is maybe run your logistic regression model with a lot of your covariates pull out the residuals and then use this as a quantitative trait and you can run Association mapping in plank to get p-values that way so what does this look like I think having a figure kind of in is instructive here so I plotted uh through a browser I’ll break this down this is our signal at the Direction one gene so in red we have our deletions uh light red deletions in our schizophrenia cases dark red deletions and our schizophrenia controls I also have duplications in blue which don’t make up much of the signal here but as you can see I’ve also plotted the log negative log 10 p-value and if you kind of look closely you can see each little break point you can see a different test being run and you can see at you know a spot like this where there’s many different break points a lot of granularity you can run many different tests and kind of get a shape of the association around this Gene now you can also collapse another test that we’ve done is collapsing across the all the exons of this Gene and so this would be Akin more to like a gene burden test where you collapse this region and then you test for overlap at that region run a similar model and then you can kind of aggregate all the cnbs and cases and controls to report say a gene-based p-value so that’s a very quick overview of how to run burden and association with cnv data and a few considerations is that you know one of the things that you don’t have access to when looking at cnbs is imputation and so it is a consideration to think about you know there isn’t kind of a reference uh you know reference haplotypes or a larger data set to do additional QC so there can be additional challenges particularly with subpar data that they can’t be rescued in the way that imputation uh can rescue snip genotypes and on that note you know genotyping chip does matter very much because you can’t impute a bunch of different new sites the variability in terms of the number of snips particularly for smaller cnbs is a very kind of important\nconsideration and so you know you can think of in a particular genotyping chip if you don’t have a good case control balance there you may not have the sensitivity to properly detect uh cmvs and so there’s a lot of work that goes into determining at what length of cmvs uh or at least at what case control balances given your genotyping chip do you have the right amount of power and sensitivity to do a proper case Control Association test another thing too ancestry PCS most of the CNB hotspots and Associated cnv that we see in in psychiatric disease aren’t very impacted and mainly that’s because these are recurrent de novo CNB areas where there’s a higher mutation rate but this is not really ancestrally um you know defined in terms of the fact that there’s not a large difference in the allele frequency across different ancestries but it is still useful to include particularly as you get to higher frequencies and you get more inherited cnbs and finally multiple testing correction permutation is more one of the more robust ways to account for the multiple tests because the nature of CNB data is such that given the type of genotyping Chip that you use the size of your data set um no no particular study is going to be very similar to similar to another study and that kind of leveraging the correlation structure within your own data set given you know your ability to detect cnvs at various of various sizes and frequencies using permutation is usually the best way to go about properly testing for Association so if you have any questions feel free\nto email me I’ve also put down some sites you could search here for the PGC CNB paper that uh we did in 2017 and also I think that the write-up and plink on how to do this is is really good and very descriptive of all its functionality thank you"
  },
  {
    "objectID": "chapter8.2_transcript.html",
    "href": "chapter8.2_transcript.html",
    "title": "Chapter 8.2: Genetic Correlations (Video Transcript)",
    "section": "",
    "text": "Title: Genetic Correlation and Partitioning\nPresenter(s): Patrick Turley, ATGU, Massachusetts General Hospital\nso now we’re going to talk about genetic correlation and and partitioning so we’re gonna we’re going to start with genetic relation I’m gonna talk more broadly about a term that that that we call just genetic overlap and so so a lot of traits have have really similar genetic architecture that is if we find a snip that is causal for some trait like educational attainment it’s it’s more likely once we found it for education attainment we might think that it’s more likely to also be associated with things like cognitive performance or a bunch of other traits you know that schizophrenia and so so we maybe want to get an estimate of how how strong that relationship is you know what if we do find a snip for one trait how likely is it that it’s going to also be causal for the other just as just as a vocab that I don’t think has has come up we haven’t defined specifically so there’s the the state where one snip is associated with a variety of phenotypes that’s called pleiotropy so some says oh there’s pleiotropic it means that that genes do more than one thing and and so because there’s pleiotropy that’s why there’s going to be genetic overlap and so why why might we care about overlap well you know it might help us untangle complicated causal relationships so if we see that all of\nthe snips that are important for\neducation are also important for for schizophrenia then you know might say oh so those those two things have have a genetic a genetic relationship you know if we see that depression is is highly genetically correlated with neuroticism I mean like okay yeah the the genetic you know additive contribution to those two traits is is similar and so that can help us as we’re thinking about what might be causing both of them it also can help us prioritize causal pathways and so so like let’s this this is sort of related to maybe the proxy phenotype method and so let’s say that we know that that education and cognitive performance are highly genetically correlated we don’t have very large samples for cognitive performance well if we know that they’re highly related then we could you know limit the space over which we’re looking at the genome by just taking you know snips that are associated with educational attainment above a certain level and look for the association with cognitive performance there and as a result we don’t have to do as large a multiple testing correction because we’re doing fewer tests you know also if we know that things are related and we’re trying to like figure out how you know it’ll just point us in the right direction sorry so I think that when at least historically when people have said candidate gene it was based on sort of a theoretical biological relationship and so he said oh yeah this is this gene we think it has to do with this and so we’re going to we’re going you know we’re gonna test the test for that whereas when I say proxy phenotype it’s it’s less of a theoretical relationship that we’re using to select about an empirical one yeah great and so there’s kind of two so when we say overlap there’s sort of two ways to think about it so ones enrichment and ones genetic correlation and so so enrichment is is this idea of kind of a proxy phenotype method you know our snips that are important for for a phenotype a also important for a phenotype B and so we could do this test we’re just gonna take the snips with a p-value of less than P naught and we’re gonna test them in an aji was--for phenotype B and so that that could be pretty successful in seeing like if you know do the snips and phenotype B in that subset are they you know more likely to be more significant that’s sort of the the very high view question but there’s a few technical\nquestions first off how would you pick\nthe threshold are we just going to look\nat snips that are genome-wide\nsignificant or should we look maybe\nlower in the distribution how do you deal with LD so let’s say you have two snips and they’re highly correlated with each other but this snip is really important for phenotype eh but not at all important for phenotype B and this snip is really important for phenotype B but not for phenotype a if we just compared Gy summary statistic we would say oh yeah these are both important and so these traits might be related but when it comes down to the actual you know the actual effect of those particular snips they’re they’re not related at all and so we should keep keep in mind that you know we’re you know we have this sense when we have gos summary statistics you shouldn’t think snip you should think sort of like region around the snip there’s another question oh yeah that’s a that’s a question I don’t know the answer to actually does anyone is there anyone with a bio background who knows this I was wondering this just the other day so the question is if if you have a gene that you know is important for some phenotype are other genes that are going to be important that phenotype may be going to be nearby like on the same chromosome or nearby yeah either close to each other on the chromosome were just on the chromosome yeah yeah so I guess for expression we know that there’s some local localness but had gee I wish I was that again that are far away then so those are ones that are associated with smoking behaviors I mean if it’s highly polygenic mm-hmm yeah yeah so III I wish I knew the answer as well and someone maybe does but I don’t know and\nit’s relevant for a lot of the lot of the things that we do like remember how I told you one of the assumptions in LC LD score regression is that these betas are independent and if genes that are important for specific things tend to be close to each other that also means that they’re in high OD with each other and so it might it might break you know might violate the assumptions about these score regressions so I so I don’t know the answer but I wish I did so here’s another question so like let’s let’s say we did was was that a question was out of stretching okay and so let’s let’s say that we you know we have our education our 74 education associated snips and then we looked them up in a height G wasps and we find that all of them are associated with height you know do we think that that is a signal that the two traits are strongly related I’m gonna say no who can give me a reason why we might not expect that you yet so so you would you wouldn’t want to interpret the fact that every single education snip is is is associated with height as well yes I guess the direction could be one thing what what else might we think about because the direction if it’s associated could go and okay yeah yeah but we’re just looking for general enrichment we don’t we’re nothing about direction mm-hmm yeah I mean if that’s the case then we actually do think they related I guess so so yes I guess maybe I should have picked something that are unrelated let’s say there’s traits that aren’t I mean I guess I guess maybe I shouldn’t make you keep guessing but maybe it’s good that you were thinking but Heights really polygenic right so why is that relevant yeah he is really probably jinuk but but specifically hyping you know if we had a trade that wasn’t polygenic let’s say there was only one gene and and it was the only gene and we find that that gene also is important for height yeah because of random snip drawn from the genome is likely to be associated with height all right and so we should keep that in mind also when we’re doing these tests so we’ve got to think what’s what’s what’s the you know what is our expected significance and and so cuz we’re talking about you know are things more significant than we expect you have to ask the questions what do we expect and saying that we expect know you know that between knows maybe not right there’s also a question of one side or two-sided test which gets to this idea you know is do we want to think about the sign or not if we see a positive effect in education if we just want to see if that leads to positive effects in height then that that’s a one-sided test but if we just want to say do we expect a large effect in height we don’t care if it’s negative or positive then that would be a two-sided test and so that’s that’s just a decision that we need to make and so here here are some some examples of kind of the thing I told you so we take the 74 education snips we’re about at a time but I will stop at the end of the overlap stuff and then I want to see if it is associated with the G wasps on the size of your of your thalamus in your brain and so the 45-degree line is what we would expect to see if if they were all no and so we see that you know it does sort of peel\naway from from no so so there are more you know it doesn’t that there might be a little bit of extra signal relative to the null but we can test the how much there’s enrichment there is based on how polygenic we think the trade is and and we you know we have a p-value of 0.02 inflation that we’re seeing is just due to chance however if we look at the education snips and a G loss of cognitive performance you know we start seeing it peel away like pretty quickly and our p-value there is is point zero to just based on you know how significant are these relative to what we’d expect if you look at schizophrenia it peels away even more schizophrenia I think’s a funny case so if we look at at cognitive performance the sign concordance is ninety percent so that means a snip that increases education you know ninety percent of the time we’ll also estimate it to increase cognitive performance and so you know is highly enriched schizophrenia we see you know it’s it’s highly highly enriched we can see just by looking at these points that there’s a lot of enrichment however the sign concordance is 51 percent which means that a snip that’s associated with education it’s likely to be important for schizophrenia but we can’t really guess if it’s gonna be protective of schizophrenia or or what’s the opposite it’s more more causal it’s gonna be more likely to lead to schizophrenia and so that’s kind of a funny case so I think that we should break for lunch and we can talk about genetic correlation briefly after lunch"
  },
  {
    "objectID": "software_geneset_transcript.html",
    "href": "software_geneset_transcript.html",
    "title": "Software Tutorials: Gene Set Identification (Video Transcript)",
    "section": "",
    "text": "MAGMA\nTitle: Gene- and gene-set analysis in MAGMA\nPresenter(s): Christiaan de Leeuw\n\n\n\nH-MAGMA\nTitle: Annotating Genetic Variants to Target Genes Using H-MAGMA\nPresenter(s): Nancy Y.A. Sey, University of North Carolina at Chapel Hill\nIntroduction\nhi everyone my name is Nancy and today\nI’ll be talking to you about annotating\ngenetic variants to Target genes using\nhigh C corporate magma or hmagmap for\nshort\nBackground\nso before I get started I would just\nlike to credit doctors um Christian and\nDanielle for developing magma Gene set\nanalysis\nso magma is widely used in our field and\nthis is for various reasons with a few\nbeing that the tool is pretty\nrevolutionary in that it’s aided in\nmaking sense of guas Finance by\nidentifying the target genes of genetic\nvariants identified from guas\nadditionally magma is very easy to use\nin that you do not have to be\ncomputationally Savvy to use the tool\nalso it is very efficient compared to\nother tools in the field so it’s it\ntakes a very short time to run magma\nhowever despite all of these benefits\nthere are a few limitations to the tool\nnamely magma relies on positional\nmapping to link variants to Target genes\nwe know from prior studies that the gene\nregulatory landscape is complex and it’s\ntherefore possible for variants\nespecially non-coin invariants to\ninteract with and regulate the install\ngenes so additionally\num the regulatory landscape can be\ntissue or cell type specific so we have\nto take into account the tissue or cell\ntype that’s most relevant to the trait\nor disease that we’re interested in\nunderstanding\nso these two limitations served as the\nfoundation for developing High c-coupled\nmagma or H magma for short\nso H magma complements magma by using\nhigh C data\nMaterials\nso as mentioned previously um H magma\ncomplements magma so the materials are\nneeded to run H magma are the same as\nthe ones needed to run magma where the\nonly difference being the gene2 variant\nannotation file so in this tutorial I\nwill walk you through how to generate\nThe hmac muscativic annotation file\nwhich links non-coding um variants to\ntheir target genes using high C\ninteraction\nso again I will um just walk you through\nhow to generate The annotation file for\nhmagma\nRequired files\nso the materials needed to run hmagma\nare similar to the ones needed to run\nmagma with the only difference being The\nannotation file so to generate the H\nmagma annotation file you need a few\nother files namely you need bed files\nfor Gene Exxon and promoter coordinates\nAdditionally you need a high C data set\nin the bedp file format you will also\nneed an ancestry reference genome and\nfor the sake of this protocol we’ll be\nusing the European ancestry\nand once you have The annotation file\ngenerated you need a Gua summary\nstatistics to run hmagmap\nso again um the focus of this tutorial\nwill be on Steps A and B which walks you\nthrough how to generate the hmag margin\nto variant annotation file\nStep 1 Load libraries and required data\nso I have broken The annotation files\ninto seven different steps with code for\neach step provided as part of this\nrecording\nso the first step is to load your\nlibraries and all required data into a\nwork directory\nthe next step is to read in your Exon\nand promoter batch files and create\ngenomic range objects for the Exxon and\npromoter using the Exxon and promoter\ncoordinates from gen code version 26.\nonce you have this loaded and the\ng-range object generated you will save\nthen as an R file\nso similarly um you would generate a\ngenomic range object for all Snips using\nthe reference genome from the European\nancestry and you will save you also save\nthis as an R file in the work directory\nfor later use\nStep 2 Overlap Snips to Gene\nso in the following step um we will\noverlap Snips to Gene starting with the\nExotic snips\nso as a reminder the Exotic Snips are\nassigned to your target genes using\npositional mapping and this is because\nthey are more likely to impact the genes\nin which they reside\nso to assign the Exotic Snips to your\ntarget genes we will overlap the gene\nrange for objects for Exon with\nengineering objects for Snips that was\ncreated in steps two and three\nrespectively\nso similarly we will also overlap the\ng-range object for promoters with the\ng-range object that was created for the\nsnips\nso once you have both g-range objects\ncreated for the exons and the promoters\num these will be saved as an R file for\nlater use\nIntergenic Snips\nforeign so after um after we’ve assigned\nthe Exotic and promoter snaps to their\ntarget genes there will be a subset of\nsnips that do not overlap with either so\nthese are known as the intergenic or the\nintronic Snips so in the following steps\nI will walk you through how to match\nthose Snips to your target jeans using\nhigh C interaction and this is shown\nhere in this diagram\nso to achieve this we will first\nidentify those in intergenic and\nintronic Snips and save them as an R\nfile\nthen we will load our high C data so for\nthe sake of this tutorial we’ll be using\nhigh C data from the adult brain as an\nexample\nso once you load the high C data in the\nbedpie format you will generate a\ng-range object for it\nso we will next find snip.physically\ninteract with excellence and with these\nset of codes similarly we will also find\nsnip that physically interact with\npromoter with the promoter regions\nso next we will combine those g-range\noutlets for the exons and the promoters\nto retrieve a unique um with to retrieve\nunique items\nso lastly we will overlap the intergenic\nand the intronic Snips with high C to\nidentify their target genes\nso once we have all these Snips um\nnamely the exonic the promoter the\nintergenic and the intronic Snips map to\nyour target genes\num the last few codes that I will show\nyou in Step seven will be used to\ngenerate The annotation file that is\ncompatible with Magma\nso to achieve this we will structure The\nannotation file so that it has the genes\nin The Ensemble format the chromosomal\nlocation and followed by the Snips that\nis assigned to each particular Gene\nso this is how you you will have um so\nfollowing these seven steps you will\nhave your hmac vibrant to Gene\nannotation file that can be used to run\neach magma for any trait or disease\nso with this um you can run hmadmav for\nany trade um so for here I will I’ll use\nan example using p um Parkinson’s\ndisorder as an example\nso this is the portion of the magma um\nfile the magma code that you place your\nhmag margin to variant annotation file\nOutput files\nso here I am showing you one of the\noutput files that you get from running\nhmagma so this is the dot genes.out file\nso with this file um you can actually\nextract the significant genes at various\nFDR thresholds for Downstream analysis\nLimitations\nso as I mentioned previously in the\nbeginning of this tutorial magma has\ntransformed the field of psychiatric\ngenetics and with each magma we can\naddress some of its limitations\nhowever it is important to note that\nthere are still some pendant limitations\nthat aren’t addressed using H magma\nnamely the directionality of effect so\neven though H magma can assign Snips to\ntheir target genes I’m using higher C\ndata we do not know whether the Snips\nthe variants are upregulating the gene\nor they are doing regular down\nregulating the gene so to address this\nyou can use various eqtl data sets to\nto analyze the directionality of effect\nadditionally we know that not all not\nall Snips are actually implicated in a\ntrait or disease so you need to do\nfurther functional validation to further\nprune down the list of genes\nand then we also know that the sample\nsize of a g was greatly impacts the\nnumber of Target genes that is\nidentifiable so this H magma also\nsuffers from this\nand lastly\num lack of diversity in genetic studies\nmost of the analysis that we’ve done\nwith h magma has mostly been from\nEuropean ancestry however it’s important\nto note that you can also run H magma\nfor other ancestries\nConclusion\nso with that um in the last slide here I\nwould just like to acknowledge where we\ncurrently are with h magma so previously\nwe have generated H magma for brain\ntissues including adult and Fetal brain\nwe have also added various salty\nspecific annotation files including\ncortical\nipsc derived neurons and ipsc derived\nastrocytes as well as midbrain\nendopliminergic neurons however H magma\nis more versatile than just brain\nrelated cells so to expand the two we\nhave created additional annotation files\nfor 28 different cells and tissue types\nto allow other researchers to use the\ntool\nand these cell types um are listed here\nand\nmost of all of these annotations files\ncan be drum can be derived from our\nGitHub page\nand with that I would just like to thank\nmy lab as well as my funding sources\nadditionally I’ll be hosting um a q a\nsection\num on October 15 2021 at 3 45 PM Eastern\nStandard Time so if you have any\nquestions or you would just like to say\nhi please drop by to see um and ask any\nquestions thank you\n\n\n\nE-MAGMA\nTitle: E-MAGMA: an eQTL-informed method to identify risk genes using genome-wide association study summary statistics\nPresenter(s): Zac Gerring, Eske Derks\nthank you for attending my talk entitled e-magma an eqtl-informed method to identify risk genes using genome-wide association studies summary statistics magma was initially developed to extract biological insights from g-was by linking risk variants to their nearby genes\nthe method assigns single nucleotide polymorphism associations to gene level associations while correcting the confounding factors such as gene length minor allele frequency and gene density while magma is a reliable and commonly used tool there is room for alternative approaches for how snips are assigned to genes for example conventional magma assigns snips to the nearest gene which is not always the most accurate approach non-coding snips can affect the expression of distal genes known as expression quantitative trait loci eqtr magma or heat magma is a modified version of conventional magma that leverages tissue specific eqtl information to assign snips to genes the genotype tissue expression project or gtex is an ongoing effort to build a public resource to study tissue-specific gene expression and regulation the current version of gtex contains samples collected from 53 non-disease tissue sites across nearly 1 000 individuals including 13 brain tissues from around 200 individuals\nthis diagram gives an example of how gtex can be used for the functional interpretation of genome-wide association signals the expression quantitative quantitative trait losse annotation from various tissues can be used to propose one or more potential causal genes whose regulation is either tissue shared shown in green or tissue specific shown in yellow for a\ntrait associated variant these associations would not be identified assigning snips to genes based on proximity alone e magma largely adopts the magma pipeline which consists of three broad steps gene annotation to assign snips to genes gene analysis to compute gene-based p-values and gene-level analysis to test the enrichment of gene-based results in curated gene sets e-magma only modifies the annotation step this is achieved by mapping significant eqtls from gtex to nearby genes in a tissue-specific manner rather than a single annotation file containing the mappings of snipster genes e-magma uses 48 annotation files one for each tissue and gtex after running gene annotation e-magman generates all intermediary files required for the gene level analysis\nthis slide shows the basic code for each step of conventional magma with alterations for e-magma outlined by red boxes in the gene annotation step the snip location file is altered to include significant eqtls for one of 48 tissues in gtx the tissue-specific eqtail annotation files are used in the gene analysis to generate gene-based p-values in addition to generating gene-based p-values we also generate intermediary or raw files for the gene set and biological pathway analysis\nwe tested the performance of e magma against other gene-based approaches including conventional magma aspertic scan fusion and summary based mendelian randomization using a simulation experiment the simulations use snip genotype information from the qimr adult twin study we excluded non-founders snips with more than one percent missingness and snips with a minor alarm frequency of less than 0.05 and we only analyze snips on chromosome this resulted in 7138nbc with around 60 000 snips eqt information was derived from gtx whole blood which included some 650 000 eqtl gene combinations for around 8200 genes phenotypes were simulated using gcta using gender type and eqtl reference data from chromosome 1. only genes with at least one significant ukchel were included in the analysis giving 651 genes and we performed 10 simulations per gene gbos on the 6510 generated phenotypes were performed using plink and we corrected the results for the number of genes in the eqtl reference set\nwe first evaluated the type 1 error rate across methods by calculating the proportion of genes that were significant in the absence of a true association between eqtls and phenotypic values the figure on the left shows all methods had good control of the type 1 error rate we subsequently evaluated statistical power to detect associations at a gene level for varying levels of phenotypic variants explained by uqtls we assess the proportion of significant associations relative to both the total number of causal genes in figure a and when accounting for the total number of causal genes included in each method shown in figure b e magmat have performed all methods across different proportions of variants explained by gene expression after correcting for the number of genes included in each gene based method e magma is still outperformed the other approaches\nwe estimated statistical power as a function of the number of eqtls per gene with one percent phenotypic variants explained by eqtls power significantly increased with the number of eqtls per gene as shown in this figure [Music] it should be noted that there was a significant association between the number of ekgles per gene and statistical power for all methods however e magma was less sensitive to the number of eqtls compared to the other approaches\nwe compared three gene-based methods e magma conventional magma and the t-was approach aspertic scan using g-wasps for major depressive disorder shown in the red circle identified 137 genes e magma the results of which are shown on the green circle identified 99 genes and desperadic scan shown in blue identified 57 genes a total of 16 genes were identified across all three methods the figure on the right shows aspertic scan minus log term p values on the y axis and e magma p-values on the x-axis with the color of the points indicating the tissue for which the gene-based dissociation was found as you can see there is good overlap between e-magma and esperidic scan and the overlap included established risk genes such as any gr-1 and team m106b\nin conclusion e-magnet is an equity home formed extension to conventional magma and can be applied to any trait with g1 summary statistics e-magma map maintains appropriate controls with type 1 error rate while outperforming other methods in detecting causal associations a tutorial and input files can be found using the following github repository thank you\n\n\n\nPRSet\nTitle: How to run pathway specific Polygenic Risk Scores\nPresenter(s): Judit García-González\nHello, my name is Judit. I’m a doctoral fellow at the iconic school of medicine at Mount Sinai today I will walk you through how to run pathway specific polygeneries course a tool that has been recently developed in the polygenic lab either by Paul O’Reilly so first of all uh it’s important to know what are and why it’s interesting to use pathway polygenic risk scores for that I’m going to use one example that most of you might be familiar with which is the ability to give us for the schizophrenia\nso we know that psychiatric disorders like schizophrenia have complex ideologies where different environmental and genetic factors contribute to the liability of these sales when we talk about genetic factors we can imagine that the US is a composite of signals where each signal might represent functional roots to disease for example uh there might be some genomic reasons that Harbor genetic degrees associated with abnormal synaptic pruning there might be other regions that are associated with biological Pathways and processes related to immune activation and other regions in the genome that pick up signal associated with cannabis consumption\nso the idea behind by the way a specific polygenic scores is that instead of aggregating effects across the entire genome we will aggregate them after those relevant pathways and because we are separating the genetic contribution of a trade accounting for the genomics and structure we hope that pathway polygenic scores might be more useful for patient stratification or to investigate the disease heterogeneity because now one single individual instead of having one single polygenic score will have as many exports as Pathways we investigate\nso to calculate these pathway scores uh I will walk you through the tool reset which is an expansion of the precise software uh which uses a glamping and p-value responding method um and it’s important to know that the pathways that can be any type of Gene set that reflects the encoding of different biological functions so this tool is quite flexible to Define pathways uh preset as well as the newest version of precise have been developed by Dr Sam Choi and you can find a software on the manual in this website precise.info and they’re in the website you will find a quick star how to download it and also some more detail guy about what are the available commands and what is the method how the method works\nso how to use preset um percent is very similar to precise in terms of use where we need as an input the Dual summary statistics and individual level data for genotype and phenotype but four percent uh we also require information about the pathways or they didn’t sets that we want to use um as I say the two the tool is quite flexible uh Pathways can be defined in a variety of ways including but always defined by existing canonical databases like Gene Anthology or reactone but also by experimental perturbations or some functional outputs like Ginkgo expression or protein protein interactions\nso I’ll go in a bit more detail on the different input options that are available to Define pathways One is using EMT and gdf files using the commands mcdv and DDF the commands that are for preset will be shown in green um so MC will contain information related to pathways so that is the genes that form each pathway and you will need a file where each pathway is specified in a different row with all the units that compose that pathway file will contain the position coordinates for each gene so in general GTA files contain different features and by default preset uses the features Exon gain protein coding or CDs but this can be modified with the command feature the second option that can be used is a bad file in this case you will need to include one bad fiber pathway so if you want to include multiple Pathways then I bet files can be included separated by a comma uh in the web file three columns are required and those are chromosome name start position and end position one important thing to know when using that files is that they are indexed uh differently as being files so whereas for Blink files are one base the web files are zero base\nso you can see in this example that we have a sequence of nucleotides I’m gonna ask for the one base indexing one nucleotide is uh one position the zero base indexing will use a range around each in nucleotide so you will need to subtract uh -1 in the zero base indexing for that values the input option 3 is an Smith list so similar as debit files each pathway will be a different file and they can be separated by comma as in this example here so when you download precise for preset there is Android data that you can use and this is the the commands that you can use like you define the keywords uh your target sample in this case I’m using the first uh option to define the pathways then you can set the number of permutations that you want to run and also the name of the output\nso after walking you through the input uh we can see a step-by-step how preset calculates the pathway polygenic scores to do that I will compare it with the genome white parents so whereas for the genome-wide PLS you clamp in mp value stressful in for a given address for a linkage equilibrium for pi vprs this will be done for each pathway separately then similarly with us for the genome ybrs each individual will have only one polygenic score for pathway PRS each individual will have KS course 4K number of pathways and for the results whereas for genome-wide PRS uh what is reported is the association method which is like the phenotype variance explained by the genome-wide PRS in the case of the pathways course there will be the same the association of phenotype variances explained by each pathway PLS but you can also report enrichment of geost signal in the pathway which is based on the RS versus the pathways\nso I’ll go a bit in more detail about these two different methods that preset can output so when we are talking about Association what preset will output is the self-contained p-value so it’s just the regression of the phenotype on pathway PRS and covariates so um Pathways that have many genes containing a Snips that are associated with the phenotype are highlighted here in pink and those will be significant but it’s important to note that for these results the self-contained p-value does not account for pathway sites and for example Avid pathway will be more likely to be significant because it’s easier to have a larger number of snips associated with the phenotype but the second output that we can have is this enrichment uh that will be output as competitive p-value this is resulting from a permutation procedure and this will test whether a pathway is more associated with the phenotype compared to null Pathways that have the same size so in this case with the competitive P value we’ll be accounting for uh the size of each pathway\nuh in this plot I’ll illustrate how we calculate this competitive p-value that accounts for uh the pathway size we can imagine a pathway a that has some Snips in the unit region of the pathway and for which we will calculate an observed the value then we will calculate as many null Pathways as permutations we run and these null Pathways will have the same number of false clampede Snips as pathway a but they will have been this isn’t it will be randomly allocated so we will obtain a distribution of null P values because it’s power will give us a value for Association and the competitive p-value will be defined as the number of tests under the null that have a p-value is smaller than our observe p-value plus one divided by the number of tests performed plus one\nso this is how the competitive competitive P values are calculated this approach and this output is similar to the type of enrichment analysis that are performed by magma and LDS Corporation but pathway PLS have the advantage that they provide individual level data that can be used for other applications\nso let’s see how the output of preset looks like similar to precise for every preset run we will have a log file with extension.log and this will contain the commands used for the analysis and all the information regarding the filtering the fields that were selected Etc then we will have another file with extension precise and this will contain the polygenic model for each pathway across thresholds so in the in this case I was running only address for one which contains all these names but you can see that um there is information for each pathway the R square the self-contained p-value coefficient for the regression standard error a number of its names you will always see this base pathway which is the background uh pathway that will is used for the competitive p-value calculation then we have um the best file which will give us the scores for each individual and each pathway uh so here each row will be one individual and then the score for each pathway will be indicated in a separate column\nso there will be as many columns as Pathways you are using in this case we see four examples from the care database if you wanted all the scores at all the people in response because this best file will report only the one with the best people in threshold uh you can use the command code finally we will have another file with extension summary that will have the information of the best model fit of each phenotype um pathway and in this case each pathway will be a row and then it will contain again information on the R square Ruby value coefficient and also this competitive p-value that I was talking about that will indicate the enrichment of this pathway compared to other Pathways of similar science\nfinally I would like to leave some heads up and useful commands um so for example some commands that might be useful for the user is the window size uh which is like for example if you are defining um a Snips that are within a game you can extend that window and so you can include some number and bases to the three prime region of the ranges region or to the five Prime region of his skin you can also exclude within a certain range with the common X range so for example the MHC complex or the Apple region if you want to do some Alzheimer’s analysis and you want to Absolute that one\nif you want to improve the computational efficiency or preset you can parallelize the process using the thread command you can also speed up a little bit uh preset um at the expense of increase in the memory using um the ultra command and um you can also use the keep and the extract commands uh to exclude individuals or its names from the analysis if you are doing permutations to calculate the competitive P values this can be computational intensive so it might be useful to adjust for the covariance like sex or age beforehand on your phenotype and then use the receivers as the phenotype because this will speed up the process\nso this was this quick introduction to preset uh thank you for listening if there is anything that is not here or you have any questions uh there is a q a session uh the 15th of October at 11 30 and this is the soon meeting ID so I’ll be happy to clarify or answer any questions thank you"
  },
  {
    "objectID": "chapter8.3_transcript.html",
    "href": "chapter8.3_transcript.html",
    "title": "Chapter 8.3: Gene Association Analysis (Video Transcript)",
    "section": "",
    "text": "MAGMA\nTitle: How do we go from genetic discoveries from GWAS/WGS/WES to mechanistic disease insight?\nPresenter(s): Danielle Posthuma\nWell, welcome back, this is part three of the session on how do we go from genetic discoveries to mechanistic disease insight, and in this last part I will focus a little bit on the software tool MAGMA for conducting gene based and pathway analysis. So in the practical you will learn how to work with MAGMA, and that’s a tool that was created by Christiaan de Leeuw a couple of years ago and it can be downloaded from this website over here.\nIt is a tool for gene-set analysis and requires you to work with the Command Line interface, which should now be quite familiar to you and as an input you can either provide raw genotypic and phenotypic data or you can also provide summary statistics from already published results, but then you would also need some reference data. Because we need information on the LD structure of between the SNPs that are part of your analysis and then the other input is gene definitions. So that’s with SNPs belong to which genes and I will come back to that in the next couple of slides and also definitions of gene sets. But if you download MAGMA, there’s some files that have, some default files that you can use, that do this for you, but you’re also free to use your own files if you want to. So just as an aside, if you want to have access to public summary statistics, we created a database and on this slide I’ve just added one example when you’re interested in looking at one particular GWAS, then it gives you the Manhattan plot that gives you gene-based plots, it gives you the QQ plot. It also gives you gene sets outcome already. And it gives you some information about this GWAS with the link to the PubMed ID and where to download the data. So that’s, you can use this database if you want to play around with any software tool that requires you to input summary statistics, then yeah you can just download summary statistics from this database but there are also some other databases that have the same purpose. In MAGMA gene set analysis, there are three main steps, so step one is the annotation where we match SNPs to genes, and so MAGMA needs to know which SNPs do I have to analyze as part of which gene. So that’s step one.\nStep 2 is the gene analysis, so that’s where we compute the association of the gene with the phenotype. So here the unit of analysis is the gene and then step three Is the gene set analysis, where the association of gene sets is tested against your phenotype. And then, because it’s a very general linear regression framework which can easily be extended, it’s very easy to use continuous sets. So instead of having a dichotomous set where genes are either a member of the gene set or they’re not a member of the gene set that you can also have quantitative, quantitatively defined set of genes where every gene has a value that indicates how likely it is to be part of a gene set, or that indicates the expression level of a gene in a cell type and then the cell type is the gene set. And it also allows you to do conditional and joint analysis and interaction analysis as was explained in part two of today’s lectures.\nAnnotation\nNow going back to the three main steps, annotation. If you download MAGMA it comes with a general annotation file and there SNPs are mapped to genes based on the physical location, and but you can also change this annotation file so you can, if you would like to have eQTLs included in it, you can map SNPs that are physically located outside of a gene but have a known eQTL link to the gene, or chromatin interaction, that’s also possible to use. You can also add a window around the gene so you can say, well I would like to have maybe 1 megabases before and after the gene and those SNPs should also be analyzed as part of this gene. An one SNP can actually be linked, can be mapped to multiple genes.\nGene Analysis\nThen if you run the analysis, there are four models that are available in MAGMA. If you have the raw genotypic data, then it will conduct a principal component linear regression analysis and that that can only be done when you have access to the raw data. So if you have, if you input summary statistics which most of you will probably do, then there are three different models that you can use to evaluate statistical significance of your genes and of your gene sets.\nSo the first model is the SNP-wise mean and it performs the test on the mean SNP association, so that evaluates the evidence for association of all of the SNPs in, that are located in gene and then uses the average association to evaluate whether the gene is actually associated. Or you can do to SNP-wise Top Model where where the focus of the analysis is on the strongest SNP association, and you can also combine these two models and get this, the SNPwise multi model where the evidence from both of these previous models is combined into one p-value for your gene. And yeah, deciding which model is best for you, that depends on what your actual hypothesis is. So what kind of sensitivity would you want? So there’s no, we don’t think that there’s a best, best model. It really depends on the situation or your research question. That’s why we provide multiple models in the MAGMA tool. So what’s being done in the MAGMA tool, so when you do a gene set analysis, that’s basically an analysis of genes. So instead of individuals being your unit of analysis or your data points, the genes are the data points in the analysis. So in this table we have listed 10 different gene IDs and each of these genes have been tested for association in the gene-based step in MAGMA. So they all have some kind of measure for the strength of the association with your phenotype based on your GWAS summary statistics. And then there’s also an indication of whether or not they are part of the set of of your gene set that you would like to test.\nSo in this case the genes are the data points and the gene set is the grouping variable and the genetic association with the phenotype, that’s the outcome that you would like to get, so this is basically a simple T-test testing whether the average association of the genes that are inside your gene set is different from the average association of the genes that are outside of your gene-set. Yeah, so that’s basically just a one-sided test of genes because you have a very strong hypothesis of what association should be stronger. Now, there are two kinds of tests. So you could do a self-contained analysis where you ask whether the mean or the average genetic association of genes in a gene set is greater than zero. Yeah, so that’s your null hypothesis and your alternative hypothesis, whereas in competitive analysis you ask whether the mean genetic association of genes in the gene set is greater that of the genes outside of the gene set.\nYeah, so that’s your competitive analysis. And compare this with with a randomized controlled trial or any experimental setup, So in a self-contained analysis we would ask if the mean improvement of patients in the treatment group is greater than zero, whereas in a competitive analysis you would have a control group, so you would ask whether the mean improvement of patients in the treatment group is actually greater than that of patients in the control group. Now everybody would agree that we would want to do a competitive analysis. We would need a control group, otherwise we cannot really say that the treatment is causing the patients to improve. So that’s also the reason why we think competitive analysis is the way to go in gene set analysis and that self-contained analyses are not informative for asking the question whether your set of genes that you tested is actually causally associated with your trait of interest. That’s why we advise never to do a self-contained analysis, but always to to use a competitive gene set analysis.\nOK. This just is stressing that same point and also in the, in part two of these lectures of today I’ve indicated this or if this is not clear than maybe you should go back to Part 2 of the lecture, so I hope that this message does come across and I’m looking forward to the MAGMA practical that is planned for later today. Thank you for listening and see you later!\n\n\n\nFUMA\nTitle: FUMA: Functional mapping and annotation of genetic associations\nPresenter(s): Kyoko Watanabe\n\num hi everyone i’m a phd student at the fair university in amsterdam my my work is mainly focusing on how to understand the genetic association in biological context and today i’m going to introduce you a web application i have recently developed which is functional mapping and annotation of genetic associations\nso i’m going to start with very quick look at what was zeros again so we basically start with genotyping a large number of individuals using snip slay which nowadays we can tag around million snips and by performing imputation with reference manual you end up with a maximum of 20 millions of snips so in a very simple case when you have case and control groups in your individual in your genotype individuals you you perform the statistical tests to see if the occulance of mineral in case and control groups are different from zero so at the end you basically get the p value for every single snip you have but as you can imagine the number of statistical tests be performed is same as the number of snips you have we of course have to correct for the multiple testing and the golden standard genomic significant p-value is five times ten to the minus eight so whenever you find the snips with p-value less than that they showed those genomic regions are called zeroes close eye\nso the very very first jiva study was published in 2005 and since then the cost of genotyping dramatically decreased which allowed us to collect much larger number of individuals and nowadays the big consortium for meta-analysis usually use more than 100 000 of individuals and by increasing the sample size we also increase the statistical power to detect relatively weak effect size so for example the height study using around two hundred thousand of individuals they are not ended up with identifying more than hundred of these glow side so we’ve been performing jivas in last decade and the nowadays in the zeros catalog we have more than 3 000 of studies including more than 38 000 of unique sniper rate associations for over 600 of phenotypes so basically we have a lot of risk loci all over the genome\nbut however especially for complex trade which is highly progenic um we know that the association of single snips is very weak to detect those effects we need much larger number of sample size and luckily the uk biobank was just released this month and the qct data database has contains 500 000 of individuals for more than thousands of phenotypes so the uk by bank has the potential to identify novelist clauses for lots of human complex traits and we are expecting more and more viewers to be published in coming few months\nso the question is what do we get benefit from jewish results so ideally we would like to identify the causal variants from genetic associations that can be used to improve diagnostics or prognostics or even you might be able to identify novel drug target or biomarkers however the association is not the cause association doesn’t tell anything about cause id and also purely based on the p value from zeros you don’t really know anything about the biology so identifying causal variance from zero zeros results is not straightforward so to overcome this problem we usually go through the several steps the first one is to correct for led which is non-non-democrats of snips so because of the led the most significant snips you find from a specific genome this glocus doesn’t have to be the one actually causing the phenotype but there must be the snips that that is actually causing the phenotype and that snips must have the higher correlation with the most significant snips\nso we don’t want to miss those snips just based on the p value so the first step is to include all the snips that have higher correlation with the significant snips so once you get the list of snips you’re interested in the second step is to check the functional consequence on the genes for example if you have a snips on the exonic regions or in the non-coding regions and there are several softwares that can perform this task however more than 90 percent of zero’s findings are known to fall into non-calling regions so just knowing that you have jewish hit in the non-coding region doesn’t really help you to understand what is actually going on in biological context um so you still need to annotate bioscale functions annotations there are several data resources that you can use for example the cut score is a directory as called as the details in score of snips and regular mdb is a categorical score tells you how likely the snips affect the regulatory elements and there are several eqtl databases for example the gtx having details in 44 different tissue types and especially for the non-coding regions you will also want to check the epigenetic status the data is available from roadmap and encode and i didn’t bring any database name over here but the three digits in the field of cd genome more and more data is becoming available so including hi-c data might also be another option to map snips to the digital genes so using those functional information at the snips level you can end up with the list of genes you’re interested in and then finally you you you need to take the expression proton in the different tissue types and also the cells share the biological functions such as pathways um so we’ve been performing these multiple steps manually as you can imagine that requires you to install different softwares and download different databases and sometimes you might have to reformat the data each time so this is very time consuming and elephant so we hope to make single platform that can perform all of them\nso we developed a web application named FUMA that basically optimized four steps that i showed in the previous slides into one single platform so in the firmware there are two main processes the first one is snip to gene starting from zero summary statistics and we provide lists of candidate snips with the annotations and also the list of prioritized genes and those scenes can be passed to the second process which is jintu funk where we provide you the further virus confrontation at the gym level and another advantage of firma is we also provide intellectual visualize visualization in the web application so you don’t have to use the external software to just for visualization so i’m going to go through what film actually does in each each process so in the snip to gene starting from the zero summary statistics we first characterize genomic disclose i by correcting for led and here we provide you the list of read snips and the genomic risk clause i and all the snips which are in led of lead snips are passed to the second step which is the notation of snips here we perform the anova and annotate several variant scores and eqdl and also the high c so using those informations we finally perform the gym mapping so currently we have three different criteria the first one is positional mapping using annotation from anova and eq thermoping and also the chromatin interaction mapping so before you perform this g mapping you can also filter snips based on the notation you obtain from step two and you can also combine different mappings together you can specify lots of different parameters when you when you submit the job and we provide the list of gene mapped by the based on the user defines parameters so this is just an example how the result page looks like so we provide manhattan pro on the top and the second one is we also perform gm-based tests using magma software so this is the manhattan board based on gmp value and the summary result per genomic risk loci and all the results are available as a table and you can also create a regional plot with all the annotations together and all the results and approaches are downloadable so this is just one example how you can utilize the eqter mapping so this is one of those cloud glocus on chromosome 14 from schizophrenia device and from from the top the most top product is the zoomed in manhattan broad and the genes and the cut score like rdb chromatin open chromatin states energy in the brain and the eq tails so as you can see the least clock the risk locus itself spans much proteins so if you don’t know if you don’t have any further information you end up with listing all the genes or you can manually take the function of genes and you can pick the one that has most interesting function in the phenotype however by performing eq demo ping we prioritize the single gene which have the eqtail in the brain so performing different types of mucus mapping you can also prioritize genes and another example is for the chromatin interaction mapping so the female currently lives in high c data set from stadium sheba at all which includes 14 different tissue types and cell lines but i already said the the field is growing very fast we also provide option to option to apply the custom chromogenetic matrix which is not limited to high c but you can also use capture high c and c5 um so the ground flight shows the risk loci on chromosome 16 from bmi zeros at the most layer is the manhattan product and the second the blue circle is the genome coordinates and these glossier highlighted in blue and inside of the circle the orange links are high ceiling and the green links are eqtls so as you can clearly see the high ck map snips to additional genes compared to the eqtl so that might helps you to identify a novel candidate genes which you have been boost so finally once you get the list of genes you can use into func process where we provide the heat gene expression heat map and tissue specificity by performing overrepresentation tests for differential express genes across different tissue types and enrichment testing gene sets and also the external link to omim and drunk bank to further investigate the individual genes\nso in summary um we optimize the postgres annotation in a single platform as a web application so this might be the very first place to stop by to get the very um broad overview what’s going on in uh in the zeros is close eye once you get the new zero zeros but also if you have a phenotype of interest there are lots of g1 summary statistics already available so you can start you can perform the firmware for the available zeros and start integrating with research and for the future updates we are thinking to extend fema to be able to accept the whole exome sequencing study and also the eros and so finally i would like to thank my supervisor core supervisor and whom is available online so please feel free to visit the website and i also have a poster this evening at b325 so if you have if you want to know for the details please feel free to visit me thank you\n[Applause]\ni’ve seen a couple of cases where even though the the british locus is associated with the same phenotype you see clear evidence of distinct haplotypes so it seems like fema would probably be able to show you cases like that where potentially you’re getting the same phenotype from distinct variants that are affecting say the promoter of the gene or a nearby enhancer uh you mean like player therapy is um yeah well so it’s more like the humor is just to annotate what’s the functional information available so it just provides you the options which snips you’re going to look at even further so it’s not it’s not removing the information so you might get multiple snips that have functions from one locus but yeah we cannot distinguish which is actually causal but i don’t think you’re going to miss that information you"
  },
  {
    "objectID": "welcome.html",
    "href": "welcome.html",
    "title": "Welcome to the PGC Video Textbook!",
    "section": "",
    "text": "Hello! I’m Cathryn Lewis from King’s College London and I’m the Education and Training lead for the Psychiatric Genomics Consortium. I’m very pleased to welcome you to the PGC’s Video Training Textbook.\nThe aim of the textbook is to provide comprehensive training materials in psychiatric genetics. We start with an introduction to mental health disorders, give a background to genetics and the technologies used to generate genetic data, and then we step through the methods that are used to analyze that genetic data, from quality control, to genome-wide association studies, to polygenic scores, and pathway analysis.\nThese videos have been collated from online resources, including those produced for the Psychiatric Genomics Consortium, and others from external groups. They range in length from a few minutes, to an hour, and cover both teaching lectures, and methods tutorials. Some videos are marked as “Basic”, “Intermediate”, or “Advanced”, so you can plan your learning program accordingly.\nYou can use the textbook in any way you choose, working systematically through each section, or picking specific topics that you want training in. If you identify areas that we don’t cover, or that you want to suggest additional videos for, please let us know! The textbook is a flexible resource that we hope to update regularly.\nFinally, a huge thank you to the team, who have given their time, their energy, and their enthusiasm to create the textbook. This is an international team from the US, Europe, and New Zealand, and it’s been a great pleasure to work together.\nWe hope that this Psychiatric Genomics Consortium Video Training Textbook fills a need in building capacity in the psychiatric genetics community, and so, enables us to train a new generation of researchers worldwide. With the rapid expansion of available genetic data, the need for skilled analysts has never been greater! And that is an essential step, if we are to realize the potential of using genetics to improve the prevention, diagnosis, and treatment of mental health disorders. And I hope the Video Training Textbook helps you achieve that aim."
  },
  {
    "objectID": "software_MR_transcript.html",
    "href": "software_MR_transcript.html",
    "title": "Software Tutorials: Mendelian Randomization (Video Transcript)",
    "section": "",
    "text": "Title: Examine causality using Mendelian randomization\nPresenter(s): Jie Zheng"
  },
  {
    "objectID": "software_gwas.html",
    "href": "software_gwas.html",
    "title": "GWAS",
    "section": "",
    "text": "Genome-wide Association Studies\nTitle: Genome-wide Association Studies\nPresenter(s): Hailiang Huang\nLevel: Intermediate\nLength: 15:47\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\nLink to tutorial GitHub and datasets.\n\n\n\nGenotype QC\nTitle: How to run quality control on genome-wide genotyping data\nPresenter(s): Joni Coleman\nLevel: Intermediate\nLength: 16:19\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\n\n\nTractor: GWAS with admixed individuals\nTitle: Tractor: Enabling GWAS in admixed cohorts\nPresenter(s): Elizabeth Atkinson\nLevel: Intermediate\nLength: 18:38\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\nLink to tutorial GitHub and datasets.\n\n\n\nGenotype Calling and Imputation with MoChA\nTitle: Genotype calling/imputation on the cloud: MoChA pipleine\nPresenter(s): Giulio Genovese\nLevel: Intermediate\nLength: 15:13\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\n\n\nSAIGE: Family-based GWAS\nTitle: Genetic association studies in large-scale biobanks and cohorts\nPresenter(s): Wei Zhou\nLevel: Intermediate\nLength: 27:17\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\n\n\nCC-GWAS\nTitle: Tutorial to use CC-GWAS to identify loci with different allele frequency among cases of different disorders.\nPresenter(s): Wouter Peyrot\nLevel: Intermediate\nLength: 21:46\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\nLink to tutorial GitHub and datasets."
  },
  {
    "objectID": "software_imaging.html",
    "href": "software_imaging.html",
    "title": "Imaging",
    "section": "",
    "text": "Title: Imaging Genetics\nDescription:\nPresenter(s): Xavier Caseras\nLevel: Beginner\nLength:"
  },
  {
    "objectID": "software_prs_transcript.html#prs-csx-to-perform-prs-analysis",
    "href": "software_prs_transcript.html#prs-csx-to-perform-prs-analysis",
    "title": "Software Tutorials: PRS (Video Transcript)",
    "section": "PRS-CSx to perform PRS analysis",
    "text": "PRS-CSx to perform PRS analysis\nTitle: Hands-on tutorial of using PRS-CSx to perform multi-ancestry PRS analysis\nPresenter(s): Tian Ge, Yunfeng Ruan, Stanley Center, Broad Institute\nYunfeng Ruan:\nhello everyone this is uniform I’m going to introduce how to calculate polygenic risk war with PRI CSX prssx combines both poetry was to increase the accuracy of polygenic risk score if you are interested in the algorithm please refer to operate on Mac archive or GitHub has a very detailed read means it covers almost all the aspects of how to run PRS says X if you are familiar with python you may figure out how to run it within no time yourself here I will walk you through the whole process and clarify some details in the addition to the readme\nthis is a workflow of the methods the input is G1 summary statistics from different populations prs6x adjusts the effect size of each population D was using their corresponding LD reference panel the adjustment of sleep effect size are performed to Snips that are shared by input G was LD reference panel and the target data therefore prss X needs the prefix of Target data so that it can read the Steep list from the dot Beam file the output is two sets of adjusted effect size\nnext you calculate the pris of your target sample based on each set of the adjusted effect size usually this is termed by plink finally you linearly combine the PRS from different operation G was using software like r or Matlab and that predicts the phenotype with the final result in pris you can also add more G was so that hopefully you can have a better result\nnow let’s see how to run prss X step by step PR status X is a command line python software you can use either person 2 or 3 and have to install Senpai and the H5 pi you can use command git clone to download and install in one step or you can click the code button to download it locally and unzip the compressed file you can test if the installation is successful by comment prr successx dash dash help it will print a long list like this\nnext download the LD reference we provide the pre-calculated LD reference panel Link at the readme getting started session we have early reference calculated from 1000 to genome and LD reference calculated from UK power bank samples you also need to download the Steep information and put it in the same directory with other audio reference files after you download all the software and the LD reference you can write your first analysis\nyou can download the formatted test data and the user example command from the readme test data to run a test the test will be finished in about one minute and you will have two txt files as the output if you use your own data the first thing to do is format your summary statistics file the information is available on using prssx session I want to highlight that the formatted the inputs must contain header line and have the right column order and the the available names now you can finally run the method here is a typical comment which contains\nthe path to the prss x the the directory of LD reference the prefix of the target data a list of formatted summary statistics please know that you put should comma between different items and there should be no space in the list and the list of G1 sample size and the list of population of the GEOS we highly recommend you run the prss x per chromosome to allow calculation in parallel you can specify which chromosome you want to adjust by specifying it in the Chrome option and then the hyper parameter file and the output\nto address the effect size of one trait you need to run 88 jobs for each job PR CSX will print something like this on either screen or the log file you will get two outputs one for each population if you adjust each chromosome in parallel you can concatricate the result in one file use the command cat all together you will have eight sets of adjust effect size you can calculate pris based on the adjusted effect size using a little like a score function in clink the last step is to predict the video type with the PRS\nfirst you normalize all the prns to mean equals to zero and the standard deviation equals to one then you optimize the hair parameter in validation data for each file you predict the phenotype with the normalized prsa and the normalized prsb and get the r squared you compare r squared from each Phi and learn the pacifier and the coefficient of the two pris under that file\nthen in the testing data you calculate the final result with the knowledge of the pacifier and the coefficient of the two pris you choose the pris under that pathfly and combine them with the Learned coefficients then you use the combination to of the PRS to calculate the r squared this will be your final result if you have any other question you can use an issue on the GitHub website and all team members and the data resources"
  },
  {
    "objectID": "software_prs_transcript.html#prs-in-ancestrally-diverse-populations",
    "href": "software_prs_transcript.html#prs-in-ancestrally-diverse-populations",
    "title": "Software Tutorials: PRS (Video Transcript)",
    "section": "PRS in Ancestrally-diverse Populations",
    "text": "PRS in Ancestrally-diverse Populations\nTitle: PRS-CSx: Improving Cross-Population Polygenic Prediction using Coupled Continuous Shrinkage Priors\nPresenter(s): Tian Ge\nprice past nine so I will you know start introduced him uh 10 is a good friend and a good colleague of me of mine as a general hospital and Harvard Medical School um he’s a assistant professor in Harvard Medical School and you know trained as a mathematician he’s contributed to many uh you know domains in science including the neural genetics neural Imaging uh statistical matters and the genetics of development so for today I’m very excited to have him talking about a PRS a new PRS method that is able to jointly model g16 summary statistics of medical ancestrys to improve the prediction accuracy so without further Ado uh Tien let’s get started\nTian Ge:\num thanks for very generous introduction and for having me today so I’m going to talk about our recent work that extends our base in Partnership production framework which is known as prcs to prcsx which can now integrate G1 summary stats from multiple populations in order to improve cross population polygenic prediction um so to start I just like to briefly recap the idea of polygenic projection and then talk about the motivation and intuition behind the piscs work which might be useful to see how PR CSX works so as many of you already know um polygenic prediction summarizes the effects of genomic genetic markers to measure the genetic liability to a complex trait or Disorder so a conventional method to compute research for particular risk score is pruning and thresholding which is also known as the Columbians thresholding so basically to apply this method usually we set up a p-value threshold or screen a range of p-value thresholds and for each of these p-values thresholds we only consider Snips reaching the significance level level we then perform a procedure called LD clamping which basically retains the most significant snip in each genomic region and discards all the Snips that are in LD with the least nib so for example in this figure we will just keep the top snapping purple and then remove all the Snips that are in red orange green light blue because they are being LD was the lead snip um and then finally we sum up the genotypes of the remaining Snips weighted by the effect size estimates from external GEOS so this pruning and thresholding method is very widely used because it’s conceptually intuitive and also computationally very efficient but it has several limitations so for example it relies on margin of GEOS Association statistics and we know that most associated in European huge region may not be causal so a lot of times we might be using the sub-optimal tagging step to build PRS and because because of the LED clamping procedure we use the method also ignores many of the secondary and tertiary signals in each genomic region and finally when we sum up the Snips we use the effect size estimates directly from external GEOS without any adjustment or shrinkage a large effect size estimates in g-was may suffer from swingers curse and and most non-codile variants will have noisy non-zero effect size estimates so by including these nibs and using these G1 effect size estimates directory without any adjustment we are adding a lot of noise to our PRS and and all these um limitations or limit the predictive of predictive performance appearance um so to address these limitations and improve the conventional pruning thresholding method a more principled framework is to calculate the product generator score by joint remodeling the genetic markers across the genome without any arbitrary pruning and stress shorting and to do this we are basically fitting this linear regression problem where the phenotype Vector is regressed onto this genotype Matrix X and beta here is a vector of snap effect size and Epsilon is a vector that captures non-genetic effects so if we can join traffic with this regression model and get the effect size estimates which is denoted as beta head here we can take the estimate to to the Target data set and where the genotypes there and compute apology score so the methodological challenge here is that in genomic prediction we often have many more Snips than the number of samples we have so this is a ultra high dimensional regression problem and we we need to regularize the effects as estimate to avoid overfitting um and so we know that this pruning and thresholding Method can actually be considered as a specific way of regularizing and shrinking the snip effect size because it in essence shrinks the effect size of discarded snap to zero and performs no shrinkage on effect size estimates of selected Snips but we have discussed that this this shrinkage scheme may be arbitrary and sub-optimal so there are many more principles that is called Methods that can import this shrinkage so for example the frequentest approach is to fit a regularized regression using methods like lasso or Ridge regression or elastic net which often encourage sparse effects as estimates and penalize large effects so if you have so if you have heard of this measure called lassos um so that’s one of the polygenic prediction methods that applies lasso to build PRS um so in the past few years we also see many Bayesian polygenic prediction methods that have been developed and the Bayesian approach to tackle this High dimensional regression problem is to assign a prior on Snap effect sizes to impose screen coach so all the models basically fit the same regression and the difference is what prior distribution to use so the question here is you know how do we design a prior or which prior is optimal for this type of genomic prediction problem um so the most widely used prior is what we call infinite has monomial prior which is also known as The Basin Ridge regression so with the effect size of each snip follows a normal distribution so this model is very widely used in many classical statistical genetics methods including like gcdta and odd score regression so all these messages assume this underlying infinite test more normal genetic architecture and one major advantage of this price and also that’s why this price is so popular is that it’s mathematically trackable and there’s a closed form expression for the posterior um so here um Lambda is a penalty parameter or shrinkage parameter which depends on these two variants one is the per snip variance of the snip effects and the other one is is the residual variance here so you can see that if the noise the residual variance is large relative to the genetic signal and then we impose a strong shrinkage on the effect size and beta is shrunk towards zero so in the extreme case if you have no genetic signal and then the beta will be shrink to zero on the other hand if the genetic signal is relatively large to the noise and then the estimate will be closer to the least Square estimator and with this penalty from The Matrix is always invertible so this is a well-defined estimator so we also noticed that this is a multivariate estimate of snip effect sizes and X transpose times x here is proportional to the LD Matrix so it’s easy to incorporate LED information in this estimator and in practice you can always divide the genome into independent LED blocks and then within each block you can do this joint estimate of snip effects um so with this being said um there are also limitations of this prior so as you can see here the shrinkage parameter is a constant meaning that under this infinite has monomial prior all the Snips are treated equal and they are shrunk towards zero at the same constant rate so this is sub-optimal because ideally we want to impose various strong shrinkage on small and noisy non-codile signals but at the same time we don’t want to over shrink large and real signals so what we really want is the shrinkage scheme that is adaptive to different Snips and different G1 signals but this cannot be achieved by this infinite Tesla normal prior because the penalty parameter here is a constant which is not snip specific um it’s in an alternative way to look at this problem is to take a look at the shape of the prior distribution which is normal um and this non-adaptive nature of the prior is equivalent to say that you know for the normal distribution um when used as a prior there isn’t enough Mass around zero to impose strong enough shrinkage on noisy signals and and because of this normal distribution has exponentially decayed Tails so these tails are two things meaning that a priority we believe there’s very low probability of large effect sizes so we don’t have a prior that can accommodate those large effect size which often leads to overshrink of real signals so that’s why the Bayesian Rich regression or in this infinite testimon normal prior is not very adaptive to different genetic architectures and usually only works well for highly polygenic trades um so there are many work um trying to design a more flexible prior so that the polygenic model is more adaptive to varying genetic architectures and one idea is that in contrast to use a single normal distribution is the prior we can use a mixture of two or more distributions so for example one pioneering approach in this field LED prep uses this Spike and slab prior which assumes that a fraction of the Snips are now so they have no effect on the phenotype while the rest of the Snips are called or Snips and and their effects are this follow a normal distribution um so if we take a look at a density prior so this prior has two components so there’s a spike component or which is a very narrow distribution centered at zero which is often used to model small signals and there’s a slab component which is much wider and can be used to model large signals and then by varying this proportion of the chordal variance which is coded in pi here so this model can cover a wide range of genetic architectures um so although this prior is much more flexible than the infinite Tesla normal prior it also has two limitations so number one so this is a discrete mixture of two components uh so we call this type of Prior discrete mixture prior so in posterior inference you can see that each snip can either belongs to this now component or this normal component normal component so you can imagine that if there are a million Snips then we have a discrete model space of 2 to the power of medium possibilities which is you know almost unlikely to fully explore and number two so unlike the infinitesimal normal prior which has a closed form multivariary update of the snip effects so the spike and slab prior does not allow for a multivariate effect estimate so in post Theory inference one has to update effects like snip by snip which makes it very difficult to incorporate LD information in this model estimation procedure um so there are many other Bayesian polygenic projection methods that have been developed and used different priors but the majority of them are discrete mixture price so for example you can parameterize um just two normal mixture differently using a additive version or multiplicative version so you can also do a now component plus a t distribution which gives you a heavier tail to model larger signals say s base in R which is another method that receives a lot of attention recently uses a mixture of four normals and and each of these normals captures the effect size distribution distribute Distribution on a different scale which which makes the model even more flexible and then finally there are non-parametric models where the effect size distribution is assumed to be a mixture of an infinite number of normals and in posterior influence the data will determine the optimal number of mixtures um so these are different variations of this discrimination normals so there are all discrete mixtures of two or more distributions so they largely share the same advantage and limitations of LD print so just to quickly summarize um so we have discussed that so the infinite has more normal prior is computationally efficient and allows for multivari remodeling of l-dependence but it’s not robust to varying genetic architectures while this cream extra price on the other hand can create much more flexible models for the genetic architecture but they are computationally challenging and it’s often difficult to incorporate LD information so our motivation was to design a prior that can combine the advantages of these two types of price\nso in our prcs work we introduced this conceptually different class of price which is called continuous shrinkage price and in contrast to this horizontal discrete mixture of normals we use the hierarchical scale mixture of normals and here Phi is a global shrinkage parameter which is similar to the Penalty parameter in Ridge regression and it is shared across all the Snips and models the overall sparseness of the genetic architecture and and different from this infinitech monomial prior we added this low cost shrinkage parameter so here J is the index of snips so this local shrinkage parameter is Snip specific and can adapt to different genetic signals and you can see that if we integrate out these hyper parameters the density function of this prior is continuous which can also be seen in this density plot on the right so here the dash black line is the normal prior for reference and then the red and blue lines are the two components of the spike and slab fire and the yellow line is the continuous shrinkage prior so you can see that unlike this two components I can slap prior the prior we used is one continuous density function but it can approximate the shape produced by this discrete mixture price and compared to the normal distribution so you can see we put substantial Mass near zero which can impose strong shrinkage on small anointing signals and in the meantime this is this this distribution has heavy polynomial Tails which can retain large and real signals so the continuous trinkage prior is almost as flexible as the discrete mixture prior but because of its continuous nature it also shares some advantages of the infinite test monomial prior that is it allows for the multivari modeling of LD patterns and it’s also computationally efficient so these are the motivation and some intuitions behind prscs so I’m not going to talk about other features of features of the method but the software is available on GitHub which you can download and test so we hope we have released both pre-computed Southern genomes and UK power bank reference panels for major populations which hopefully has made the application easier um and in the initial application of the prcs method so we applied it to some existing guas of six common complex diseases and six quantitative traits and then predicting to the Mass General Bergen biobank and you can see that kind of prsc is substantially improved the prediction over this conventional pruning and thresholding Method and often outperform LD print um and a second application that might be relevant to this group is the polygenic prediction of schizophrenia and in this study led by Amanda so we aggregated the schizophrenia cases and controls across for Healthcare Systems so Geisinger Mount Sinai Mass General Bergen and Vanderbilt so as part of the cycle emerge Consortium and you can see this polygenic risk score calculated by prscs correlates with the case prevalence and schizophrenia diagnosis and can be used to identify diseases that are genetically correlated with schizophrenia using a free watch design so that’s the review of the ideas behind prcs and some of its applications but one limitation of prcs is that it was designed and tested in homogeneous populations but now it is well well recognized that cross population predictive performance of polygen and growth spores decreases dramatically especially when the target sample is genetically distant from the training example due to the predominant European samples in the current guar studies so there are many factors that can limit the transferability of powers learned transferability of PRS learn from European GEOS so for example there may be population specific variants or variation in the snap effect size estimates across populations the allele frequencies and LD patterns are different across populations um and also the differences in the phenotyping OR environmental factors can all affect the prediction accuracy so in the past few years there have been many efforts to expand the scale of now European Duos and to diversify the samples in genomic research in general although the sample size of most non-europe NG was remain considerably smaller than European studies so they cannot right now they cannot be used to fully characterize the genetic architecture in non-european populations and dissect relative contributions of these factors to PRS predictive performance but one question we can ask is can we leverage these existing non-european Jewelers to improve trans ethnic prediction of the pris even if they are smaller than European unions um so we have been working on this um method called prscsx which is a very simple extension of the prcs framework so here to model existing now European guas we assume that we have data from K different populations and then we still use this continuous shrinkage prior to model snippet effects but this time this product is shared across populations so you can see that these shrinkage parameters do not depend on K which is the index of population so they are shared across populations and to use this coupled prior we have implicitly made assumption that causal variants are largely should across populations so we think this is a reasonable assumption given that many recent Studies have estimated the transdent genetic correlations for many complex traits and diseases to be moderate to high um and with this coupled prior so we can borrow information across summary statistics and increased accuracy of effects as estimation particularly for non-european populations whose G1 s emphasized relatively small um and the other advantage of this coupled prior is that we can leverage the Audi diversity across populations to better localize the G1 signal so this is very similar to the idea of transcending file mapping so although we are not doing any form of file mapping analysis here we we’re actually using this idea um so although we use this shared prior cross populations the effect size for snip are still allowed to vary across populations and so we believe this gives the model more flexibility so we don’t constrain effects has to be the same across populations and we also allow for population specific variants meaning that so if a snip is available in one population but absent in other populations due to for example the low frequency other populations we still include the snap in the model although in this case there’s no effects to couple or we still included Snips in the modeling um so finally um PR CSX incurts many features from prcs so it allows for this multivariate modeling of LD patterns using population specific reference panels and also computational efficient so in practice PR CSX takes the G1 summary stats and the ancestry matched LED reference panels from multiple populations it then joins a models or these data that fits the model and then I’ll put one set of the snap posterior effect size for each Discovery population and then these snap effect science estimates can then be taken to a validation data set and calculate one PRS for each population and we then learn an optimal linear combination of these PRS in the validation data set and evaluate the predictive performance of the final PRS in an independent testing data um so as a comparison and also in the results I’m going to show so we’re examine two alternative methods that can combine G1 summaries that’s from multiple populations uh one method which we call PT met here it performs a fixed effect meta-analysis of the GEOS and then applies the pruning and thresholding method to the method you was um and since the LD pattern is mixed after this meta-analysis so it has different LD reference panels in this case and then select the one with the best predictive performance to evaluate in the testing data set so the other method which we call PT multi this method was developed by August price group a few years back so they applied prunian thresholding separately to each G1 summary statistics and then the resulting PRS are linearly combined in the validation data set and then taken to the to the final PRS for for evaluation\nokay um so here are some results so we selected um 17 quantitative traits that are shared between the UK power bank and about Bank Japan and in this analysis UK power bank G1 sample size is typically three to six times larger than the BBJ biobank japani was so we then train uh different PRS using BBJ g1s only so these are the PRS measures that applied to the wild Bank Japan g1’s only and then these are the PRS that was trained on UK biobank data only and then the last three methods are those PRS that combine the G1 summaries that’s from the UK bio bank and biobank Japan and then we train these different PRS and then predict into um different populations in the UK power bank that are independent of the UK power bank training to us so you can see so here in the first panel when the target sample is the UK power bank European population and you can see that PRS is trained with the ancestry match the UK power bank Gwas performs better than PRS trained with the BBJ juice which is expected and in this case combining the UK power bank and bbj-g was doesn’t help too much so you can see it is a very small probably five around five percent of improvement in prediction accuracy when we combine UK power bank and biobank Japan that’s likely because the UK power bank G was was already quite powerful so adding a smaller East Asian G was doesn’t help too much in a prediction uh in a prediction of the European samples um but when we predict into the UK biobank East Asian samples you can see PR CSX can increase the prediction accuracy um here the bar shows the median medium variance plane that was in increased by about 25 percent when comparing with When comparing prcsx with these PRS trained on the European GEOS and then if you compare with this ancestry matched PRS trained in the biobank Japan he was the Improvement was even larger it’s around 80 percent so these results show that we can leverage this large-scale European G was to improve the prediction in non-european populations and then when we predict into the UK biobank African samples the target population didn’t match any of the discovery sample the biobank Japan sample or the UK power bank sample and and both the European and East Asian samples are genetically distant from the African samples so in this case the Improvement in predictions was again limited and predictions in the African population remain quite low relative to the predictions in European and East Asian populations um so we asked whether we can add some African samples to the Discovery data set to improve the prediction in African population and among the 17 trades We examined here seven were also available in the page study which largely comprised of African-American and Hispanic Latino samples but you can see the sample size of the page study was much smaller than the UK power bank and baobank Japan so the question here is whether adding a small African jewels to the Discovery data set can improve projection um and you can see in the right panel of this figure so when integrating this UK power bank power bank depends and Page summary stats using PRN CSX the prediction the African sample was quite dramatically improved and Improvement in media and variance brain was about 70 When comparing with the prcsx applied to UK power bank and barber and Japan she was only and the prediction was also much better than the PRS train on Ancestry matched page study so these results suggest that we can leverage samples that have matched ancestry with the target population to improve prediction even if the now European training was a considerably smaller than European studies so adding the page study of the discovery data set also improved the prediction in other Target populations although the Improvement was to a much less extent so in the last example so we evaluated um different purse methods in the prediction of schizophrenia risk so in in this analysis we use the GEOS summary statistics derived from the pgc2 schizophrenia G was in the European samples and also the reasons schizophrenia G was in the East Asians and of course led by Max lamb Cheyenne and Hylian and colleagues so we have access to the individual level data of sixth East Asian cohorts um and we left out one cohort as the validation data set so this is the data set we use to learn hyper parameters or linear combinations of pis and then for the remaining six cohorts we apply the leave one out approach so meaning that we in turn use one of the six cohorts as the testing data set and then meta-analyze the remaining five cohorts with the other cohorts to generate the discovery to us for the East Asian population we then build again build different PRS using the East Asian GEOS only or using the European G was only or using methods that can combine these two guas and you can see that the prcsx can increase the prediction accuracy relative to messages trained on a single GEOS and then the um medium rarings playing here had approximately 50 increase relative to guas using ancestry match these Asian Duos and almost double the prediction accuracy here when the pris was trainually using European GEOS and on the right panel um you can see that at the tail of the PRS distribution and prcsx can also better stratify patients at top and bottom PRS percentiles relative to other methods um okay so I think I’ll stop here and send call my collaborators so in particular viewing phone has led many of the real data analysis in this project and how long has critical inputs in every aspect of the project and he also led the Stanley Center East Asia initiative which make the schizophrenia analysis in the East Asian cohorts possible and then our preprint is our Med archive and we have also released the software on GitHub so any feedback on comments will be much appreciated so I will stop here and I’m happy to take any questions great thanks a lot here and that’s a great talk so um we have 20 minutes for questions foreign terrific talk Tiana really appreciate it so um I’ll just start things off I’m sure there’s other questions um but I guess one question I have is it seems like with these methods uh Beyond pruning and thresholding one of the big barriers is that some of them are harder to implement than others Printing and thresholding is so easy um and so I was wondering if you could just say a little bit about um how difficult it is to implement this approach and what um what information people need to be able to use PRS CSX um yeah so um I guess in many of the analysis we still use pruning and thresholding as a baseline because it’s computationally faster and also a robust approach so you can use that as a comparison so in terms of those phasing methods um depending on the different implementations and different methods will require different computational costs and usually takes longer than premium stretch holding but um we have trying to you know hopefully make this software easier to use so we have released these reference panels so users usually don’t need to calculate their own reference panels and then we can parallel the computation of clouds chromosomes and then usually for the longest chromosome it takes around an hour or one and a half hours to finish so I think it’s\num populated and add too much computational burden on end user that’s really helpful that’s really useful right and so people just summarize statistics from multiple populations of course the other the other things are provided with your software yeah great so on that topic 10 uh could you perhaps discuss a little bit about the mixed population uh you know I think not every population has a relief reference panel so what are the considerations here and the what are things you know these people can do if they want their you know uh analysis being done you’ll see this matter great question there are a lot of challenges in terms of how to handle Amex populations because um you know the LD patterns might depend on specific studies and and the proportion of the samples in each study so I don’t think there’s a universal reference panel that can be used for all that makes populations um so right now so for example in this study when we try to model the page dual summaries that’s um so it’s sort of a mixture of African-American examples and also Latino Hispanic samples so it’s kind of atmixture so we try to use a African reference panel to approximate in this situation and turn out to work okay but clearly there are still a lot of work to do and think about how to better model the admix populations and how to build reference panels in this case all right maybe I can ask a question so um solely on the deal with on summary statistics it’s always simpler but at the same time more difficult right because you lose a lot of detail information uh so but I’m curious if you have individual level do not have data will you be able to do that better uh for the advanced population because you should be able to have much higher individual level resolution incredible uh population local structure right um right so I think there are two aspects of this question so number one is you know do we lose any information when we use summaries that’s relative to individual data whether it’s a homogeneous population or at next population right um so the question that the answer to that question is so if you only look at um only use the second order information it’s basically the LD information and then you assume you have a LD reference panel that can accurately approximate the true LD patterns in your dual sample and then there’s actually no information lost when we use the reference we use the summary stats data relative to the individual level data um so the question here is can we find a G1 ref can we find a reference panel that can closely approximate the LD patterns in your G1 sample um and so a lot of times you know when the G1 sample G was was conducted in a homogeneous population we think um you know the the reference panel was accurate enough but that also you know warrants you know if in the future it’s possible to release in Sample LD information with the geother summaries that we can of course do better and get more accurate LED patterns so we’ll have less information loss or less reference sample mismatch um in this case and then the second part of the question was you know if we have individual level data can we do better to handle app mix populations um and I think sure because with individual level data you can go beyond LD you can look at local ancestry and do those decomposition and build pris using those local ancestry information which can of course be much more accurate than uh treating the whole genome in a homogeneous way um so I think going forward releasing LD inflammation and local genetic information with the GEOS summaries that might be the key to further improve the polygenic prediction\nthese topics populations just follow the um so you have an implementation and to deal with individual level potato and we don’t actually but but if you have individual level data you can just compute the ink sample LD and then do a g was so then I’ve tried a message to the GEOS summary stats that should give you um you know highly similar results to a message using individual level information thank you question from Laura slootman we did but it was answered because I didn’t uh I thought the the answer was uh truncated before uh the individual level conversation uh I was going to ask specifically about what you just clarified actually girl this is could I ask a practical question um we usually transfer as ratio to look at ratio before calculating pis based on LD pruning and p-valuation approach so my question is do we need also do the same process in piscs because we know that the posterior effect size in this approach is relative small if we don’t um process the ratio to low guard ratio before calculated posterior effect size right so prcs can take alteration odds ratio estimates but it basically just take out ratio and take the lock and convert it to standardized beta so if you you know G1 summaries that’s it’s odd ratio then it’s by nprcs can take that I think I thought it was really encouraging to see how much better the prediction was with the page samples for the African ancestry individuals um do you have any ideas about why that worked as well as it did um yeah that’s a good question so I think number one so doors do population specific variants and information that data containing in the page study Geo summary is that that’s not available in the US of other populations the other possibility might be you know I integrate these Duos so because the LD pattern that the Audi block is smaller in African samples so we have a better localization of the Dual signal which also improved the production accuracy but I I think there’s much work to do to dissect these contributions and see\nyou know whether we can improve on that it’s very encouraging because I think sometimes when you see samples for European ancestries that are are over an order of magnitude or more larger than the other ancestries you think well maybe it’s not worth it including these other ancestries but it sounds like these data are suggesting that definitely you should I just have a question uh I think someone said about the phenotype I just wonder if you think that you know for the Latin population it’s because you know I think the sample size is small if if you have a better phenotype something like that should you have more chance to you know to I don’t know but to predict I think this meta-analysis or phenotype is something it’s a variable that matter here or not you know it’s just I don’t know if you start a project in Latin American that is totally mixed and you don’t know and you have simple there’s more simple size do you think it’s you know it’s good to spend I don’t know money or time having a deep phenotype or this is doesn’t matter so the video type typing that’s definitely influences the production accuracy of um pris and then if you have very different phenotyping in say or training and Target populations that might reduce the production accuracy and in many of our work so when we try different phenotyping methods so for example try\nto predict depression and then there are different ways to you know type like using ICD codes um using any rule based or algorithm based definition of depression cases and controls and they do give us meaningfully different production accuracy and a lot of times um you know there’s a balance here because if you use the simpler icd-based method you get you know more cases and controls that you use the more Exchange in definition you get higher specificity but sometimes lower power because the case number is reduced so I think again there’s a manufacturers that contribute here the sample size and how the phenotyping matches between the discovery and Target data set and how specificity specifically the phenotyping algorithm is um a lot of times you know when you conduct the PRS analysis these phenotyping issues beyond our control because we only use Geo summary stats or tests the pr as the existing cohorts but if you have control over these genotyping algorithms um I think some kind of results phenotyping and sometimes boost the projection of pris okay so other question is just today I saw you know a talk in a conference that they use a family apology trios or families to predicted this polygenic risk score and then you know they have this analysis using trios what do you think about that it’s a good strategy or right I think Trio of family studies provide additional opportunities that can’t be done by uh PRS of unrelated individuals for example you can better control environmental factors and sometimes you can even decompo decompose and dissect the QR transmitted and non-transmitted abuse interesting questions that you can only do in family or truths that is so I think both methods are so so it’s so a lot of work we do is to do this PRS analysis in population-based cohort and trying to stratify patients for example but in terms of memory study they also give a different or unique aspect where you can look at the relative contribution of genetics environment so I think both study designs are useful and can be used to answer different questions it should your method can be used in this approach um that’s a good question so right now probably not because when we build the model we assume the G1 summaries does or calculated on a large unrelated G1 sample so if we want to conduct any um any PRS analysis that is specific to your family or Trio design probably need to look into you know more specific methods that can you know better address the questions there and those areas I remember of their paper about about estimating the cross population prediction activity\nand um he said the most tricky part of the smart population predictions to different housing matrices and those variances so your message how these two parts are handled so I actually have some trouble here in this first part of the question well maybe I I see a louder a little bit better now I would say can you hear me the voice quality is not great um speaking up it seems like helps a little but it’s still a little bit difficult to hear um sorry I think it’s better sending the question in the chat because it’s okay then I will type the question sorry I don’t know sorry I don’t know if you I will have time but yeah no problem if you have any questions you can also you know email me afterwards [Music] yeah foreign how are the two matrices handled in your method um two different LD matrices um right so we use so there are you know if you have G Watson raised that’s from different populations you’ll have one without the some population specifically the matching that tensor trail that she was and then when we do this effects not estimates they are actually taken care of and then so different um effects sides in different populations were modeled by the matching LD reference panels and there’s a follow-up question that how are the variants with different minority frequency being handled into different matrices um how minor little frequencies are handled um so we don’t well so I’m not sure how to answer this question so how minor your frequencies are handling so when you come to the LD Matrix um they’re just using population specific reference panel to compute that penalty Matrix and that gives you um The Matrix for each population and then when we model the effect size\nand those effect size are and the relationship between snaps in each population was mapped to the population\nspecific LD reference panel um I’m not sure if that answers the question all right so I think we’re at the hour thanks uh Tien for giving this great talk and thanks everyone for joining us for this uh for for this meeting and I we look forward to seeing each other again in amongst the next meeting is May 5th May 5th May 5th and I think believer back at the uh 105 p.m Eastern great thanks so much Jen and great to see everyone bye folks bye"
  },
  {
    "objectID": "chapter6.1_transcript.html",
    "href": "chapter6.1_transcript.html",
    "title": "Chapter 6.1: Polygenic Risk Scores (Video Transcript)",
    "section": "",
    "text": "Polygenic Risk Scores\nTitle: Polygenic Risk Scores\nPresenter(s): CPM Oxford\nour genes provide valuable insight into our family history our ancestry and also our health as we learn more about our dna we identify new opportunities for healthcare one such opportunity comes from using polygenic risk scores in this video you will learn what a polygenic risk score is see how they can be used in healthcare and find out how you can participate in research studies that use them\npolygenic risk scores why might you care about whatever they are knowing our risk of developing particular diseases can be a really powerful way to help us live healthier lives imagine you knew that you were at a high risk of having a heart attack that’s pretty clear motivation to do something about it such as changing your diet or taking risk lowering medication but how could you know that you’re at a high risk a major risk factor for common diseases such as heart disease cancer and diabetes is our own genetic makeup new studies show that we can now analyze an individual’s genes and actually measure that risk using something called polygenic risk scores\nour genes vary from person to person it’s why we’re not all the same but some of these genetic differences can contribute to our risk of complex diseases there are a few rare diseases that are caused by changes in a single gene but we now know that for the most common diseases such as heart disease and diabetes it’s often not just one or two of these genetic changes that are important it’s many of them each having a small effect on the polygenic risk hence poly many genic to do with genes risk scores scoring a risk scientists have compared dna among gigantic groups of people with and without disease and have identified thousands of genetic variations that influence risk for hundreds of diseases this important reference dna from the studies can then be compared with individual patients dna to calculate their risk\nso how could polygenic risk scores actually help you? a doctor’s usual assessment of a patient might indicate one course of action but when armed with a polygenic risk score a different approach may become the better way to go these scores could help doctors better identify specific medicines or therapies a patient is likely to respond well to or whether they’re likely to benefit from earlier screening for specific cancers and because our genes don’t change as more of these variations are identified your polygenic risk scores can be updated without having any more tests also remember this same genetic information can be used to give you a polygenic risk score for lots of different diseases\nso we’re stuck with the genes we have all throughout our lives but even when they do mean our polygenic risk score of a particular disease is high that doesn’t mean our fate is sealed there’s lots we all can and should do to reduce our overall risk of disease particularly if our polygenic risk is high for example patients with a high polygenic risk of type 2 diabetes can significantly reduce their overall risk to exercise and eating a healthy diet so what’s next well you can see how much promise polygenic risk scores have but there’s still work to be done further studies are needed to assess the clinical impact of prs in improving the diagnosis treatment or prevention of specific diseases also studies must be extended to cover a wider global population but already polygenic risk scores are a promising new tool by taking them into consideration along with other risk factors they have the potential to help us live longer healthier and happier lives to learn more about polygenic risk ors and how you can take part in research studies involving prs please visit us at cpm.well dot ox dot ac dot uk forward slash prs\n\n\n\nWhat is a Polygenic Risk Score?\nTitle: What is a polygenic risk score?\nDescription: A brief introduction to polygenic risk scores\nPresenter(s): Till Andlauer\nin this video i’m going to introduce you to polygenic risk scores prs also often now called only polygenic scores pgs because you can also calculate them on quantitative traits like for example brain volumes\nPolygenic disorders\nall complex common disorders are polygenic if you want to quantify genetic risk for a complex disorder you thus have to assess the effects of many genetic variants at the same time the basis for appears is a g was and there’s a separate video explaining what that is\nCalculating PRS\nfor the calculation of prs this gmas is considered the discovery or training data set the jiva’s effect size are used as the weights when calculating a prs to get stable effect size estimates you need gmail’s generated on large samples as for example conducted by the pgc but\nProblem with PRS\nthere’s one problem neighboring variants are correlated because they get inherited together and thus show similar associations the snip density could thus bias the pure s at any given locus classical prs thus only use the variant with the lowest p value correlated snips are removed using a method called ld clumping\nLD clumping\nhow many snips to use that’s a difficult question you might only want to use the snips that show do you know my significance 44 in this study on depression but what about these ones here are they not relevant likely they would get significant if a larger sample size was used for the gmas therefore typically p-values of 0.05 or 0.01 are used as thresholds for the calculation of classical prs\nOther methods\nin recent years many other methods have been published that use bayesian regression frameworks to model the linkage disequilibrium and thereby calculate ld corrected weights these methods like ld pratt pier scs and sbazar they have been benchmarked in a recent manuscript and they perform much better than classical prs\nno matter how you choose your weights the next step is always the same for each snip you multiply the weight by the number of effect alleles if a is the effect allele then the multiplication factor would be 0 in this case here one and here two you do this multiplication for each snip and you sum over all variants to get a single score a prs by itself for a single person is just an arbitrary number that cannot be interpreted well in order to be able to interpret the prs you need a large group of people that share the same ancestry and possibly even the same socio-demographic background among each other and with the people used for the training divas and only with this group you’re able to interpret the prs of an individual relative to that group and if you calculate prs for many people you will see that the scores follow a normal distribution the prs of a single individual may then map to a lower an average or a higher percentile within that distribution patients suffering from a disorder will on average have higher prs for that disorder than healthy controls but only on average individual level risk predictions from prs thus have a very poor sensitivity and specificity basically you can only say anything about the people mapping to the lowest or highest percentiles they have significantly lower or higher risk for the disorder and for everyone in between you can’t say much you can however get fairly good predictions if you contrast the extreme ends of the distribution but the odds ratios achieved are of course still much lower than for monogenic disorders nevertheless prs can be used for risk stratification to identify people at high risk that might need more medical attention\nApplications\nin research prs have many highly useful applications from the assessment of the polygenic risk load for common variants in families especially affected by psychiatric disorders to the genetic characterization of bipolar disorder subtypes and of course many many more also many reviews and methods articles on prs have been published recently and you will easily find a lot of material to keep on reading\n\n\n\nPolygenic Risk Scores: In detail\nTitle: Polygenic Risk Scores\nPresenter(s): Adrian Campos, Adrian.Campos@qimrberghofer.edu.au\nHello, my name is Adrian Campos and today I’m going to talk to you about polygenic risk scores for the Boulder Workshop 2021. Special thanks to Sarah Medland, Lucia Colodro Conde, and Baptiste, Couvy-Douchesne, for all the input for preparing these slides.\nOutline\nThis is the layout for today’s talk. We’re going to start with a brief introduction on GWAS and allele effect sizes. Then I will give a brief overview of what a PRS is and how it’s calculated with the graphical example. After that, we will discuss which variants to include and how to account for linkage disequilibrium when estimating a polygenic risk score. We will dicuss the more traditional approach named clumping and thresholding which is widely used in the area. Then we will discuss some applications for polygenic risk scores, other methods for polygenic risk scores and a brief summary at the end.\nGWAS\nSo without further ado, let me get out of the picture and let’s start with this talk. As we have previously seen, a Genome Wide Association Study (GWAS) allows us to identify which genetic variants are linked to a trait of interest. A GWAS allows us to identify not only which genetic variants are linked to a trait of interest, but also their effect size. If we imagine, for example, this to be a height GWAS and we focus on the highlighted genetic variant, we would identify an effect size of two centimeters per copy of G allele. So the effect size of this variant would be approximately 2, which is also the slope of a linear regression between the mean height and the genotype groups. What this basically means is that if we had access to a new sample with Genotype data and height data, we would expect AG individuals to be an average 2 centimeters taller than AA individuals and two centimeters shorter than GG individuals. But we know that complex traits like height, are highly polygenic. There are many genetic variants that contribute to the phenotype. Furthermore, we know that common variants have a small effect size and that the example that we were using was an exaggeration. This would cause this single [locus] based prediction to be basically useless. However, we can combine the information that we gain from several genetic variants to estimate an overall score and gain a better estimate of the trait. This is essentially what a PRS does.\nNow let’s keep using this example to understand what a PRS really is. In our previous example, we started by focusing on this genetic variant. Which had an effect size of two centimeters per [copy of the] G allele. So if we were to score this participant based on this genetic variant, we would sum 0 given that he has no copies of the G allele. The same would be true for all participants that are homozygous for the A allele at this [locus]. Participants that are heterozygous at this [locus], have one copy of the G allele. So in order to score them we multiply 2, which is the effect size times the number of copies of the G allele, which is one. Finally, we score participants with two copies of the effect [allele] by two times the effect size, which also happens to be two in this example. For polygenic risk scores we want to aggregate the information of several genetic variants. So now let’s focus on another one. This one has an effect size of minus one per effect allele. Following the same process, we would score participants by weighting the number of copies of the T allele they have, times the effect size of that allele. So in this example, participants with a TT genotype will have a score of minus two for this [locus]. Participants with a CT genotype will have a score of minus one and participants with the reference CC Genotype will have a 0. We can do that for a third genetic variant. This one has an effect size of 0.5 per G allele. Again, we proceed to score this [locus] by multiplying the effect size times the number of copies of the effect allele. We can repeat this process, including all other variants and sum across all loci. These will give you an estimate of the polygenic risk for the trait of interest.\nSo a working definition of polygenic risk score is a weighted sum of alleles which quantifies the effect of several genetic variants on an individual’s phenotype. As a word of caution, the sample for which PRS will be calculated should be independent from that of the discovery GWAS. That means there has to be no sample overlap between the sample with which you calculated the effect sizes for the variants, and the sample in which you were calculating a polygenic risk score. Although in this example we have focused on a quantitative trait, which is height, it is important to mention that polygenic risk scores can also be used to calculate the genetic risk for a disease or a binary trait. It is important to remember that genetic material is organized on two complementary strands of DNA, which is made up by nucleotide bases. These bases are four: basically A T C G, but they are complementary to each other. That means that if one of the strands has an A, the other strand will have a T in the position that is complementary to that A. The same is true for C and G. Whenever the reference and alternative alleles of a genetic variant are not complementary to each other, we can tell which strand they came from. However, when the reference and alternative alleles are complementary to each other, it is hard to tell which of the strands we are actually measuring and therefore which is the effect allele. That can have severe consequences on polygenic risk scores. Sometimes it is possible to solve this ambiguity using information on allele frequency, but this can be tricky if the allele frequencies are close to 0.5. Now we’re going to discuss how to decide which variants to include for a PRS, and also how to account for linkage disequilibrium. I know that I previously said that we should repeat including all the other variants and sum across all loci. However, there are things to consider. The first one is that we know that there are many GWAS that are underpowered. That means that there are many more true associations that those that are reaching genome wide significance. The second one is that linkage disequilibrium creates a correlation structure within the variants and it is important to use independent SNPs for polygenic risk scores, or to account for their correlation somehow. To do this, we try to identify near independent SNPs using a method called clumping. Clumping basically selects all the SNP are significant at a certain P value threshold and forms clumps of SNPs within a certain distance to the index SNP only if they are in LD with the index SNP.\nAfter clumping, genetic variants are approximately independent, but there’s still a question of whether we [should] include only genetic variants that reach genome wide significance, or do we relax the P value threshold for including them? One solution is to calculate many PRS, including more and more variants, by becoming more lenient on the P value threshold that we use to filter them out. Here is an example of eight P value thresholds, starting with the most strict, which would be just genome wide significant variants, all the way to the most relaxed which would be including all variants. After “solving” the problem of which genetic variants to include, the polygenic risk score can proceed as we previously saw and then we will end up with a set of scores that depict the genetic liability or the genetic risk. in an independent [population] (sample) for a certain trait of interest, then we can perform PRS-trait association analysis. For this, it is important to think about your sample. If it is a family based sample like a twin registry, it is important to adjust for relatedness. If it is homogeneous in terms of ancestry, even then it is always a good idea to adjust for genetic principal components to make sure you’re getting rid of population stratification effects. It’s also important to think whether the target sample matches the GWAS [sample] ancestry, because there are known issues of portability between ancestries.\nThen you also have to consider your trait of interest. Is it continuous? Then you can use a linear regression to perform PRS-trait association. If it is binary, you can rely on logistic or probit regressions and if it is ordinal, you will have to find something like a cumulative linked model or cumulative linked mixed models for family based samples. Always remember that there are potential confounders of the trait and of the discovery GWAS and you have to think about them and adjust for them. Before performing a polygenic risk score analysis is important to keep in mind that the power of PRS depends on the power of the GWAS that will be used to estimate the PRS. In this example. The same target sample was used to calculate polygenic risk scores for depression. And they are comparing the variance explained [by] a polygenic risk score based on the first MDD PRS by the PGC and a subsequent update; and what they found is that there was a substantial increase on variance explained which was sample size dependent. The clumping and thresholding approach allows us to explore the pattern of variance explained and its relationship to the number of genetic variants that we are including. For example, here we can see that using the most strict cutoff is not having a significant variance explained, and the more variants we include, the more variance explained we are getting. This is a typical pattern [of a GWAS] of a PRS. constructed from a GWAS that was that still not fully powered upon a fully powered GWAS we expected a pattern were including just the genomewide SNPs performs really well in terms of variance explained and then when we start including more and more noisy SNPs, we are losing variance explained.\nApplications\nNow we will discuss some of the applications for polygenic risk scores. I have listed here some of them, but I think you can use your imagination and think of others. The first one is something very typical. To test for the GWAS association and quantify variance explained. It is basically a safety check in a genome wide association study where you want to demonstrate that your GWAS is really predictive of the trait of interest. Polygenic risk scores can also be used for risk stratification, which would be identifying people to test later for a specific disease. That should reduce the burden to a health service system. It can also help in clinical diagnosis of rare diseases. We can also use polygenic risk scores for testing genetic overlap between traits. For example, is a genetic risk for depression predictive of cardiovascular disease and vice versa?\nWe could also think of using PRS for trait imputation when a trait is not measured. For example, if you wanted to impute a smoking phenotype in a [population] sample. This is obviously imperfect and dependent on the heritability of the trait, but it might start gaining traction as polygenic risk scores become more and more predictive of the trait of interest. As there are many more GWASes of treatment response and they’re gaining power, personalized treatment based on polygenic risk scoring could become a reality. And basically any hypothesis where you rely on a genetic risk or a genetic liability. There’s been many publications where polygenic risk scores are used to examine gene by environment interactions.\nSoftware\nSo far we have discussed the traditional clumping and thresholding approach. However, there are other methods that are worth mentioning. But first, let me mention the software that you can use to calculate clumping and thresholding polygenic risk scores. The first one is PLINK [and PLINK2]. The second one is PRSice2, and then there is an R library called bigsnpR which contains not only clumping and thresholding, but many other options. There are other types of PRS which we will discuss briefly like LDpred2, which is implemented in bigsnpR BbaseR which is implemented in GCTB. Lasso sum and lasso sum two, which are implemented also in bigsnpR and there’s PRS-CS, and JAMPred. Which I believe are standalone software, but I’m not quite sure. All of these methods share a common motivation, which is to substitute the clumping step with something more elegant. We basically want to approximate the effect sizes that we would obtain if we would have run a multiple linear regression GWAS. That is, a GWAS that simultaneously estimated the joint effects of all the SNPs. The problem is that we cannot do that. So what we do in a GWAS is we run m regressions. And we obtain the SNP ‘marginal’ effect sizes, that is, the effect size of each SNP without taking into account the correlation with other SNPs. And the lack of adjustment for these correlation is obvious from the Manhattan plots having these well defined towers.\nTo solve this problem, we need to find a method that approximates the multiple linear regression results based on the GWAS summary statistics. There are many methods that implement estimating the multiple regression, SNP effect sizes, and we really don’t have time to cover them all in detail. So today I’m going to quickly mention some of them, and then I’m going to give a couple of details in the two that I considered the most commonly being used in the area, which is LDPred2 and SBayesR. LDpred2 is implemented in bigsnpR, and it uses a Gibbs sampler to estimate the joint SNP effects. SBayesR is implemented in GCTB and it estimates the joint SNP effects using Bayesian, multiple regression. Lasso sum and lasso sum two are also implemented in bigsnpR and they are based on performing a penalized regression that basically shrinks the SNP effect sizes. Then there’s PRS-CS, which also uses a Bayesian regression to estimate the joint SNP effects and then JAMPred, which uses a two step Bayesian regression framework. In SBayesR they combine a likelihood function that connects the joint effect sizes with the GWAS summary statistics coupled with a finite mixture of normal distribution priors Underlying the marker [variant] effects. This basically means that we are modeling the SNP effect sizes as a mixture of normal distributions with mean zero and different variances. This is typically done using four normal distributions all with mean zero and distinct variances. The first one is variance of zero, which basically captures all the SNPs with a 0 effect, and then from there we allow increasing values of effect sizes to exist in this model. What this does then is performing Markov chain Monte Carlo Gibbs sampling for the model parameters which are basically: The joint effect sizes, the probabilities of the mixture components and error terms. Of course, the parameter that is of our main interests are those joint effect sizes that then we can use [for] as effect sizes or weights in our polygenic risk score analysis.\nLDPRED II\nLDpred2 is a recent update to LDpred, which was a method that also derived an expectation of the joint effects given the ‘marginal effects’ and the correlation (LD) between SNPs. This method assumes that there is a proportion P of causal variants on that trait of interest, and then assumes that the joint effect sizes are normally distributed with mean zero and variance proportional to the heritability of the trait. Importantly, the proportion of causal variants and the heritability of the trait are estimated independently, at least in the classical approach and for P there is a grid of values that are explored, whereas for heritability or SNP based heritability, it is estimated using LD score regression. Then it uses a Bayesian Gibbs sampler to also estimate the joint effect sizes for the GWAS. However, LDpred2 adds 2 new models to the traditional LDpred approach. The first one estimates P and the heritability from the model. Instead of testing several values and using LD score regression. This is useful because before P and h2 squared were estimated through a validation data set and this new approach which is called ‘LDPred2 auto’, doesn’t require this intermediate validation data set anymore. And there is another one called LDpred2 sparse which allows for effect sizes to be exactly 0, which would be similar to the first mixture component of SBayesR. LDPred2 is also good for modeling long range linkage disequilibrium such as that that is found near the HLA region. Other methods rely on removing these regions to account for this problem. However this method (it’s authors) adequately points out that these regions are important for certain traits, and that removing them would reduce power in them.\nKey take home messages\nAs key take home messages, these approach is usually perform better than, or at least as well as, clumping and thresholding; and when they don’t, it is important to be weary as sometimes the models don’t converge and they might fail silently. Performing better PRS is still an area of active research and there is a clear battle between complexity and power versus scalability and ease of use. There’s many publications comparing across these methods, so try to read them and pick one that best fits your needs.\nSummary\nSo in summary: A polygenic risk score is a weighted sum of alleles. It’s basically a tool that estimates the genetic liability or risk to traits. It can be done for both quantitative and binary traits. Before performing PRS, it is essential to have quality controlled your GWAS (discovery) summary statistics. To have quality controlled the (target) genotype dataset and to be wary of the potentially problematic ambiguous SNPs. Furthermore, practically you find that you need to match the SNP identifiers between your GWAS data and the genotype data. Remember that discovery and target samples need to be independent and to consider statistical power before starting any analysis. When using polygenic risk scores do remember to be aware of related individuals in the sample and to properly adjust for them. As well as for population stratification. Also consider that differences in ancestry might underlie differences in predictive ability of a polygenic risk score and always be wary of jumping too fast to conclusions. Always consider potential biases in the discovery GWAS and in the target sample.\nFurther reading\nIf you’re interested, here are some further reading on polygenic risk scores. Some of these are historical papers that mark the milestones of actually achieving polygenic prediction in complex traits, and some of them are discussion on the possible biases and the different methods that exist for polygenic risk scoring. And that’s it for the intro to polygenic risk scores. Thank you and see you next time.\n\n\n\nPolygenic Scoring Methods\nTitle: Polygenic Scoring Methods: Comparison and Implementation\nPresenter(s): Oliver Pain\nhello my name is oliver payne and i’m a postdoctoral researcher working with professor catherine lewis at king’s college london today i’ll be talking about polygenic scoring methods comparing various methods to one another and also describing resources we’ve developed for the calculation of polygenic scores for research and clinical purposes i have no conflicts of interest to declare so my presentation is split into three sections first i’ll give a brief introduction to polygenic scoring then describe our work comparing different polygenic scoring methods and finally introduce an easy and reliable pipeline for polygenic scoring that we’ve developed called genopred pipe\nso first a brief introduction to polygenic scoring a polygenic score is a way of summarizing an individual’s genetic risk or propensity for a given outcome typically calculated based on an individual’s genetic variation and genome-wide association studies summary statistics referred to as g-words sum stats polygenic scores are a useful research tool and could also be useful for personalized medicine as their predictive utility increases for polygenic scoring the dual summary statistics are typically processed to identify variants overlapping with the target sample account for linkage to equilibrium between variants and adjust the g word’s effect sizes to optimize the signal to noise ratio in the geoserum statistics therefore an individual’s polygenic score can vary due to the genetic variance considered and the alloy of frequency and linkages equilibrium estimates used to adjust the dual sumstats often in research these factors vary depending on the target sample using the intersective variance between the target sample and the g was and using estimates of linkages equilibrium and alloy frequency from the target sample itself now this isn’t ideal and an alternative strategy is to use a reference standardized approach whereby a common set of variants are considered for all target samples and the linkage and algebra frequency estimates are derived using a common ancestry matched reference sample this reference standardized approach is advantageous when using polygenic scores in both clinical and research contexts as it allows polygenic scores to be calculated for a single individual and it avoids variation in an individual’s polygenic score due to target sample specific properties which might influence the result\nnow i’ll be presenting our work comparing polygenic scoring methods this table shows the leading polygenic scoring methods as reported in the literature the pt plus clump approach is the traditional approach whereby ld based clumping is used to remove the linkage to equilibrium between lead variants and g-was and a range of p-value thresholds are used to select the variants considered the others are more recently developed methods that use ld estimates to jointly model the effect of genetic variants and typically perform shrinkage to account for the different genetic architectures like the p-value thresholding approach several of the newer methods apply multiple shrinkage parameters to optimize the polygenic score when multiple parameters used a validation procedure such as 10-fold cross-validation is required to avoid overfitting whereby the variance explained estimate is artificially high due to trying many different p-value thresholds or shrinkage parameters in contrast some methods provide a pseudo-validation approach whereby the optimal parameter is estimated based on the g-word summary statistics alone not requiring a validation sample another approach that doesn’t require a validation sample is to assume an infinitesimal genetic architecture though this approach works best for highly polygenic phenotypes a third option is to model multiple polygenic scores based on a range of parameters using methods such as elastic net to account for the correlation between the polygenic scores\nwe compared methods using a range of outcomes measured in two target samples uk biobank and the twins early development study referred to as teds we used two samples to ensure our results not target sample specific and we selected the traits on the right as they have a well-powered publicly available g was and represent a range of genetic architectures as i described previously we used the reference standardized approach when calculating the polygenic scores ld and allele frequencies were estimated using two reference samples of differing sample size to evaluate the importance of reference sample size across methods and these reference samples were the european subsets of 1000 genomes phase 3 and an independent random subset of 10 000 european uk biobank participants hatmap three variants we used the route as these variants are typically well captured by genome-wide arrays after imputation provide good coverage of the genome and also reduce the computation times several methods already provide ld matrices including only hatmap 3 variants for use with their software in line with the reference standardized approach the predictive utility of each polygenic score approach was evaluated based on the pearson correlation between the observed outcome and predicted values the statistical significance of differences between predicted and observed correlations was determined using the williams test which counts for the correlation between predictions from each method this figure shows the average performance of each method compared to the best pt plus clump polygenic score as identified using 10-fold cross-validation the transparent points in the figure show the results for each target phenotype separately i’m only showing the results based on the uk biobank target sample when using the thousand genomes reference as the results were highly concordant when using teds or the larger reference when comparing methods that you use 10-fold cross-validation to optimize parameters shown in red you can see that the best methods are ld probe 2 lasso sum and pure scs all outperforming the pt plus clump approach providing a 16 to 18 relative improvement in prediction ld pro 2 showed further nominally significant improvements over the su-sum and pure scs on average when comparing methods that use a pseudo-validation approach or infinitesimal model not requiring a validation sample highlighted in blue and purple you can see that pscs and dbs lmm methods perform well providing at least a five percent relative improvement over the other pseudo-validation methods pure scs is better than dbs lmm providing a four percent relative improvement over dbs lmm it’s worth mentioning that especially’s\nperformance improved when using a larger ld reference on average then doing as well as dvs lmm when comparing the pseudo-validated pscs performance to 10-fold cross-validation results in red the prcs polygenic score performs only three percent worse than the best polygenic score identified by 10-fold cross-validation for any other method and performs better than the pt plus comp approach for all phenotypes tested highlighting the reliability of this approach the multi-prs approach shown in green which uses an elastic net model to model to model multiple polygenic scores based on a range of p-value thresholds or shrinkage parameters consistently outperforms the single best polygenic score selected by tenfold cross foundation shown in red with the largest improvement for the pt plus clump approach of 13\nlastly we tested whether fitting polygenic scores across multiple methods improved prediction and we found it did to a small degree though the\ncomputational burden is obviously substantial because then you have to run all of the methods in terms of runtime these methods very substantially this graph shows the number of minutes to run the method on chromosome 22 alone without any parallel computing you can see the prcs and ld pro 2 take a lot longer than other methods however since our study ld pro 2 developers have substantially improved the efficiency of their software halving the runtime moving that down to around here just under 25 minutes for chromosome 22. it’s worth noting that the time taken for pure scs when using a single shrinkage parameter is one-fifth of the time shown here meaning its pseudo-validation approach is actually reasonably quick dbs lmm is by far the fastest of the newer methods taking just a few minutes when run genome-wide without any parallel computing which is impressive considering how well it performed against other methods in terms of prediction something i wanted to highlight is that when using sbaza i’ve been using the robust parameterization option available in the newest version of esbazar as i found sbasal was not converging properly for some g was using this robust parameterization was most reliable and did not make the esbaza performance worse except for depression using the smaller 1000 genomes european reference you can see this here in the figure the second point is that since our published study i’ve also compared prcs methods using geosummary statistics for 17 different phenotypes derived using only the uk biobank sample avoiding possible quality control issues that occur from large scale meta analyses the results are highly concordant to the results when using the largest publicly available meta was results which is reassuring that our findings are are reliable however the performance of essbase r did improve again highlighting that especially is more sensitive to g1’s quality than other methods but when the quality is good esposa performs very well appears to be the best method in this analysis we were also interested to see whether one particular method did better when predicting across ancestries as some methods might highlight causal genetic effects better than others using the same 17 g was i briefly mentioned on the previous slide within the uk biobank sample alone i’ve tested the performance of prs methods across populations\nso using the european g-wars but predicting in non-european subsets of uk biobank as you can see the results are very similar in each population with the best method identified in a european target sample also being the best method in a non-european target sample\nso the advice for the future research regarding polygenic score methods is for optimal prediction we recommend modeling multiple parameters from ld pred 2 lasso sum or pure scs but if you’re looking for a simpler option for looking at genetic overlap perhaps i’d recommend using the pure scs pseudo-validation method also referred to as its fully bayesian method alternatively if you need to compute polygenic scores for many g-words or have limited time or computer power then dbs lmm is a fast and good alternative although especially does very well when the g was summary statistic’s quality is good its sensitivity to that quality means it doesn’t always perform well when using the largest meta g was results\nokay so now i’m going to move on to the last section of my talk which describes our recently developed pipeline for polygenic scoring called genome pred pipe so most of the work i’ve just presented is contained in the study shown in the top left where we also described the reference standardized approach polygenic scoring in the study we provide a schematic representation of this reference standardized approach shown in the figure on the right whereby target genetic data is first imputed if it hasn’t been already then only happened three variants are retained as these are typically well imputed and provide decent coverage of the genome and then the sample is split into ancestral superpopulations determined by comparison to the thousand genomes phase three reference and lastly polygenic scores are then calculated based on jewish summary statistics processed in advance using a g was ancestry matched reference when we were carrying out this study and writing the code for the reference standardized approach we wanted to ensure the results were fully reproducible and we wanted to develop a resource that other researchers could use to calculate reference standardized polygenic scores so we used a combination of our markdown git github and github pages to create a publicly available website containing a series of readable documents describing the code and results of our various studies\nthe website is called genopred as it focuses on genotype-based prediction and there is a qr code here if you would like to take a look i’ll now briefly show you the website so this is the home page of the website which shows a list of the pages available i’ll now click on the link describing how to prepare the reference data for the reference standardized approach at the top it provides some information and then lists the required software below that and then it goes through each step uh one by one with code chunks for users to use to reproduce the results or create their own data now whilst we think this is a great resource for others to see what we’ve done and replicate the results you can see there are many steps and many separate code chunks to follow making its use quite lengthy and possibly prone to user error so i’ve recently written all of the steps to calculate reference standardized polygenic scores into a pipeline using snakebig along with what is called a condor environment which will automatically download and install all of the required software meaning it’ll create fully reproducible results the pipeline just requires three input files first a list of g was summary statistics that you want to use indicating the population in which they were derived and optionally information regarding the distribution of phenotype in the general population i.e prevalence or mean and standard deviation then you provide a list of target samples you want to polygenic scores four with the pipeline currently accepting pink one binaries so bed bim fam or b gen files and also the 23andme format lastly you’ll need a file called config.yaml which describes the location of the gwas\nand target list files then actually running the whole pipeline just takes a single line of code i provided some test data for new users to experiment with which can be downloaded and then step two here is where the pipeline is actually run just writing snake make then two parameters indicating the computational resources you want to use and that you want to use condor environments and then the name of the output that you want in this example i’m requesting the final output which is an r markdown report of the ancestry identification and polygenic score results for all target samples which involves running the full pipeline\nhere is an updated version of the schematic i showed you earlier based on what is implemented in the geno pred pipeline the only difference is that at the end now we generate a report summarizing the results of the ancestry identification and polygenic scoring i’ll show you these reports briefly now this is the report produced to describe the results of a single individual first there are some descriptives about the number of snips before and after invitation and the number of reference snips available then the report describes the results of the ancestry identification analysis first showing the probability of being from each suit population in 1000 genomes reference in this example the individual has a probability of almost 100 of being from a european population you can also see the individual’s position on the first few reference projected principal components relative to the thousand genomes population i’m showing now and the individual individual in this case is that black circle then the individual’s ancestry is broken down into more specific populations within the assigned superpopulation in this case showing the individual was most similar to the great british population and individuals broadly from northern and western europe and at the bottom shows the individual’s polygenic scores in this case for body mass index and cardiovascular disease showing the results in relative terms compared to an ancestry matched reference and then in absolute terms for improved interpretation by considering the variance explained by the polygenic scores and the distribution of the phenotype in the general population then this is an example of the report produced to summarize the results for a sample of individuals again starting with some descriptives about the overlap with the reference snips then showing the number of individuals assigned to each superpopulation using a hardcore threshold of 50 with the underlying probabilities shown as a histogram below again you can see the position of these individuals on projected principal components compared to the thousand units reference and then each individual is assigned to specific populations within each assigned super population i’ll skip past these though and show you the polygenic scores which is summarized at the bottom just simply using histograms to show the distribution of polygenic scores for each population and g was\nso i’ll conclude with why i think people should use this pipeline first it performs ancestry classification which is really important as non-europeans are often discarded from studies and as sample sizes increase usable non-european populations can be identified and analyzed also it’s important to consider the ancestor of an individual when calculating the polygenic score second it provides reference standardized polygenic scores which are scaled according to an ancestry matched reference and are independent of any target sample specific properties which is useful for research and clinical prediction purposes third it can efficiently implement any of the top performing polygenic scoring methods using a single line of code saving time and reducing user error fourth it’s been tried and tested i’ve put uk biobank through it and other samples and compared the prs to the observed phenotypes assuring me that it’s working as it should finally it’s well documented online and produces fully reproducible results both of which are important for progressing science i’m showing the qr code again for the juno pro website on the right please do check it out if you’re interested lastly i’d like to thank my amazing colleagues for their help with this work in particular catherine for her brilliant supervision throughout thank you for listening\n\n\n\nPolygenic risk scores: PGS comparison\nTitle: Polygenic risk scores: PGS comparison\nPresenter(s): Guiyan Ni\nwelcome the topic of this video is how\nto run polygenic risk score comparison\nuntil now we already have watched the\nindividual talk on prscs LD Pro 2 and\nspsr and other methods I believe you all\nhave a good understanding about\npolygenic score and each of those\nmethods\nso this slides here is just to set up\nthe scene to make sure that we are on\nthe same page\ncollagen risk score of an individual is\na weighted sum of the counties of risk\nallele So based on the jivas summer\nstatistical results the basic method for\npolygen resource score is pwalu clamping\non the thresholding\nthis method is simple but it doesn’t\nfirmly model different genetic\narchitecture so there are many new\nmethods try to model the genetic\narchitecture for the trade of Interest\nfor example using different paraders in\nthe base in regression like our LD\nprinting infinitesimal model LD Pro 2 SP\nSBC\nprscs and acid base arm\nand also matter like La Susan is using\nthe lasso regression\na mega PRS is another method is runs on\ndifferent prayers for example if it runs\non a probably using a mixture of four\nnormal distribution it will be the same\nlike a space r\nor similar\nand if a zoom order snip have a\ncontribution to the phenotype variance\nthen it will be similar to LD product\ninfinitesimal model or S block it can\nalso run a prayer like both a similar\nlike both LM and also can run also some\nregression\nso the difference between Mega PRS and\nother method is the expected first name\nheritability can vary between\num can vary by LD and manual frequency\nso in this talk we will compare all\nthose message and we know that when the\nmethod is proposed they already compared\nwith other methods but the fundamental\nquestion we are trying to answer here is\nwhich method should we use in the PGC\ndata\nthen we use the course validation we\nwant a cohort out of course validation\nto\num to answer this question and to\ncompare the performance of different\num methods here’s the toy example\nshowing uh for the course of validation\neach cell here is one cohort and the\npink cell is for the discovery cohort\nand the target cell is as green cell is\nfor the Target cohort\nand in first uh first round of the\nanalysis we’re using the four pinks\ndiscover cohort as a discovery says and\nthen while it is a performance of each\nmethod in in the Target\nsample and then we repeat this process\ninto each of those cells or each of\nthose cohorts serve as a Target cohort\nif it’s needed by the by the um some\nmethod we have another cohort will serve\nas a tuning sample to select the optimal\nhyper parameters\nso in the real data analysis we use the\nGeo summer statistic from schizophrenia\n2 as a discover sample and all those\ndata we actually have access to uh\nthrough a service through cohort where\nthe individual level genotype is\navailable and we use each of them as a\nTarget sample\nfor the tuning cohort we use uh these\nfour cohorting term to tune the hyper\nparameters\nand then we can predict the polygenic\nscore\ninto each of the targets sample\nhere we used a search statistic to\nmeasure the performance of different\nmethods one is AUC another one is\nproportion explaining\nin the liability scale and third one is\nall the ratio\nI will go through each of them to show\nhow to calculate each of those\nstatistics\nso first start with AUC here is a toy\nexample\non how to calculate AUC back hand\nso AUC is actually a short for the area\nunder the RC curve which is shaded by\nthe Ping here so the ROC curve is made\nby plotting\num\nagainst the true positive rate to the\nfalse positive rates at each possible\ncut off\nso what that mean\nit means as\nassume that\num this is the density of plot for the\npolygenic score in the control sample\nand here is for the case samples\nand this vertical line is the current\ncutoff\nand in this case this graph can be\ndivided into four groups and true\nnegative\nfalse negative\ntrue positive\nand a false positive and true positive\nand then we can calculate the proportion\nof each each group and then we can\ncalculate the true positive rates and\nthe false positive positive rate\nwhich other coordinates used in the RC\ncurve so in the in the current cutoff we\nuse here it means that we have roughly\nabout 17 percent of cases the the uh\ncorrectly classified as case\nand then there are about 10 percent of\ncontrol they are running classified as\ncase\nand which give us the coordinates\nfor this dot here\nand then we’ll vary the this vertical\nline This cut off we will uh we will get\nthis Roc curve as shown in this slides\nhere\nand this you see the first statistic we\nuse to measure the performance of\ndifferent methods\nand the second one is variance explained\nin the liability scale when using a\ncertain case control studies\nso this variance is a function of\nvariance explaining the observed scale\nthis r squared observed\ncase control study and another two\nparameters C and Theta\nso the variance explained on The\nobserved scale is actually a function of\ntwo likelihoods from the new model and\nthe phone full mode which is designed\nin this two equation\nand this parameter C is a function of k\nz and P and this K parameter is actually\nthe proportion of the population that\nare diseased\nis also means the prevalence of the\ndisease and Z parameter is a density at\nthis threshold T here and this curve is\na standard normal distribution\nand the p is a proportional case in your\nG was a result or in your case control\nstudy\nand Theta parameter is a function of the\nsame\nkzt and threshold T but with different\ncombination\nso in this slides I just give the final\nresult of how to calculate the variance\nexplained in the liability scale the\nfull Direction with of this equation can\nbe found in this reference\nforeign\nstatistic is called Oz ratio\nand also ratio is a ratio between two OS\nand the OS is a probability being a case\nover the probability being a control\nso here is a toy example showing how to\ncalculate all the visual backhand\nand that\nsaying that we are ordering the\nindividual based on their polygen risk\nscore from a lowest to highest and we\nare interested in the observation\nbetween the uh 10 stairs and\num first day so with a number of cases\nand the controls show in this table so\nthe always being a case in the first day\nso is 23 over 103 and us being a case in\nthe 10 states of is 83 divided by the\n43. the old ratio of between the two\ndecimals is 9.3\nso this this value means um when we\norder individual based on their\npolygender score\nthe individual in the top 10 days in the\ntop 10 percent or in the 10 states or\nhave 9.3 Times Higher of us being a case\ncompared to the individual in the bottom\n10 percent\nand this this old version can be easily\nestimated from the logistical version\nusing the logic link function\nso using the U1 cohort strategy we can\naccess the AUC variance plan and also\nratio for each of those Target cohort\nand here show the result for AUC and for\ncohort of each method and different\ncolors here stand for different method\nwe used and the y-axis here is a UC\ndifference compared to the P plus T\nwhich is a benchmark we used\nand as you can see of course different\nvalidation cohorts there are lots of\nvariations\nand\nthat’s why we think our comparison is\nmore robust compared to other comparison\nwhen they are only use one target cohort\nif we summarize these uh bar products\nby each group by method\nwe can see we can observe this bar plot\nthe y-axis here is AUC and\neach of the group stands for each of the\nmethod we compared and each of the bar\nin each of the group stands for\ndifferent tuning cohort we use\nand we noticed that the methods that\nhave formally modeled different genetic\narchitecture they actually have quite\nsimilar performance this is because the\ngenetic architecture of psychiatric\ndisorders they are quite polygenic\nif we look at the results for the\nAlzheimer’s disease Which is less\npolygenic compared to psychiatric\ndisorder we will observe a big\ndifference across different methods\nand then we also observed the similar\npattern for our various explained in the\nlaminate scale and also ratio between\nthe top 10 percent and bottom 10 also\nthe alteration between top 10 medium\nbut uh we observed that LD player 2 as\nbase R and mega PRS they rank the\nhighest amount in most of the comparison\nand to summarize in this talk I show how\nto calculate AUC various explaining\nliability skill and also ratio backhand\nand based on the comparison we made we\nobserved that\nfor security disorders which are very\npolygenic all the methods are perform\nsimilar but some are rank higher than\nothers for example I would refer to as\nbase R and mega PRS\nso the result actually here is part of\nthis study which is published recently\npublished and in this paper we also did\nthe comparison for major depression and\nalso other sensitivity analysis and we\nalso provide the code for each to run\neach other method and for each\ncomparison and also each of the\nstatistics\nstatistic use for comparison\nand with this I would like to give a big\nthank you to Professor Norman Ray who\nalways give me a huge support of\nwhatever I needed and thanks to all\nother pccg members\nand thank you all\nforeign"
  },
  {
    "objectID": "software_prs.html#prs-csx-to-perform-prs-analysis",
    "href": "software_prs.html#prs-csx-to-perform-prs-analysis",
    "title": "PRS",
    "section": "PRS-CSx to perform PRS analysis",
    "text": "PRS-CSx to perform PRS analysis\nTitle: Hands-on tutorial of using PRS-CSx to perform multi-ancestry PRS analysis\nDescription:\nPresenter(s): Tian Ge, Yunfeng Ruan, Stanley Center, Broad Institute\nLevel: Intermediate\nLength: 8:28\n\n\n\n\n\n\n\n\n\nLink to video transcript here."
  },
  {
    "objectID": "software_prs.html#prs-in-ancestrally-diverse-populations",
    "href": "software_prs.html#prs-in-ancestrally-diverse-populations",
    "title": "PRS",
    "section": "PRS in Ancestrally-diverse Populations",
    "text": "PRS in Ancestrally-diverse Populations\nTitle: PRS-CSx: Improving Cross-Population Polygenic Prediction using Coupled Continuous Shrinkage Priors\nPresenter(s): Tian Ge\nLevel: Intermediate\nLength: 56:46\n\n\n\n\n\n\n\n\n\nLink to video transcript here."
  },
  {
    "objectID": "chapter2.2_transcript.html",
    "href": "chapter2.2_transcript.html",
    "title": "Chapter 2.2: Genetic Variation (Video Transcript)",
    "section": "",
    "text": "The differences in our genes come about through a natural process called mutation. Mutation is extremely common. As a cell copies its DNA before dividing, it makes occasional typos. Most commonly, a single base is substituted for another.Sometimes a base is deleted or an extra base is added. The cell is able to repair most of these typos. But each round of cell division introduces a few changes that are not repaired. When DNA changes occur in cells that will give rise to eggs or sperm, they can be passed down to offspring. Each one of us has about 60 new variations that were not present in our parents.\nPeople commonly use the terms mutant and mutation to describe something undesirable or broken. But mutation is not always bad. Most DNA changes fall in the large areas of the genome that sit between genes and they usually have no affect. When variations occur in areas of the genome that code for proteins, they often have consequences. These changes can effect the protein product itself or they can effect when, where, or how much protein is made.\nMost of the time, mutation generates variations that are neither good nor bad, just different. While a mutation is defined as any alteration in the DNA sequence, biologists use the term \"single nucleotide polymorphism\" (SNP) to refer to a single base pair alteration that is common in the population. Specifically, a polymorphism is any genetic location at which at least two different sequences are found, with each  sequence present in at least 1% of the population. Note that the term \"polymorphism\" is generally used to refer to a normal variation, or one that does not directly cause disease. Moreover, the cutoff of at least 1% prevalence for a variation to be classified as a polymorphism is somewhat arbitrary; if the frequency is lower than this, the allele is typically regarded as a mutation.\nSNPs are important as markers, or signposts, for scientists to use when they look at populations of organisms in an attempt to find genetic changes that predispose individuals to certain traits, including disease. On average, SNPs are found every 1,000–2,000 nucleotides in the human genome, and scientists participating in the International HapMap Consortium have mapped millions of these alterations.\nThe DNA in any cell can be altered through environmental exposure to certain chemicals, ultraviolet radiation, other genetic insults, or even errors that occur during the process of replication. Sickle-cell anemia is a disease caused by a change in a single nucleotide, and it represents just one class of mutations called point mutations. The alteration of a single nucleotide in the gene for the beta chain of the hemoglobin protein, the oxygen-carrying protein that makes blood red, is all it takes to turn a normal hemoglobin gene into a sickle-cell hemoglobin gene. This single nucleotide change alters only one amino acid in the protein chain, but the results are devastating. Molecules of sickle-cell hemoglobin stick to one another, forming rigid rods. These rods cause a person’s red blood cells to take on a deformed, sickle-like shape, thus giving the disease its name. The rigid, misshapen blood cells do not carry oxygen well, and they also tend to clog capillaries, causing an affected person’s blood supply to be cut off to various tissues, including the brain and the heart. Therefore, when an afflicted individual exerts himself or herself even slightly, he or she often experiences terrible pain, and he or she might even undergo heart attack or stroke - all because of a single nucleotide mutation. \nMutations can also alter the way a gene is read through either the insertion or the deletion of a single base. In these so-called frameshift mutations, entire proteins are altered as a result of the deletion or insertion. This occurs because nucleotides are read by ribosomes in groups of three, called codons. Thus, if the number of bases removed or inserted from a gene is not a multiple of three, the reading frame for the rest of the protein is thrown off. \nChanges in the DNA sequence can also occur at the level of the chromosome, in which large segments of  chromosomes are altered. In this case, fragments of chromosomes can be deleted, duplicated, inverted, translocated to different chromosomes, or otherwise rearranged, resulting in changes such as modification of gene dosage, the complete absence of genes, or the alteration of gene sequence. The type of variation that occurs when entire areas of chromosomes are duplicated or lost, called copy number variation (CNV), has especially important implications for human disease and evolution."
  },
  {
    "objectID": "chapter3.1_transcript.html",
    "href": "chapter3.1_transcript.html",
    "title": "Chapter 3.1: SNP array genotyping (Video Transcript)",
    "section": "",
    "text": "Title: SNP Chips (Introduction to genomics theory)\nPresenter(s): Gábor Mészáros, Genomics Boot Camp\nIntroduction\nHi everyone. Welcome back to the introduction to genomics lecture series. We continue now with the second part, and we will talk about SNP chips. Before we do so, we do a bit of a refreshment from previous lectures. So we talked about the DNA and its structure that is based on certain building blocks, and the set of rules of how these building blocks connect. We also established that if we cannot look at everything at once, molecular markers are good surrogates, because we actually know their genotype and their exact position on the genome. And also, they are connected to genomic regions of interest, for example genes, that influence the traits that we are actually interested in. There are multiple possibilities for genomic markers, but the most widespread and most widely-used ones are the so-called “SNP markers”.\nSNP chips\nSo a bit of refreshment also on this. So the SNP markers are the single nucleotide polymorphism markers that are single base pair positions that are different between the genomes of two individuals. So, here we have Individual 1 and Individual 2, and we compare their sequence, and we will find that most of the sequence is totally identical all of the time, except some variants. And one of these types of variants are the SNP markers that are single base pair mutations. And the good thing with these SNP markers is that there are many of them throughout the genome, so we can cover the entire genome with these SNP markers and use them to our advantage. Now, there are really really a lot of these SNP markers, in the millions, and not all of them are consistently appearing within populations. So what we really want to do actually is identify just those SNP makresr that are consistently appearing, so we can genotype them all the time and analyze these consistent data from many individuals. If we have really a standardized, consistent set of SNPs, we can genotype these ones in a straightforward manner, and also in a cost-effective way. This cost-effective way is genotyping these standardized set of SNPs with the so-called SNP chips. These SNP chips have multiple names, or you can find multiple expressions for it, so the beadchip, beadarray, SNPchip, microarray - all of these are basically meaning the same thing.\nSo this is how the SNP chip looks like. As I mentioned they have multiple names, but one thing is common that the SNPs that are selected on them are biallelic by design. For example, there is allele A and allele B, so we have three possible genotypes: homozygous AA, homozygous BB, and heterozygous AB. If we look at these SNP chips from a very close perspective, we will find that on these SNP chips there are hundreds of thousands of tiny wells as shown on the right side of the screen. So we have these tiny wells and then in these wells there are these beads, and therefore the name beadchip or beadarray. What it is actually doing, that each of these wells and beads is coated with multiple copies of oligonucleotide probes targeting a very specific locus on the genome. Therefore, each of these wells, and each of these beads, is designed to capture a very specific SNP for the particular species for which the SNPchip is developed.\nNow how does it work? So obviously we need DNA that we want to genotype and these DNA fragments pass over the beadchip. Each probe binds to a complementary sequence in the DNA and stopping one base before the locus of interest. After that, they come single base extensions that incorporate one of the four labeled nucleotides. Now these nucleotides are very special because when they are excited by a laser, so when the laser shines on them or points on them, they emit a specific signal, and the intensity of the signal actually conveys information about the genotype on that particular locus or in that particular bead. So this is actually shown on the picture on the left side here. So here we see the wells, and also the beads, here is the sequence, and then at the end of each sequence there is the labeled nucleotide. And this will be three SNPs here with the RS code and for each of these beads there is a certain genotype that emits a certan signal. So if there is a homozygous one it emits one signal very strongly and not the other one. Similarly, for a different locus there is a different homozygous genotype so it, again, emits a different signal, but again, just on the one side, and if there are some heterozygous genotype there is the signal intensity somehow in between the extremes.\nNow this is how the SNP chips look closer to reality. so basically we have these lanes here, and you see that there is a tiny fraction of the lane is magnified and you see here these tiny dots that are each of them here is a well and the bead that it is emitting some kind of signal. of course these signals do not tell us anything just by looking at them, and they need to be analyzed in a very specific way, so that we know what is the exact meaning of the signal at each of these dots. This analysis is done by a specific genotyping cluster, so there is an algorithm in place that automatically clusters the samples into two homozygous and one heterozygous group. So there are circles around each cluster where the genotypes should fall, and also there are wider kind of shaded areas where we still accept the genotype calls, and then SNPs that are falling outside even these shaded areas are the ones that are not given a genotype. This is what i had in mind. So this is such a graph for a single SNP. Each dot here is an individual genotype for that particular SNP, and here are the circles. So this would be one homozygous, other homozygous, and in between them are the heterozygous genotypes. Whatever is falling into these circles is fine, so this is called as such genotype, and also you see these wider shaded areas, they are still OK, so the individual falls into this area is still called, for example here, as heterozygous. There are some individuals that are outside of these areas, for example this one, and this one, in this case, the algorithm is not certain about the actual genotype call, and this is how we get these so called “missing” SNPs or “missing” calls into our data. So just something went wrong and rather than giving a very inaccurate result, the genotyping algorithm determines that rather, it would not call this SNP, and put it as a missing one.\nOn a SNPchip we have pre-selected SNPs, so we have SNPs that are working very well as, in this case, so we can clearly determine the homozygous, heterozygous, and other homozygous genotypes. I show this example of a so-called bad SNP also just for comparison. So there are also cases like this. Again what we here we have circles, the homozygous, other homozygous, and heterozygous, but you see here that this is somewhat problematic. Here, some the of genotype calls are really really close to each another or even overlapping, so if an individual falls somewhere here, for example, its not really safe to determine if it is heterozygous or homozygous. So there are also SNPs like this, they are generally problematic, but they do not appear on the SNP chips, because, actually, on the SNP chips we will talk about and we will analyze, they are basically these sets of pre-selected well-working SNPs.\nAfter these genotyping process is done, then basically everything gets transferred to a text file that is called the final report. Now, I made a few videos already about these final reports, and you can find them on the channel. But the short story is that everything from the genotyping routine is saved in this final report, which is basically a large text file, and part of this final report are also the genotypes, and these genotypes then can be transferred to other file formats, for example standard PLINK files, and then these files and these data could be analyzed with either PLINK or various other software as you see also a bunch of examples of this on this channel.\nNotes on data handling\nThis series of videos is supposed to be more on the theory side, so I don’t want to spend too much time on this right now. If you are interested in the practical applications there are lots of other videos on the channel, but still I would mention that this is how the data then looks like. So here each line is one individual, and here we have the actual genotypes. And, of course, we know also the locations of these SNPs, we know which chromosome they are on, which exact base pair position they are on, and what is their name, so we can actually conduct routine analysis of various kinds. And afterwards, when we have our data, we can transform these data, by using appropriate methodologies with some kind of signals, and these signals might reveal something about the organisms, or the individuals, or populations we are interested in.\nNow, when it comes to handling of the genomic data we do not rely entirely on our knowledge of biology, because we are talking actually about large data sets and these large datasets are handled exclusively with computers and various softwawre, then i dare to say that some kind of or some degree of knowledge of computers or information technologies is also really really useful if you want to do serious research in this area. I’m not saying you need to be a hardware or software expert, but you need still you need to know the basic jargon, and know your way around computers and also computers servers. It is really useful to have this kind of knowledge in the long run. When it comes to genomics, in our daily work we deal a lot with software because, as I mentioned, its really not possible to analyze this type of data by hand. While the programming skills are useful, well i say here essential, maybe i would rephrase that in a way that , yeah its really useful, and maybe not programming but scripting. if you’re really serious about this kind of work, or type of work, you really need to know some kind of a scripting language, and you need to be able to write some really basic scripts that tailor the data as you want, or modify the data in a way you want, or you be able to run software that you didn’t really use before, all kind of things. So you need to have some kind of knowledge of the computers and how to use them.\nDepending on what you do you can rely on your own scripts, but there are also a ton of established programs that do all kinds of things. So, especially at the beginning, there’s really nothing wrong with relying on these established programs or packages that do the stuff that you want. For any given methodology or approach there is a large number of approaches and software solutions, so I would really encourage you to look around and see which ones fit your needs the best way. But at the end, we will all come back to the same thing, so we will come back to large text files that will have SNP genotypes in them, which can be homozygous, other homozygous, or heterozygous. So this is a different kind of graph, don’t worry about that, but basically what we are after are these SNP chips and SNP genotypes in a text format that we need to analyze.\nAllele and genotype codes\nIn these large text files with the genotype data, you might find alleles and genotypes in different types of coding and these different coding types I want to detail in this slide. One of the most common ones is of course nucleotide coding. So we know that the DNA consists of four nucleotides: guanine, cytosine, adenine, and thymine. And these are also the abbreviations G, C, A, T for this type of coding. Now, you might notice that there is in brackets here is a TOP format coding, because in the SNP chips for some reason there are 2 types of nucleotide coding, usually that is called TOP and FORWARD. Actually both of them are nucleotide coding, so you would see the same type of codes, but genotypes for the same SNPs could be denoted a bit differently when it comes to TOP and FORWARD coding. If you analyze a single population this is not a problem, so you actually don’t need to care too much which coding it is. This question, or the question of TOP and FORWARD allele codes, come into play mostly when you want to merge datasets. Again, there is a video on data merging on this channel, so if you are really interested in that, I would just encourage you to look up that video. But for right now, just information that there is nucleotide coding and there could be TOP and FORWARD coding on the SNP chips.\nNow I mention that each SNP chip is biallelic, meaning that there are exactly two alleles possible for each SNP. So you can actually simplify that, so actually you don’t need four letters, or 4 possibilities, because each of the SNPs is only biallelic, so you can actually recode or rename one allele as A and the other allele as B. So there is another type of coding, character allele codes, with so called AB coding. Also, sometimes you need to use programs, or software, or approaches, or otherwise its somehow beneficial to store the alleles codes not as characters, but as numbers. In this case, very often the numbers that are used for this purpose is 1 and 2. “1” is one of the alleles on the SNP, and the number “2” is the other allele on the SNP. Sometimes also you can find or come across numeric allele coding that uses “0” for one allele and “1” for the other allele. So in all cases, after you get the genotype file, you look it up, what is the coding style that is used, and also you need to ensure that you know what these allele codes mean, or what are the actual allele codes that are used in your particular case, because it could be different, and there is not one single rule or one single scheme that is used all the time. So there are some schemes that are used more often, but of course this doesn’t guarantee that the file that you have uses this particular allele or genotype coding conventions.\nAlso, while I mentioned all these allele codes, but there are also what i mentioned before are the missing alleles. These are often coded with “0”. Of course if the numeric coding is 0/1, then its coded something else, but most of the time, or many times, the missing alleles are coded “0” or something else. Also this is the other thing you need to check is that first what is the codes that is used for alleles, and the second thing is what are the codes that are used for missing genotypes. For example, for the final report it is customary, or I very often come across coding for the missing allele as a “-” or a minus sign.\nNow in the previous slide I mentioned allele codes, but again, I underline that the SNPs are biallelic, meaning that they are two alleles that make up a certain genotype, and this could be homozygous one, homozygous other, and the heterozygous. Again, depending on the allele coding type, there could be different codes for the genotypes. So this would be an example of a nucleotide coding. This would be the example of the AB coding, so AA, AB, and BB. In case of numeric coding when the allele codes are 1 and 2, then these are the numeric coding genotype codes. Here, I would underline that this is not pronounced “eleven”, “twelve”, and “twenty-two”, but actually we refer to these genotypes as “one-one”, “one-two”, and “two-two”. And there is also a different type of genotype coding when you use just one number for each genotype, and this is customary to have it as 0, 1, and 2. And these numbers are used, so the 0, 1, and 2, because this is actually the numbers of the so called “2” alleles. So the “0” is used for the genotype 1/1 because there are zero “2” alleles, the heterozygous is often denoted as “1” because it’s from 1/2, so there is just one “2” allele here, and the 2/2 is denoted by “2”, because there are two “2” alleles here. And obviously, if this type of genotype coding is used, the code for the missing genotype must be something else than zero because zero is already used for one of the homozygous genotypes.\nSNP chip types\nSo the SNP chips are specific for each species, and here I show possibilities of SNP chip types in cattle. I mention cattle as the first species because, well, I work mostly with livestock, and cattle are the most widely genotyped among livestock species. And because of this, it has also a lot of possibilities in terms of chip types. So what we have most commonly, or most often, is the so-called mid-density SNP chip. Funnily enough, it has about 54,000 SNPs, but it is still being referred to as 50K or mid-density, but anyways there is this chip that is very often used for many purposes, from population genetics, to genomic selection. there are also SNP chips that have a higher or lower density depending on what you want to use it for. so the high density SNP chip has around 800K SNPs and the low density around 7K, but this could also be different ones, I just really put it out as an example. also there are custom SNP chips that might have, for example any of these ones as a base, and adding additional, special SNPs that the people, or researchers, the breeding organizations, are very specifically interested in. This is just a quick comparison of the 50k and the HD SNP chips in cattle. So you see that each of the coloured dots here is a SNP on all of the chromosomes in cattle, and you see that the entire genome is covered. Of course, we have much more SNPs in the HD so it is much more covered, so the inter-SNP distances are much shorter, but all-in-all, both SNP chips do the job, and they are covering the entire genome, so we can use these data to conduct various types of analyses.\nOf course, SNP chips exist for a wide variety of other species, and here I just mention some of these species, and some of these chip types. So there is a lot more on the market, but for you, just to have an idea. So I mention a few of these. So there is a human SNP chip with around 900,000 SNPs, in horses, ovine, porcine, companion animals, for example dogs, but also there are cats, and birds, and all these kinds of stuff, and also there are for mice, for all kinds of research experiments, also in plants, wheat being one of the major crops, and i just included strawberry because i just found it funny that also for strawberry there is SNP chips already existing, well i just wasn’t expecting it to"
  },
  {
    "objectID": "chapter8.6_transcript.html#introduction-to-expression-quantitative-trait-loci",
    "href": "chapter8.6_transcript.html#introduction-to-expression-quantitative-trait-loci",
    "title": "Chapter 8.6: Quantitative Trait Loci (Video Transcript)",
    "section": "Introduction to expression quantitative trait loci",
    "text": "Introduction to expression quantitative trait loci\n\nMPG Primer\nTItle: MPG Primer: Introduction to expression quantitative trait loci (2021).\nDescription: This video introduces expression quantitative trait loci (eQTL) analysis, discusses how to address batch effects and covariate correction, and control for false discovery rate.\nPresenter(s): Francois Auget - Broad Institute\n\n\neQTLs, genes, and molecular networks"
  },
  {
    "objectID": "chapter9.5_transcript.html",
    "href": "chapter9.5_transcript.html",
    "title": "Chapter 9.5: Family-based analysis (Video Transcript)",
    "section": "",
    "text": "Title: Univariate/MonoPhenotype Twin Modeling in OpenMx\nDescription:\nPresenter(s): Hermine H. Maes\nto help you understand how to go about modeling twin data in terms of estimating sources of variance in a phenotype of interest. Now this presentation, as well as various of the slides wouldn’t have been possible without help from a variety of my colleagues, including Drs Nick Martin, Elizabeth Prom-Wormley, Lindon Eaves, Tim Bates, Mike Neale, and many others. You can find the files as well as the code with the openMx scripts on this website here that you can freely access and we’ll talk more about how it’s organized and how to get to the various bits and pieces later on in this talk.\nQuestion\nSo what are we going to address? Question is, does the trait of interest run in families and if so, can this familial resemblance be explained by genetic or by environmental effects, or both? Which sources of variance contribute significantly to the variance of the trait and how much of that trait variation is accounted for by either genetic or environmental factors?\nRoad map\nHere I’ll provide a little road map for what we call univariate or mono-phenotype analysis. These are analysis of a single phenotype using twin data, so there are basically different steps in this process. The first step is used the data to test some basic assumptions about the models, including whether or not means and variances between twin 1 and twin 2, as well as for MZ and DZ pairs. mono- and dizygotic twin pairs can be equated. We call this a saturated model. Next, we will want to estimate the contributions of genetic versus environmental factors on the total variance of a phenotype. So we’ll delve into both ACE or ADE models? And I’ll just explain in a minute what these acronyms stand for and then finally we test various sub models of these ACE and ADE models to identify and report significant genetic and environmental contributions. So these would be considered an AE, CE or E only model. Hopefully this will become clear during the rest of this presentation.\nData\nThe practical example that we’ll be using relies on a data set from the NH&MRC twin registry in Australia, kindly provided by Dr Martin. Data come from the 1981 questionnaire and we will focus on data of BMI which stands for Body mass index and is measured as weight divided by height squared and it’s a measure of obesity. Today we will focus on the young female cohort. Those are between the ages of 18 and 30 years old and you see the sample sizes here. We have 534 monozygotic female twin pairs and 328 by dizygotic female twin pairs.\nVariables\nThis is what a data set looks like, and this is obviously straight from R where we just are showing the first top rows of this data set, representing in each row a pair of twins. As you can tell by the list of variables where we have, for example, wt1, wt2, representing the weight measured in twin 1 and twin 2 of the same pair. Of course we also have zygosity as well as age. part indicates participation, so we include both pairs were both have been completed measurements as well as those were only one completed measurements.\nNaming conventions\nHere are some of my naming conventions and these are my naming conventions, which means that you can change them and you can use your own, but I’ve tried to use a number of consistent ways to explain these scripts and use the same ones across, so it makes it easier to translate from one script to the next. So the name of variables is called vars and nv stands for the number of variables and nth for number of twin variables, because most of the analyses here will deal with twin data. selVars for the vars or the variables that are selected for the particular analysis, or covers for definition variables which typically are used to include covariates, and sv for start values, lb for lower bounds. I mean, these are pretty obvious. Also, the model names are quite important as you’ll see and so we will try to name them specifically to the kind of model that we’re applying, and so we will change name with something that is more descriptive about each of the models.\nClassical twin study\nSo today we’re going to fit some basic twin models, so it makes sense to talk a little bit about the classical twin study and provide you a little bit of background for those who are not that familiar with it. The Classical Twins Study, CTS is sometimes also referred to as the classical twin design or CTD uses MZ and DZ twin pairs reared together where MZ or monozygotic twins share 100% of their genes while DZ twins share on average only 50% of their genes. So, given that we know this information, there is the expectation that genetic factors are assumed to contribute to a phenotype when MZ Twins or more similar for a trait than DZ twins. Let’s unpack that a little bit.\nVariance\nSo what we’re interested in is the variance, how much people differ from one another and better. Those differences can be ascribed to either genetic or environmental factors, so we want to partition the phenotypic variance into genetic and environmental components, where the total variance of V is the sum of the variance components assuming that these effects can be added up and are independent of one another and we’ll talk about some ways in which that can be evaluated or tested later on. You will often hear about the concept of heritability denoted as h^2, which is the proportion of this total variance that’s due to genetic influences. It’s important to remember that this is a property of the group not an individual, and as it’s also specific to that group in place and in time.\nSources of variance\nSo you’ll hear me talk a lot about different sources of variance in the rest of this talk, and I’ve color coded them consistently. Such as the red refers to the genetic factors where we have additive genetic factors as the majority of the genetic factors, they basically are the sum of all the average effects of single alleles at all the individual loci. But there could potentially be some dominance resulting from the interaction between alleles at the same locus, which we refer to as D or VD or d^2. These are alternative ways in which people refer to them with they typically mean the same thing. The environmental factors can also be broken down into two sources, one that we refer to as C, the common environment. These are aspects of the environment that are shared by family members which contribute to similarity between relatives, and that’s the key part here. So they are shared and they increase similarity. In contrast, the other environmental factors that I denote in yellow here, the E component, these are unique environmental factors, unique to an individual that contribute to variation even within a family. They are also referred to as the specific, unique, or within family factors.\nAssumptions\nNow what we’ll be doing is fitting models today, but every model comes along with a variety of assumptions, and so does the classical twin study design. And some of these will list here. The equal environment assumption is quite critical, and it assumes that MZs and DZs experience the same degree of environmental factors or that they create the same level of similarity in MZs and DZs, with respect to factors that have a direct impact of the trait of interest. Other assumptions include those of random mating, no genotype by environment correlation, no genotype by environment interaction, no sex limitation or no genotype by age interaction. Several of these can be addressed with more complicated models that we will address in future videos.\nClassical twin assumptions\nNow, in addition, there are some basic data assumptions associated with the classical twin study. We assume that monozygotic and dizygotic twins are sampled from the same population. Therefore, we expect equal means and equal variances in twin 1 and twin 2, which are supposed to be randomly assigned. We also expect to find equal means and variances in monozygotic and dizygotic twins. Now there could be further assumptions needed that we will introduce when we introduce more complicated models that include other for example, male twins or opposite sex twins or a variety of other variables.\nDescriptive statistics\nNow let’s have a look at some actual numbers that you could get out of collecting data from twins. So what we have here is some descriptive statistics we could come up. We could calculate the means for both twin 1 and twin 2 in MZ twins as well as in DZ twins and we can workout what the expected covariance matrix looks like. Of course a covariance matrix is always symmetric with variances on the diagonal, and the covariances on the off-diagonal elements. Now when we look at these observed values, the question is, does it look like the means can be equated across twin 1 and twin 2 and across MZs and Dis, and the same for variances? We can do some old fashioned data checking, but we really need to test this properly, which is what we’re going to do next.\nSaturated model\nSo how do we test this? We do this by fitting what we call is a saturated model that basically just estimates the means, the variances and the covariances in our data, and so because we’re dealing with two groups of data that potentially have different expectations, at least with respect to their covariance, we are inherently fitting a multigroup model, and so we have a separate group for MZs and for DZs. In each group we estimate the mean of twin 1 and twin 2, the variance of twin 1 and twin 2, as well as the covariance. This model is described in the code called oneSATc.R, indicating we’re having one phenotype, sat for saturated model and c because currently we’re dealing with continuous data. These models can also be applied to ordinal or binary data, but we’ll talk about that later.\nFitting models\nNow before we go on and look at the script, let’s talk just a tiny little bit about how we go about fitting models and you’ve seen a little bit of this in a different video that introduces the various concepts of likelihood and parameter estimation. But just a quick refresher. Likelihood is the probability that an observation or data point is predicted by a specified model, which is basically what we’re doing here. So what we’re doing is trying to estimate by maximum likelihood, the best values for the parameters in the model. In this case for a saturated model, the mean, the variances and covariances, possibly some covariates as well. So how do we go about this? We typically define a model first. We then define the probability of observing a given event conditional on a particular set of parameters, and then we choose the set of parameters which are most likely to have produced the observed results.\nLikelihood ratio test\nAfter calculating a likelihood, we can also use this likelihood to compare it to the likelihood of a different model and construct what we call a likelihood ratio test, which is a simple comparison of the likelihoods under the separate models, and actually, in practice, it typically is a comparison of the log-likelihoods under two models, because that has slightly better properties. So we typically have an unconstrained model, which we call here Mu that has more parameters than the constraint model which has fewer parameters. And then the likelihood ratio statistic equals the difference or basically twice the difference between the likelihoods. or the log-likelihood of the unconstrained and the constrained model. And this likelihood ratio is asymptotically distributed as a Chi square with its degrees of freedom equal to the number of constraints. Hopefully this will make more sense when we use this in practice.\nProbability density function\nSo it’s also useful to remember is that this is based on the probability density function, which describes the distribution of a range of values of a continuous measure that are considered to be normally distributed. So Phi of xi is the likelihood of a data point, for example xi, for a particular set of mean and variance estimates. So based on the values that we provide for the mean and the variance, we can workout the likelihood of any particular data point and then we can sum across all these likelihoods of all the different data points to get the total likelihood of our model. And basically for any particular data point, it’s the height of this probability density function that provides us the likelihood of that particular data point.\nMultivariate situation\nNow remember that we’re dealing here with not a single distribution of a single variable, but we have variables for twin 1 and twin 2, and so we’re talking about the likelihood of a pair of data points X & Y, that we will work out in the context of a particular set of means, variances and correlation estimates, so we’re talking here about the multivariate situation or the height of the multi normal probability density function which is described here.\nConclusion\nSo this is our quick basic introduction with some background as to how we go about fitting or setting up models in openMx and the next video will show you the specific steps we go through in the code to set up a saturated model that estimates the means, variances, and covariances of continuous data in MZ and DZ Twins. Thank you very much."
  },
  {
    "objectID": "chapter9.2_transcript.html",
    "href": "chapter9.2_transcript.html",
    "title": "Chapter 9.2: Mendelian Randomization (Video Transcript)",
    "section": "",
    "text": "Title: A two minute primer on Mendelian Randomization\nPresenter(s): George Davey Smith, TARG Bristol\nepidemiologists are interested in understanding factors related to health and disease the smokers tend to die younger than non-smokers it certainly looks that way but disentangling cause-and-effect can be difficult smokers are different on average from non-smokers they’re more likely to drink heavily and have less healthy diets but even if we measure these confounding factors we may not measure them perfectly or there may be others we haven’t measured also as people become ill they may cut down give up smoking which could wrongly suggest the reduce smoking leads to worse health we could randomly assign 50,000 people to smoking 50,000 people to not smoke and follow them up to monitor their health this removes the possibility that any other factor could be responsible for any differences we see in their long-term health but this is neither ethical nor practical fortunately we’ve all been recruited into an experiment without knowing it at the point at which we were conceived our genes which have passed on randomly from generation to generation influence how much we eat drink smoke and more these genetic influences are not affected by anything else you may or may not choose to do in your life they’re not related to confounding factors we can use this knowledge to learn about cause and effect grouping people according to their genetic code this method is called Mendelian randomization for example smokers are carrying one version of a gene called CH RNA v tend to smoke less heavily than those who carry a different version when we group people according to which version of this gene they have we find that the people with the version of the gene associated with heavier smoking do indeed die younger but is the gene influencing how long we live in some other way we don’t think so when we look at the same gene in non-smokers there’s no effect on life expectancy so if that must be driven by smoking using this method you can show that smoking causes lung cancer heart and respiratory disease and many other diseases but has not seemed to influence the crashing or anxiety Mendelian randomization has already begun to tell us about factors that influence our risk of disease now we’re using the same approach in other ways to look at several risk factors together and to look at what influences disease progression which may help us to develop new treatments"
  },
  {
    "objectID": "chapter8.1_transcript.html",
    "href": "chapter8.1_transcript.html",
    "title": "Chapter 8.1: SNP Heritability (Video Transcript)",
    "section": "",
    "text": "Title: Heritability and SNP Heritability\nPresenter(s): Alkes Price, Broad Institute\nagain good morning everyone and welcome to our first virtually presented mpg session we’re fortunate today to have Dr. Alkes Price from Harvard School of Public Health speaking he’s a professor of statistical genetics in the Department of Epidemiology and today we’ll be talking about heritability and snip heritability and the relationship to the genetic architecture of disease a few notes about questions so I guess has kindly offered to answer questions both at the end and also importantly during the talk itself to sort of make sure that those are are voiced in this this virtual format and please plan on typing them into the Q&A box which at least on my screen is at the very bottom there you can type your question and also upvote questions posted by others that are of particular interest to you and I will do my best to keep an eye on that and to interject as as needed during the talk also if you have questions that you prefer to save until the end of the talk I will keep an eye on that and we can aim to address those then so again\nAlkes Price:\nthanks very much for joining and I think we can get started with the talk all right well good morning everybody I’m alkis price from the Harvard School of Public Health and welcome to this first virtual mpg primer session I’m going to talk about heritability and snip heritability which is basically a sort of introduction to the genetic architectures of disease and as Diane mentioned I do encourage everybody to jump in with questions throughout the talk in the format that Diane communicated so let’s get started I think that everybody in this audience will be aware that genome-wide Association studies have already been very successful at producing important biological insights this is just one example from the schizophrenia landmark schizophrenia G was published in the year 2014 in nature with a lot of contributions from people at broad and this landmark study had a huge number of interesting and important findings and yet at the same time I think everybody in the audience will be aware that even though genome-wide Association studies have found a lot of things they certainly have not found everything and in the case of schizophrenia we know that the findings of the 2014 paper explained about 3% of heritability whereas the heritability of schizophrenia has been estimated at about 64 percent from twin studies so there’s this big gap between what we found and what we believe is out there and this gap is classically known as the missing heritability and this story about missing heritability goes back to around 2008 or so this is a commentary of mehar 2008 nature and in the year 2008 people didn’t really know what the cause of the missing heritability was now I think it’s much better understood and this talk introducing the concept of snip heritability will also provide a review of what we know about the answer to this mystery of missing heritability so with that in mind here’s an outline of my talk I’ll start with an introduction and a definition of heritability then I’ll talk about genome-wide Association studies and missing heritability and then we’ll delve into this idea of heritability explained by snips also known as snip heritability and if we happen to have extra time there are some extra bonus topics pertaining to heritability that we might have time to scratch the surface on so let’s start with an introduction to heritability\nso heritability is generally defined as the proportion of phenotypic variance that is due to genetic effects and most of the time when people are talking about heritability they’re talking about narrow-sense heritability which is the proportion of phenotypic variance due to additive genetic effects you could also talk about broadsides heritability which could include epistatic or recessive dominant effects but those are generally harder to estimate and so generally when people are talking about heritability they’re usually talking about narrow-sense heritability denoted as lowercase h squared and so people have been trying to estimate narrow-sense heritability for a long time at least back to the year 1886 and one way that you could do this is you could take some relatives and see if they are somewhat phenotypic ly similar because if a trait is genetically heritable then you would expect that relatives are going to be somewhat phenotypic ly similar and there’s a method called Haseman Elston regression but I’m not going to describe in detail but here you’re basically regressing the phenotypic similarity on the genetic similarity or the expected genetic relationship amongst a particular pair of relatives so you know a parent-child pair or a sabzi pair have an expectation about 50% share genetics and you could ask how a phenotypic Lee similar are those pairs what’s their phenotypic correlation and so on this graph this is sort of a an amalgamation of different results that were compiled in the paper of issue at all 2010 in which they each points or represents one study and you can kind of see this is a study this is a sort of amalgamation of studies of height that generally people who are very closely related to each other tend to have very correlated values of height and this is generally sex adjusted standardized height that people are studying whereas on the other hand people who are a little bit genetically similar to each other tend to have height that’s a little bit you know typically correlated and that sort of stands to reason of what you’d expect for a trait that’s largely but not completely heritable and the slope of this line in this paper is estimated at point seven four seven which might mean that height is something like 75 percent heritable in terms of the narrow sense heritability but there’s a little bit of a surprising finding here which is that there’s an intercept that you know as you’d think that it as your as your familial genetic relationship goes to zero the phenotypic correlation ought to go to zero right if you’re sharing close to zero genetics you should have close to zero phenotypic correlation so you’d really expect this red line to go through the point zero zero and yet surprisingly this red line seems to not go through the point zero zero there’s this large intercept and there’s a lot of speculation you know different hypotheses why what could this be due to probably the simplest explanation is shared environments that you know even people who are just cousins they come from a you know a type of socio-economic background or something like that that makes them more disposed to be taller or more disposed to be shorter as a consequence of environmental effects and that is one possible explanation for why this line does not go through zero zero and there’s more complicated expect explanations as well involving genetic ancestry and Anna sort of mating and so on and so on but we just have to keep in mind as we think about narrow-sense heritability that is complicated and there’s a lot of room for different sorts of confounding and complex effects as we try to estimate this quantity that I have defined and probably the most popular way right now to try to estimate narrow-sense heritability is using the classic monozygotic die gods dizygotic twin study and so the idea here is the hope not the guarantee but the hope is that monozygotic twins versus dizygotic twins have basically the same amount of shared environment and really the only thing that differs between monozygotic twins and dizygotic twins is that monozygotic twins share 100% of their genetics whereas dizygotic twins share only 50% of their genetics and so the difference in how much phenotypic correlation you see between monozygotic twins on one hand versus dizygotic twins on the other hand should give you a sense of how you know how heritable the trait is and this twin based approach is really the it’s widely considered as the gold standard in ways to estimate narrow sense heritability and it should work unless there’s a difference in the amount of shared environment between monozygotic twins on one hand and dizygotic twins on the other hand for example due to effects in the womb or due to sort of societally you know effect for the family influences differences in the ways that man has got a twins versus dizygotic twins are treated and we have to take seriously this possibility that there could be still some confound into the differences in the amount of shared environment between monozygotic twins and dizygotic twins where by monozygotic twins have more shared environments and that might actually inflate the twin based estimates of heritability and I will provide some evidence later in the talk that decision fact likely to be the case that that the twin based estimates may in fact be inflated all right and so keeping in mind that these estimates might be inflated there have been a lot of a lot of work published you know that probably the broad and recent reference worth looking at is polar men at all 2015 Nature Genetics but for example for height the most quoted value from twin studies is 0.8 and then you know recent studies of cancer have have produced estimates of around 0.3 to 0.5 and generally speaking most of the disease diseases and complex traits that people tend to be interested in height is a bit of an outlier that’s extremely heritable but most of the diseases and complex traits that people tend to be interested in seem to have narrow-sense heritability estimates from twin studies that might be on the order of around 0.5 or a little bit less than 0.5 typically somewhere in that range all right so that does a brief introduction to narrow-sense heritability and now i’m gonna delve just a little bit more into the missing heritability problem from\ngenome-wide Association studies that I’ve already defined early at this talk so again the missing heritability is originally was defined as this gap between what we discover from the genome-wide significant low-side 4g wasps versus the estimated narrow-sense heritability from twin studies and again using schizophrenia as an example 3% is the hair ability explained by the 108 gos Llosa from PG c 2014 nature versus on the other hand 64% the narrow-sense heritability that’s been estimated from twin studies that’s a very large gap and in the early days of gos people were really very interested to try to understand what the cause of this gap is like why doesn’t G us find everything why is it so incredibly far away from finding everything and there’s a lot of explanations out there but these are the four that I think are worth most worth discussing in this MGP primer format so\nI’m gonna go over these four explanations one at a time so the first explanation is common causal variants of exceedingly low effect size and so we could imagine some different possible values of the genetic architecture of a disease or trait one possible genetic architecture is that you have ten common risk variants which each explained about 1/10 of the heritability and this is what people thought you know a long time ago you know way back only maybe early 2000s this is what people thought disease architectures might look like there’d be 10 low sigh you’d run into us you’d find the 10 low sigh and if story that’s what people were expecting but it might be more complicated than that you know there could be 10 there could be 100 or a thousand or even 10,000 common risk variants that each explained a tiny tiny tiny tiny proportion of heritability and in this extreme case at the bottom of this slide were we have 10,000 common risk variants that each explained on the order of one ten-thousandth of narrow-sense heritability maybe more in some cases and less in some other cases you can imagine that even at a very large sample size Egeus is going to be very underpowered to find them and you might only find a very very small fraction of the true common causal risk variants and that’s why you’re only going to explain a very very small proportion of the narrow-sense heritability with the ones that you’re actually lucky enough or well powered enough to actually find and this is what a lot of people believe that you know schizophrenia happens to be an example of a particularly polygenic trait with a lot of causal variants and so this may be what we’re looking at we may be looking at something like 10,000 common risk variants they need to explain a tiny tiny tiny proportion of heritability and even at very large sample sizes you’re going to be under powered to detect most of them you’re only going to detect the ones with largest effects or you’re going to attack a few because you get lucky or something like that but most of those common risk variants you’re just not going to detect them as being genome-wide significant in the G loss and we can even go to a greater extreme the so called the infinitesimal model nobody believes that this is realistic but it can be useful to think about as a theoretical construct and so the infinitesimal model is a model image all of the common snips in the genome are causal risk variants causal risk variants with causal effect sizes something like you know normally distributed with mean 0 and variance H squared divided by M where m is the number of snips and so there you’re sort of spreading the the narrow-sense heritability across literally all the common snips in the genome so this is one very plausible explanation as to what is going on with missing heritability we’ve got a lot of causal a lot of common causal snips of tiny effects gos even at large sample size are not finding most of them and you know we do know that there’s a lot of traits that are extremely polygenic I’ve already said that schizophrenia is a particularly polygenic trait this is a Manhattan plot from the blood pressure G wasps of Evangeline at all 2008 teenager genetics and this is another another illustration of an extremely polygenic trait which clearly has an extraordinarily large number of of causal Llosa so a second explanation that people have been interested in for a while is the explanation of rare and low frequency causal variance and we know that G wasps are not well powered to identify rare and low frequency causal variants because the power sort of scales with the allele frequency if you have a really rare variant it doesn’t occur very often in the sample and so you’re not going to be well powered to detect it and mathematically we know that etta had a specific fixed per allele effect size our our power scale something like with the sample size times P times 1 minus P where P is the minor allele frequency and so if you have a really rare variant then you’re definitely not going to be well powered to detect it as being genome-wide significant energy loss where you’re conducting single marker tests I mean I’m not I’m not going to get into the topic of sort of multi 8 marker gene level test which is a topic for a different MPG primer session and so G wasps are not well powered to identify these rare and low frequency causal variants as being genome I significant and maybe that’s where a lot of the missing heritability is and that’s an explanation people have been interested in but recent work from Zhang a tall 2018 nature genetics as well as other work from our group shook at all it is all at all are suggesting that there’s not really a lot of heritability coming from rare and low-frequency snips so according to shook at all it’s less than 10% of snip heritability I know I haven’t defined snip heritability yet we’ll get to that later but less than 10% of snip heritability is coming from snips with minor allele frequency less than 1% now there’s a little bit of a complicated story here involving negative selection it under under simplified assumptions where you don’t have any selection and you might have to also assume equal a constant effective population sizes across time but under some some assumptions I’m not going to get into you might expect that snips with math less than 1 per sentence only\nought to explain about 1% of snip heritability so there is some excess here where they actually explain I couldn’t do this paper about 9% of snip heritability which is a lot more than 1% and that excess where by 9% is more than 1% is a consequence of the action of negative selection which sort of causes really important snips that have important effects too generally those important effects are usually bad for the organism and because they’re bad for the organism those snips cannot rise to high frequency and stay as rare snips and so you might expect that rare snips will tend to have larger effects and in fact that’s exactly what you see and that’s why snips at math minor leave frequency less than 1% explains somewhere around 9 percent of snip heritability which is a lot more than 1% of snip heritability but even so at the end of the day according to these papers these rare and low frequency snips are not really explaining a lot of heritability and they are probably not the primary explanation for missing heritability now to be clear I don’t I specifically do not want to say that rare and low frequency causal variants are not important and we shouldn’t study them rather what’s going on here is that if you’re interested in heritability or in explaining a lot of heritability then rare and low frequency causal variants don’t contribute a lot and by extension if you’re interested in polygenic prediction which kind of piggy box piggy backs off of heritability then rare and low frequency causal variants aren’t very important but on the other hand if what you’re interested in is discovering interesting disease biology that could lead to a drug target rare and low-frequency calls of variants may be really important you may identify a rare coding variant that explains a miniscule amount of heritability but it has a really biologically interesting sort of mechanism behind it that might lead to a drug target totally fine if it explains a miniscule amount of heritability if it’s going to lead to an actionable drug target so let’s just keep in mind that even though they’re not so relevant for heritability they could be very important for disease biology and developing drug targets ah the next explanation that people I see that there’s a QA Diane do you want to read out the QA thank you yes um I will do that now so the question from now son is how they heard how doesn’t heritability of rare how is the room sorry how is the heritability of rare variants estimated for a jiwa study that is underpowered\nokay so the question is how is the heritability of rare variants estimated for a jiwa study that is underpowered and I the topic of snip heritability that I will delve into in the third part of this talk delves into estimating the aggregate heritability contributed by all the variants in the genome may be I mean mostly what I’m going to talk about is estimating the heritability explained by all common collectively explained by all common variants in the genome but something related to that that you could do is you could estimate the heritability explained by all rare variants in the genome and so these studies that I’ve quoted on this slide shook it all saying it’ll cos all and there’s others they’re doing something sort of like that they’re doing something they’re sort of extending the methods to estimate snip heritability or heritability explained by all snips in the genome which I will be talking about in the third part of this talk they will be extending that to estimate the heritability explained by all snips in a particular minor allele frequency class such as the heritability explained by all rare variants and I will aim to return to this question in the third part of the talk when I talk about snip heritability hopefully what I just said will become more clear after I delve into what’s going on with sniff heritability all right so moving on to explain explenation number three on my list which is copy number variation and it is in principle possible that copy copy number variants are biologically important and contribute a lot to heritability but are not well tagged by common snips and there’s another question\nsee I don’t see the question in the Q&A box okay in that case I will keep going\nthank you so it’s possible that the copy number variance could be important for disease but their effects may not be so well tagged by snips so that if you’re only looking at snips you won’t see them and we have to take that possibility seriously way back in the year 2010 this paper that I’ve cited suggested that common copy number variants that could be tight on existing platforms and that’s an important qualification did not contribute much but that may be more at saying more about the technology than about the biology and more recent work of segments at all 2015 is suggesting that structural variants are enriched on haplotypes identified by G Y so some of the common snips that we identify as genome-wide significant in G wasps may be tagging causal copy number variants so I don’t think we have a conclusive answer to this question right now as to whether you know untight and an untagged or only partially tagged copy number variants are responsible for a lot of the missing heritability III would like to go out on a limb and hypothesize if this story might maybe be sort of similar to this story with rare variants and some of these copy number variants of course are likely to be rare because you can’t you know you can’t knock out a big chunk of genome and and and have it not have a huge effect which would then due to negative selection keep the variant rare I hypothesized that it might be true that just like rare variants copy number variants might not explain a lot of heritability and might not be important for quantifying heritability or by extension and for apologetic risk prediction but on the other hand they might be really important for understanding disease biology and they might lead us to examples where we understand the biological mechanism and can develop drug targets so I hypothesized that they might be not so important for heritability but at the same time very important for disease biology and drug targets and then finally the fourth explanation that I wanted to talk about is the possibility that narrow-sense heritability was overestimated in the first place and I already alluded earlier in this talk to the possibility that narrow sense estimates of narrow-sense heritability might be inflated due to shared environment and there’s some other complicated explanations whereby if you have G by G interaction which is not supposed to be included in estimates of narrow-sense heritability which is defined as including only additive effects and not G by G interaction effects then according to look at all 2012 that could inflate your twin based estimates so there’s people out there who made who believed that the twin based estimates may be inflated despite the fact that they’re sort of the best well until recently at least they’re the best thing we have I’d like to highlight this paper of young at all 2018 a nature genetics which introduced a new method called relatedness disequilibrium regression which is predicated on having at your disposable at your disposal a really large data set with a lot of related individuals which those authors happen to have because they analyzed the decode genetics data set from Iceland and they claimed that they have an estimation procedure that is robust to these types of effects of shared environment and they claim that the narrow-sense heritability of height is only about 0.55 and we have to take those claims seriously they are John fairly consistent with a couple earlier studies of Zaillian at all from our group and I think people at this point in time tend to believe that point eight four for height for the twin based estimates of a neuro sensor to be live height that point eight probably was an over estimate and the truth might actually be some number closer to about point six or so I think that’s what people most people believe and so summing it all together I think in terms of heritability the two explanations for missing heritability that are most prevalent are number one you’ve got common variants of exceedingly low effect size that cannot be detected by G wasps as being genome-wide significant and number two the narrow that the twin base narrow-sense heritability estimates are somewhat too high obviously this is not going to get you you know a slightly overestimated narrow sense notability is not going to explain a difference between point zero three and 0.64 for schizophrenia which is a humongous difference and that’s probably more dominated by number one so where does this leave us well there there’s a sort of fundamental question that was asked originally in a landmark paper of yang at all 2010 Nature Genetics from Peter Fisher’s group maybe we can try to estimate the heritability explained by all the snips in the genome or maybe all the common snips in the genome not just the snips that are genome-wide significant but in fact all the snips collectively even we don’t know which ones are the causal ones we can still try to estimate the heritability jointly explained by all of those snips together and so that’s this concept of snip heritability is really the main focus of this primer and I will start to now delve into that and so the distinction between on the one hand narrow-sense heritability and on the other hand snip heritability the hair ability explained by snips specifically uh it really sort of rests on the distinction between two important ideas\nIBD or identity by descent and IBS or identity by state so identity by descent or IBD means two people are related they have similar genetics IBS or identity by state means two people who are not related may still by chance have genetics they’re a little bit similar and you might be able to do something with that to learn about complex trade architectures so let me delve into a little bit of detail let’s start with IBD let’s suppose you take two individuals who are related so the two individuals I depicted on this slide are brothers and we could ask ourselves what is the proportion of the genome that these two brothers share identical by descent which means inherited from a recent common ancestor and the answer well the answer is not exactly point five the answer is approximately point five because it can vary a tiny bit from one step to the next but an expectation is 0.5 and you can imagine if you have sort of a large set of related individuals who you’re analyzing genetic data from you can build an IBD matrix quantifying the IBD for individuals and for a pair of individuals who are brothers the entry would be 0.5 because point five is the IBD of those two brothers and this cave matrix or this IBD matrix you could use it to estimate narrow-sense heritability and no worries if you if you don’t feel like sorting through all these equations but this is what it would look like in terms of math that you have a vector Y of phenotypes and you’re you’re sort of decomposing the the phenotypic variance into the part coming from genetic effects called U and the part coming from environmental effects called epsilon and the variance of U the genetic effects isn’t it sort of proportional to the IBD matrix and where’s the environmental effects on the other hand if we make a strong assumption of no shared environment then the environmental variance is proportional to the identity matrix that is saying we have no cross terms at all between distinct individuals and then you you sort of parameterize the overall phenotypic variance of E in that way estimate these parameters and estimate the narrow-sense heritability and there’s methods you know max likelihood methods or restricted max and likelihood methods for estimating that parameter or actually you’re estimating the two parameters Sigma square G and Sigma square e from the previous slide I’m going to just kind of skip most of mathematical details you can read about it in these various papers and you know no worries if if you’re not following all the math so on the other hand we could have two unrelated individuals such as the two individuals depicted on this slide and even though these two individuals are completely unrelated it is possible that they might be just a little bit more genetically similar than average or just a little bit less genetically similar than average just by statistical chance and we can sort of quantify that in using the equation on the right half of this slide where we can compute something this is sometimes called the genetic relationship matrix or it’s sometimes called could be called an IBS matrix but it’s basically you just sort of like compute the correlation across snips between these individuals genotypes suitably normalized and typically if you have two unrelated individuals you can practically guarantee that that number is going to be really really close to zero but in these standardized units in which the mean is going to be zero on average it might be a tiny bit bigger than zero like zero point zero zero four or it might be a tiny bit less than zero like- it’s zero point zero zero four and you might expect that if you have a heritable trait then two individuals who just by chance are a tiny bit more genetically similar than average hot-hot to have slightly concordant phenotypes whereas on the other hand individuals who are slightly less genetically similar than average ought to have slightly less concordant phenotypes and this is something that\nyou can use to estimate the heritability explained by snips or heritability explained by genotype snips in the way that it was originally employed and the real question here is what is the set of snips and well the answer is related to the snips that you used to compute this genetic relationship matrix if you only use snips on chromosome 1 to compute the genetic relationship matrix then you’ll get an answer that has something to do with snips on chromosome 1 if you use all common snips to compute the genetic relationship matrix then you’ll get an answer that has something to do with all common steps and so on and so on and so forth and so once again we can model the phenotypic covariance of V as sort of a linear combination of this genetic relationship matrix or IBS matrix that’s sort of the the genetic part of the genetic part coming from snips and then everything else which is the environmental part or strictly speaking it’s everything except the genetic part coming from snips and and then we can estimate this quantity called snip heritability and again reverting to a little bit of math here I feel that when we if we’re going to talk about estimating a quantity we really wanted to find that quantity in the entire population and this is a formal definition of that entire quantity of that quantity in the entire population and we can skip the mathematical symbols here and in words this is just the maximum amount of phenotypic variance that you could explain using any linear combination of snips and that’s the\ndefinition of snip heritability that’s a definition in the entire population that definition does not depend on a particular sample although it does depend on which set of snips you’re looking at if you’re looking at you know just snips on chromosome 1 the answer will be smaller than if you’re looking at all snips in the genome or you might be looking at just the genes of just a small just a few hundred thousand genotype snips or a large number of imputed snips or common snips or common and rare snips or whatever and the answer will be different in each case but this is a quantity that you can define in the entire population and then after you’ve defined it in the entire population then you could obtain an estimate of that quantity an estimate with noise of that quantity in a finite sample and this is what was done in this landmark paper viewing at all 2010 Nature Genetics and this is the same math that I showed on an earlier slide for IBD this is just the same math with the IBD matrix K replaced with the IBS or gr M matrix called capital A and we’re now intuitively thinking about this in terms of unrelated individuals and snip heritability but mathematically all the math is the same and all the maximum likelihood or restricted maximum likelihood computations are the same so I’m going to choose to not focus on the math in this talk and I’m gonna gonna focus more on intuition and so now we have these two quantities H squared that’s total narrow-sense heritability and that’s sort of that’s corresponds to\nthe question how phenotypic ly similar are two relatives there’s a question there is yes thank you I think the question is asking or the question savanna lewis is asking the following when you say the entire population do you mean the entire human population okay so this is this is a good point\nthis is something I’ve really glossed over and you know if I wanted to delve deep into the population genetics that could be a separate MPT primer but generally when people talk about a population they’re talking about a populace if ik population of a particular continental ancestry one example the population would be European Americans another example the population would be the British ancestry individuals from the UK biobank and so on and so forth now if I want to be a strict population geneticist I might define a population as a set of Penn matically randomly mating individuals now know believes that European Americans are a set of ethnically randomly mating individuals nobody believes that British ancestry individuals from the UK or you know East Asians from Japan or you know Nigerians or whatever population you’re talking about nobody believes that that’s strictly speaking of magnetically mixing set of individuals we might just choose to pretend that’s the case as a sort of approximation and I mean I mentioned earlier at the beginning of this talk that there’s opportunities for all sorts of confounding one sort of confounding has to do with confounding due to population stratification do the differences in genome-wide ancestry amongst different individuals that again could be a topic for another MPG primer we should be aware of the possibility that if you are studying a population such as European Americans or any population in which there are differences in genome-wide ancestry amongst different individuals in that population we have to be a little bit careful because there is the possibility for confounding to the population stratification but the short answer is we’re thinking about a specific population of a specific continental ancestry all right and so getting back\nto this slide narrow-sense heritability it corresponds to the question you know how phenotypic ly similar are to relatives and it’s kind of implicit that two relatives could be phenotypically similar due to like whatever genetics they happen to be caring maybe they’re carrying the same rare variant maybe they’re selling the same copy number variant maybe they’re carrying the same common variant it’s going to include all of that on the other hand we have a smaller quantity snip heritability originally this was called the heritability explained by genotype snips because people liked it to estimate this just using genotype snips before imputation became kind of universally popular so that’s why you’ll see this terminology on some of these slides but you can apply this to any set of snips and this this corresponds to the idea if\nI have two unrelated individuals and I use a specific set of snips and the set of snips is important to quantify how genetically similar they are just by chance then how phenotypic ly similar will they be and again this is a function of a very specific set of snips and because it’s a it’s only capturing the heritability causally explained by a very specific set of snips in general it’s expected to be lower than the total narrow-sense heritability which is the capturing additive effects from all possible genetic variance all right and then finally there’s a third quantity that has already appeared in this talk and I’m going to call that H for G us and H squared G us is the heritable explained by genome-wide significant snips and I mean if you’re not worried about LD then it’s basically just the sum of the variants contributed by each of your genome-wide significant snips in turn but it’s a little bit more complicated if you have LD and strictly I can define it as the maximum proportion of phenotypic variants that you can explain with any linear combination of genome-wide significance snips so just focusing on those snips that are genome-wide significant and that’s that’s the number that’s the number that was in a particular study at point zero three for schizophrenia and I have to be a little bit careful when I say that I’ve defined this it’s not really a pump it’s not really a true population level parameter because it’s a function of which snips up which snips pop up is being genome-wide significant in a particular gos at a particular sample size so it’s really a function of a specific G watch that identifies a specific set of genome-wide significance snips it’s really a function of what the set of genome-wide significance snips is in a particular study and so now we have we have sort of inequality with three different quantities we have h4g watch that’s the smallest number which is just the genome-wide significant snips we have H squared G that’s the snip heritability that’s all the snips in the genome maybe all the genotype snips maybe all the common snips whatever flavor of snips you want to be choose to study at a particular point in time but it’s in some level the heritability explained by all the snips in the genome including all the snips that are not genome-wide significant but might contain some signal and then funding to largest quantity total narrow-sense heritability that’s the additive heritability explained by all genetic variants which includes not only every possible type of snip but other types of variants as well copy number variance and so on and so so this has been a little bit of a dry technical discussion so now we might want to look at some real data to sort of get a sense of\nhaving some intuition of how this works in practice and so I’m going to start with height which is probably the the trait most well studied by geneticists and in terms of narrow-sense heritability well from the twin studies we have 0.8 as I mentioned earlier that’s probably an overestimate we’re probably closer to 0.6 but for now I’m just going to say 0.8 from the twin base studies and then we have snip heritability this is the quantity that yang at all 2010 Nature Genetics in their landmark paper estimated at 0.45 and then the hair adjust the heritability explained by Gy snips well way back in the year 2010 that was at about zero point one zero although it’s gone up a little bit with some studies of wood at all 2014 Nature Genetics and enu at all I forget which year 2018 or 2019 human likely genetics it’s a little bit higher than 10% now but you can see that this concept of snip heritability can explain most not all but it can actually explain most of the missing heritability and this goes back to explanation number one of my four explanations that I quoted earlier where you know there’s just a lot of snips with really small effects that you know that gee wasps do not detect as being genome-wide significant and if you define and estimate this quantity H to RG which quantifies the heritability explained by all snips or all snips of a particular category such\nas genotype snips not just the ones that are genome-wide significant then you get a much much much larger number like such\nas 0.45 then you get from just the genome I significant ones such as point\n1 0 and I like to use the terminology\nhidden heritability as do others hidden heritability to explain this gap between h4g wasps in each for a G so that\nheritability we know it’s there we know it’s in those snips we just don’t know which snips it is because we don’t have\nenough sample size to do an infinitely power G loss to figure out which which you know which snips are which are all\nthe causal snips in the genome but we know that heritability is there it’s just kind of hiding and then on the other hand there’s heritability that’s\nstill missing which is this gap between 0.45 and 0.8 although as I said a moment ago maybe it’s really a gap now between\n0.45 and 0.6 if we believe point 6 for height now and that’s heritability that’s still missing and then we don’t\nhave an X we don’t currently have an explanation for so this is just a\ngeneralization published by yang at all 2011\nto some other quantitative traits and I’m not going to go over this in detail other than to say that I mean of course\nthese traits are less Harold other than less heritable height but qualitatively the story is pretty similar to what’s\ngoing on with height we still have one question in the Q&A that I think it’s maybe timely at this\nmoment which is from Matthew Worman who says hey because if one were to use a p-value of 10 to the minus 5 and instead\nof 10 to minus 8 for D us what fraction of snips exceeding this less stringent threshold are false positives okay so I\ndo think I do think it is of interest to say hey what happens if I choose some other threshold let’s say 10 to the\nminus 5 and I could define a set of snips which are snips that come in at P\nless than 10 to the minus 5 in my G wasps and I guess I would like to say\nthree different things about that well the first thing that I want to say is that just as is the case with the set of\ngenome wisely he would snips at five times ten to the minus eight the nature and characteristics of the snips that\nwould come in at ten to the minus five is very much a function of your sample size I mean it’s a function of a lot of things like the genetic architecture of\nthe trade and so and so forth but it’s really very much a function of your sample size so that’s sort of like very dependent on a particular study\nyou know the snips that come in at P less than ten to the minus five killer set of characteristics that’s the first\nthing that I wanted to say the second thing that I wanted to say is that although it might depend on that the\nparticular study in a particular genetic architecture and and so on and so forth that the sample size I think intuitively\nit’s appropriate for us to would think intuitively that most of the snips that come in you know at P less than ten to\nthe minus five most of those are going to be false positives in the sense that they’re not truly causal variants or they’re not truly tagging causal\nvariants that is that in general that is what I suggest is likely to be the case and the third thing that I want to say\nis that even if most of them are false positives a very important fraction of\nthem will be true positives and those true positives will be contributing more heritability and so if I if I define\nsomething called H square G was 10 H squared G was less than 10 to the minus 5 which is the heritability explained by\na particular set of snips that comes in at P less than ten to the minus five in a particular G wasps then that will lie\nsomewhere in between h4g wasps which is just the john of genome I seen big ones and h4g and it would probably be\nsubstantially greater than each square G loss because even though I would I’m going to say that in general make you’d\nexpect that most of those coming in at ten less than ten of - I would be false positives a very very important fraction\nof them would be true positives and that would get you a lot closer to H square G the true snip irritability although I’m\nstill going to say that in general you’d expect that there’s still a lot more signal that is not even captured by P\nless than ten to the minus five and you’d still have a gap between h 4g whilst 10 to minus 5 and h 4g so h 4g\nwest 10 to the minus 5 might lie somewhere in the middle between h 4g west and h 4g with the details that\ndepending on how polygenic the architecture is and what the sample size is and so on and so forth all right very\ngood and so so generally speaking H for G is less than H squared so it’s an\nimperative builds us the narrow-sense heritability that could be due to rare and low frequency variants as well as other types of variants like copy number\nvariants that are not captured by the snip heritability of a very specific set of snips usually either genotype common\nsnips or genotype and imputed snips that are mostly common or something like that that you’re estimating this an impaired\nability of and then on the other hand we have a completely different phenomenon each board gos\nless than H squared G so that’s each word us less than snip heritability where we need larger G while sample\nsizes or as Matt alluded to maybe we need just a less less stringent genome-wide its significance threshold\nor whatever I mean H for G is just sort of in the limit of you know any snip with P less than or equal to 10 to the 0\nand your G loss right and and so in the limit of large sample sizes we might\nhope that as our jaws gets hugely large then we’ll identify all the you know\nassociated snips and each for gos will approach H squared G but there is a bit of a caveat here that even if you’ve got\na humongous sample size like 758 thousand samples for blood pressure if it’s a really polygenic trait like blood\npressure 758 thousand samples is not enough and it’s not even remotely close to being enough and each board gos\npoint zero six may still be a lot lower than this nip heritability of point two one all right so I’ve been I think I’ve\nbeen saying repeatedly that snip heritability is a function of the set of snips and that’s definitely true but we\nshould also keep in mind that it’s a function of the particular population that we’re looking at and again as I\nmentioned earlier one population that some people sometimes study is British\nancestry individuals from UK biobank which is mentioned on this slide and another population that people sometimes\nstudy is European Americans and that happens to be the population that was analyzed in reference to on this slide\nand it turns out that if you estimate snip heritability in British ancestry UK biobank samples you consistently get\nhigher numbers than you do if you estimate heritability in other cohorts such as European American cohorts and\nthere’s different possible explanations for this but probably the most likely explanation is that snip heritability is\ntruly higher in a British ancestor UK biobank population than it is in a\nEuropean American population and that might be due to just less environmental variance in a set of British ancestry\nindividuals residing in the UK or more precisely less environmental variance in\na set of British ancestry individuals residing in the UK that the UK biobank captures which is actually it’s not a\nperfectly random subset of of British individuals it’s British individuals who choose to respond to surveys and might\nhave higher than average SES and so on and so on and so forth and there seems to be less environmental noise and thus\nlarger snip heritability in that population than in for example a general\nEuropean American population so let’s just keep in mind that’s an impaired ability depends on the population studied as well as on the set of snips\nand of course I should say that total narrow-sense heritability also depends on the particular population that you’re\nstudying so one question that comes up a lot is\nwhich assumptions do we need for estimates of snip heritability to be valid and one thing that the world now\nunderstands very well is that le dependent architectures can lead to bias and estimates and because I see that I’m\nrunning out of time I’m going to choose to just gloss over the details of that I want to say very carefully what I mean\nby LD dependent architectures le dependent architectures we’re not talking about tagging here of course we\nknow that you know a system that’s not a causal snip can tag a different snip that is a causal snip but LD dependent\narchitectures means it means that causal effects vary with the amount of LD s\nsnip has and this could be just due to minor allele frequency that’s kind of trivial that rarer snips you know\nexplain less heritability percent because they’re rare and they also have less LD that’s kind of trivial but even\nif you condition on math even in a specific fixed math we know that low LD\nsnips actually have larger causal effect sizes than hi LD snips and the reasons\nfor this are quite complicated you know we believe they have something to do with negative selection but it turns out\nthat that that violates some of the assumption of some of these estimation methods and it can lead to biased in\nestimates and it’s something that we have to watch out for and it’s something that the world now understands well and\nknows that we have to watch out for and I’m just going to leave it at that and gloss over some of these other slides\nand then a second question that comes up which is the second bullet fifth point now on this slide over here is the\nquestion is it okay if effect sizes have a non infinitesimal distribution and so I haven’t really emphasized this point\nbut the sort of the the estimation procedures that I’ve described that were you know computer genetic relationship\nmatrix from unrelated individuals and then use restricted maximum likelihood the fit variance components to estimates\nand impaired ability there’s kind of some assumptions underlying their underlying that about an infinitesimal\narchitecture where infinitesimal as I defined earlier refers to a genetic architecture in which all snips are\ncausal with a normal or Gaussian distributed distribution of causal\neffects and the methods assume that but we now know that even though that assumption is clearly an incorrect assumption that\ndoes not lead to any bias it’s merely the case that you might be leaving some precision on the table we’re by methods\nthat are a little bit more sophisticated in accounting for the fact that that through the distribution of causal\neffects is a little bit more sparse than that can actually produce more precise estimates but that’s an issue of precision of getting a lower standard\nerror more precise estimate it’s not an issue of bias and this does not lead to bias and that is well understood all\nright um I do want to mention at least briefly the important point that whereas\nI’ve been talking about the math in terms of quantitative traits like height people are very interested in case\ncontrol traits because most of the disease traits that were really ultimately interested in terms of\nmedical action ability and drug targets and treatment or whatever it is we’re aiming for you know even polygenic\nprediction we’re generally aiming for it in the context of medically important disease traits like schizophrenia or type 2\ndiabetes or whatever disease you like to study and we need a little bit more math to get this to work right and the type\nof math that is most commonly used is called the liability threshold model and again I’m going to try to avoid going\nover the mathematical symbols here but I want to provide the intuition because this is so important that the liability\nthreshold model models that there’s some unobserved underlying continuous number\ncalled the liability and if the liability is above some number you have the disease and fortunately we have an\nexample which is type 2 diabetes which provides some intuition here where maybe you’re doing a G loss of type 2 diabetes\nand all you know is that somebody has told you whether or not they have type 2 diabetes or whether or not they’ve been\ntold by a doctor that they have type 2 diabetes but there’s an underlying continuous quantity which we call the\nliability which is your fasting blood glucose level which is one of the the diagnostic criteria for type 2 diabetes\nand we could think of it as you know we don’t get to see the liability but if you’re if you’re fasting blood glucose\nis above some level then you are you are typed you have type 2 diabetes and so what we want to do is we want to do some\nmath that operates on this unobserved continuous valued liability and that’s\nwhat liability threshold modeling is about and I’m not going to go into the details but all the work that’s been\ndone in this space on snip heritability has relied on the liability threshold model and contingent on using the\nliability threshold model it has generally produced a qualitatively similar story to the story that I\ncommunicated earlier for height and I’m not going to go into those results in detail I do want to mention briefly that\nI’ve been talking mostly about methods that use individual level genotypes there’s been a lot of interest dating\nback to the year 2012 and methods that instead only requires summary Association statistics as input and\nthere are methods now that they do make that do input summary Association statistics to learn about various\nflavors of heritability and snip heritability and I just want to provide one point of intuition that this is\nrelated to the fact an important observation made by yang at all 2011 that the average chi-square statistic in\na Jie wasps is very closely related to snip heritability you should not be surprised if your average chi-square\nstatistic and in G wasps is above 1 that does not imply that you have confounding but rather that is directly related to\nthe amount of polygenic signal in that G wasps and an increasingly sophisticated set of methods can use that observation\nto estimate heritability from summary statistics and I’m going to gloss over all of the details of that so\nconclusions we can use family data such as twin studies to estimate the\nnarrow-sense heritability of a trait for even now in the year 2020 most G wasps\nhave the property that the genome-wide significant low-side that they discover do not explain all the heritability we\nhave missing heritability and there’s this concept of snip heritability that can really close that gap so these are\nsome of the bonus topics really within the heritability space that I think are super interesting and that I have some\nbonus slides on and that as expected I’m not going to have the time to delve in to in detail but you can check out those\nbonus slides if you want to as you know the slides were sent out to the whole group and I’d like to acknowledge all the members of my group who have figured\nall this stuff out and explained it to me and I’d like to put in a brief advertisement for my course advance\npopulation medical genetics which would be offered in spring 2001 and with that I’ll take any additional questions that\npeople may have thank you very much ok as there are no questions in the Q&A box at the moment\nplease do post to the whole group please do posting a few if you have further questions it was a question that was\njust asked oh great um it’s from Matthew Orman who’s commenting thanks Elka I really appreciate the clarity your\npresentation so I know the second to that and thank you and I do have one question on which is I’m curious about\nthe with the identity by state definition the concept of unrelated individuals does this require that the\nvariant shared by the two individuals be of independent origin or just okay so\nthank you for raising that point and and unrelated individuals that this is another term just like the term\npopulation where it’s a little bit tricky to define it and if you define it strictly then it’s probably nothing like\nwhat you have in real life so if I want it to be real strict about it I could say unrelated individuals is like let’s\nthink I have a simulation and in my simulation I’ve got some minor allele frequencies and I’m you know or\nhaplotype frequencies and I’m you know generating the the genome of individual number one and then completely\nindependently and generating the genome of individual number two and the whole process is completely independent but in\nreal life of course it’s not like that you know you take any two people who think they’re unrelated to each other no\nyou could probably go back you know five or ten generations and find some common ancestor that’s usually the way that it\nworks and so the the detailed answer to that question is if you’ve got cryptic relatedness you’ve got this set of\nindividuals from a G wash they’re supposed to be unrelated to each other you don’t think there’s any siblings in there but there’s probably some cryptic\nrelatedness that I mean I believe that yang at all 2010 Nature Genetics I came\nup with some threshold that I think might have been 0.05 or maybe they later they changed it to 0.025 or something\nlike that and as long as he as long as you apply this threshold where you stringently check that there are no two\nindividuals in the data set who are related at a level level greater than 0.05 or greater than 0.025 then it is\nhypothesized that the met that the method is sort of robust to that and then even though the people are not strictly unrelated you should be okay\nnow on the other hand what if you have a data set that consists of like related and unrelated individuals and you don’t\nyou want to throw away half of them because then you’d be zapping your power then what do you do well it turns out you can fit two variants components to\njointly estimate narrow-sense heritability and impaired ability I’m not going to go over the details there’s a reference date at all 2013 plus\ngenetics from our group so I think that I mean that in summary the shortest answer to your question is as long as\nyou make sure that there you know that any particularly related people are only a little little bit related then you’re\nkind of okay and everything that I said still kind of holds thank you very much there were three more questions posted\nbut I’m afraid we’re out of time because MGP talk will be starting shortly so I think we’ll keep those questions in\nreserve and share them with you all because if you if you wish all are welcome to email me offline a price at HS pH don’t harbor any deer great thank\nyou so much for your talk and thanks to the audience for joining in today so thanks okay"
  },
  {
    "objectID": "chapter9.html#genomic-sem-introduction",
    "href": "chapter9.html#genomic-sem-introduction",
    "title": "Chapter 9: Advanced Topics",
    "section": "Genomic SEM Introduction",
    "text": "Genomic SEM Introduction\nTitle: Genomic Structural Equation Modeling: A Brief Introduction\nDescription:\nPresenter(s): Andrew Grotzinger\nLevel: Beginner\nLength: 10:43\n\n\n\n\n\n\n\n\n\nLink to video transcript here."
  },
  {
    "objectID": "chapter9.html#running-genomic-sem",
    "href": "chapter9.html#running-genomic-sem",
    "title": "Chapter 9: Advanced Topics",
    "section": "Running Genomic SEM",
    "text": "Running Genomic SEM\nTitle: Short Primer on Structural Equation Modeling (SEM) in Lavaan\nDescription:\nPresenter(s): Andrew Grotzinger\nLevel: Intermediate\nLength: 11:34\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\nTitle: GenomicSEM: Input/Explaining how S and V are estimated and what they are\nDescription:\nPresenter(s): Michel Nivard\nLevel: Intermediate\nLength: 23:18\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\nTitle: Working through examples on the Genomic SEM wiki one by one: munge, ldsc, usermodel functions\nDescription:\nPresenter(s): Michel Nivard\nLevel: Intermediate\nLength: 22:54\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\nTitle: Multivariate GWAS in Genomic SEM\nDescription:\nPresenter(s): Andrew Grotzinger\nLevel: Intermediate\nLength: 30:08\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\nTitle: Using Genomic SEM to Understand Psychiatric Comorbidity\nDescription:\nPresenter(s): Andrew Grotzinger\nLevel: Intermediate\nLength: 1:01:02\n\n\n\n\n\n\n\n\n\nLink to video transcript here."
  },
  {
    "objectID": "chapter9.html#statistics-of-the-interaction-term",
    "href": "chapter9.html#statistics-of-the-interaction-term",
    "title": "Chapter 9: Advanced Topics",
    "section": "Statistics of the Interaction Term",
    "text": "Statistics of the Interaction Term\nGoals of this section:\n\n\nUnderstand the statistical basis of the interaction term\n\n\nTitle: Dummy variables: interaction terms explanation\nDescription: Explanation of the interaction term Part 1. This two-part short video series briefly describes how to use and interpret interaction terms.\nPresenter(s): Ben Lambert\nLevel: Beginner friendly\nLength: 4:35\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\nTitle: Continuous variables: interaction term interpretation\nDescription: Explanation of the interaction term Part 2. This two-part short video series briefly describes how to use and interpret interaction terms in the case of continuous variables.\nPresenter(s): Ben Lambert\nLevel: Beginner friendly\nLength: 4:53\n\n\n\n\n\n\n\n\n\nLink to video transcript here."
  },
  {
    "objectID": "chapter9.html#gene-environment-interaction-in-psychiatric-genetics",
    "href": "chapter9.html#gene-environment-interaction-in-psychiatric-genetics",
    "title": "Chapter 9: Advanced Topics",
    "section": "Gene environment interaction in psychiatric genetics",
    "text": "Gene environment interaction in psychiatric genetics\nGoals of this section:\n\n\nUnderstand how gene environment interactions can be detected in statistical population genetics.\nUnderstand how statistical power impacts detecting gene by environment interactions.\nUnderstand additive vs multiplicative interactions.\n\n\nTitle: Gene-environment interaction analysis\nDescription: MPG Primer: Introduction to gene-environment interactions in the context of statistical genetics. Introduces gene environment interactions, statistics, variant prioritization, variance-QTLs, and different types of interactive effects.\nPresenter(s): Kenny Westerman, Broad Institute\nLevel: Intermediate (content will be easier to those with a statistical genetics background)\nLength: 42:28\n\n\n\n\n\n\n\n\n\nLink to video transcript here."
  },
  {
    "objectID": "chapter8.html#magma",
    "href": "chapter8.html#magma",
    "title": "Chapter 8: Post-GWAS bioinformatics",
    "section": "MAGMA",
    "text": "MAGMA\nTitle: How do we go from genetic discoveries from GWAS/WGS/WES to mechanistic disease insight?\nDescription:\nPresenter(s): Danielle Posthuma\nLevel: Beginner friendly\nLength: 9:53\n\n\n\n\n\n\n\n\n\nLink to video transcript here."
  },
  {
    "objectID": "chapter8.html#fuma",
    "href": "chapter8.html#fuma",
    "title": "Chapter 8: Post-GWAS bioinformatics",
    "section": "FUMA",
    "text": "FUMA\nTitle: FUMA: Functional mapping and annotation of genetic associations\nDescription:\nPresenter(s): Kyoko Watanabe\nLevel: Beginner friendly\nLength: 14:19\n\n\n\n\n\n\n\n\n\nLink to video transcript here."
  },
  {
    "objectID": "software_MR.html",
    "href": "software_MR.html",
    "title": "Mendelian Randomization",
    "section": "",
    "text": "Title: Examine causality using Mendelian randomization\nDescription:\nPresenter(s): Jie Zheng\nLevel:\nLength: 9:26"
  },
  {
    "objectID": "chapter8.8_transcript.html",
    "href": "chapter8.8_transcript.html",
    "title": "Chapter 8.9: PheWAS (Video Transcript)",
    "section": "",
    "text": "PheWAS: Discovering gene-disease associations\nTitle: PheWAS: Discovering gene-disease associations\nPresenter(s): Dr. Joshua Denny, All of Us Research Program\nthank you very much it’s a pleasure to\nbe here as always and talking about just\nthe amazing stuff that’s happening here\nin the UK biobank\nalright great so to start with you know\nwe’ve talked a lot about genome-wide\nAssociation studies and sequencing and\nand we’ve talked to them about phenome\nwide Association studies as well that’s\ngoing to be the focus of my talk and\njust to orient us you know essentially\nwhat we’re doing is thinking about an\nindependent variable than exploring what\nphenotypes and the range of phenotypes\nthat are available and associated with\nthat really anchoring on the fact that\nwe have richly systematically phenotype\nsets of individuals such as in the UK\nbiobank and other electronic health\nrecord data sets which is where this\nstarted usually that’s based on things\nlike billing codes but I don’t want to\nlimit us there you can think about\nlaboratory values you could think about\nnatural language processing and things\nlike that as well most of its been based\non billing codes so to start with I want\nto orient us to a discovery study four\nout of the electronic medical records\nand genomics Network emerge in the US\nand it was five sites that work together\nfor a carefully validated phenotype that\nwe used codes labs medications natural\nlanguage processing to find these cases\nand manually validate who was a case for\nautoimmune presumptive autoimmune\nhypothyroidism and in control and we’d\nidentify the thyroid transcription\nfactor that was associated and\nreplicated this and then we did it took\ntook a variant the same variant that was\nfound here and did a fee wass on that\nvariant in a slightly larger population\nand you know that was unselected our a\nmuch larger population unselected for\nany given phenotype and hypothyroidism\nwas the the you know the highest\nassociate a phenotype there but we also\nhad some other thyroid diseases that\ncame up and things like atrial flutter\nas associated which we know that there’s\nyou know hypothyroid or less likely to\nmanifest with atrial flutter and so so\nthis gives opportunity to look at the\nperformance of these two methods\nand so you know on the left we have the\nschematic of the algorithm we use and\nthen we use these mappings called fee\nwass codes or fee codes and we usually\nsay there have to be two or more that\nmap into that fee code and you know you\ncan see the odds ratios are essentially\nthe same between the two approaches\nwithin our population of individuals\nwith an emerge\nyou know we identified more cases and\nwith the fee wast codes than we did with\nthe algorithm and so you know there’s\nmany approaches that fee Wasson just\ntalking about that you know said most\nused billing codes in the u.s. that’s\nbeen historically icd-9 with the\nclinical modifications and now icd-10\nafter 2015\nthere’s about 65,000 icd-10-cm codes and\non the right you can see some of the\nways this works so the fee codes have\nnumbers that kind of look like icd-9\ncodes but they’re actually not\nand and then what we do is we group you\nknow like codes now across icd-9 and\nicd-10 and icd-10-cm codes to you know a\ngiven fee code and and so all the type 1\ncodes come together which is not obvious\nfrom the icd-9 coding group system and\nthen each of those also define ranges of\ncontrol groups in addition to the feed\ngroupings or other groupings in the US\nthere’s some the AHRQ has released some\nsoftware that groups things into about\n300 diseases tree wasps is another thing\nyou can also use raw ICD codes for\ninstance that gives you a challenge\nacross of course mapping between icd-9\nand icd-10 and you can do many other\nthings survey data has been run across\nthe UK biobank and other things that\nI’ve talked about like the procedures so\nhere’s another example fee wasps driven\nby on EHR data looking at imputed HLA\ntypes into the two and four digit types\nof HLA and you can see you know quickly\nit highlights the fact that there are\ndifferent associations between class one\nand class two HLA alleles and what helps\nyou think about the range of\nassociations and and overall I think\nthere was a hundred or so significant\nassociations most of which were known\nand a few of new ones but what’s more\ninteresting as by doing in a single\npopulation you can actually look across\nthose phenotypes\nand and then look for pleiotropy and see\nyou know if you adjust in condition on\none and the other do you see you know\nthat they’re that they’re truly\nindependent associations and you can\nalso see we can a given HLA type that\nyou know one two HLA types that may put\nyou\nsimilarly at risk for rheumatoid\narthritis it may had differential effect\nfor your risk on type one diabetes for\ninstance and so so that is a tool that\nyou know you can rapidly explore using\nthis kind of technique an important\naspect is validating its efficacy and so\none of the early things we did using our\nicd-9 codes across the emerge was\nreplicating known associations in the G\nwasps catalog we found eighty-six\nphenotypes that were could be\nrepresented in the electronic health\nrecord and a number of snips about seven\nhundred and fifty overall snip phenotype\npairs overall we replicated in two\nhundred ten of them across the number of\ndisease classifications and sixty-six\npercent of those for which we were\nadequately powered in this population of\nthirteen thousand people as well as\nvying some novel associations on the top\nof which we replicated it also allows us\nto actually compare the effect size so\nhere you might you see something that\nyou would expect to see in that that the\neffect sizes from the the--why studies\nare typically a little bit lower than\nwhat’s in the G wasps catalogue now some\nof that’s probably due to the winners\ncurse but some of it’s also due to the\nphenotype being not quite as accurate\nand it helps you think about the ones\nthat where you have the most error so\nthe most common error and it was really\nI universally type one diabetes is often\nmiss code in fact ninety six percent of\nthe time we found type one diabetics had\ntype two diagnosis codes and so so you\nknow it made it it made it and the\nreverse is true fifty six percent of the\ntime so so it caused a lot of inaccuracy\nin the type 1 diabetes phenotype and we\nhad trouble replicating some of those\nsnips and we’ve actually instituted\nmethods to fix that problem and we can\nrecover those associations so here’s\nhere’s a way you can use fee wasps in\nconcert with eg wasps so we did a GYN\nemerge looking at longitudinal risk of\nheart of a CO disease on a statin\nand found variants that are tied to\nexpression of lipoprotein a were\nassociated with that outcome as a\nlongitudinal analysis and that risk is\nincreased for those that have you know\nkind of ideal cholesterol levels at less\nthan 70 so we looked at a loss of this\nlowest locus and you know as you’d\nexpect you see coronary atherosclerosis\nnear the top and and fortunately we see\nmost of the phenotypes are ones we would\nexpect to see which gets that the\nquestion of if you were to target this\nwith a medication you know what\npotential effects would you see and so\none of the things that’s interesting and\nwouldn’t have been on a radar screen is\nthis point over here which is not quite\nstatistically significance was lung\ncancer so you know this is a relatively\nsmall population from 13,000 people as\nit’s explored more maybe that will turn\nout to be true or not but it is a rapid\ntool for highlighting especially when\nyou think about the scale you could buy\na bank I mentioned mapping these to\nicd-10 and icd-10-cm codes just shows a\nlittle bit of a process and the\nvocabularies and systems that we used in\nthe process with some manual validation\nit is in what we call it’s still in beta\nform but you can see it covers about 90%\nof the billed ICD codes in the UK\nbiobank\nand amongst the 10% that aren’t there\nmost of those are not actual disease\ncodes only a small fraction those\nrepresent true disease codes and we did\nan evaluation at using our data with\nicd-9 and icd-10 codes in terms of fee\nwasps and you can see that the effect\nsizes between this pandas population was\nessentially the same for these two known\nassociations with that snip so I want to\ngive a few examples actually Kristen\nshowed this earlier doing a fee wass in\nthe UK biobank and just tons of\nAssociation associated with atrial\nfibrillation genetic risk score for afib\nand and I when they condition for the\nphenotypes the cardiovascular phenotypes\nessentially those associations went away\nand but it shows the power of a huge\npopulation to show lots of things you\nexpect to see here’s another one for\nsystolic blood pressure on a\na large G wasit was done across the\nmillion veteran program as well as the\nUK biobank and just a number number of\nassociations showing up with systolic\nblood pressure they also did with\ndiastolic blood pressure and pulse\npressure to show that some of these\nphenotypes overlap and you see\nphenotypes that are not exactly\nassociated with a cardiovascular disease\nin here coming out as well\nendocrine being one of the more common\nones here’s a resource Kristen also\ntalked about the sage approach - I’m use\nsaddle point approximation - I created\nan efficient and accurate way of\ncalculating these kind of results at\nscale to the UK biobank they have\nproduced a website where you can explore\nthose phenotypes calculated using the\nsame approaches for fee codes across the\nUK biobank and this just shows a\nparticular a-fib snip in that website\nand the URL there is there at the bottom\nso you know we’ve talked about this and\nand and in looking at individual\nphenotypes I want to spend the last few\nminutes talking about phenotypes in\nclusters and how we think of them so if\nyou think about a Mendelian disease is\nis a classic example that are often\nsyndromic presenting with many different\nfeatures and those features you know are\nwhat we may bill in the electronic\nmedical record as physicians but it\ndoesn’t necessarily represent you you\nknow the the disease is not always\nrecognized or you know may be recognized\nlater into the disease course as we you\nknow heard about earlier with\nhemochromatosis and so through the\nonline Mendelian inheritance and man\nresource and the linked human type human\nphenotype ontology you know we can go\nfrom a Mendelian disease to a list of\nfeatures of that disease which have a\nvocabulary behind them and then so so\nour lab mapped those hpo features to fee\ncodes so basically allowing you to\ntranslate oh men features into EHR\nphenotypes and then similar to a\npolygenic risk or you know creating a\nphenotype risk or that that looks\nsimilar in process so aggregating\nPhoenix\nby their weights I’m to produce the\nscore for individual and essentially you\ncan crank this out across anything for\nwhich you have a map and do it at scale\nso let’s look at the Cystic Fibrosis a\nnumber of features from omen and each of\nthose is mapped to a human phenotype\nontology code and so I’m using our fee\ncode ontology of around 1,800 phenotypes\nand you can map the ones that line up\nfairly well to the CF they’re not all\nexact matches some are better matches\nand others and then some that we don’t\nhave in the HR which you know we’re\nfamiliar with and so let’s play that out\non a couple individuals hypothetical\ndifferent conditions I mentioned they’re\nweighted so features like bronchiectasis\nhave a higher weight than features like\nasthma and so when you go across this\nyou know individuals get a different\nscore and what you find is you can\nseparate case in the controls 460\nfibrosis\nyou know just using the features of\ndisease so we’re not using the disease\nlabel but in this example we use\nmanually validated at cases versus\ncontrols who don’t have any evidence of\nthe disease in the in the text record\nand we see a very significant result and\nwe’ve actually done this for 15 other\ndiseases now and in in every case except\nfor one we’ve seen very strong\nseparation between cases and controls\nthe one exception is phenylketonuria\nwhich as you know in the u.s. is on\nessentially every newborn screening test\nand if you avoid and feel an exposure\nyou don’t actually see the you know the\nmanifestations of the disease so it sort\nof gives you a test of the effectiveness\nand newborn screening in removing the\nfeatures of Z’s in the population\nbecause they generally do not have\nelevated scores so we turn this on a\npopulation of 21,000 people that had\neczema rate genotyping and looked at\n6000 variants that were rare at at 1%\nlevel or less and we found 18\nsignificant Association most of which\nwere novel and we were able to change\nthe ACMG clinical interpretations for\neight of these variants towards likely\npathogenic or pathogenic and so so this\nusing our population as a paradigm that\nI think can be explored with larger rich\nphenotype populations such as what is in\nthe UK biobank so I want to end and with\nrepresent a recognition of some of the\nmany people contributing to this work\nand the middle row is probably the most\nimportant row as as the folks actually\ndoing the work thank you very much\n[Applause]\n\n\n\nPheWAS and EHR\nTitle: Using EHR-based genomic approaches to understand the relationship between mental and physical health\nPresenter(s): Dr. Lea Davis, Vanderbilt University\nIntro\nand the bio bank that’s attached to them at Vanderbilt and so this opened up part\nof a newer area of investigation for me something that I’ve been interested in a long time but but hadn’t had the\nresources to investigate and so that’s basically using EHR based genomic\napproaches to try to better understand the relationship between mental health and physical health so that’s kind of\nthe story that I’m going to be talking to you about today so if I can advance my slides oh there we go okay so so this\nrelationship between mental health and physical health is kind of well-known\nthat it’s important and in one of the I think nice summary statements about the\nimportance of this comes from the World Health Organization on their website they state that there is no health without mental health and it’s been\ndocumented for some time that poor mental health is a risk factor for a number of physical conditions particularly chronic conditions and so\npeople with severe psychiatric illness are at much higher risk of experiencing\nchronic physical conditions and vice versa people with chronic physical conditions\nare also at risk of defining mental health and so I I’ve actually come to\nPeople with severe psychiatric illness experience health disparities\nthink about this much like a disparity in fact that people with severe\npsychiatric illness experience these kinds of healthcare disparities there we\ngo so this is really sort of punctuated by the observation that the lifespan for\npeople with severe psychiatric illness or schizophrenia bipolar disorder but also neuropsychiatric disorders like\nautism spectrum disorders and cognitive impairment on average the life span is\nshortened by 10 to 12 and a half years and it’s thought that this is due to\nmany possible reasons one being a difference in the access to health care\nso related to employment and insurance issues here in the states and that’s\ncertainly been shown to play a role particularly for for psychiatric illnesses such as\nschizophrenia where there’s a much higher rate of homelessness but also\nissues related to difficulty in expressing pain and so for example for\npeople who are non-verbal there’s also some suggestion that there may be\naltered interoception or among people with neurodevelopmental disorders and so\nif for anybody who’s not familiar with that term interoception is kind of your internal perception of pain discomfort\nhunger thirst even sort of sensing your own heartbeat and and that appears to be\nsomewhat altered in people with developmental disorders but it’s also\ncertainly possible well that there’s some increased genetic or biological risk that’s actually related to the\ngenetic risk for for the psychiatric disorder itself so imagine sort of\npleiotropic mechanisms and then of course increase exposure to environmental risk factors poor diet\nlack of shelter lack of access to medication and those kinds of things and\nI think it’s also important to highlight that this is really a an under studied area for a couple of primary reasons\nactually one is that until fairly recently a lot of people with severe psychiatric illness and\nneurodevelopmental disorders received most of their health care in an institutionalized setting whether that\nwas a psychiatric institution or or prison or some other kind of group\ninstitutionalized setting that was separated from community-based health\ncare and so there’s this population has been under studied in the kind of\nepidemiological and community-based studies and then there’s also as I’m\nsure everybody on this call is probably well aware has been kind of a historical\nseparation between psychiatry and the rest of Medicine and so it ends up being functional separation\nso that refers to the fact that you know\noften the mental health facilities are separate from the primary care\nfacilities and you might have like a community-based mental health clinic and a community-based primary care clinic\nand even at a hospital there’s often a separate psychiatric hospital you know\nin a completely different building from the the general hospital so there’s this\nfunctional and cultural and financial separation of psychiatry and medicine\nthat I think has also contributed to this kind of healthcare disparity and in\nthe lack of research about it okay and\nso like many of us in this field this is also a very an area of personal interest\nfor me so that this is a picture of me with my son Dylan Dylan is 24 years old\nhe has autism and severe cognitive impairment he’s nonverbal he requires\n24-hour support staff and he lives in a community integrated group home and so\neven with a whole team of people who are therapists on Dylan’s health we still\nreally struggle to get proper healthcare for him and so you know this is I think\nan issue that it’s important to me personally as it is to many families of\nindividuals with developmental disorders and our psychiatric disorders who are\ngetting older and aging and we don’t really know what chronic health\nconditions they may be at risk for so\nwhile on the whole this has been a you know an under studied area there have\nbeen a few kind of areas of focus particularly related to cardiovascular\nand cerebrovascular disease and in recent years there have been some really\nlarge meta analyses looking at the prevalence and cumulative incidence of\ncardiovascular disease in people with severe psychiatric illness primarily\nbipolar disorder schizophrenia and major depression and so this is actually a\nrecent paper with the meta-analysis of I think 92 studies the had really\nimpressive sample size so over three million patients with one of those three\ndisorders and and over a hundred million controls where they investigated the the\nincreased risk of both cardiovascular disease and cerebrovascular disease in these populations and so basically the\nthe take-home is that there is significantly increased risk for both disorder for both circular and\ncardiovascular disease and that this risk persists even after adjusting for\nsome of the health behaviors but that may be related to to the incidence of\ndisease so things like smoking or poor diet or BMI and so based on these kinds\nof epidemiological studies that are just now starting to be published we also are\ninterested in asking the same kinds of questions and better understanding the\nrelationship between men of mental and physical health using our EHR so in\nparticular we want to understand if there is some shared biology if it’s the\ncase that for mental health causes for physical health or that for physical health causes for mental health and\nreally our model is that it’s it’s gonna be all of the above but understanding\nyou know what the primary risk factors are for for each type of chronic disease and\nwhere their shared biology yeah I think this is going to be an important area of research another question that we have\nis do these relationships between mental health and physical health transcends our diagnostic boundaries is it really\nbecause that it’s just people with severe psychiatric illness or diagnoseable more developmental\ndisorders that are who are at risk or is it the case that risk is continuous and\nand that actually across the entire spectrum of genetic risk for these traits that there’s also increased risk\nfor these chronic health conditions and\nthen are there particular health conditions for which people with developmental disabilities and severe\npsychiatric illness are at high risk there’s been a lot of work done on cardiovascular disease and cerebral\nvascular disease but and and I think actually a fair bit of work on type 2\ndiabetes but outside of those kind of primary chronic health conditions um there’s there’s really not been very\nmuch and so we really want to look genome-wide at these relationships and\nthen finally the wanna see if this can help us understand the best point of\nintervention and and of course to identify any interventions that are typically used in healthy populations\nthat that may cause particular problems in patients with severe psychiatric\nillness or developmental disorders so\nA few of our research questions\nmaybe I’ll pause there for just a minute if there are any questions\nand don’t forget you to unmute yourself if you have one and I mean one short\nquestion - to move to the first slide so this is the shortening of the lifespan\nfor severe psychiatric diseases and so\nthe one thing that that was from my point was not missing on this list is\nlike suicide like big toe to to the psychiatric disease here is not the main\nno it isn’t actually um it’s definitely a contributory cause but but actually\nthe primary causes chronic health condition so suicide is definitely a\nincreased risk in this population absolutely um but it doesn’t account so\nyou mentioned already this like like being facilities being also prisons etc\nso and is there other comparisons\nbetween for example also like the US and Europe are the comparisons between\nCaucasians and other industries have\nhonestly the literature is pretty scant so I’m I’m not sure if that’s really\nbeen investigated sort of systematically and with sufficient sample size um it\nreally hasn’t been I think most of the papers looking at that these\nassociations have been published you know from the 90s and on so there’s not\nthe kind of you know eighty year body of literature like there is for you know\ncardiovascular disease and in healthy populations so I guess that yeah the\nshort answer is I’m I’m not sure that it’s really been systematically investigated\ntwo others have some questions if not\nthey said they’d like two or three on the car that are not muted so if again\nso this is question if you own the car not muted please mute yourselves like right contractors um so we believe that\nEHR based genomic approaches are actually a great way of investigating\nseveral of these questions that we have so the EHR the electronic health record\nallows us to investigate the relationships between phenotypes and the\nbiobank that we have also facilitates investigation of the genetic relationships between theatre phenotypes\nand so we can then also compare the the genetic correlations to the phenotypic correlation better understand you know\nwhat is there where there are the environmental risk factors that contribute to the matific relationships\nand genetic risk factors that contribute to the genetic relationships between traits and it also allows us to utilize\nthe polygenic architecture of these complex traits and develop quantitative models so that we’re not necessarily\nrelying relying completely on diagnostic categories but we can look at how\ngenetic risk as a quantitative trait is related to risk for various phenotypes\nso maybe some of you have heard me talk about the Vanderbilt EHR and the bio\nbank but just in case you haven’t it can be kind of thought of as three entities\nwe have what we refer to as the synthetic derivative which is this\nde-identified and continuously updated mirror image of the EHR or EMR that as\nof now has a little over 2.8 million individuals if we look across just that\nset of 2.8 million individuals the median length of the EHRs is only about\na year even though the EHR has been in existence now for 20 years and part of\nthe reason for this is that we’re a tertiary care center so we get people coming in from all over the state of Tennessee and Kentucky and\nand sort of all over the southeast particularly you know if they have a\nthey if they have the need to come see a special specialist in a specialty care\nclinic um and so we end up drawing pretty sick people from all across the\nstate so in comparison to like the UK biobank that has maybe a healthier on\naverage population I think at Vanderbilt we have a sicker on average population\nthat said there is also a population of people who make Vanderbilt their medical\nhome so to speak and they get most of their primary care at the end result as\nwell and so our biobank which consists\nof DNA samples that have been collected from just routine clinical blood draws\nis enriched for that population of people but that actually make Vanderbilt\ntheir medical home and so this is illustrated by the median length of the\nEHR or bioview subjects which is about 10 years and so we have now somewhere\naround 270,000 DNA samples that have been collected and a little over 50,000\nof those subjects have been genotyped with some kind of G loss platform and on\naverage the age of those subjects is around 58 years old but we are trying to\ngenotype and accumulate pediatric care goals of all so any quick questions\nabout the structure of the bioview biobank or EHR that’s central to the\nrest of the so this is like all different kinds of diseases right all\ncases of these are they are also healthy subjects here or is this all cases well\nI mean yeah so there are people who don’t have you know chronic diseases\ncertainly there are people who come in who’s just for you know routine health care and they’re in in the bio bank as well so\nthere’s no ascertain meant at the bio bank level that said there it’s a it’s a\nhospital ascertained population so so I think probably most people will be you\nknow a case for something okay and so how many of these $50,000 psychiatric\npatients how many of the 50,000 have\npsychiatric codes I’m not exactly sure\nyeah I actually don’t have access yet to all 50,000 samples it’s it’s not an it’s\nnot enriched for psychiatric cards that but but we’ve said you know the\ngenotyping data is actually it’s a large project eventually we’ll have a hundred thousand people genotype and so the the\ndata is coming through in waves and we actually were involved in pushing\nseveral of the psychiatric diagnoses through but we haven’t gotten those\ngenotype samples yet so um so they’re right yeah we know where are they being\nyou know type here no on the mega\nplatform okay okay so as I mentioned\nbefore we were very interested in taking advantage of the fact that most complex traits have a complex genetic\narchitecture with a measurable polygenic component and so we can look at how the\npolygenic risk for all kinds of psychiatric disorders is also related to\ndisease status for other chronic health conditions\nso I think probably everybody on the call is familiar with this approach but just in case one of the methods that\nwe’re using is basically to calculate koala genetic risk scores for everybody in our bio Bank so using some kind of\nlarge discovery gos and taking the effect sizes from from that gos to\ncreate a linear weighted sum of the number of risk alleles and then looking\nat how those polygenic risk scores segregate cases from controls across a\nnumber of different phenotypes genome-wide so we started with actually\ninvestigating this and both psychiatric disorders and in some of the previously\npublished chronic health conditions and so I’m starting with showing you the\ncoronary artery disease polygenic risk scores that are significantly associated\nwith the EHR definition for coronary artery disease so here we took the beta\nweights from the cardiogram plus C for D consortium study that had about sixty\nthousand cases and 123 thousand controls and applied it to a small subset of our\nmega data target sample just to make sure that we that indeed coronary artery\ndisease in carefully ascertained research samples was related to the HRV\ndiagnosis for coronary artery disease and so this included covariates median\nage across the EAP our sexes the top 10 pcs and actually it’s not listed here but also genotyping batch and so you can\nsee that our apologetic risk score accounted for almost 3 percent of the variants and CID diagnosis within our\nEHR so this was encouraging and we actually applied this model also to the\nlipid traits that have been studied by the global lipids consortium so HDL LDL\ntriglycerides and try to also model the relationship the known relationships\nbetween those risk factors and coronary artery disease oops\nso I’m going to take just a short methodological detour here because one\nof the other things that’s pointer my attention in working with the Department\nfor biomedical informatics you know often when I present telegenic analyses\nI get the question from people could be a lot of machine learning where is your\nfeature selection step right so in your genome-wide Association study training\nthe the weights but then how do you know which snips to include in your model and\nthe target data and typically really what we often do is just work across a\nnumber of thresholds and you know and see how the r-squared might change if we\nin just the genome-wide significant snips or you know everything at a p-value of less than 0.5 or less than\none and so kind of investigator cost different thresholds but we wondered really how well we would do if we\nactually took a training set and use that to select a threshold for inclusion\nfor including snips in the apologetic risk score and then applied it to a validation set because this is actually\nI think you know ultimately what we would do if we were they’re really just\nkind of out of curiosity we you know look at that now but we have our\nLipid traits and Coronary Artery Disease\ndiscovery gos phenotype in the first column and the target phenotype and the\nsecond column so the discovery gos either as the global the potential for the HDL LDL and triglycerides and you\ncan see my\nthis one person that is not muted or if it’s Yulia sure no it’s not me I heard\nit too okay if we see you know me to see\nyour cars is okay all right great and so right so for for these lipid trades this was people and then our\ntarget phenotypes were all measured in the bio Bank and so we we set up a\ntraining sample of about 9,000 people and a validation sample oh yeah alright\nso we had a training sample of about 9,000 people a validation sample of about 16,000 people and and so you can\nsee the key value threshold that was the best fit and the training sample the number of snips that was included in\nthat best friend we’re truly r-squared value or the proportion of variance I have to say and and the p-value for that\nbecause I have no experience so I’m not\nsure I mean they they seem to be like they don’t have the speaker on kro they don’t even listen so it’s a little bit\nannoying so please everybody again here so if you hear us please mute yourself\nor just if you don’t listen if you’re not listening just leave the call that’s\nfine as well okay so we used the same in\nfresh hold that was identified in the training symbol to define the PRS in the\nvalidation sample there’s a you’ll notice there’s a different number of snips included and we think that this is\nbecause the training sample was a me platform and the validation sample was\nthe mega platform even though they were both imputed to the same reference panel\nand they’re both European populations the mega sample seems to have better\noverlap with the original discover AG loss and so the are squirts for you know\nboth the training and validation sample particularly for the lipid traits are really pretty impressive and and\nactually start to approach the Senate based heritability or in particular for\nHDL and so I put an asterisk here for\nthe p-value fold in the training sample was actually the same as the best fit\np-value threshold in the validation sample so I can pause there for any\nquestion we were actually kind of surprised that there were any traits where the best of it men told was the\nsame across multiple samples and so I think this is actually really\ninteresting and potentially indicating that we’re starting to approach kind of narcs and them information or an LDL in\nour jaws\nright in the interest of time I’ll just move on so we took the same best-fit\nthresholds and genetics course you’re\nlooking at here is a Manhattan plot from us yaws where along the x-axis we have\nkind of classified that the phenotypes and along the y-axis the minus log time\nthat the p-value for the PRS twin direction of the arrow indicates the\ndirection of risk so if there were higher polygenic risk scores among cases\nthis up arrow control to see a Down and so this view else was for LDL sorry for\nHDL and so we see a protective effect of yell on type 2 diabetes and this has\nbeen previously shown this was a good concept and we did a similar type of\nanalysis for our LDL polygenic risk score where again we see a strong\nassociation with this lipid yneouws and\nwith coronary artery disease and so these are also known associations\ninterestingly so I don’t have it here and in this presentation but but there’s\na the phenotypic correlations between a chair between LDL and the diseases\ntested in our genome wide analysis were much stronger so we saw lots of\ndifferent phenotypes associated with the measured LDL but these are the only phenotypes that are associated with\ngenetically predicted LDL so getting\ninto our primary interest which is the genetic risk for psychiatric disorders\nassociated with diseases across the phenome we asked\nwhether genetic risk for MDD was associated with heart disease codes and and actually all phenome wide codes and\nso he took the most recent 2018 DD\nmeta-analysis results that are posted on the website and did the same kind of thing where we calculated a genetic risk\nscore and all of our 16,000 people in the mega example and we see really\nstrong associations with mood disorders and depression which we expect associations with bipolar disorder and\nanxiety disorders but then we also see some of these party avascular rates\nrising in significance as well and so we see an association with nonspecific\nchest pain which is really kind of a catch-all you know as it states\nnonspecific code but was definitely interesting to us and so we wanted to\nkind of investigate this a little further and we asked whether we saw some\nDo we see similar associations for related mental health traits?\nsimilar associations for related mental health traits so related to major depression is also individual’s\nperception of loneliness and this is something that we’ve been working on with a consortium group now a Palmer\nDorit blooms fund and myself and others would call ourselves the lonely\nconsortium and we’ve amassed almost 500,000 samples and I think those of you on the MDD call have heard some reports\nof this already and so we looked at the relationship between polygenic risk scores for loneliness and phenom wide\nassociations as well and so this is our Manhattan plot for our loneliness\nseawalker sorry gee wasps and you can see also that we are observing some\nenrichment of gene expression for our loneliness low sigh within\ntissues that we expect to see some enrichment so this is in brain tissues in particular and we also have observed\nseveral genetic correlations and I just have a few posted here but genetic correlations with phenotypes that are\nassociated with poor mental health including general tiredness lower\nself-rated health and coronary artery disease and one of the reasons that we\nwere really interested in and actually looking at the genetic relationship between loneliness in these other traits\nis that loneliness has in and of itself and identified as a risk factor for\nincreased morbidity and mortality and and so there have been several kind of\nepidemiological studies looking at the temporal relationship between a person’s self-reported loneliness and later\nhealth consequences and so often the causal mechanism is inferred from that\ntemporal relationship but I think that’s a little tricky because of course you know by the time I’m somebody actually\nhas a heart attack or has diagnosis of coronary artery disease that that\ndisease has been developing for for many years and so having that temporal\nrelationship is not always an indicator of a cause-and-effect relationship so a\ngraduate student in my lab julia c-loc has kind of led the effort on this work\nwell she looked at again the innate propensity to loneliness apologetic risk scores for loneliness to see whether\nthey were associated also with poor health outcomes and so in this case because we didn’t have loneliness measured anywhere in our biobank we\nweren’t able to do this kind of foot training on a separate sample for a best fit so we just took everything at a\np-value of less than one and even with that we actually see a strong association with mood disorders and\ndepression with tobacco use disorders but then also you can see with a whole host of coronary artery disease type\nphenotypes and so this is also getting at that question that we had about the relationship between sort\nof dimensional traits and and diagnosis itself so again these are not\nnecessarily so the sample is not enriched for major depression and we’re\nnot looking at the diagnosis of major depression our diagnosis of you know chronic loneliness or we’re just looking\nat the genetic risk factors so we\nBMI and MDD diagnosis influence associations\nactually one of the I think benefits of having this type of data as opposed to just looking at genetic correlations\nwith summary stats is that we can do a lot of conditional and sensitivity analyses to try to tease out some of the\nrelationships and so we looked at how\nBMI and diagnosis of major depression might influence associations and so when\nwe adjust for BMI definitely we see an attenuation of the signal for coronary\nartery disease although we do still see some of these phenotypes rising above\nyou know might significance and we also see again a strong association with the\nyou know disorders depression and tobacco use disorders when we adjust for the diagnosis of major depression we we\nstill observe a significant association with obesity and and even though our\ncoronary artery disease codes kind of fall below phenome wide significance they’re still of course enriched among\nour results and so this was actually a really important analysis because it’s\nalso known well-known that after having a coronary event people become much more\nsusceptible to a major depression episode and so so he wanted to make sure\nthat our associations weren’t completely driven by the major depression that may\nbe diagnosed after the fact and while we do see definitely an attenuation of the\nsignal I think we’re think resample sighs these associations will probably still remain\nphenome aren’t significant we also were interested in seeing if\nDifferences between males and females?\nthere was a difference between males and females and so we stratified our few us\nsample and and looked separately and while we do see some qualitative\ndifferences so in females definitely the depression and mood disorder codes remain phenome like significant and in\nmales the mi and atherosclerosis codes\nremain feel might significant these differences between them were actually not statistically significant so I think\nthis is really just reflecting the fact that more males have myocardial infarctions and more females\nare diagnosed with depression Stephan\nhow am i doing on time good 10 to 12\nminutes okay all right okay great so so\nthere’s since we were really primarily interested in following up the associations between polygenic risk for\nloneliness and the coronary artery disease codes we we focused in on the males and and looked there at\nAssociation after adjusting for again MDD or BMI and and so again we see that\nmyocardial infarction remains significantly associated after adjusting\nfor either BMI or MDD and actually also remains associated after adjusting for\nboth MDD and BMI and so this now I think\nis a really nice substrate for a\nMendelian randomization analysis as well\nPart 2: Strategy to screen for biomarkers in lab data\nso the the second part here is is actually just kind of an introduction to some of this the work that we have\nplanned and so I just wanted to sort of briefly go over it and and invite any\nideas or collaborations of people are particularly interested in certain\nbiomarkers for phenotypes that they that they’re studying so within the EHR we\nhave access also to actually thousands of labs but many of those are kind of\nunique special snowflakes so they may only have been ordered on you know a small handful of patients so when we\nstart looking at labs that have a larger sample size it turns out that we have\nabout 350 labs with over a thousand individuals and actually I should say\nthat all of these labs have at least a thousand observations so if we say that\nyou know we’ve got 350 labs with at least a thousand individuals at least a\nthousand observations then everybody has been measured at least once but then\nwe’ve got a larger number of labs where we have a smaller number of individuals but a larger number of observations per\nindividual and so this data is also you know really rich for looking at\nlongitudinal associations between the relationship with you know psychiatric\nillness and and changes over time in various biomarkers so in total we’ve got\nabout 500 labs with at least a hundred people measured at least a thousand\nobservations so this work has sorry this\nClinical lab data\ndata source really has not been utilized very much in the EHR space partially\nbecause it’s really messy data so it’s taken us close to two years actually to to really carefully QC all of the the\nlabs that that had sufficient sample size it’s\nalso challenging because again it’s a hospital population and so you know a lot of times the the labs are being\ndrawn because somebody is actually sick and so this can be a challenge to interpretation but at the same time the\nfact that we have the entire EHR allows us to investigate the relationship between diagnosis and and changes in lab\nvalues some of the benefits of using this data are that we have a really\nlarge sample size that is really rich for clinical data so it’s longitudinal\nwe’ve got over 20 years worth of data and we can as I mentioned a couple of\ntimes now test the effect of many possible mediating and moderating variables and then we can also go into\nthe charts themselves and validate by chart with you so developing tools to\nboth QC and visualize this data has been a really tremendous effort by Peter\nStraub a programmer in my lab so he’s developed a shiny app and a whole set of\ntools that actually we’re planning on making publicly available and and will\nbe allowed we’ll be able to make the summary data for all of these different labs available so that we can look at\nhow they vary by age by sex by by race\nand and that groups with bio banks and\nand labs you know locally can also download these tools and and apply them to their data\nand this is sort of the concept map for what we would eventually like to do so\nthis is you know that we would take the you know beta values from geo so many\npsychiatric illnesses calculate polygenic risk scores across everybody\nand bio view and then look at how those risk scores are related to median values\nin hundreds of routinely collected labs and as I mentioned before we can\nso use longitudinal models to see how they’re related to change in lab values\nover time so we’ve started doing this a\nlittle bit for some of these traits that I’ve been talking about so far so this\nis just focusing in again on HDL LDL and triglycerides and looking at how the\ngenetic risk scores for coronary artery disease loneliness or major depression\nis associated with median lab values and\nso in each of these plots we’re looking at the r-squared values the proportion\nof variance explained in HDL LDL and triglycerides on the y axis and then the\nDiscovery gos p-value threshold on the x axis so that we can see kind of across\nthe board how well do you know MDD risk scores CA D risk scores and loneliness\nrisk scores predict HDL LDL and triglycerides and so interestingly we\nsee that for HDL our loneliness risk scores actually tend to outperform our\nCA D risk scores and for LDL you know it\nlooks like there’s some difference but I don’t think these are actually really meaningful because below 0.1% variance\nit’s not not a significant association with the between the risk score it’s\noften and the median value and then in our triglyceride analysis we do see the\nthe CAG risk score outperforming the loneliness risk score which is kind of what we would think about maybe\nintuitively expect but the loneliness risk score does actually significantly\npredict triglyceride levels as well\nso like I’ve said a couple of times we\nalso are interested in testing for the mediating effect of these quantitative\ntraits and the moderating effects of sex and medications and and other diagnoses\non these relationships as well so this is kind of our an example of a general\nmodel that we’re interested in testing the relationship between polygenic viability and disease diagnosis that may\nbe again mediated through quantitative traits so kind of our future interests\nare to mine this lab data for biomarkers for our psychiatric disorders using genetic risk scores from them are\npublicly available jaws and I should say that you know we don’t think it’s likely\nthat we will identify a biomarker for you know major depression out of all of\nthese routinely collected labs but that actually we may identify by several quantitative traits that together may be\npredictive of an individual’s genetic twist or form a major depression or\nschizophrenia for example we also are really excited\nabout using bi-directional Mendelian randomization analyses to better understand some of these possible causal\nmechanisms and to compare the phenotypic correlations with the genetic\ncorrelations to try to identify comorbidities that may actually be more of a consequence of environment then of\ngenetic causes which you know we’re also equally interested in understanding so I\nAcknowledgements\nthink that is it I’ll wrap up with an acknowledgement of everybody in my lab a\nreally great group of people to work with and wonderful students and the work\nof the lonely consortium again this has been like a just a phenomenal\ncollaboration that I think has yielded some really interesting results and so I\nthink with that all and and hopefully there’s some time the questions thanks so much the other\nfabulous presentations really it’s a lot of data that’s coming - therefore - it’s\nreally exciting to see that you’re diving into that with full speed here um\nso is there I mean we have like around 30 people on the call we probably have\nquestions and they don’t know how to unmute themselves or I still have\nquestions but I don’t want to always step in did I hear somebody\nokay so then here I still have likes like two or three night more short\nquestions I’m on the technical side so so two things here so first of all and these are you a because you’re speaking\nabout to do know stuff but also is there\nchance to actually give them the fact you might have seen some people with\nespecially high score screen your result is their chance to get in contact with\nthem again on the next visit or so or is this something that’s so enormous there’s no chance to to actually get in\ncontact with these individuals no they’re yeah it’s it’s a completely de-identified data set yeah there is\nalso a what’s called the research derivative so we work with in the synthetic derivative because we’re also\nworking with the genetic data if we if we were to restrict ourselves just to\nthe phenotype associations we could apply for access to the research derivative which is an identified data\nset but even then I think there are rules I don’t know that that we could\nactually recontact patients and and there’s no although I don’t understand\nit why this is the case you can’t work with genetic data in the identified\nenvironment well I think most of the bio\nbanks within the US are de-identified that that issue about not working with\ngenetic data in the context of an identified environment that might be specific to Vanderbilt I’m not actually\nsure what the what the you know rules around that where those\ncame from very interesting and a little\nbit sad of course because that would be the and so the other issue a little bit\nmore technical side I mean like special project with scores are actively sensitive to so how is there\nany chance for you or how do you want these dealing with this issue like yeah\nyou know that it’s a good question we’ve been thinking about this a lot so so\nthere’s a method I can’t remember now the first author’s name but I think it came from from Peter Fisher’s group to\nin the context that we’re using where we have some individual level data and geo\nsummary statistics they’ve they’ve published an approach to actually first\njust check to see if there is yeah what the probability of overlap actually is\neverything up until now have to be honest is based on just the knowledge\nthat no investigators have contributed actively from Vanderbilt to these G wast\nbut I mean we haven’t yet done the careful checking to make sure that\nthere’s no sample overlap but that is on our list yeah it’s it’s really something\nthat I think we have to just actually baked into the pipeline and and we’re\nalso part of a collaboration with other\nbio bank sites through the emerge Network where we’re doing a similar kind\nof thing you know replicating these polygenic few associations in other bio\nbanks and so so we’re really trying to develop a you know a robust pipeline for\nfor doing that across several EHR environments and and one of the things\nwe need to make sure to build into the pipeline is this kind of checking\nespecially when did you see it’s good for for just testing reasons doesn’t try to\nsee if we see some unexpected overlap there and we don’t and this gives you a\nlittle bit more security that yes like 200 cases coming up that we shared from\nsomebody else to PTC something that you need to be super careful at least did\nyou see everything in place we could test that we couldn’t test done even without you know we have to check some\nmethod where we can without sharing the genotype so it’s something that could be\ndone on a relatively low level so so in\na genome-wide type of analysis how would you then basically like for each fee\nwass category conduct a G was--and check all of those some stat relationships\nit’s also possible that we have say somebody with schizophrenia who doesn’t\nactually have the diagnosis in our electronic health record right they have\na diagnosis of type 2 diabetes in our health record but maybe they’ve been inclusive in a geo somewhere else so so\nI think you know I mean first we like\nthe check on the summary statistics level works for like a significant\noverlap for for if you have the same field I think you’re raising exactly\nright questions that I think if you want to be really sure you need to really\ntest it on a genotype and the burning at\nleast you can do this like with a couple of these consortiums you at least\nmeaning if there’s something unexpected or something expected and I think this gives already some support here and\nmaybe when you actually see the summary statistics level when you see actually\nsome significant overlap there Fineman somewhere then you you probably\nwant to go there ask these guys for can we actually test this on genotype 11 who are these individuals this is actually\ncorrect is this I think we can test this on a couple of different levels yeah yeah that’s that’s a good idea\nand that’s basically what we wanted to start with was just to see first like do we see any evidence of overlap and then\nif we do you know how should we go about dealing with it I mean to be honest the\noverlap cannot be really really big because then it’s so quickly we’ll see\nwith the exploding a square values that’s really impressive even if like if\nyou have a small data set like individuals all of these it’s just 300\ncases red ones are exploding so if there\nis something little small thing can have\nthis small impact on these on these on these that seemed to fit so well right\nyeah so do you have an intuition of like what you expect if there’s an overlap of\nsay you know a dozen people you know like a really small overlap no this is\nalways something that I really wanted to test yeah somebody wants to go for it please no I don’t have that I have these\nexperiences that it’s like a couple hundred overlap it’s really it’s really\nstrong so much surprise surprisingly strong really that’s why I’m always\ncareful about this it’s not that you see dying are spread this is just jumping a little bit around it’s really exploding\nand so that’s why I think my hunch is that even like it does know 20 could\nhave a small I mean some of the p-values I really like just passing the spin of white\ntrash out there right so that’s why especially for these cases yeah\nokay but that’s the cheapest keep this ominous this discussion and I’m very\nhappy to it to be yeah and and so for\nyou know anybody on the call who’s interested in looking at particular phenotypes and we’ve we’ve now see us\napologetic scores for all of the publicly available summary stats from PG\nC and and so you know anybody who’s like\ninterested in talking more about what schizophrenia looks like or what you know bipolar looks like or whatever\nplease feel free to get in touch with me because I would love to kind of have you know more folks to discuss these with\nI’m sure the people will be more grave\nokay so we are actually over the hour so I will have to close to the call now thanks so much for presenting yeah\nthanks and this will continue here so\nall the best"
  },
  {
    "objectID": "chapter2.4_transcript.html",
    "href": "chapter2.4_transcript.html",
    "title": "Chapter 2.4: Linkage Disequilibrium (Video Transcript)",
    "section": "",
    "text": "What is linkage disequilibrium?\nTitle: What is linkage disequilibrium?\nDescription:\nPresenter(s): Gábor Mészáros, PhD\nhello everyone welcome back to the genomics boot camp to a series about lincoln this equilibrium in this video we just start to talk about this genomic phenomenon so therefore the question what it is exactly so what is linkage diskequilibrium and you will know it from this video before we start let’s activate some of our prior knowledge that you might know from this channel or from other sources\nand the two most important issues to highlight here is one that the single nucleotide polymorphisms or snips exist these are some markers that are widely used and currently in my opinion the most important marker types as for the genomics tens of thousands of such snips are being genotyped in a cost effective way on the entire genome so basically it is a very good way to get to know something about the genomes themselves and the other bit of information that is important here is the existence of the recombinations or recombination events that are of major biological importance this means that the nucleotides on our genome are not being inherited independently but in a form of shorter or longer genomic segments from the paternal and maternal side\nMendels Law\nsuch statement about the non-independent inheritance is in conflict with the mendel’s law of independent assortment which says genes do not influence each other with regard to the sorting of alleles into genes and every possible combination of alice for every gene is equally likely to occur mendel’s law is of course valid when the genes or the parts of the genome are far from each other or for example on separate chromosomes but as you will see it’s not valid when these genes or sneaps or parts of the genome are very close to each other\nLinkage Equilibrium\nthe next few slides i took from the presentation of professor henner simiano from university of getting in to demonstrate what happens in such occasions and what in fact the linkages equilibrium is so for the sake of simplicity let’s have an individual with two alleles and we ensure that this individual is entirely heterozygous then we mate it with another another individual which is entirely homozygous for these alleles a and b and when the law of independent assortment is valid then we get the four possible genotypes and if our sample is big enough then we have all these possible genotypes with a 25 of probability so in the our population we would see that all four genotypes appear in a proportion of 25 in some cases however we might notice that these four genotypes are in fact not appearing in an equal proportion but very very differently from each other in this case the alias denoted by the capital letters a and b seem to appear much more frequently together and also the alleles denoted by the lower case a and b are also appear to occur much more frequently with each other so in this case and 45 in comparison to a situation when the lower case a for example and a higher case b would be together in a single individual the reason for this is of course recombination so we have a non-recombinant gametes on the one side and the recombined gametes on the other side so in other words again in case a recombination happened between these two loci then we have a different occurrence for these for these gametes the degree of recombinations is measured by the recombination rate so\nLinkage Disequilibrium\nwhat we see is a departure from this equal distribution denoted as a linkage disequilibrium or ld which is the very frequently used abbreviation i wanted to underline that ld is in fact a parameter of the population so you need more individuals and in the best case scenario a large number of individuals to determine the ld between two loci in a population it is in fact a non-random association between the loci within this population which could be measured so we can tell that the two loci is strongly linked or weakly linked or not linked at all together in other words the ld tells us something about the strength of the information if we see an allele in a certain locus and what it can tell us about the occurrence of an other allele on another locus there are various methods to measure ld and we will talk about these measurement possibilities in the next video so here is an example of a part of\nHeatmap\na genome with a heat map so here is basically the darker colors are high ld and lighter colors are low ld so it’s measured with d prime in this example don’t worry about this right now but basically what i want to show you that the snips that are close to each other tend to have a higher ld between each other and there are for example snips for example number nine and number 20 you see that there are relatively lower ld between them also there are segments on the genome that are so called ld blocks where all the pairwise combinations of the snips yield a high ld so basically this whole block is in inherited in one piece i really like this picture about the ld block structure in this particular case in a chicken genome that demonstrates a similar thing as in a previous picture or previous slide but on a much larger part of the genome also it compares to chicken lines where we see that the same part of the genome could be very different even within the species when we talk about different breeds or in this case the different lines and the reason for this could be that for example in this chicken breed in this particular part of the genome there are some genes that are very important for this breed therefore this part of the genome is quite well conserved with a very high pairwise lds in this part and the same genes are of course present also in the other breed but in this breed are not important or at least not selected for therefore we do not see such a strong ld block occurring in this breed this\nWhy is LD important\nsimple picture supposed to illustrate the use of ld and why is it important in the genomics as such so what we have here is let’s say a chromosome and we have a snips on them and let’s say we did some kind of analyzes where we found the significance of each snip so this would be the higher the snippets the higher the significance level now we see that there we have some kind of a signal here so the question is is this the gene of interest that influences our trait of course because we are speaking about snips that are themselves just markers and not the causal variants so this is in fact not the gene of interest but actually just shows what region of the genome is interesting for our case so we have a region of interest here and most likely the exact gene of interest resides somewhere here now the size of this region is determined by the linkage this equilibrium so ld because most likely this snip them the highest significant snips is connected to all of these nucleotides around also that the parts of the genome that were not observed and therefore helps us to find the gene that is actually interesting for us\nSummary\nso to sum up icl the important it actually defines how how far we are allowed to or supposed to look from the detected markers when we are looking for the causality on our genome and it shows the association strength between the observed snips and the unobserved genes or qtls also i want to underline that the ld itself is actually dependent on a population or species so most likely these association strengths or the in these regions or the size of the regions might differ between the different populations as for\nApplications\nthe applications of ld in genomics it is really really wide ranging so i just mentioned a few examples here so in evolutionary biology it allows us to reflect on past events and gives insight into evolutionary history when it comes to genetic diversity it’s a well similar as in a previous point but we often compute the effective population size for our populations and this one of the computation methods is via ld itself then we can tell something about the artificial or natural selection events on the genome when it again comes to the population with the so-called selection signatures and the genetic hitchhiking these are events when the actual selected gene drags along part of the surrounding genome and appearing in a population as a selection signature so this size is determined by the ld itself my small schematics showed an example of a genome-wide association study where we could search for causal genes and as it was mentioned there we could locate our search to these interesting regions where the most important or most significant snips reside and ld is also used in a genomic selection in an indirect way so here the question is that how many snips we need that are more or less equally distributed on the genome so the whole genome is covered so we could detect these very small qtls and very small genes that affect our trait of interest which is usually a quantitative trait so it is influenced by many genes of small effect we need to ensure that the whole genome is covered properly so that all of these small genes are connected to at least one snip in a high enough ld and therefore taken into account when the genomic reading value is calculated so again a slide to give an answer to the initial question what is linkage disequilibrium so linked to this equilibrium is a parameter that quantifies the non-random association between the low psi it shows if the frequency of a different alleles between two loss psi is higher or lower than it would be expected if the lows say loss i were independent from each other and associated randomly and\nConclusion\nthe very last summary slide for the entire presentation we talked about snips markers that allow to track the associations between parts of the genome and such associations and such connections could be non-random especially if the snips or markers or parts of the genome are very close to each other such non-random association is called the linkage this equilibrium or ld in a shortened abbreviated form and there are wide-ranging applications of lde within genomics in the following videos we will continue with the exact measurement techniques of the linkage this equilibrium also i will provide a very nice hands-on example but of course also show the way how the ld is computed for large data sets using blink for today i thank you for your time and see you at another video on the genomics bootcamp channel i wish you a very nice day\n\n\n\nMeasuring linkage disequilibrium\nTitle: How to measure linkage disequilibrium?\nPresenter(s): Gábor Mészáros, PhD\nhello everyone welcome back to the genomics boot camp today we will continue to talk about linkages equilibrium before we start however i want to thank you for the 500 subscribers or the over 500 subscribers on the genomics boot camp channel it seems that there is a continued and ever increasing interest about the contents of the channel which is of course a very good news for me and a good motivation to move forward of course if you are not subscribed yet you can use this opportunity to do so so thank you thank you thank you and without further delay let’s move on to further discussions about linkages equilibrium so let’s start with\nLinkage disequilibrium\na little bit of activation of our prior knowledge namely that single nucleotide polymorphisms or snips exists and these are actually the main marker types we speak about on this channel linkage disequilibrium as such also exists and we spoke about this in the previous video and in short it actually measures the non-random associations between snips and there is a wide ranging applications for ld in genomics but for today’s question how to measure linkages equilibrium we saw this graph in the previous video and basically what we see is that some of the allele combinations are appearing much more frequently than others thus leaving two linkages equilibrium this non-random association between some alleles of a different loci can be visualized in such a two by two table where we have on one side locus a and with the alias capital a and small a and on the top side locus b with alias capital b and small b each of the combinations is then represented with a proportion so basically the p capital a capital b is the proportion of the combinations for these alleles if we are interested for example in the total proportion of the allele a then we have it in the right side the same we can tell about the proportion of all the other alleles so so the the lowercase a the capital b and this lowercase b there is one rule if we are talking about biolic systems that the sum of the proportion of the two alleles sums up to one in case of linkage equilibrium when the low psi a and b are not linked we can actually compute the genotype proportions based on the allele proportions this is shown for example in the first line and we can actually do the same for all the other genotypes one additional comment here so we use this feature of this previous two by two tables so i told you that basically the proportions for the two alleles are summing up to one so basically the p lowercase b we can also express as one minus p capital b this is of course a preferred setting because we reduce the number of unknowns from four to two so basically we just have to work with two variables p a and p b now this situation is valid for the linkage equilibrium as i mentioned so when the alleles are unconnected in case of linkage disequilibrium however these equations does not hold true in other words from the proportions of the alleles we cannot compute the proportion of the genotypes we cannot compute this because there is a difference between these two metrics that is the disequilibrium coefficient or capital d and it can be computed as the proportion of the a b together minus the proportion of a time proportion of b and when we include this this equilibrium coefficient to the equations as before we get the proportions of the genotypes so in other words this d or this equilibrium coefficient is a measure of the linkage this equilibrium or a coefficient that can be used to express it\nD prime\nbut we have a bit of a problem with this this equilibrium coefficient because it’s a bit hard to interpret and the sign is arbitrary so it’s depending on which allele actually you consider first of course there is a common convention to set the capital n capital b to be the most common allele and the smaller case a and b to be the rare allele but of course the rarity of the alias or the minor allele frequencies can change from population to population even within the same species so that is not such a big win after all now there is a better version of this d or the disequilibrium coefficient called the d prime that is a scaled version that is computed as shown in this slide so basically is divided by the minimum of these two values in case the disequilibrium coefficient is lower than zero or the minimum or of these two values if the disequilibrium coefficient is higher than zero now we end up with a coefficient which is another a more popular version of the linkage this equilibrium and ranges between minus one and plus one extreme values in this case imply that at least one of the haplotype was not observed it has several advantages that is the d prime one or minus -1 means that the two snips are not separated by recombination so that they are in a complete ld it also means that if the earlier frequencies of the two loss i are relatively similar then the high d prime means that the markers are good surrogates for each other but in this case we have also some disadvantages namely that the d prime estimates are inflated in case the sample size is small and the d prime estimates might be also inflated when one of the alleles is rare then\nCorrelation\nthere is a follow-up question if there is a more intuitive way to measure linkage this equilibrium which to which the answer is yes and that it can be measured straight with the correlation coefficient or an adapted value anyway these correlation coefficients as we would expect expresses the mutual relationship between the alleles of the two loci now a little bit of refreshment on the correlation so this is the correlation as we know it from the statistics so it has a standardized abbreviation of lowercase r and ranges from minus one to plus 1. basically there could be a different values for the correlation coefficient we describe how the two variables behave in relation to each other so basically we have an x and a y and if with the increasing x the y decreases then we have a negative correlation coefficient and if the two values are increasing so with the increasing x also y increases then we have a positive correlation coefficients in between minus one and one so the extreme values so the correlation coefficient expresses the strength of such relationships and of course it could be situations when the two values are not correlated so then it’s the correlation coefficient is zero now for linkages equilibrium we use the squared correlation coefficient so this is the r square so the squared correlation between the markers and therefore it ranges between zero and one r square of one implies that the markers provide exactly the same information and the r square of zero implied that the two markers are not connected at all so independent from each other so in other words the r square measures a loss of efficiency when a marker a is replaced with the marker b now the r square value could be computed from the allele frequencies themselves and what we need is actually just three values from here the joint appearance of a and b the proportion of capital a and the proportion of capital b based on these values we can put them into this very nice equation and we can compute the correlation coefficient r square now i realize that there is quite a jump from the correlation itself to this equation and i provide actually all the details in an additional video which i call the advanced or a quotation mark advanced video so you can look at it if you’re interested it is already uploaded on the genomics bootcamp channel and once we have our measure of linkage d6 equilibrium of course we can compute the pairwise linkages equilibria between markers such it was done in canbary at all 2010 and for this example so in here what we have in on the y-axis is the link is this equilibrium measured in r square in the x-axis there is a distance between the markers in this case measured in morgans which can be of course translated to megabases so this would be one megabase two megabase difference three four and five megabase difference each dot on this graph is a linkage this equilibrium value for a marker pair so you see that if the two alleles are very close to each other then the linkage desecration could be very high up to one but then this value quickly goes down to a lower values as the differences between the markers or the genomic distances between the markers increase this sudden decrease of the average linkage disequilibrium between markers is shown with this dashed line and in fact its shape is very typical for all the organisms we refer to this sudden decrease as linkage disequilibrium decay the shape of this lddk is in general similar between the organisms but the starting values and the further developments can be different when it comes to different species or in this case as shown in perez o’brien 2014 it could be different even when we consider bostoros and boss indicus breeds this of course has some relevance when the ld is used for further genomic analysis as discussed in the last video so\nSummary\nto summarize this video the ld characterizes the degree of relationships between the need by loci there are various methods and possibilities to measure it one of them is the disequilibrium coefficient denoted by d or as a d prime that is a metric to use to quantify the ld but it has some disadvantages and then the other more commonly used metric is the r square or the squared correlation coefficient that is a robust measure of the ld so this is the end of this video if you’re interested how we arrive to the equation to compute the r square then be sure to check out the next one a follow-up to this and if you say that is enough for today then of course i thank you for your time and wish you a very nice continuation of the day.\n\n\n\n(Advanced) Measuring linkage disequilibrium\nTitle: How to measure linkage disequilibrium? (ADVANCED)\nPresenter(s): Gábor Mészáros, PhD\nhello everyone welcome back to another video on the genomies boot camp this time is a follow-up or an add-on to the previous video how to measure linkages equilibrium this video is entitled advanced because we have a bit more equations here as on the average of the channel the reason i did not included these ones into the previous video is that perhaps not everybody is interested or they don’t like to see you know too many equations in a educational video therefore i thought it makes sense to divide the two but you are here you are interested so let’s get to it i repeat here a few slides that were in the original video in order to provide context but we will go through these ones in a quicker manner\nRsquare\nwe start with this video where we ended last time and this is our prior knowledge now and the most important part is the r square so there we have the commonly used robust measure of ld in the previous video i was just including the summary equation but in this video we will have more information how we arrive to this r square so again we have the correlation coefficient from a general statistics and abbreviated as a lowercase r ranges from -1 to 1 and there are various values of the correlation coefficients that are expressing the relationships between the two variables x and y of course the correlation coefficient is also computed somehow and it is actually computed as a covariance between x and y divided by the square root of variance x and variance y if we want to write out this one in a further detail so we have this would be the covariance between x and y and the variance of x and the variance of y in these brackets basically the goal of this video is to provide you with the link between this equation and the r square equation to compute the linkage disaquarium that was shown at the end of the last video and also will be shown more times also in this one so for refreshment we have the r square value that is the squared correlation between the markers therefore ranges between 0 and 1 and the value of 1 implies that the markers provide exactly the same information while the value of 0 for the r square implies that the markers are not connected at all to each other therefore it measured the loss of efficiency when a marker a is replaced by a marker b the r square itself could be computed based on the allele frequencies from a this two by two table so we have a locus a and the locus b and each of them has two values capital b and small b and a capital a in a small a so these values are valid for a bial analytic system so these kind of computations are valid only in a case that the markers have a two alleles but because the snips on the snip chips are biologic by design then we are good to go there are in fact only three values that we need that is the joint occurrence of a and b and the respective frequencies of capital a and capital b now we have the r square which is the measure of the linkage this equilibrium and that can be computed with this equation previously it was stated that the correlation itself so the r could be computed with this equation so basically the main question of this video is how to get from this to this now to answer this question we have to reformulate our example a little bit and we consider two loci a and b again on one gamete but each with a possible random realization we give the value of 1 to locus a with the probability of p capital a and the value of 0 with the probability of p 1 minus p capital a so this is basically the other allele so this is the p a and p capital a and then this is the b lower case a but because b lower case a and capital a equal to one so the sum of them is equal to one so then the b lower case a could be expressed as 1 minus capital a the very same thing happens with the locus b so we assign a value of 1 with the probability of b capital b and the value of 0 with the probability of one minus p capital b so this is a very particular example that we will follow up on in detail in the next video but still i thought it’s useful to show it here so perhaps it’s easy some understanding so basically what we have a two loci here locus one and locus two so we have uh allius capital a a lowercase a and for locus two it’s a allele capital b and lowercase b and basically what we do is we replace with the capital a with that once and the lowercase a is with the zeros so then it looks like this so basically we just count how many capital a’s we have so that would be a 5 and how many capital b we have that would be a 6 and the joint occurrence of capital a and capital b would be 4 in this particular example now of course we are interested in in a general example so we introduce a population size of n and we can compute the proportion of the allele a as the sum of occurrence of allele a divided by the population size n the same way we can compute the proportion of b with the sum of the allele capital b divided by the population size n and the joint occurrence of a and b we can compute when the jointly occurring a and b together divided by the population size n okay then the follow-up is what\nFollow up\nis the correlation between the realizations of zero and one of a random variable at the two loci and here comes again the equation for the correlation coefficient so that is the r this equation basically has three parts so that is one in this top and then the so basically the covariance and the variance of x and the variance of y and basically we have all these parts figured out already in the previous slide as shown here so it can be replaced by n times the joint occurrence of a and b and the sum x can be replaced by n times the proportion of a and the y could be replaced as the n times proportion of b and similarly all the other parts could be replaced and then in some cases also adjusted so we end up with such a beautiful and much more simple equations to provide you a link to the next slide i also color code them so basically we have this orange the green and the blue part which we then put back to the original correlation coefficient equation so this is the top part here is the correlation coefficient and then we put in the orange the green and the blue part and then you see here that the n is present all the way so it can be actually removed from the equation and then only this part remains so this part is basically the simple correlation between two loci one and two containing the alleles a and b and if we square that equation because we are interested in r square rather than a simple r so then we have this correlation or a squared correlation coefficient that is the measure of the linkage disequilibrium as we have shown in the previous video and also in this\nSummary\none so for the summary the r square is a commonly used and robust measure of ld and the computation of the r-square is adapted based on the equation for the correlation coefficient and for the allele and genotype frequencies in reality of course we talk about computation of linkage disequilibria between a large number of markers or marker pairs so we use software for it for example blink or other software and also in the follow-up videos we will actually show how it is done using the plink itself for today i thank you for your time thank you for your interest also in this advanced content and i wish you a very nice continuation of the day\n\n\n\nComputing linkage disequilibrium\nTitle: Compute linkage disequilibrium (Part 1)\nPresenter(s): Gábor Mészáros, PhD\nhello everyone welcome back to another video on the genomics boot camp back again with another video on linkage disequilibrium this is the first part of the finishing two when we are actually computing the linkage disequilibrium this time by hand i believe this is an interesting piece of knowledge so it’s worthwhile to look at so what we know so far so ld characterizes the degree of relationship between two loci as always and r square is the commonly used measure of the ld computed with the equation below and\nhere we are at the first of the two examples i will show you today the first one is the very basic one and actually you might recall this two by two table from the previous videos when we look at the r square equation it is clear what we need to compute and that is the proportion of a proportion of b and the proportion of the joint occurrence of a and b therefore we are interested what is in this segment or cell in in this other one for the b and the joint occurrence of a and b together because we are speaking about proportions we are need to divide them with the sum of all alleles so this is already how we compute the proportions so the proportion of a is the 5 divided by 10 so 0.5 in this case proportion of the b allele is 6 divided by 10 so 0.6 and the joint occurrence is 4 divided by 10 0.4 when we have our usual equation we basically have all the unknowns so the proportion of a b and a b and we put them in and after the computation we get the ld between these two low psi a and b as 0.167\nso after this warm-up example i show you the real deal that is this ld calculation exercise that was shown by professor henry simiano at the\nlivestock conservation genomics data tools and trends workshop back in 2012 in pug island croatia i really like this example because it puts the ld calculation into context of haplotypes and all the allele combination using snips so first i give you the initial example how it was given back in 2012 then i give you the breakdown of the example and you will have the chance to stop the video and do the computations yourself and after that you can look up the solution in this video so the exercise looks like this so we have our genome here and we have a four snips first one is an eighty snip second one a cg snip third one is also a cg snip and the fourth is an at snip we have also our population and this are already the haplotypes where this haplotype for our genome or part of the genome appears 17 times this other one 14 times three times three times two times and this haplotypes appears just one time in the population the exercise is to calculate the ld between all pairs of loci now in this video i will show you how to compute ld between locus 1 and 2 in detail then i will show you the solutions for the two other pairs of loci and the last three you can follow up yourselves if you want\nso to give you a bit of a heads up information i list here the low side that you need to compute the proportions for so for a locus one is the a the locus 2 is a c local 3 is also a c and locus 4 is an a you need to compute it for these alleles specifically because these are the major alleles for these loci so if you count them up so the a is the major allele for locus one uh dc is for locus two and the c is also for local three and a for locus four and obviously then you need to compute the joint occurrences between the two low side that you are following up and you want to compute ld for so if you actually have all these proportions you can compute the ld between all pairs or actually from four pairs one two one three and one four as we will list it in this video and putting these proportions into the well-known equation then you get the r square value for that pair of loci now if you want to go ahead and do the calculations yourself on with pen and paper then this is the moment to pause the video and well just go ahead with the computations and after you are done or at least you are done with the locus for example locus one two then to resume the video and see if you have done well or if there is something to be corrected\nokay so we are continuing with the solution in three two one now actually what needs to be done is pretty similar to the previous smaller case but in this case you need to build up the two by two table yourself so here we have a locus one with an a t snip a locus two with the c g snip and because we told that the a is the actual major locus then you actually need to count them up so basically you need to fill up this spot in the 2x2 table for the locus 2 the c is the major logo so you need to fill up this spot on the 2x2 table and of course the joint occurrence as well so how to do that of course we will use the actual haplotype numbers as they are given in the exercise actually right now we are looking at locus 1 and locus 2 so basically these two columns are interesting for us so we can actually hide the other ones so that they don’t bother us too much now perhaps the first thing you might consider is that well we need to calculate the proportions so we basically need to know how many haplotypes are there total so basically we add up 17 plus 14 plus 3 plus 3 plus 2 plus 1 and we when we do that we actually arrived number 40 so this is the total number of haplotypes now for locus 1 we need to count how many times the allele a appears so in this haplotype it there is an a so it’s 17 times so it’s 17 here in the locus one is a t so that is we don’t count this also there is an a here an a here here there is a t here so we don’t count this and the a here so basically we have a 17 plus 3 plus 3 plus 1. that is together 24. following the same logic for locus 2 in this case we count the c allele so here we have a c so if we count this here we have a g that so we don’t count this count this this and this so everywhere where is the c so there is 17 plus 3 plus 3 plus 2 and that is together 25. the last thing we need to find out that how many times the a and c appear together so here the a c appear together so that we count this we don’t count this but also ac here ac here here als although there is a c but there is not an a so we don’t count\nthis part and also here is an a but there is a g here so it’s not an ac combination so so we don’t count this so basically we count this 17 these three and this three which is together 23 and because we are actually interested in proportions rather than the numbers themselves so we divide for the locus 1 the proportion of a is 24 divided by 40 that is 0.6 and therefore locus 2 the proportion of c is 25 divided by 40 that is 0.625 and the joint occurrence the same way so the 23 divided by 40 and that is this number 0.575 so with this we already have everything so we have a proportion of a proportion of c and the joint occurrence so we have our beautiful equation here so this is the joint occurrence and this would be the proportion of a and proportion of c so we put them in into the equation itself and then we end up with an ld between locus 1 and locus 2 as 0.711 so this is the ld between these two loci on this slide i give you all the results for the other low size so locus one two three and four and the joint occurrences of these three pairs and if we fill them in into the equation the well-known equation then we end up with well this is just a repetition from before and then there is the ld between locus 1 3 and the ld between locals 1 and 4. so this is basically it this is how you compute\nld by hand so for the summary an example of the ld computation by hand was shown which is not a very widespread piece of knowledge so i argue that is really good to have it in your inventory but on the other hand we also have to note that this manual mode of computation is not very effective for that we need to use software solutions that are able to handle computations of lde between thousands and hundreds of thousands of loci and such computations will be shown in the next video for today i thank you for your time and have a very nice rest of the day"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Software Resources",
    "section": "",
    "text": "Analysis\nStep\nSoftware\nAdditional Tutorials\nLanguage\nPubMed ID\n\n\n\n\nGeneral\nFile management/formatting\nSAMtools\nManual\nC; Python\n\n\n\nGeneral\nFile management/formatting\nBCFtools\nManual\nGitHub\nC; Python\n\n\n\nGeneral\nFile management/formatting\nBEDtools\nGitHub\nC; C++\n\n\n\nChapter 5\n\n\n\n\n\n\n\nGWAS\nGenotype calling and Imputation\nMoChA\nGitHub\nC; R\n29995854; 32581363\n\n\nGWAS\n\nRicopili pipeline\n\n\n\n\n\nGWAS\nGeneral; QC\nPLINK1.9/PLINK2\n\n\n\n\n\nGWAS\nGeneral; QC\n\n\n\n\n\n\nGWAS\nRelatedness/Kinship\nKING\n\n\n\n\n\nGWAS\nPopulation Stratification/PCA\nGENESIS/PCAiR\n\n\n\n\n\nGWAS\nPopulation Stratification/PCA\nPC-Relate\n\n\n\n\n\nGWAS\nLocal Ancestry and Admixture Inference\nRFMix\nGitHub\nC++\n23910464\n\n\nGWAS\n\nTRACTOR\nGitHub\n\n33462486\n\n\nGWAS\nAssociation Testing\nSAIGE\nGitHub; More documentation\nR; C++\n30104761\n\n\nGWAS\nAssociation Testing\nBOLT-LMM\n\n\n\n\n\nGWAS\nAssociation Testing\nREGENIE\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGWAS\nImputation panel\nTOPMed\n\n\n\n\n\nGWAS\nImputation panel\n1000 Genomes\n\n\n\n\n\nGWAS\nImputation panel\nHRC\n\n\n\n\n\nGWAS\nPower calculations\ngenpwr\n\nR\n32721961\n\n\nGWAS\nqqplot and Manhattan plot\nqqman\nGitHub\nR\ndoi link\n\n\nChapter 6\n\n\n\n\n\n\n\nPRS\nPRS methods\nPRS-CS\nGitHub\nPython\n30992449\n\n\nPRS\nPRS methods\nPRS-CSx\nGitHub\nPython\n35513724\n\n\nPRS\nPRS methods\nPRSice2\nRunning PRSice-2\nR\n31307061\n\n\nPRS\nPRS methods\nLassosum\nGitHub\nR; C++\n28480976\n\n\nPRS\nPRS methods\nLDpred2\nManual\nR\n33326037\n\n\nPRS\nPRS methods\nSBayesR\nManual\n\n31704910\n\n\nPRS\nPRS methods\nMegaPRS\n\nPython\n34234142\n\n\nSNP heritability\n\nGCTA-GREML\n\n\n\n\n\nSNP heritability\n\nLDSC\n\n\n\n\n\nGenetic correlation\n\nLDSC\n\n\n\n\n\nGenetic correlation\n\nGenomic SEM\n\n\n\n\n\nGenetic correlation\n\nLAVA\nGitHub\nR\n35288712\n\n\nTWAS\nTranscriptomic Imputation\nPrediXcan\nGitHub; Tutorial\nPython\n26258848\n\n\nTWAS\nTranscriptomic Imputation\nS-PrediXcan\nGitHub\nPython\n29739930\n\n\nTWAS\nTissue models (GTEx, CommonMind, PsychENCODE)\nPredictDB\nGTEx models; PsychENCODE models; CommonMind DLPFC models\n\n\n\n\n\n\n\n\n\n\n\n\nGene/Gene set identification\n\nMAGMA\n\n\n\n\n\nGene/Gene set identification\n\nH-MAGMA\nGitHub\n\n\n\n\nGene/Gene set identification\n\nE-MAGMA\nGitHub Tutorial\n\n\n\n\nGene/Gene set identification\n\nFUMA\n\n\n\n\n\nGene/Gene set identification\n\nMAGMA\n\n\n\n\n\nGene/Gene set identification\n\nPRSet\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMiXeR\nGitHub\nC\n32427991; 31160569\n\n\n\n\nMTAG\nGitHub\nPython\n29292387\n\n\nFine-mapping\n\nSuSiE (used with coloc)\nR package\nR\n37220626; 34587156\n\n\nFine-mapping\n\ncoloc\nGitHub; R package\nR\n24830394; 32310995; 34587156\n\n\nWGS/WES"
  },
  {
    "objectID": "software_correlation_transcript.html",
    "href": "software_correlation_transcript.html",
    "title": "Software Tutorials: Genetic Correlation and SNP Heritability (Video Transcript)",
    "section": "",
    "text": "LAVA\n\n\nLDSC\n\n\nGCTA-GREML\nTitle: Estimating SNP-based heritability with GCTA-GREML\nPresenter(s): Jian Yang"
  },
  {
    "objectID": "chapter9.3_transcript.html#sec-video1",
    "href": "chapter9.3_transcript.html#sec-video1",
    "title": "Chapter 9.3: 9.3 Genomic Structural Equation Modeling (Video Transcript)",
    "section": "Genomic Structural Equation Modeling: A Brief Introduction",
    "text": "Genomic Structural Equation Modeling: A Brief Introduction\nTitle: Genomic Structural Equation Modeling: A Brief Introduction\nPresenter(s): Andrew Grotzinger\nIntroduction\nIn this first video, I want to provide a brief overview of genomic structural equation modeling, including some of the background and motivations that led us to develop this package.\nGraphs\nI’m going to start here by showing these two different graphs that show on the X axis, the discovery sample size from different GWAS studies and on the Y axis, the number of hits identified in this studies. And what you’ll notice is that as the sample size have increased, we have begun to identify hundreds if not now, thousands of different genetic variants associated with both complex traits like height and BMI and disease traits, like Crohn’s disease and prostate cancer, which really reflects this gradual realization that we’ve had in human complex trade genomics, that many of the outcomes that we’re interested in are highly polygenic, meaning that they are associated with many genes and not just some small handful of core genes. For those of us that are interested in the shared genetic relationships across traits this comes with the caveat that it’s not simply a matter of identifying the five or six overlapping genes.\nLimitations\nSo for my self, as somebody who’s interested in clinical psychology outcomes, I couldn’t simply look at these two Manhattan plots for schizophrenia and depression, which just to orient you to these, to the chromosome on the X axis and the negative log 10 P values with values that are higher up over here indicating genetic variants that are more significant. That I can’t just count up the ones that are above this red dash line here for genome-wide significance. Cause there’s simply too many. So we needed at some point to develop methods that find ways to estimate the aggregate shared information across these really polygenic outcomes.\nLD Score Regression\nAnd thankfully A team from the Broad, including some people who are talking at this workshop developed this method called LD score regression which can be used to estimate genetic correlations across participants samples with varying degrees of sample overlap, using what is often publicly available, GWAS summary data. As in, you can go online right now and directly download that data without going through any sort of strenuous request process or going through an IRB.\nGenetic Heat Maps\nWhen LD score regression is applied, it can be used to produce what is often referred to as genetic heat maps. Two of which are shown here. So on the left, we have the genetic correlations estimated across psychiatric phenotypes with the squares that are shown with darker shading, indicating stronger levels of genetic overlap. And of course the squares on the diagnoal and all are shown in dark blue because that indicates the genic overlap of the phenotype with itself. So you see that across a number of these disorders, that there are high levels of genetic correlation. And that this is also reflected in general brain and behavioral, cognitive phenotypes shown this heat map over here on the right. And so this reflects one of the second things thatwe’ve realized in human complex trait, genetics, which is that there is both pervasive, polygenicity, which is say that the traits are affected by many genetic variants and there’s pervasive pleiotropy, which is to say that many of those variants are actually shared across traits. And this shared genetic architecture that we can see depicted here in these heat maps is really what motivated myself and others to come together and develop methods that allow us to actually analyze that joint genetic architecture. So again, genome-wide methods are clearly suggestive of these two things, high polygenicity necessity, and pervasive pleiotropy. And we’ve viewed genetic correlations as data to be modeled. We want it to be able to ask what kind of data generating processes give rise to these correlations and how can we use publicly available data, to examine systems of relationships, to really start to interrogate that data more. And so that what led us to develop genomic structural equation modeling, which we introduced in this nature, human behavior paper that involved a lot of key members on the team. But I’ll specifically highlight Michel Nivard and Elliot Tucker-Drob, who supervise this project and continue to work with me to develop extensions on it and of course, Michel is also one of the people presenting here.\nGenomic SEM\nSo our solution to this problem of how we model the genetic correlation matrix is genomic SEM which applies structural equation models to these estimated genetic covariance matrices, and then allows the user to examine traits that oftentimes could not be measured in the same sample and it provides a flexible framework for really estimating a limitless number of models using only these GWAS summary statistics. That again, can be applied to summary stats or what is often referred to as sumstats with varying and unknown degrees of sample overlap.\nExample\nI just want to focus on one specific example for time reasons related to my main interest in clinical psychology and this reflects our most recent application of genomic SEM to psychiatric outcomes, where we produce this genetic heat map across 11 major psychiatric disorders. And what you see is there’s a pretty high level of genetic overlap across these disorders. And what is really unique about this is that many of these disorders cannot be measured in the same sample. So bipolar disorder and schizophrenia, the way that we structured our clinical diagnoses, you can’t have that same set of disorders within the same person. You’re either assigned one or the other based on your presentation. And so that means we’ve been limited in clinical psychology research to making inferences about what is shared across these disorders based on patterns of findings from separate samples. Where now for the first time, genomic SEM offers this unique opportunity to actually model the set of relationships across these rare and even mutually exclusive clinical presentations. Because again, The summary statistics that were used to produce this heat map are not from the same sample, but instead reflect the aggregate summary level data from these different cohorts . When we then applied genomic SEM to model these relationships, we found that a four factor model fit the data best. And this actually mapped on pretty well too some level of what we might expect based on the clinical presentations of these disorders. So for factor one, we have anorexia obsessive compulsive disorder and Tourette’s syndrome that we might characterize as being really defined by this kind of compulsive, like presentation. With the psychotic disorders of schizophrenia and bipolar disorder. What we’re calling a neurodevelopmental disorders factor because it’s defined primarily by ADHD and autism, but also notably includes PTSD and alcohol use disorder. And then this internalizing disorders factor over here that includes MDD, anxiety and PTSD. And I would note that these factors are all correlated, but they also segregate into these more tightly defined clusters of sets of disorders, giving rise to some level of understanding about what is shared across these specific subsets of psychiatric presentations. We can also use genomic SEM to examine individual SNP effects on the factors. And so when we then apply what we referred to, as multi-variate GWAS to these factors, it can be used to produce these 4 Miami plots here. Where I’m depicting with the black triangles, the GWAS hits that overlap with the univariate hits and in red triangles, the GWAS hits that were actually not hits for the individual disorders. And this is reflecting that by leveraging the shared information across the disorders that define the factor, without collecting any new data, we can actually discover new genetic variants associated with these factors. On the bottom half of these Miami plods, I’m showing the effect of Q SNP which is a heterogeneity index that we’ve developed that is designed to pick out those genetic variants that do not fit the factor model. As a classic example, we generally pull out the ADH1b gene that is associated with alcohol use disorder, specifically as something that is a Q SNP variant, which is to say that this is something that does not operate via these more general factors, but it’s highly specific to alcohol use disorder. And we’ll talk more about Q SNP in some of these later videos it was. And so sales pitch to genomic SEM I think one of the really. Amazing opportunities that genetics and genomics SEM building on methods like LD score regression presents is the ability to examine systems of relationships across a wide array of rare traits that could not be measured in the same sample. To give an example of research question, you might have that you could not do outside of a genetic space.\nLet’s say that you’re interested in the genetics of early and late onset schizophrenia and anorexia nervosa. You have some sense of research literature that indicates that a particular onset stage of anorexia nervosa can sometimes contain a psychotic like presentation and you also recognize that these things can have distinct developmental onset periods that means something about the course of these diseases. And so you want to look at this cross-lagged model that examines the systems or relationships across these presentations within the disorder and across these two disorders.\nYou could collect a sample where you look at the relationship between early onset schizophrenia and anorexia nervosa and in the same vein, late onset schizophrenia and later onset anorexia nervosa. But that would take you a long time to collect just because of how rare these two disorders are. What you could not do is look at the relationship outside of a genetic space between the early and late onset versions of the same disorder. But again, when GWAS summary data, you could split the GWAS data by age of onset and then you could estimate the genetic overlap of the signal between the early and late onset versions of this disorder and so now with genomic SEM You can actually estimate this model and answer these sorts of research questions that again, would not even be possible outside of a genetic space.\nSummary\nThis final slide is just to highlight the different ways in which genomic SEM has been used. Including some work that I’ve been involved in, but also works from other outside research groups that are published in high-impact journals, like molecular psychiatry, nature genetics, cell, and nature human behavior. I really hope that some of you see some opportunity to use genomic SEM in your own research. In the next videos, we’ll talk more about how structural equation modeling works and provide some hands-on examples of how to apply genomic SEM to actual genetic summary data."
  },
  {
    "objectID": "chapter9.3_transcript.html#sec-video2",
    "href": "chapter9.3_transcript.html#sec-video2",
    "title": "Chapter 9.3: 9.3 Genomic Structural Equation Modeling (Video Transcript)",
    "section": "Short Primer on Structural Equation Modeling (SEM) in Lavaan",
    "text": "Short Primer on Structural Equation Modeling (SEM) in Lavaan\nTitle: Short Primer on Structural Equation Modeling (SEM) in Lavaan\nPresenter(s): Andrew Grotzinger\nIn this video, we’re going to talk about some of the basics of structural equation modeling and more specifically how to do  structural equation modeling and Lavaan which is the art package that genomic SEM uses to estimate the models.\nTo start, let’s go over some of the basic model syntax for Lavaan, beginning with how to specify a regression relationship, which you could write as Y till the X, which visually depicted as a path diagram would mean X predicting Y with this single headed area. And depending on how you think about variables in your model, you might think of that as reflecting a ~ DB. Dependent ~ the independent outcome ~ predictor. But of course he would name these according to the actual names of the variables in your data set. This is just to drive home the point that the outcome is on the left-hand side of this equation and the predictor is on the right-hand side of the, ~ the over here. To specify a variance of a variable. You would write the name of that variable twice with two ~ in between. So X ~~ the X would specify the variance of X or the covariance of X with itself to specify the covariance between two different variables you would also use two ~ and on the right-hand side, you would write the name of that second variable in this case, Y. Which is a path diagram would reflect this two headed arrow between X and Y. And then the standardized case would of course reflect the correlation between these two variables.\nTo specify factor loadings which refers to the regression relationships between this unobserved lightened factor and these observed variables, a through E, which we’ll talk a little bit more about later in this presentation, you would write F and then equal sign ~ and then a through E you can name the latent factor however you want. In this case, we’ve just called it F and of course, A-E should again, reflect whatever the names of the actual variables are in your data set but this is generally speaking the schematic for how you would specify the factor loadings in a model. Just a note to say that the default behavior in Lavaan  is to fix the loading of the first variable to one and this is necessary because since F is not a variable in your dataset, Lavaan and structural equation modeling more generally needs some sort of anchoring point so that it knows what kind of scale to put this latent factor on. So again, unless you tell it to do otherwise, Lavaan we’ll fix that loading for the first variable to one. So it knows how to scale the latent factor. And as another note, just in general and structural equation, modeling squares are used to depict observed variables, such as the variables here for A through E and circles are used to depict an observed or what is often referred to as latent variables. And that can include latent factors like the latent factor F over here or, for example, the residual variances of observed variables where the observed variable is something that is observed in your data set and the residual variance is not. And so for that reason is generally depicted as a circle.\nTo fix a parameter in a Lavaan model, you would write the value that you’re fixing it to on the right-hand side of the equation, followed by an asterisks, and then whatever variable name comes on that side. You always put the value you’re fixing into on the right-hand side. So you would not write one star X, ~~ Y but what this specifically does here is it fixes the covariance in between X and Y to one. You can also name a parameter in lavaan and to do that, you also put an asterisks on the right hand side of the equation, but then, you would also include whatever you’re naming that parameter, which should just be some set of letters. So I could call this dog, cat, a, rain drop. You just have to name it something that doesn’t overlap with the actual names of the variables in your dataset. So this names, the covariance between X and Y to be a, and that’s useful because then you can, in the later part of your model use what is referred to as model constraints for a particular parameter. So let’s say that for some reason, you know, that X and Y is covariance should be above zero, but you don’t know exactly what that number should be. So you’re not fixing the value to anything in particular, but you are telling the lavaan that that value should be above this particular number of .001. Just a cautionary note on naming parameters, I’ve seen a number of people use the same parameter label for multiple parts of the model and oftentimes this was done unintentionally on their part and the reason that that can is a problem is because when you name it with the same label, a here, that’s not just naming the covariance between X and Y. And the covariance is between Y and Z. It’s also telling Lavaan that you want to fix these two covariances to be estimated at the same value. So if you do want to name multiple parameters in the model. You just want to make sure to name them different things, unless you are intentionally trying to use what is referred to generally as inequality constraint. So this is a, what not to do unless you are trying to make these parameters be the same. Having reviewed all of these Lavaan basics. And I just want to go through some of the concepts and basics of structural equation modeling, and then continue to use this lavaan syntax along the way as we review those basics.\nTo start, I want to Begin in this hypothetical space where we’re talking about a situation that we would not run into in which we actually know what the relationships are between the variables. So here let’s say that we know that the regression relationship between X and Y is 0.4 and that Y as a residual variance of 0.84, which would imply this particular set of relationships were Y equals 0.4 times X plus this residual U. If we extend that out to say that, right we know that Y causes Z of 0.6 that would add on this additional notation of Z equals 0.6 times Y plus U. And so this would imply a particular covariance matrix in the population. This set of relationships where this isn’t a standardized space on the diagonal we have the variances of the variables, which are one. And then on the off diagonals, we have the Relationship between X and Y at 0.4, between Y and Z of 0.6 and the relationship between X and Z in a structural equation model. And this case would be 0.4 times 0.6 or .24. In practice, we, of course don’t have access to the covariance matrix in the population, but instead we have a particular covariance matrix within a sample, which is intended to be a rough approximation of the population that we’re interested in studying.\nAnd so again, in practice, we have this observed covariance matrix in our sample. Instead of knowing these relationships, we’re then flipping that process on its head. And we’re saying that we want the model to pick the values within this system of relationships that we’ve specified that most closely, approximate the covariance matrix in our sample. With an structural equation modeling. You’ll hear people talk about the degrees of freedom at the model, and that refers to how many parameters you estimated relative to how many parameters you could have estimated given the unique elements in the covariance matrix. So for three variable covariance matrix, you have six unique elements which refers to the three variances on the diagonal and the three covariances on the off diagonal. And in our model, we’ve only estimated five of those parameters where specifically we did not estimate the regression or covariance relationship between X and Z. So that means we have one degree of freedom and can be thought of as specifying a model that provides a more parsimonious or simplified set of relationships relative to what we observe in the data. To specify this model Lavaan using that notation that we just went over for the regression relationship between X and Y. We would write Y ~ X over here in blue and for Y and Z the same thing, but Z ~Y.\nSo in practice again, what lavaan and what SEM softwares are doing more generally is they’re taking this observed covariance matrix and the system of relationships that you’ve specified and they’re picking the numbers that get as close as possible to that observed covariance matrix, which in this case would be.35, and 0.61. And that would imply a certain covariance matrix. And the level of difference between these two matrices is often what is used to produce what is referred to as model fit, which  is something we’ll talk about in later videos. As we talked about earlier, you can also specify latent factors in a data set, which reflects the shared variation across a certain set of variables, such as Y one through Y five here. And I would just note that we’re actually employing a different notation than we used earlier, where were specifying the factor loadings again with the name of the factor F =~ And then the name of the variables that define the factor, but we’re overriding that default Lavaan behavior to fix the indicator the first variable to one by writing NA*Y1, and then we’re instead anchoring that part of the model or scaling it by telling Lavaan that we want to fix the latent variance of the factor to one. So we’re using what is referred to as unit variance identification instead of what we used before, when we fixed the loading of Y1 to one, which is referred to as unit loading identification. Not something that you need to keep in your working memory at all times, but just something for you to know as an option. And this particular part can be important if you’re specifying, let’s say correlations between factors, where if you set the variance of the factor to one, then that means they’re on a standardized scale and you can interpret those relationships as correlations.\nNow here, we’re just showing what if you want to expand this out to include the relationships between two latent factors. And so again, in Lavaan syntax, you would right for the latent factor F1 here this part of the model here in blue F1 =~Y1+Y2+Y3+Y4+Y5 the regression relationship between F2 and F1 this part here in purple and for the latent factor model for Factor 2, F2=~Z1+Z2+Z3+Z4+Z5. What genomic SEM does is it uses the principles of structural equation modeling to fit a model to the genetic covariance matrix. So it’s a two-stage process where in stage one, that genetic, covariance matrix and its associated matrix of standard errors and their co-dependencies are estimated where we specifically use LD score regression and stage two, we fit that structural equation model to the matrices from stage one using Lavaan and the basic principles of structural equation modeling. I know this was a fairly brief overview, but hopefully it provided the basic notation to use Lavaan syntax and understand a little bit about what the SEM process looks like. And so in the next talk, we’ll go over how it is that we actually estimate this genetic covariance matrix and that set of standard errors."
  },
  {
    "objectID": "chapter9.3_transcript.html#sec-video3",
    "href": "chapter9.3_transcript.html#sec-video3",
    "title": "Chapter 9.3: 9.3 Genomic Structural Equation Modeling (Video Transcript)",
    "section": "GenomicSEM: Input",
    "text": "GenomicSEM: Input\nTitle: GenomicSEM: Input/Explaining how S and V are estimated and what they are\nPresenter(s): Michel Nivard\nWhat goes into genomic SEM before you can actually fit a structural equation model. So it’s basically a class on how the sausage  is made, right? So some of these things are details that are good as background, Good for you to understand, once you use genomic SEM. But basically stuff that’s largely handled by internal functions of genomic SEM. so it means you don’t have to do   anything like this by hand and to discuss what goes into genomic SEM. It’s good to discuss two other things. Namely, what kind of information goes into a structural equation model in general? Right. You can use raw data, but you could also use the covariance matrix of the raw data to fit the structural equation model or a regression model, as we’ll see in the example. And it’s good to discuss what LD score aggression is exactly. It’s a very commonly used technique to estimate genetic co-variance and or heritability. And it actually is one of the main ways genomic SEM can estimate from raw GWAS data. So raw, summary statistics the genetic covariance between traits and the heritabilities, which you can then use to fit a structural equation model too. Okay. So first we’ll discuss what kind of information goes into a structural equation model. And, show you that basically you can either use raw data to get a structural equation model to work, but you can also use, the covariance matrix of that raw data. And that’s an important conceptual step, right? Because if you understand that you can later understand why we can fit structural equation models while we don’t observe any raw data in genomic SEM. And with raw data. I mean, phenotypic observations, we don’t observe those at all in genomic SEM. The only thing that goes in is the GWAS summary data.\nSo now we’ll switch over to, R and i’ll through a little example in which I create a dataset and then fit a structural equation model. And then I’ll take only the covariance matrix of the dataset and fit the same structural equation model to illustrate. what goes on in such an example, I didn’t really clear my workspace before, so let’s do that so we have a fresh start. The first thing we’ll do in this, R script this we’ll will require some, packages I need to generate some data and to fit structural equation model. So the mass package allows me to generate data from distributions and lavaan will allow me to fit structural equation models. Okay, then we’ll run these, we’ll also run this line, which States that variable N is 10,000 we’ll use the variable N for sample size all throughout the example. Then we’ll generate three random variables, X one X, two, and X three. And we’ll also generate random variable, Y which is a function of X one X, two X three, and some residual variance that’s not attributable to all those variables. So what does variable Y look like something like this, what does X one look like? Well X one was normal, right? This, we just generated from a random, normal variable. This is what it looks like. Okay. We’ll combine all those variables into a dataset, right? The dataset just keeps together all the variables in one place. Now the variables also have a relationship because that’s the way we define them. Right. So we could make a plot of Y against X one, and we’d see there be some sort of  relationship where if X one is higher, Then Y is also, which is exactly what we expect given that we just made Y a function of X one. Right? Okay. Now on these data, we can fit linear regression model, which I do right here in which we regress Y on X one on X two, and X three. And we know what will come out of the linear regression model. Why? Because we just defined the relationship between Y and X. Right. We defined the relationship as Y being equal to 0.4 times X one, .5 times X two and minus 0.2 times X three. So if you run the model, Oh, I forgot to run the ‘dataset’ line, this is instructive. You know, when you, when you, when you’re programming, you’ll always make mistakes. I’m not going to cut it out of the video. Cause I think it’s instructive to know just that we also had just, you know, messed this up all the time. Maybe it’s not that instructive, but I messed this up all the time. Okay. So now I’ve run the lines we need to, to run a linear model, as you can see, this is the way you would define a model in R Andrew has a video up on how you can define Models in lavaan, but in R, the regression model uses a tilde as an equation sign and “+” to add additive terms. And so this linear model using the function, LM it’s fit, linear regression, and we can get a summary using the summary function, applied to the object in which the linear model is stored. Now, as you  may expect the coefficients we get from the model are very close to the values we use to generate the data their not exact because generating random data, also introduce a slight bits of noise. so X one gets an estimate of point 40, which is its true value. Thereabouts, X two gets point 49 , while the true value is 0.5 and then X three gets minus point 16 while the true value is minus 0.2 so they’re all close to there, true value, which is great. the linear model works, which is, I guess not entirely unexpected, you can fit the linear model. The same model; whoops, spelling error. You’ll get this code by the way. I’ll make sure it’s available. You can fit the same model in lavaan using dis as a, as a descriptor of the model. Okay. Andrew went through the syntax for lavaan. So you should be able to understand this, but I’m going to read it out. Anyway. It basically defines one regression. Y is regressed on X one X, two and X three, and then explicitly defines the covariance between X one X two X one X three. And actually I just noticed it should also include the covariance between X two and X three okay. And then using “sem()”, which is the lavaan function and adding the dataset, we created, the raw data the the observed data, to the data argument we can fit the SEM model and let’s have a look. Now, a structural equation model is capable of estimating the same parameters as  a regression model. So here you go. Y regressed on X 1, 0.4 Y regressed on X 2 point 49 and Y regressed on X 3 minus 0.16.\nSo, this is just a very convoluted, indirect way to fit the same model. Now notice, because this is a, this is a structure equation model we could fit many, many more models with these, four variables, we’d be flexible. We could, we could say: “well, actually, Y is an outcome of X one, two and three, but we suspect X two to be an outcome of X three.” Right. And we could fit a mediation model in which Y is regressed on all three variables. And the X two was regressed on X three. we could define that model and we could see what the parameter fit is. That’s the flexibility you have in lavaan or a structural equation model, or later in genomic SEM, but which you don’t have in a linear regression model. That’s not the point of this tutorial. So let’s get back to the point.\nI can create the covariance between the variables in the dataset using cov() function in R right? And I’ll get this object Sigma, which has the covariance in it. Now, if we look at Sigma, we’ll see a matrix with covariances between Y and X one and X two and X three. Okay. We can actually feed that covariance matrix. To lavaan so we use the same SEM function, the same model. Now, instead of giving it raw data, we’re giving it the covariance matrix and we’re giving it the number of observations because it needs to know how precisely all the elements in the covariance matrix are fit. And we can run these and we’ll get exactly the same or very similar outputs.  Right. It is, I am running this on SEM model 2. Right. So is this the model that doesn’t know about the raw data just knows about the covariance and, you get the same regression parameters. Now, this works because in A SEM model you defined a covariance, the cells of the covariance matrix in terms of the regression parameters. And then you ask the model to seek out the regression parameters that minimize the distance between the observed covariance matrix in the data and the covariance matrix implied by the model. And so we don’t need the raw data we could do with the covariance. Now it’s nice to have the raw data, because sometimes you have missing data points or other sort of things going on where there’s extra information hidden in the raw data. That’s not hidden in the covariance matrix. So it many cases it’s, it’s far more valuable to have to raw data, but if you don’t, you can fit these kinds of models on the covariance matrix.\nOkay. Let’s go back to the presentation. So when you go over to that, you can, fit structural equation model based on the covariances only. And that’s a valid input for, a structural equation model. Now the input for genomic SEM are genetic covariances, which we get from GWAS summary data, right? So only from a vector of each SNP “rs” number the effect alllel, so which of the two alleles is actually increasing or decreasing the trades the Z statistic associated with that allele so that’s the statistic of the linear regression association in the GWAS, and that’s all, all we’re puttingin to getting the covariances. Now we’ll get those genetic covariances and heritabilities. Using LD score regression now, what is LD score regression? LD score regression actually tries to explain the signal that is created in a GWAS. So this is a Manhattanplot of a GWAS of schizophrenia and these hits highlighted in green. They only explained 4% of variability, but the heritability, according to twin studies, Is 80%. So how do we go about explaining the rest? So if we visualize the same GWAS as a QQ-plot, we’d see that the observed P values are much smaller.so therefore the minus log P values are bigger than what is expected under the expected distribution of a Z statistic, and so the question we ask is, is this, this inflation right? Is this true signal or is it type one error that we’d like mess something up that we’re getting all these false positives. And that’s the question we’re asking ourselves when we’re using LD score regression. And so just really important ingredient for LD score regression is the, the LD structure of the genome. So what I’ve tried to depict here is a part of the genome and each dark. Blue square is a SNP and the lighter blue squares depict the correlations between the adjacent SNPs and LD introduces correlations between adjacent SNPs, which I’m sure has been covered in the days before this person, patient. This also is what creates these towers in, in Manhattan plots right? Because. If one of these SNPs is associated truly with the trait, then then do to LD other ones become correlated with the trade. So you can summarize these LD patterns into a score, which is basically for every SNP, just the sum of its LD with all its neighbors. That’s basically to reflect how well the SNP is correlated to all the SNPS around it, and then consider there is a true genetic effect. Right, so on the left you can see I called this beta. So it’s a true effect. So none of these SNPs has an effect, except for this one, it has a tiny effect. If this were the true effect and we were to do a GWAS, we would get estimated betas, and so we get something like this where we estimate the beta  for the SNP with the true effect to be non-zero, but also for all the SNPs that are in LD with the SNP, with the true effect, we’d expect a estimate of beta. Those are sort of raised. Now, what happens on the genome wide scale is that those test statistics you could derive from, from the linear regression of a trait on every SNP, they go up for SNPs that have more LD and they’d go up precisely because. So many SNPs in the genome are associated with the traits, right? ” that if you “tag” more SNPs as a SNP, you are more likely to tag more true causal SNPs, and therefore your that’s, your signal goes up and your test statistic goes up and it goes up in a very specific fashion. It goes up proportional to the heritability, to the number of with a true effect M, and with the sample size, right? Because Power goes up when the N goes up. And so if the sample size goes up, the relationship between test statistics and LD scores goes up as well.\nSo, and by the way down, there are Hilary Finucane, Brandon Bulik-Sullivan, Ben Neale, and Alkes Price, who basically wrote the first few papers on this relationship. So going back to the little squares i made the Chi square statistics you get from your GWAS they are regressed on LD scores, which reflect how well each SNP tags it’s neighbors SNPs that tag more neighbors are expected to have higher test statistics. And then the slope of that regression is reflective of the heritability because the other big unknowns in that equation sample size and M are known to us, right, because we know how big the GWAS was, and then there’s an intercept which actually reflects things that do not correlate with LD scores such as population stratification. Which is a very neat feature. So now we can separate the true signal from the signal introduced by population stratification.\nSo how did they go about validating this? They actually did a GWAS of Swedish controls versus UK controls. So neither of these sets of people have this disease, but they different mainly in their ancestry. And as you can see on the right hand side the QQ plot of the GWAS, it does look inflated. It does look like there is signal there that is inconsistent with the distribution of test statistics under the null, however, if they plot the LD score, Of all the SNPs, in bins against the death statistic in these bins, then there’s no relationship. In other words, the test statistic doesn’t go up with the LD, which means the signal is probably not a function of heritability, but a function of something else in this case, population stratification.\nNow, if you do it with the real  GWAS, like the schizophrenia GWAS we’ve discussed you see that there is a steep and consistent correlation between the LD score bin a SNP is in the mean chi square the SNPs in each of these bins have. So the relationship is strong and it’s actually consistent with a SNP heritability for schizophrenia of like 40% and 90% of the signal is true. Okay. So that’s how we get an estimate of heritability, GWAS summary statistics, the slope estimates, the heritability. And if we have two traits, we get two slopes, but can also, instead of using chi square test statistic, used a product of the Z statistics of the two traits, regress that on the LD score to get an estimate of genetic covariance between traits. So, this is an example where the rg is 0.5. We got a slope consistent with that RG genetic correlation. And this is an example of why the RG is zero. We get a slope consistent with there being zero correlation. Okay. And it’s robust, this, this entire technique to sample overlap between the GWAS. Now, why is that? Because this intercept in the case of genetic covariance will actually absorb the sample overlap. So it’s  great. We can use GWASes from the same sample to estimate genetic correlation between traits or we can use entirely different samples, so we can correlate some MRI study to some metabolite study. Right. And that’s insightful because it’s really expensive to measure MRI and metabolites in the same. People and not always feasible.\nOkay. Phase three of this lecture, what kind of information goes into a genomic structure equation model? Well, so these heritabilities and genetic covariances, we have estimated are actually assembled into a matrix S which holds the genetic variance covariance matrix. So the top left entry is the heritability, the first trait and all across the diagonal, we get heritabilities of the other traits and the off diagonal entries are the covariances between the traits. Now as we’ve seen at the very beginning of this lecture, a covariance matrix is sufficient to estimate a structural equation model. However, if we use a covariance matrix to estimate a structural equation model in lavaan, we need the sample size. Now these estimates don’t necessarily really have a sample size. Yes, the underlying GWAS has a sample size, but that doesn’t translate directly into a precision or standard error for these heritabilities. And so we need that information to be presented in a different way. So for every entry in this matrix, S we actually need to know it’s standard error or its variance, and we also need to know the covariance between the different estimates. Right. So imagine I estimate the heritability off height and BMI in one sample then those two heritability estimates are interdependent, right? Because it’s the same people in that sample that feed into those two estimates, and that dependency needs to be taken account off. That’s what we do in constructing this matrix V which basically has the squared standard errors or the variances off the heritabilities and genetic covariances on, on the diagonal. So all the elements of S have a diagonal element in V that corresponds to their standard error or their squared standard error, or variance. And then the off diagonal elements in V are actually the dependencies between the elements of S at this matrix takes a while to compute, but it’s basically computed in a very smart fashion that’s called Jack knifing in which we basically estimate the LD score regression in chunks of the genome, 200 chunks of the genome. And then we sample from those 200 chunks of the genome. We sample combinations of 199 chunks, and we estimate the matrix S 200 times. Each time omiting a different part of the genome, which gets us an estimate of the variance of all these elements in S and their covariance. Those, we store in V and then S and V are the matrices that actually go into genomic SEM now that sounds really hard and complex and computationally it is luckily others have solved the wonders of how to compute this for us. And so we can very easily implement it let me just show you.\nIn a browser. How is this done in genomic SEM it’s in the Wiki page of the genomic SEM GitHub chapter three: models without individual SNP effects. And it basically starts by preparing the GWAS summary data we get, which is couple of lines of code. Right, which gets us like the summary statistics in a standardized format. So LD score will know how to read them. And then after that’s done all, we need to tell LD score regression within genomic SEM is where to find, in this case, the summary statistics for psychiatric diseases. What the sample prevalence is of these diseases are in the respective GWASes what the population prevalence of the traits are in the population where it can find these, these LD scores I’ve been discussing what I want my traits to be named. And then I just run the function “ldsc()” and everything we just discussed. The estimating of S the estimating of V is done automatically, and it’s stored in an object, which you can then use to start running genomic SEM models. It also means you don’t need to rerun all those steps. You can store the object, you create it, so you don’t need to rerun LD score all the time.\nOkay. Thank you for joining us for this lecture And catch you in the next one."
  },
  {
    "objectID": "chapter9.3_transcript.html#sec-video4",
    "href": "chapter9.3_transcript.html#sec-video4",
    "title": "Chapter 9.3: 9.3 Genomic Structural Equation Modeling (Video Transcript)",
    "section": "Examples on the Genomic SEM wiki",
    "text": "Examples on the Genomic SEM wiki\nTitle: Working through examples on the Genomic SEM wiki one by one: munge, ldsc, usermodel functions\nPresenter(s): Michel Nivard\nIntroduction\nHi. And welcome to this tutorial on   genomic SEM and specifically a tutorial\non fitting genomics and models without  an individual snip effect. And today’s  \nvideo is on. how to perform those models. We also  have written a Wiki page on the GitHub. Right.  \nSo navigate to Github.com/GenomicSEM/GenomicSEM  then click on the header Wiki. And you’ll find.  \nA number of tutorial, pages or pages with  instructions, how to perform analyses in  \nGenomicSEM. And actually the third chapter of the  Wiki is models without individual snip effects.  \nAnd that’s what we’re discussing. In this video  and actually we’re running the code from this  \ntutorial. Right. And what the code will do it  will. First download GWAS a summary data for,  \nfor a number of psychiatric disorders,  specifically bipolar schizophrenia,  \ndepression. PTSD and anxiety disorder.  We’ll clean those summary statistics   \nin, in the sense that , we’ll take them from their  format as they’r oploaded by the authors in those  \nformats can vary quite a bit between authors.  And groups and put them into a single uniform.  \nformat This step we call munching. Then we’ll  take a step in which we’ll compute The genetic  \ncovariance matrix using LD score regression  and we’ll compute the matrix of. Standard  \nerrors associated with. Genetical variances and  covariances as discussed in a previous video,  \nright. We had a video about computing, the  matrix , S, the covariance matrix and the  \nmatrix V. Which is the covariance matrix of all   the elements in the covariance matrix once those  \ntwo steps are done. We can actually start fitting  models. Well first fit the Common factor model.  \nAnd then we’ll manually fit a model with a  common factor that loads on to schizophrenia.  \nBipolar MDD, PTSD and anxiety and add a second  factor. As an illustration. And basically allow  \nyou guys at home to follow along now to follow  along. You’ll need to download Summary data.  \nAnd this is a neat trick. If you’re on  a Mac or Linux machine. You can actually  \nrun. Command line scripts. In a terminal  in “R” right. So I’ve written this script.  \nTo download the LD scores we’ll need for LD score  regression, a list of HapMap SNPs. Which is the  \nlist of high-quality SNPs. We’ll use for LD score  regression. And also, the GWAS summary data from  \neither the PGC website or from James Walters  group. Who did a Cloz UK? GWAS of schizophrenia.  \nAnd unzip all those GWAS summary data. Now, this  will take quite a while to run across. It’s like  \nthree gigabytes of data you have to pull  in from the internet. I’ve already ran it.  \nSo that’s the step we’re going to assume you do  \nyourself at home Okay. Without further ado,  let’s move over to the script for this video.  \nAnd as a first step in this script, I’ll  require genomic SEM and the package data.Table.  \nAnd I will require data table because one  of the files. Has a column. That’s sort of.  \nNot easy to work with in it’s SNP column.  It actually has RSID and basepair and   \nchromosome and allele A1 and allele A2  concatenated together as one variable.  \nAnd that’s not what we want. We want. A  specific variable. That’s only the RSID  \nfor the SNP. And so step zero is to prepare.  Specifically that schizophrenia GWAS and   \npull out the rs id’s from that one column. And  I’ve written a bit of code here to do that.  \nAnd it will take quite a while to run and it will  heat up my computer a bit. So there. We go and  \nI’ll. Then have to patiently wait for this code  to run before we can actually take the next step.  \nmunge\nOkay. That code has managed to run and we’ve  prepared the summary data for further processing.  \nNow we are ready to munge. So the munge function  takes a number of arguments. Let’s look at the  \nhelp page real quickly. And we try to be good  about updating the help pages, but all of us.  \nDeveloper for Genomic SEM, have an actual career  on the side in which you have to do science. So  \nthey may lag behind Sometimes . Or they miss.  Certain information. So basically munge is a   \nfunction that processes. GWAS data and prepares  them for LD score regresison. So we’ll take  \nin those large GWAS summary data files and it’ll  write out. Smaller processed. Files, which are.  \nReady for use in LD score regression. Okay. So the  first. Arguments is called files and it actually  \njust is a vector of file names. Which are the  names of the Gus files you’re going to process.  \nRight. And the second argument is  HM3, HapMap3. And it’s basically.  \nI don’t know why we call it. HM3. We could have  called it filter or, you know SNPs or selection  \nor whatever, but it’s argument you use too provide  a list of SNPs you think are high quality,  \nsNPs or highly efficiently imputed SNPs. And  for LD score regression people have commonly  \nused those HapMap3 SNPs. Because they’re well  imputed. They behave well So we trust. LD score  \nregression analyses based on these SNPs will  be. Good reflection of heritability or genetic  \ncorrelation between traits. Okay. The next  argument. Is a vector of names for your traits.  \nGotta be said: pick memorable names because  you’ll need those names later. They’ll The prefix  \nfor the file names. Okay. So very long strings of  unintelligible things are probably not useful.  \nEspecially if you return to a project in  seven months and you’ve called something  \nflip flop, flip. Then you won’t know what  it is, whereas if you call it. SCZ for  \nschizophrenia or BIP for bipolar. You might  still recall. What the file was you processed,  \nthough? It’s better to rely on scripts. then too  rely on file names then. A vector of sample sizes.  \nAn info filter. For. Files that if these, GWAS   files, have an info column. It will filter SNPs   \nwith an info below 0.9. And a minor allele  frequency filter. If these files have a minor  \nallele frequency column, it will filter SNPs  with an minorly frequency. Below 0.01. Now I say  \nif, because not all GWASes come with info  and MAF. Columns. And so you will have to.  \nWork with what you’ve got. Right. And this  is another reason to consider those high  \nquality HapMap3 SNPs, because sometimes you are  simply unable to filter. On things like info,  \nwhich reflects imputation quality. Or minor  allele frequency. Okay. Let’s run this code.  \nI should have started running it. While I was  talking, but I didn’t. So. Now we’ll have to wait  \na bit to. And as you can see. While its running  it will keep you up to date via the terminal.  \nAnd it will also write. A log file. And. Usually  takes a few minutes. About a minutes per file.  \nWhich isn’t that long, but if you’re  fitting models with like 50 traits,  \nit will take 50 minutes. Right. So as a, as a  rule of thumb, As many minutes , as you have  \ntraits and depending on how many traits you   have, he may go for a cup of coffee. You may just  \ncheck your Twitter, or you may just want  to, you know, take the afternoon off.  \nAnd have a nice long hike. Okay. So the munge. in  LD score regression is done it took. 12 minutes.  \nA bit longer than I expected, but that’s probably  because I’m screen recording at the same time. And  \nif you scroll back up a bit and, or look at  the log file, it has produced. You’ll see. It  \ntells you what it’s doing and munging five.  summary statistics files. Time it started.  \nNames of the files, which may be important if  you have a lot of file names that are similar,  \nmake sure the right files are read in and it  will then tell you for each column in each file.  \nWhat it’s interpreted in it as so it finds a  column. “SNP”. And it says I’m interpreting   \nthat as the SNP. And other files. I  may find something like SNPID. And it  \nwill actually know that that’s probably SNP.  Makes sense for you to go through this file  \nand check whether the things. It’s doing. Are  correct right So important to check whether these  \ncolumn names are? What you want them to be?  Or are interpreted as you want them to be.  \nThen it will start filtering. remove rows  that are in. The summary statistics file,  \nbut not in the HapMap3 SNP lists , the snips  we think are high quality SNP of want to  \nkeep. It will then determine effect Column. To  either be an odds ratio or a beta, it will do  \nthis by determining the mean or the median of the  column. Overall snips of the median is around one  \nor two meanness around one and we’ll  think, well, that’s an alternate show.  \nIf the mean, or the median is around zero. it  will asuume that that’s actually a. Beta. Right.  \nAnd. Makes sense for you to double check   whether it’s doing this correctly. Okay.  \nAnd. So usually for a paper. You would take  an hour, half an hour to go through these.  \nLog files and make sure everything’s done  correctly. Once you’ve run. LD score regression.  \nYour working directory will contain sumstats.gz  files which are like 12 or 13 megabytes. They’re  \nfar smaller than GWAS summary statistics  because they only contain five columns.  \nRemoving all the excess information you don’t  need. And they only contained 1.3 million SNPs.  \nNow mind you, if the overlap between yourGWAS  and the HapMap SNPs is only like 150,000 SNPS.  \nYou don’t want to run LD score regression. You  want to figure out why the overlap is so small.  \nRight. So it’s also one of the things,   the log will report to you. It’s how many.  \nSNPS are deleted for what reason? Now. This  example I’m in here. Three and 260,000 SNPs  \nwhere removed from the bipolar GWAS because  the info. Doesn’t cross the threshold of 0.6.  \nRight. So that’s. The reason for exclusion.  You can go back to the bipolar file and  \nmanually check whether that’s true. If, if  you somehow suspect. There’s something amiss  \nwith your bipolar file or your analysis. For LD  score regression needed to find some arguments.  \nFirst for argument is which traits we’re going  to use. The sample prevalences of the GWASes,  \nNow Andrew has been so kind. At some point  to check these out and look this up and.  \nWell, he had to. We were using them in the   original genomics SEM paper. And then,  \nthen also the associated population  prevalence of these disorders. So here,  \nwe’re assuming schizophrenia as a 1% population  prevalence bipolar has a 1% population prevalence.  \nAnd for example, MDD, has a 16%? Population  preference. And then you will also need to point  \nLD score regression to where it can actually  find the LD scores, which as we’ve shown before,  \nhave been downloaded from the internets and  then you define trait names. at this step,  \nthe trait names you define will be your variable  names in your model later. So definitely here you  \nwant to choose. Memorable names. Okay, let’s run  this. Which again will take a few minutes. But,  \nyou know, But the magic of video editing, I can  omit those minutes and make this video move along  \nrapidly. But you may want to go an have cup of  coffee. Okay, that finished running. And if you  \nscroll back up a bit, you can see that. It will  produce sort of information for you to consider,  \nright. It will tell you. Okay You mean chi- square  across the SNPs for the MDD sumstats is one  \npoint in 26, Lambda GC, a metric off genome-wide  inflation. The LD score intercept, which is   \nsupposed to be one, an excess of one is actually a  metrical population stratification in your GWAS.  \nIt will report the heritability. On the observed  scale and the heritability’s Z statistic now.  \nIt will do so for all the heritabilities   and genetic covariances right. And at the  \nbottom. I will report Genetic correlations and  heritabilities. All of this is those are written  \nto a log. Which you can consider at your leisure   and make sure that everything’s correct. Before  \nyou consider even publishing. Genomic SEM results.  Now let’s have a look at what was created by this  \nLD score regression function. So we’re going to  do some ad hoc. Inspection of this R object the  \nLDSCoutput is where you’ve stored. the output a  few of your LD score regression and it contains  \na matrix V. And the matrix S and the matrix Five  by five and it’s a genetic governance matrix.  \nSo let’s have a look. The row names of the matrix  are the variable names and on the diagonal,  \nyou’ll find the heritabilities of the  traits on the liability scale, because it  \nconverted the observed scale heritabilities to   reliability scale using the population. And.  \nSample prevalences. You can find the details of  that in Grotzinger et al on how that does that.  \nAnd on the offdiagonal elements, you’ll   find the covariances between these traits.  \nNo. As Andrew was discussed in another video.  And I have discussed in the other video as well.  \nStructural equation model actually takes the  covariance matrix as observed in the data. And  \nthe covariance matrix implied by your model and  tries to pick the parameters for the model such.  \nThat it minimizes the distance between those two  covariance matrices. And that’s what we’re going  \nto do right now. The first model will fit. I’m  going to safe. This output. Right. So we don’t  \ncommon factor\nhave to rerun all those slow steps again. The  first model will fit. is a common factor , which  \nis basically the model you’ll see here. Which  says that a single latent factor explains.  \nThe covariance between the psychiatric  disorders we’re considering now such a model.  \nIn some people’s eyes. And I think that’s  a really reasonable perspective. carries a  \nstrong causal implication named me that there  is a shared cause. of these traits. Right.  \nAnd we can get into how you can actually test  whether that’s a reasonable hypothesis. Later.  \nNow we’re just going to run the model okay. Let’s  run the common factor now running. Muddle doesn’t  \nmodel output\nreally take that long. 0.7 seconds to be  exact. Now you can imagine if you run   \na GWAS with 10 million SNPs. 0.7 seconds. times  ten million. It’s actually quite a bit of time.  \nAnd then we’d want to parallelize it and   run it on, on a cluster computer perhaps.  \nBut right now for this. SNP less model. It doesn’t  matter. And so the output you get from. Let me  \njust resize the window here so we can like check  the output out. From. LD score regression is.  \nNo object within it. some model fits. That you  can consider. To compare two models. For example,  \nthe difference in chi-squares and degrees of  freedom. You can test significance, test. I’m  \ndifferent than your models. If the mdeols are not  nestedit. You can consider the AIC. To compare  \nmodels and then there is the CFI. and SRMR,  which are. Not relative metrics of two models,  \nbut absolute metrics. And I refer you to the  literature or the Wiki page or our articles for  \ndetails on how to interpret. These metrics and  wherher you consider something good. This will  \nalso depend on your application. And many factors.  I also encouraged you to be strict on yourself.  \nAnd not just consider something good   cause you wants the model to be good, but  \nI can only encourage you to do so. And if you  pick me as a reviewer, I will compel you to do  \nso. So, you know, Careful what you wish for in  listing me as a reviewer on your genomic SEM.  \nArticle. I’m pretty sure the other Genomic   SEM authors are similarly inclined. So,  \nyou know, you’ll have to deal with someone’s.  Neuroses about fit statistics. Then the other.  \nPart of the model. Output is actually the  parameters, right? So this is the loading  \nof Schizophrenia the first factor F1.  And this is the value of the loading.  \nIt’s a standard error and this is the  standardized loading.. Standardized loading   \nalso has standard error. And there’s a B value.  Very significant parameter. If you are too.  \nYou were interested in. Destiny department  you’ll get factor loadings. But you also get  \nresidual variances. And so.residual variances, are  variances in the traits that are not explained.  \nby. The factor and they’re on the scale of the  original heritability. So 9% of the variance. in  \nbipolar disorder is not explained by the latent  factor. Okay. Good to know. Goods let’s move on.  \nThis was a single common factor model. And we’ve  been so kind to define a function specifically.  \nfitting models\nFor you to use to fit the common   factor model, but in many cases you  \nmay want to fit a different model.  Right. So to get into, get into the   \nhabits or fitting our own models. We’re going to  start by defining a common factor model. Now. You  \nshould really, if you haven’t watched Andrew’s  video on lavaan syntax, you should pause now  \nand go watch that video. And then come back  because we’re going to use lavaan syntax,  \nand I’m going to assume you understand  it because he has already explained it.  \nRight. So please do that and comeback. Okay, well,  If you’re still here. You had watched the video  \nready? You’re just back. Welcome back. Here we  define a common factor model we define it as.  \nlatent factor F1. Which is measured by that’s  what this, this. Equal sign plus tilde means   \nmeasured by schizophrenia, bipolar, MDD, PTSD, and  anxiety. Right. factor causes variance in those.  \nDisorders. And then we also define factor  variance. to be one. We fix it to be one. Now  \nit’s an identifying characteristic. We need  to fix the variance of the factor to one,  \nor we need to fix one of the loadings on one of  the indicator for disorders to one. And let’s  \nrun. common factor model. Using. The code for user  model. . The codes are actually designed to read  \nyour model. Which may be different than a single   factor model and fit it to the data. And so  \nwe’ve already seen the fit of the model and  those of you who’ve seen, seen model fits  \nbefore will know. That’s. S R M R Point 22 is  pretty high in a CFI. point 85 pretty low. So  \nthe fit of the model to the data is not great.  And we can try and improve the fit to the model.  \nOkay, so we can try and fit. Another model.  And to determine what kind of model we should  \nfit. We should definitely try and check out  what the, what the residual co-variants is, are.  \nIn the model. And so one of the arguments we  can add to udermodel. Is imp_cov and it will give  \nus the implied covariance matrix. We’ll set that  argument to true. And we’ll rerun the model. Now.  \nDoing so. gives us. The model implied covariance  matrix and a difference between the model implied  \nand the observed covariance matrix, right. Which  we calculate as observed minus implied. And it  \nwill tell us. Where there is still covariance   between traits that is not explained well by the  \nmodel. And if we look at this matrix, we’ll see  that there’s actually. Slides. Well, actually,  \nmaybe a substantial residual correlation between  MDD and anxiety, which are very similar disorders.  \nAnd their covariance isn’t explained by only the  common factor. And so a model we could consider.  \nIs a model in which we allow a residual   covariance to exist between MDD. And.  \nAnxiety. And we’ll call it.common factor model  two. We’ll call it output. common factor self  \ntwo. Right. To distinguish it from the previous  model. And we can check whether this model fits  \nthe data better. lets go, lets run it and scroll  up and figure out. Whether there’s anything. In  \nterms of model fit tat improves. Well, Wow.  So the CFI of this model, it was point 98,  \nwhich is way higher than the one before.  Right. We can look back was like point 85. And  \nS R M R is . Way lower. So that those are in   the correct direction. That’s what you want.  \nThe model fits the data better. If we allow for  residual covariance between MDD and anxiety.  \nSo the last bit of this video went a bit chaotic.  But that’s what you get. If you ad hoc try to  \nfit a new model. You can. Try and improve the  model further. Or yourself you could consider.  \nWhether they are still residual. A genetic  correlation between schizophrenia bipolar.  \nMaybe you removed to genetic residual correlation  between anxiety and MDD, all kinds of things you  \ncan consider. And I encourage you to try.  And some of those things we may introduce  \nin life practicals in June. Thank you for  watching and catch you in the next one."
  },
  {
    "objectID": "chapter9.3_transcript.html#sec-video5",
    "href": "chapter9.3_transcript.html#sec-video5",
    "title": "Chapter 9.3: 9.3 Genomic Structural Equation Modeling (Video Transcript)",
    "section": "Multivariate GWAS in Genomic SEM",
    "text": "Multivariate GWAS in Genomic SEM\nTitle: Multivariate GWAS in Genomic SEM\nPresenter(s): Andrew Grotzinger\nIn this video, we’ll be talking   about how to perform multi-variate GWAS   using genomic SEM. Multivariate GWAS  consists of four primary steps. The first\none being munging the summary statistics,  which we’ll talk about in just a second,   the second to run multi-variable LD score  regression within genomic structural equation   \nmodeling to obtain the genetic covariance and  sampling covariance matrices across the GWAS  \nsummary statistics. And note to say that these  first two steps mirror the steps that you would  \ngo through to estimate a model without individual  SNP effects. Including for the user model and   \ncommon factor functions that Michel talked about  in the previous video and do not need to be   run again. Just for the purposes of running a  multi-variate GWAS. And the third step you’ll  \nprepare the summary statistics for multi-variate  GWAS using the sumstats function. And finally,  \nyou’ll actually run the multi-variate  GWAS using common factor or userGWASs.  \nFor this example, we’re going to use the five  psychiatric traits from the GitHub example for the   P factor across schizophrenia, bipolar disorder,  major depressive disorder, PTSD, and anxiety.  \nAnd these are all publicly available summary  stats that are directly available for download   The first step again is to manage the data  where munge literally just refers to the general  \nprocess of converting raw data from one form to  another. Munge primarily converting the summary  \nstatistics, Z statistics. It’s aligning all  the summary stats, the same reference, allele.  \nAnd it’s restricting them to hapmap3 SNPs, both  because these tend to be well imputed and even  \nwith just those 1.1 million hapmap3 SNPs you tend  to get a reasonable estimate of the heritability.  \nSo sometimes people will be really   concerned when they have this large set   of eight to 10 million SNPs and then they run  it through munge and they only have about one  \nmillion SNPs left, but this   isn’t cause for concern,   because that is enough to get an accurate  estimate of heritability using the LDSC equation.  \nWhen you run munge it’s going to produce   a dot log file for each of your traits.   And this is something that’s important to check,  just to make sure all of your columns are being  \ninterpreted correctly. I think in general, there  can be this push to plug forward with the results  \nand not really take a look at your data or some  of these log files that are produced by different  \npackages, but you definitely want to make sure  before going through all of the additional steps  \nthat the data is being read in appropriately.  And one particular thing that I’ve highlighted   here is for case control traits, you really  want to make sure that that effect column  \nis being interpreted correctly  as either an odds ratio or not.   So for MDD I know that this is an odds ratio and  I see that the munge function is interpreting that  \nas such. I just want to go over to R now just  to walk through this code as we go along.  \nSo up here, I’ve just set the working directory  to where I’ve stored all of these files. This will  \nlook a little bit different in terms of how you  do this if you’re on something that’s not a Mac   operating system and then I load in genomic  SEM and also this function data dot table.  \nBefore running munge something I want to  highlight is that I actually have to do something   to this schizophrenia summary statistics so  that munge can read this data appropriately.  \nSo I’ve already read in the schizophrenia   summary statistics using F read. And if you  \nlook at a particular row within schizophrenia,  you’ll see that within that SNP column, it’s    not just the RSID identifier for that SNP but it’s  in the format of RSID. Colon based pear colon   \na one colon eight two. And that’s not something  that munge is going to know how to read. So   prior to actually running munge I use these two  lines of code to first split that SNP column so  \nit’s just pulling out the RSID using string split  and sapply, and then writing out a new GWAS file  \ntitled scz_withRS. And then for munge we list the  files, the hpmap3 lists the names of the traits  \nand then the total sample size before running  munge this is not something I’m going to do right   now just for time reasons and because Michel will  have gone over it in the previous video, but just  \nto show you what the code looks like for this  first step. Switching back over to our slides,   \nthe next step is, is going to be to run  LD score regression, which computes that   genetic covariance and sampling covariance matrix  that was discussed in one of Michel’s videos.  \nSo this is The level of genetic overlap across  these different traits is estimated using LD  \nscore regression. And then also the standard  errors and dependencies across those estimates  \nas will be the case when there is sample  overlap. And this sampling covariance matrix    is what allows genomic SEM to produce accurate  estimates, even in the face of unknown levels of  \nsample overlap across your traits. I’ll just note  that before going on to steps three and four,  \nI would highly recommend pausing at step two  and actually fitting what I sometimes call  \nthe base model using the user model or common   factor functions, that model that doesn’t include  \nthe effect of an individual SNP on different  parameters in the model, just to make sure that   you’re getting reasonable estimates, that it fits  well and that lavaan or genomic SEM don’t produce  \nany warnings or errors about this particular  model, because odds are when you then carry   that model forward to multivariate GWAS a lot of  the same problems are going to start to show up.  \nSo you just want to diagnose that right. Make  sure you’ve got this solid base model and then   carry that forward to multi-variate GWAS and  step four. So going back over to the R script.  \nLD score regression takes the names of the month  summary statistics for the case control trades. It  \ntakes the sample prevalence of cases over the  total sample size, the population prevalence,  \nwhich can be pulled from the research literature,  the LD scores and the weights used for LD score  \nregression. Oftentimes this will be the same   folder for both of these and the trait names for  \nyour summary statistics. This particular argument  trade names is important because this is how  \nyou’re going to name these traits when you  specify the model in lavaan. So you want to   make sure you don’t name it something with  a bunch of upper and lowercase characters,   \nsomething that’s easy to write out when  you go to write your model on later steps,   and then you just run. LDSC this’ll take  about a minute with only five trades. Again,  \nI’m just not going to do it here for time reasons  and I’m going to load in that LDSC output that  \nI created before which is something that I just  saved using this command here. The third step is  \nsumstats and before I go back over to the slides  to talk about some of the arguments for sumstats,   I just want to reading these arguments and set  sumstats up to run. So we’re going to just let  \nthis run and go back over to the slides to  talk about what sumstats is actually doing.  \nSo just like munge sumstats to make sure that  in all cases that the GWAS summary statistics  \nare aligned to the same reference to allele  and further the coefficients and their standard   errors are transformed so that they’re scaled  relative to unit variance scaled phenotypes.  \nWhat that means is that it makes sure that  the GWAS estimates are scaled relative to  \na standardized outcome. Or what is sometimes  referred to as STDY or partially standardized  \nregression, coefficients and standard errors.  We are not standardizing with respect to the   predictor. I E the SNP, but just to the outcome.  And the reason that’s important is because  \nwe’re going to take this some steps output, and  we’re going to add it to the genetic covariance   matrix from LD score regression that we just  created in step two. And that genetic covariance  \nmatrix from LD score regression is itself on a  standardized scale where the heritabilities on  \nthe diagonal are by definition scaled relative to  a standardized phenotype. So we want to make sure  \nthat when we add this, sumstats out, put to that  LSDC matrix, which I’ll show you visually in just   a couple of slides that they’re on the same scale  so that we can produce the appropriate estimates.  \nIn order to do that rescaling appropriately.  sumstats needs to know a little bit of    information about the scale of the outcome and  how the GWAS was specifically run. So this takes  \na number of arguments in order to make sure  things are done appropriately. And I just want   to walk through those arguments here. So the  first argument for sumstats is the name of the  \nsummary statistics files. This is not the munged  files and should be the same as the name of the  \nfiles used for the munge function. So it’s the  raw summary stats that you provide to munge.  \nAnd it should be listed in the same order that  you listed them for the LDSC function in step two.  \nThe second argument is the reference file that’s  used to calculate SNP variance and aligned to a   single reference allele across traits. Here  we’re going to use an a thousand genomes  \nreferenced file from a European population.  The third argument is the name of the traits.  \nThis’ll probably be the same as how you’ve been  naming the traits for the ldsc and munge function.   And the fourth argument is se.logit, which  is a vector of that includes TRUE or FALSE  \nfor each trait that indicates whether or not the  standard errors in those GWAS summary statistics  \nare on a logistic scale. The reason that we make  this a required argument is because oftentimes  \nGWAS summary statistics somewhat   counter-intuitively will list an odds ratio,  \nbut then they will list a standard error of a  logistic beta and so we want to make sure that  \nthe user is being sure to double check this. And  this information, if you’re unsure, can often be  \nfound in the readME file for the GWAS summary  statistics for those case control outcomes.  \nThe fifth argument is whether the phenotype  was a continuous outcome analyzed using an   observed least squares or what is referred to  as an OLS or more commonly linear estimator.  \nThe following the argument is linprob, which  refers to an instance where a phenotype was a  \ndichotomous outcome analyzed using an OLS  estimator. This is referred to as a linear   \nprobability model and is often run just for  simplicity sake, becauseis computationally  \nmuch easier to analyzea dichotomous outcome  using OLS, but in order to do that rescaling,  \nwe need to know whether or not this   particular situation is occurring. Proportion  \nis something that is specified in  conjunction with the lineprobargument   and it’s necessary in order to perform the  linear probability model I E LPM conversion above.  \nSo it takes the proportion of cases over the  total sample size. N is a provided sample size  \nin the order the traits are listed and as only  needed when OLS or linprob is true for any of the  \ntraits. Info and MAF filter are standard filters  use to filter on amputation quality for info  \nand to filter on the minor of legal frequency  with package defaults of 0.6, 0.01. Keep indel  \nrefers to whether you want to retain insertion  deletions with the default being FALSE. Parallel  \nrefers to whether or not the function should be  run in parallel and utilize multiple cores on the  \ncomputer with the default be to run FALSE and if  you are running in parallel, you can specify the  \ncore’s argument that’s indicates whether you want  the computer to use a certain number of cores.   The summary statistics or sumstats argument  will typically only take in this case,  \nit’ll take about eight minutes for 30 trades it  might take. Upwards of an hour. So you certainly  \ncan run in parallel and speed things up, but it’s  not necessary to run in parallel by any means.  \nSo I know that the, sumstats argument can be  a little bit confusing and for that reason,  \nI’ve created a schematic on the GitHub Wiki.  So this is on the second page of the Wiki   \nimportant resources and key information, that just  walks you through how to think about specifying   these arguments. And it starts with this first  question is the GWAS outcome binary continuous,  \nand just lets you know, what, how you   should specify these different arguments.  \nIf we go back over here to R you can see that  I’ve specified those file names. The name of  \nthat reference file, which is available in a  box link listed on our Wiki. The trait names,  \nfor all of these, these are case  control outcomes and they are   reporting standard errors and logistic scales  so I set se.logit to TRUE for all of them and  \nI use the default info filters. For completeness,  I’ve listed all of the different arguments here,  \nbut you can certainly write this in a more  compact form, you don’t have to write OLS   equals NULL linprob NULL prop equals NULL if  you don’t have any OLS or linprob outcomes.  \nAnd here I am running in parallel using four  cores. We’re just going to let this finish up    here and while that’s happening we’ll  move on to talking about the functions.  \nBefore doing that a note that the, sumstats  function will also produce a log file like munge  \nand so, again, it’s imperative that you look at  these log files and just make sure everything’s    interpreted correctly and much like major  depression that I showed you for munge we   \nwant to check here that for bipolar, the effect  column is in fact, appropriately interpreted as  \nan odds ratio. So I’m going to first talk about  common factor GWAS a function and then I’ll   end by talking about the userGWAS function. What  commonfactorGWAS is doing is it’s automatically  \nspecifying a common factor model where the SNP  is specified to predict the common factor.  \nWhat’s happening behind the scenes for both of  the GWAS functions is it’s automatically combining   the output from step two from LD score regression  and step three, which we’re running right now  \nfrom sumstats. So what it does is it one by one  takes the LDSC matrix, it takes a particular row  \nfor an individual SNP from sumstats, and it  adds it to that matrix. So now that you’ve got  \nthe LDSC matrix and then this appended  column or vector of individuals SNP  \ncovariance affects between the SNP and these  five psychiatric traits. And what it’s going   to do is it’s going to create this matrix,  run the model, and then discard the matrix.  \nAnd so it’s going to create as many covariance  matrix sees as there are SNPs across the trait.  \nSo effectively, if we then take that matrix  and run the model, it’s then specifying this   model where the SNP is predicting this general  factor that indexesrisk sharingacross these five  \npsychiatric traits. So this is an example of  just one model that’s being run, but again,  \nthis first vector here is swapped  out as many times as there are SNPs   and it’s re estimated to get that updated   estimate of the SNP effect on the factor  \ncommonfactorGWAS takes a couple of arguments.  The first is covstruc, which is the output  \nfrom LD score regression. The second is  SNPs, which is the output from sumstats.  \nThe third optional argument is estimation,  which specifies whether models should be   estimated using DWLS, which refers to diagonally  weighted least squares, or ML, which refers to  \nmaximum likelihood estimation where the package  default is DWLS. The way to think about the  \ndifference between these two is DWLS will look  to produce model estimates that are trying to  \nrecapture the parts of the observed covariance  matrix that are estimated with greater precision.  \nThis does not mean that DWLS is going   to automatically produce things like  \nlarger factor loadings for the GWAS   traits with a larger sample size.  \nInstead if you’ve got a particularly   well powered GWAS that goes into this model   and that GWAS does not correlate very highly  with the other traits, then the model will  \nactually prioritize producing, in the context of  a common factor model, a particularly low factor   \nloading for that well powered trait. So again,  it doesn’t mean that the well powered trait  \ndominates the model per se, in the sense  that it’s producing larger estimates,  \nit just means that DWLS is taking into account   the information that’s available. Depending on  \nhow you think about statistical modeling, you  might have a different preference between them,    but to our mind, if you’ve got more information  about a particular cell in that covariance  \nmatrix, that’s going to reflect a GWAS that’s  better powered, why not use that information   appropriately and let the model appropriately  favor reproducing that part of the matrix.  \nCores is how many computer cores   to use when running in parallel,   where the default is to use one less core than  it’s available in the local computing environment.  \nBut you can specify as many cores you want  using this argument. Tolerance is only relevant  \nif you start getting errors or warnings  about matrix inversion, but beyond that,   it’s not something that you need to be concerned  about . Parallel is an optional argument  \nspecifying whether you want the function to be   run in parallel or to be run serially. GCs, the  \nlevel of genomic control you want the function  to use. The default is to adjust the univariate   GWAS standard errors by multiplying them by the  square root of the univariate LDSC intercept.  \nWhat that does is it takes this univariate LDSC  intercept, which is intended to index uncontrolled  \nfor population stratification and that it  appropriately corrects those standard errors by   the intercept so that you’re producing estimates  that pull out that uncontrolled for pop Strat.  \nIf the LDSC intercept is below one, I’ll just  note that what the package is going to do is   not correct for the intercept at all. So it’s  never going to produce more liberal estimates  \nthan a univariate GWAS going in, but it’s  going to be more conservative as a default.  \nMPI is whether the function should   use message passage interfacing which   allows you to use multi-node processing. Which  we’ll talk a little bit more at the very end,  \nwhen we talk about runtime considerations   for these different functions.   So now if we go back over to R we can see that  the sumstats function has finished up running.  \nIt took about seven minutes . And now going on  to step four of actually running the common factor  \nGWAS just for the purposes of this exercise,  I’m just going to subset to the first 100 rows.  \nSo that’s sumstats output just so we can see how  the common factor GWAS functions are running. So  \nwe’re just going to let this run here. And as it’s  doing that, I’ll just show you that we’ve got.   covstruc that lists the LDSC output, SNPs,  that list, that subset output from sumstats  \nthat we’re using DWLS we’re using four cores  and we don’t need to set the tolerance lower.  \nWe’re not changing the SNP standard error that  it uses. We’re running in parallel. We’re using   the standard GC correction and we’re not using  MPI. SNP standard error just refers to whether or  \nnot you want to change the standard error of the  SNP, this is just set to a really small value to  \nreflect the fact that that’s coming from our  reference panels, so we essentially treat it   as fixed. But it is not something that   really affects your model estimates out to  \nthe fifth decimal place. So that finished running.  So let’s just take a look at the first five rows.  \nAnd what you can see here is that it’s pulling  the SNP RSID, the chromosome, the base pair,  \nthe minor allele frequency from the reference  file that you fed to sumstats, A1 and A2,  \njust the run number, the particular estimate from  the model that was saved, which for common factor  \nGWAS is always going to be the effect of the SNP  on the common factor. The corresponding estimate  \nfor that parameter, the sandwich corrected  standard error, the Z statistic, the P value. And  \nthen this Q statistic and it’s degrees of freedom  and P value. There’s also this fail and warning  \ncolumn at the end. That can be good to check  using something like the table argument in R   just to see if any warnings or runs were   just failing to produce any output. Zero  \nindicates that there was no warnings run. And  we can see here that for these a hundred SNPs,   that there were no issues that were raised before  I switched back over to the slides to talk about  \nQ some more. I’ll just highlight that for a lot   of this code I’m just including for completeness,  \nthe arguments that are listed, including their  defaults. So MPI is automatically set to false   \nGCs, automatically set to standard. So we could  produce the same output in a much more compact  \nform, writing this code here below. So this  will just produce the same results. And it’s   just a highlight that , if you’re setting  the arguments to the default behavior,  \nyou don’t have to list them. So what you saw on  that output was three columns related to this  \nQ SNPs statistic, which is an estimate of SNP  level heterogeneity. And the way to think about  \nQ SNP is it asked the extent to which the effect  of a SNP operates through a common factor. It’s  \na chi-square distributed test statistic that is  essentially comparing the fit of a common pathways  \nmodel in which this SNP operates strictly via the  common factor to an independent pathways model in  \nwhich the SNP directly predicts the indicators of  that common factor. If this independent pathways  \nmodel fits much better than this common pathways  model, then that suggests that the SNP is not   really operating through the common factor,  that this single regression relationship is  \nnot sufficient for capturing the pattern of the  relationships across these five indicators here.  \nInstances where you might expect Q  SNP to be significant include when,   for example, there are directionally opposing  effects of the SNP on different indicators.  \nSo let’s say the SNP is risk conferring   for the first two indicators  \nand is actually protective for the last three  indicators . In that case, it’s clearly not   operating through some general common factor  and we would expect Q SNP to be significant.  \nAnother instance might be at the SNP has a  really strong effect on one of the indicators.   And I sort of null or at least a much weaker  effect across the remaining indicators.  \nAs a canonical example of this if we ever include  alcohol use disorder or any alcohol use phenotype,  \nreally within a genomic structural equation  model, we often find that the variants that    exist within the alcohol dehydrogenase genes  will pop a significant for Q SNP. And that’s  \nbecause those tend to be genetic variants that  are not associated with the general factor that  \nalcohol use is loading on but are instead of  highly specific to that alcohol use phenotype.  \nAnd what is cool about Q SNP is that when you’ve  got a set of really highly correlated trades,   In fact, what might be more interesting is  what actually causes these traits to diverged.  \nIs to identify via this Q SNP statistic, what  it is that really genetically differentiates  \nthis disorders. Wouldn’t it be nice if we  could use Q SNP to gain some novel insight  \nabout what causes these things to have   a slightly different presentation.  \nif you’re a specifying a model that is   something that is not a common factor model,   then you’re going to want to use userGWAS, which  allows the user to specify any model that includes  \nthe effect of individual SNPs, such as a SNP,  predicting multiple correlated factors in the  \nmodel. userGWAS takes all of the same arguments  that I just showed you for commonfactorGWAS,  \nalong with two additional arguments. One of  those is the model that you’re estimating   written using lavaan syntax. And for this model,  the way that you’re going to include the SNP in  \nthe model is just to literally refer to it as SNP  or SNP in all capital letters. And I’ll show that  \nmodel over in the R script. in just a second.  The second is the sub argument and this is an  \noptional argument specifying whether or not you’re  going to request to save only specific components   of the model output. The reason I would recommend  almost always setting this argument to something  \nis that there’s a lot of different rows for any  given model that lavaan is going to produce. So  \nfor example, for the common factor model, there’s  the five factor loadings, the five residual   variances of the indicators, all of which are  going to be fairly similar across all of the SNPs.  \nAnd it would take up a lot of space to save all  of that output for each individual SNP. And what  \nwe’re really interested is just the effect of the  SNP on the common factor. And so what sub allows   us to do is say, I just want you to save that  output for the SNP effect on the common factor.  \nAnd if you’re specifying a model in which you’re  interested in multiple different parameters,   you can also set sub to include more than just  one parameter, but again, it’s rarely going to  \nbe the case that you’re interested in saving every  single piece of the model output for each SNP.  \nAnd instead you should think about just  saving those pieces that you’re interested in.  \nIf we switch back over to Rstudio to run  userGWAS, what I’m going to do is I’m going to   run the userGWAS, but for the exact same model as  common factor, GWAS is automatically specifying.  \nSo here we’ve written the common factor model  with the factor regressed on the SNP here on the  \nbottom and we’re also setting that sub argument  to just save the effect of the SNP on the factor.  \nSo let’s set that up to run. And what this should  do is produce the exact same estimates that we  \nsaw for common factor GWAS. And I’ll show that  to you in just a second, when this finishes up.  \nSo if we look at the first five rows from the  user GWAS output and the first five rows from  \nthe common factor, GWAS output, you can  see that these are exactly the same 0.413.  \nAnd so on the user, GWAS, output’s going to look  a little bit different. It’s not going to include  \nthe Q statistic, but instead it’s going to  include model fit statistics. What’s the overall   fit of that model. So it’s going to include Chi   square, Chi square degrees of speed, chi-square P  \nvalue, and AIC. And those can be used to compare  to what are referred to as nested models.  \nSo you could examine a independent pathways  model where that SNP is predicting each of  \nthe five indicators and then if you did a  model comparison across those two models,   \nyou would find that that produces the same thing  as Q SNP down here. So if we take a look at the  \nrun times across this. I know for the first two   steps, I didn’t run them now. But if you look at   the output files, you’ll find that for munch  on my own laptop and took about eight minutes.  \nLDSC took a little over a minute. sumstats took  about seven minutes. The common factor GWAS took  \nabout 17 seconds and the user GWAS 10 seconds.  For those GWAS functions, of course, we only ran  \nit on the first a hundred SNP and we did run it in  parallel. This is not necessarily indicative of   \nhow long it would take for a million SNPs  you wouldn’t just multiply these numbers  \nby a certain amount because there’s certain  initial steps that the GWAS functions need   to go through. At the same time, the GWAS   functions do take a while. How long that takes  \nexactly. It’s going to depend on the number of   traits and how complicated your model is. So,  \nI never know exact runtime considerations,  but these are things that you are going to  \nideally be running on a computing cluster and I  want to talk finally about how to really speed  \nthis up so that you can get results as quick as  possible. So both parallel and MPI processing  \nare available for userGWAS and commonfactorGWAS.  Where parallel and serial processing are doing  \nthe exact same thing with the exception  that parallel processing is utilizing   the core is available in your computing  environment to send off different chunks of SNPs,  \nto the different cores, to then run the GWAS on  those cores. MPI takes advantage of a multi-node  \ncomputing environment. It does require that Rmpi  is already installed on the computing cluster,  \nbut that then adds this additional layer that  it sends the output off to multiple nodes and  \nthen often multiple cores within those nodes.  And then finally, you can speed this up just  \nthat much more by sending off separate jobs that  then themselves use MPI processing, where they  \nsend it off to different nodes in different  cores within those nodes. The important thing   to know is that all runs are independent of one  another. So you can dice up that sumstats output,  \nhowever you want, and you’ll still get the  same output. So for me, anytime I run a GWAS  \non the computing cluster , I will send off 40  jobs that then run in MPI. And for this common  \nfactor, GWAS example for two ish million SNPs  that only ends up taking about two to three hours.  \nSo again, if you reach out to me and ask, what’s  the exact runtime I should expect for this model,   I’m not going to know because it’s really gonna  depend on the type of model you’re running.  \nFor sure there are indicators that something is  going wrong. Like if you submit a hundred SNPs   \nand it’s taking 12 hours to run that suggests that  something is just breaking apart on the computing  \ncluster. And you’re certainly welcome to reach  out on the Google group to see if we have any   input about how best to speed things up. So with  that, I’ll just end here and in the next videos,  \nwe’ll talk about some of the newer functionality  available in genomic SEM including the ability to   examine multi-variate enrichment, using   what we call stratified genomic SEM."
  },
  {
    "objectID": "chapter9.3_transcript.html#sec-video6",
    "href": "chapter9.3_transcript.html#sec-video6",
    "title": "Chapter 9.3: 9.3 Genomic Structural Equation Modeling (Video Transcript)",
    "section": "Using Genomic SEM to Understand Psychiatric Comorbidity",
    "text": "Using Genomic SEM to Understand Psychiatric Comorbidity\nTitle: Using Genomic SEM to Understand Psychiatric Comorbidity\nPresenter(s): Andrew Grotzinger\nso andrew thanks so much for coming we are just super excited to hear about your work many of us know about genomics sound but\nreally want to get a more in-depth knowledge so uh very very\nuh happy to have you here and welcome thank you um yeah it’s a real pleasure to be speaking\nto this group in particular um and today i’m gonna be talking about\ngenomics sem overall um but i really want to focus the second half of my talk on the application of\ngenomic sem to psychiatric traits um and i know a number of you have heard me talk about\ngenomics and before so i’ll try to keep this first part relatively brief and more just a refresher\num but there is a new package update within the last couple months that i’ve put on the github for\nstratified genomics m so hopefully that’ll be kind of a new thing for some people here\num so just to start giving an overview of genomics them and kind of the motivation for developing this package um\ni’m just showing here a old plot from kendler at all 2012 showing g was hits on the y-axis\nand different traits on the x-axis and what this is demonstrating is something that’s well known to\neveryone here that as the sample sizes have increased for different gewa studies that the\nnumber of hits that we’ve identified has increased in a corresponding way pretty rapidly where we’re now identifying\nhundreds of different genetic variants under underlying complex traits that are of interest to people\nand so for me as someone who’s interested in psychiatric outcomes like schizophrenia and depression if we\nlook at these two manhattan plots from some of the more recent efforts from these groups these traits are so\npolygenic that if you’re trying to figure out what actually is shared across these two traits it’s not just a matter of\nidentifying um five or ten overlapping genes or loci that are shared across these manhattan\nplots and it really required people to think about how we could actually quantify the level of genetic overlap\nand that was done really beautifully in 2015 when the group from the broad introduced\nld score regression and more specifically bivariate ld score regression for examining\nshared genetic overlap across traits which allows you to estimate genetic correlations between samples\nwith varying degrees of sample overlap using what is often publicly available gwas summary data\nand when people apply genomic sem like they did in the brainstorm consortium paper in 2018 you can produce these genetic\nheat maps across different sets of traits where on the left you’ve got psychiatric phenotypes\nand on the right across a wider range of kind of brain disorders and behavioral cognitive phenotypes where the darker\nshading indicates higher levels of genetic overlap um and unsurprisingly we see that\nthere’s certain clusters of traits within these heat maps where for example psychiatric disorders tend to show\na pretty strong level of genetic overlap which is something we might expect based on high levels of comorbidity that\nwe observe among psychiatric disorders well at the same time some genetic correlations were maybe\nparticularly or even surprisingly high such as genetic correlations between bipolar disorder and schizophrenia\nand the range of 0.6 or 0.8 as you can kind of see indicated by this particularly dark blue box\nbetween those two disorders um so when these sorts of papers were coming out i was\nin the midst of running twin models and helping um navigate and manage a twin study down\nat ut austin and running different multivariate models of twin correlation matrices and\nso seeing these correlation matrices coming out based on genomic patterns of convergence\nthere seemed to be this kind of strong need to really develop and apply multivariate methods to actually model\nthese constellations of genetic overlap and so that led us to develop and\npublish this paper that introduced genomic structural equation modeling in 2019 and nature human behavior which is kind\nof broadly speaking a framework for modeling patterns of genetic associations\nacross wide constellations of traits and so in general genomic sem uses a\ntwo-stage estimation procedure where in stage one you estimate the genetic covariance matrix\nand associated sampling covariance matrix of standard errors and their code dependencies we use ld\nscore regression within the context of the package but you could hypothetically use any sort of\ngenetic covariance matrix such as you might get from greml um and then in stage two we actually fit\nthe structural equation model to the matrices that are estimated from stage one\nand so just to kind of show you visually what those stages look like and so in stage one we’d create this\ngenetic covariance matrix or what we call s that has the heritabilities estimated from the genome-wide data\non the diagonal and those genetic covariances are cohered abilities on the off diagonal and then critically\nwe’re also estimating this sampling covariance matrix v that contains the squared standard\nerrors on the diagonal and the sampling dependencies on the off diagonal which indexes\nthe dependencies between estimation errors and that’s what allows us to apply genomic sem\nfor samples that have different levels of sample overlap and that doesn’t need to be known that\nis directly estimated from the data using a block jackknife procedure but again\nif you have different sets of summary statistics and you’re worried about some level level of overlapping controls\nthis sampling covariance matrix is going to allow you to produce unbiased standard errors even in the presence of that\nsample overlap and then in stage two you take those two matrices\nand feed it into the genomics mr package and specify some sort of model like this genetic\nmultiple regression model in which we have schizophrenia and bipolar disorder as correlated predictors of\neducational attainment and then parameters are estimated that fit the observed genetic covariance\nmatrix as close as possible and since this is a fully saturated model and the model parameters are\nsimply a transformation of the observed matrix um one thing that i like to highlight\nwhen i’m talking about this and i know that this kind of first part of this phrase is not\napplicable to this group but even if you are not interested in genetics i think genomic sem offers some kind of valuable tools\nbecause it allows you to look at systems of relationships across a wide array of rare traits that could not be\nmeasured um in the same sample and so if we think about just a case example of what that might look like\nlet’s say that as someone interested in clinical phenotypes you have a real interest in the relationships between\nschizophrenia and bipolar disorder you’ve read the research literature and see a lot of convergence across\ndifferent risk factors for these disorders you have a pretty good understanding that these phenotypically can often look pretty similar\nand you also notice pretty similar kind of age of onset distributions for these two disorders so in that case you might\nbe interested in looking and quantifying um in a sort of cross-leg panel like this\nthe relationships between both early and late onset versions of schizophrenia bipolar disorder\nthe issue is that these two disorders are actually not you can’t diagnose them together in\nthe dsm these would be rule outs of one another so you couldn’t in a phenotypic sample actually observe these two disorders\nwithin the same individual [Music] and so you wouldn’t be able to estimate\nthis part of the model using phenotypic data and of course you also can’t observe\nboth early and late onset versions of a disorder within the same person so you couldn’t look at this part of the\nmodel but with genomics m you can actually stratify the g was\nsummary statistics by early and late onset versions of these disorders and you could actually fit this sort of model\nand actually start to test the sorts of relationships that we’ve been left to really just hypothesize about and make\nqualitative conclusions around based on convergence from different separate univariate studies\nat this point stratified genomic sem can be used to look at convergence at three main levels of\nanalysis and i just want to walk through those so at the genome-wide level um this is actually just a\nrecapitulation of that multiple regression model i showed earlier as an example of stage two\nestimation where we’re just looking at the system of relationships between schizophrenia and bipolar and\neducational attainment but this is just to highlight that for people who are interested in sort of\nmediation type hypotheses that within genomic sem you can kind of quantify things like a total indirect and direct\neffect um between different disorders um within genomic sem\nyou can also get standard model fit indices like aic model chi square cfi and srmr\nand that allows you to go through kind of classic model fitting procedures to try and decide between different\ncompeting models [Music] and decide what kind of patterns of relationships best fit the data\nso in that original paper we looked at a series of models fit to the covariance matrix estimated across\nthese 12 neuroticism items and what we found is that a common factor model fit the data pretty well a\ntwo factor model did a little bit better but a three factor model really seemed to balance\nthat um relationship between fit and parsimony in terms of how we were representing the data\nand what i would highlight here is that the way that the items kind of segregated across these factors actually does make a lot of post-hoc\ntheoretical sense so you see on factor one this kind of mood misery and irritability items\nmaybe on this kind of negative affect factor guilt hurt and embarrassment around\nmaybe this kind of social reactivity factor and nervous worry tense and nerves maybe this kind of more\nanxiety type factor and while these factors are still highly correlated they are somewhat\ndissociable and in fact when we run multivariate g wasps to look at snip\neffects on these factors we do find somewhat divergent biological pathways that we might miss\nif we were we were to employ approach where say we just kind of sum across these items and run a g was on a\nsum score of the 12 neuroticism items so i think this really highlights the ability of these sort of model\nfitting procedures to pull out some pretty interesting patterns in the data\nmoving on to this new level of analysis stratified genomic stem that we introduced in a paper that’s\ncurrently on meta-archive and out for review right now but the code for that is live on the github\nalong with a tutorial page for that um is really designed to think about how we can start to make sense\nof gwas findings characterized by thousands of genes that really individually explain only a\nvery small portion of the phenotypic variants so just as sort of a kind of hypothetical example let’s think\nabout these future manhattan plots where the gy sample sizes are getting in the millions and now we’re starting to see\nthat um basically the entire genome is somehow associated with the phenotype again i know this is just sort of a case\nexample but it just highlights that you know at an individual snip level we’re going to get to a point where the\npicture is so complicated that it’s really hard to make sense of what’s going on just based on a manhattan plot\nalready a number of methods out there that in the univariate case can be used to\nbasically partition heritability by using collateral gene expression data such as\nwhat you might get from rna sequencing methods to try to lump associated genetic variants um and\nportions of heritability into meaningful categories so in this partitioned ld score regression\npaper um from 2015 they showed that disorders like schizophrenia and bipolar disorder\nare enriched in the central nervous system which you can see here based on these orange bars for that\nannotation including for years of education as well which of course um makes a lot of sense\num so we extend that same model to be able to look at partitioned coherent ability\num so this will look familiar to anyone who’s um worked with the bivariate ld score\nequation before and really all we’re doing now is that instead of using\num the ld scores here we’re using the partitioned ld scores so the ld scores within a particular functional annotation so\nwe develop this and validate it um as a means to an end for really being able to then feed these\npartitioned covariance matrix matrices into genomic sem to then be able to examine\ngenetic enrichment of any model parameter estimated in genomics m for this kind of new extension that we’re calling stratify genomics m\nso you can look at enrichment of residuals in a structural equation model enrichment of correlations between\nfactors and i think what people would typically be interested in enrichment of the factor variances\nto see where these kind of overarching factors that explain variance in the indicators\nare really enriched so in that way we can take a manhattan plot of\nthese common factors and start to look at these kind of top peaks and ask whether or not these hits are really enriched within\ngenes that are expressed during certain developmental periods such as during early kind of even prenatal periods or\nlater in life whether they’re enriching specific brain regions or even in certain neuronal subtypes um and this\nmethod really starts to and you know any kind of partitioning method starts to become increasingly\nexciting as the gene expression work starts to move at a rapid pace and our ability to build\nthese categories um in meaningful ways starts to also really increase and become quite\nexciting so again this method is about asking whether there are certain biological\nfunctions that can characterize genetic variants with plyotropic effects which you know there’s a lot of kind of\nconvergence of findings across disorders that tend to cluster together unsurprisingly psychiatric disorders\ntend to be enriched within brain regions but now we’re really trying to quantify that using this multivariate method\nfor stratified analyses and at the most fine-grained level of analysis genomicsum can be used to\nperform multivariate g wasps to look at snip effects on any model parameter that you would\nestimate in genomics m and the way we do that is that we extend that s matrix i showed you earlier\nwhen walking through the two-stage process we’ve got that same genetic covariance matrix from ld score\nregression here in blue and then we append this snip column\nthat include the snip variance from a reference panel and the betas from the g-was summary statistic scaled to covariances\nusing that same snip variance from the reference panel and so what you would do is the package\nbuilds this matrix as many times as there are snips present across the indicators and then runs\nany particular model that you might specify that includes an individual snip um so for example we look at snip\neffects on a p factor that’s defined by these five psychiatric indicators of schizophrenia bipolar major depression\nptsd and anxiety we take this ld score regression matrix and append\nthe snip column and then we’re able to look at the effects of an individual snip such as this particular snip that\nwe find is genome-wide significant with respect to its effect on the p-factor\nin the context of multivariate g-wasps we also have developed this snip heterogeneity metric which we call\nq-snip because of its similarity to the meta-analytic cue statistic that really ask whether or not the\nassociations between a given snip and the individual disorders like i’m showing here in panel a\nis sufficiently captured by a model that posits a single association between the snip and the factor so really is it\nkind of if you just fit this one relationship is that obscuring the relationships that\nyou might get if you fit all of these individual relationships so we compare this common and independent pathways model\nand if this model and panel a fits much better then that suggests that this snip is not really operating through the\nfactor and an instance where you might see that is if for example there’s highly disorder specific effects so if a snip\nreally is specific to one of the indicators and has a null effect or an opposite effect on\nthe other indicators then you would expect to get a significant q-snip metric and this really highlights that\ngenomics and is not about boosting power for the individual traits and the way that some other multivariate methods are\ndesigned like mtag but it’s really about finding the snips that operate through the factors\nand the snips that are highly specific to the disorders so we can start to get a sense of the multivariate picture\nacross the different disorders that we include in the model\num this is one particular application of a model that you could fit in genomic sem that\npeople have shown some interest in namely g wasps by subtraction where you fit this sort of koleski model\nfor two traits and then you have the snip predict the two latent factors within the\nkoleski model so in this particular example we’re looking at the effects of\na snip on cognitive performance and on educational attainment minus the genetic overlap with cognitive\nperformance for what we’ve called this kind of non-cognitive element so that then you can start to\nkind of break apart something that’s really multifaceted like educational attainment and ask what snips underlie this overall\ngenetic signal that are separate from the cognitive component this is in a paper that is currently on\nbioarchive but is forthcoming and this is just a highlight that then you can produce these manhattan plots\nfor the cognitive um phenotype and the non-cognitive phenotype down here\nand identify these kind of dissociable genetic signals and then in the paper they also look at\nkind of polygenic score prediction for these two different phenotypes and find divergent patterns of prediction and also genetic correlations with outside\ntraits so this is just kind of one way that you can apply genomic sem\nin interesting ways um in general it’s just a kind of sales pitch for genomics\nm the different groups that have used it and using the open source r package have\nbeen fairly successful in publishing um in a lot of different outlets\nso this is just kind of a flavor of the different ways that people have been using it and the outlets that they have been\npublishing in using genomic sem um so with that i want to transition as\nsomeone who’s trained in clinical psychology to my main interest in psychiatric phenotypes\nand how i’ve used genomics m to really understand the multivariate genomic architecture across psychiatric traits so\ni just want to kind of start by explaining why i think um multivariate work and\npsychiatric disorders is so important to begin with even ignoring the genomic piece so in this orange circle i’m just\nshowing kind of all individuals with mental disorders um that will meet criteria for a\ndisorder in their lifetime and among that group of people about two-thirds are going to meet criteria\nfor a second disorder half will meet criteria for a third disorder and 41\nwill meet criteria for a fourth disorder and while we know that um our just our categories are not\nperfect we do kind of think of them as sort of these mutually exclusive things where i think when clinicians\nsometimes talk about people having multiple disorders there’s this sense that they’ve just kind of\nhad bad luck in terms of maybe the environments or genetic risk factors that they’ve been exposed\nto but really at the end of the day i think this speaks to how much our categories overlap pretty substantially\nand how much we should think about kind of refining future versions of our diagnostic manuals so that we can create\nmore mutually exclusive categories and i think there’s a lot of really compelling reasons to think about why we would want\nto do that um but before i get to that this is just kind of depicting the extent to which\nwe might kind of loosely think about um these patterns of comorbidity and\nkind of the loose um rough borders that we’ve drawn between the disorders as being somewhat\ngenetic in nature so on the x-axis here we have parent disorders across depression\ngeneralized anxiety panic substance use and antisocial personality disorder and on the y-axis the odds ratio for\ntheir child developing a particular disorder and what we see is that the children are really at risk for any disorder and not\njust the disorder present in the parent now of course that could just mean that a parent having a mental disorder is\nsort of generally stressful through an environmental component that kind of generally leads you at risk to any disorder or\nit could mean that the genetics passed down from the parents are really kind of unspecific in terms of how they convey\nrisk but at the very least that suggests that the boundaries between the disorders are pretty blurry\nat the end of the day um and so again that’s important because um as someone who also um\ncurrently practices it can be very stressful to meet with someone and tell them that they’ve got every disorder in the book i think this\nhas real clinical implications in terms of the message that sends to the patient in the room and also what it means for a treatment\nprovider to think about what kind of intervention to give somebody when they’re meeting criteria for multiple\ncategories and from a scientific perspective i think this also has\na lot of important ramifications where if you have scientists a studying bipolar disorder scientists be studying\nschizophrenia um and this fellow is studying ocd but at the end of the day we know that these\ndisorders are all related in some pretty substantial way it probably isn’t the best use of grant\nfunding money to give all these people a separate pool of money and then just kind of send them off in separate directions when they’re really\nstudying these phenomena that are actually interrelated in pretty substantial ways in some level\num and so that is why as a kind of backstory i’m particularly interested in\nunderstanding the genomic architecture across different disorders which i think\ndoing it in a genomic space has a number of advantages one it can give us insight into what we know is a pretty important\ncomponent of these disorders since they’re all estimated to be pretty heritable but one limitation when people are\nunderstanding what’s called as this general psychopathology factor a trans\ndiagnostic p factor is family-based research on p is inherently limited to relatively common\ndisorders because it’s going to be next to impossible to obtain genetically informed samples on\nrare disorders and in just a basic cross-sectional sample again i noted that some of the disorders actually disallow one another\nso it would be not possible to actually examine disorders within the same sample\nso for the first time methods like genomic sem building off ld score regression allow us to look at the\ngenetic structure across both common and rare disorders because ld score regression is able to\nestimate the genetic relationships across samples that are potentially independent\nat the end of the day um i want to walk through the different iterations of these factor models that\nwe’ve done now so i showed you this earlier in the context of the snip model but um at the base model\nwe modeled in that initial paper the relationships across these five disorders and pulled out what we called at the\ntime this kind of general psychopathology factor um i kind of am referring to it as a\nso-called general psychopathology factor because um we also only had five disorders at\nthe time that were sufficiently powered to put in the model and as we started to\ninclude more disorders this model started to look a little bit more nuanced than just a single common factor so in the second major iteration of this\nwe worked with the cross disorder group from the psychiatric genomics consortium to publish this paper in cell where we\nlooked at the genomic relationships across these eight disorders of anorexia ocd tourettes\nschizophrenia bipolar mdd adhd and autism and what we found is that these kind of segregated into these three factors\nthat we loosely call a compulsive disorders factor a psychotic disorders factor and a\nneural developmental disorders factor and let’s briefly mention too that um\nyou know things like an ocd and tourette’s loading under the same factor is very consistent with what we might\nexpect based on comorbidity rates and kind of convergent patterns across different research groups same thing for\nschizophrenia and bipolar um one kind of lone wolf here is mdd\nand we were able to improve on that when we in our most recent iteration of this\nmodel now include 11 major disorders now with alcohol use disorder here in red ptsd and anxiety and with the\ninclusion of ptsd anxiety we’re now pulling this fourth internalizing disorders factor\num but i would highlight also that for a number of these disorders the sample size is updated pretty substantially\nrelative to the previous iteration and even with that update we were still pulling out the same\nthree factors when we going through the exploratory factor analyses um so again this kind of indicates that\nthere is a lot of intercorrelations across these disorders but there is some kind of nuance in terms of how these disorders\nkind of segregate into these subgroups and in fact if you just kind of look at the genetic correlation matrix across\nthese 11 disorders you can kind of see these factors kind of pop out when you\norder the matrix according to the factors that we modeled where you’ve got this kind of subcluster up here factor one this really tight cluster\nbetween schizophrenia bipolar and factor two and in particular this really um apparent cluster between ptsd and mdd\nand anxiety on that internalizing fourth factor because there were you know correlations\nacross these factors and because of the kind of growing interest in this overall p factor we did model\nan overarching p factor that explained the genetic relationships between these four\ndifferent factors and found that this fit the data pretty well but given these kind of partially\ncompeting models of either kind of four correlated factors or an overarching factor we wanted to go on to do\nsome other analyses to really evaluate the utility of each of these factors for understanding shared biology\nacross their indicators and we did that in a number of ways one is we took\nthe same logic that we developed for q-snip and applied it to examining the relationships between\nrelevant external correlates for psychiatric disorders and so again in a very similar manner to\nq-snip we ask whether or not the associations between an external trait and the individual disorder shown\nin panel a is sufficiently captured by a model that just shows a single relationship between\nthe external trait and factor one and we compare the fit of these models and would expect there to be\na really significant decrease in fit by just modeling this common pathways model when these associations are really\ndiscrepant across one another um we fit these sorts of models for 49\ndifferent external traits and i just want to walk through um a number of these findings so\ni’m showing a couple different things here and color-coded at the top are the four different factors from the correlated\nfactors model and then the p factor in turquoise and then whether or not the bar is shown with a solid or dashed\noutline indicates whether or not it was significant for that cute trait metric where bars with a dashed out line\nwe found to be significantly heterogeneous across the indicators so within socioeconomic outcomes we see\nthat there’s a relatively homogeneous relationship between a lot of these outcomes and the compulsive disorders\nfactor in the positive direction when we look at health and disease we see\na lot of positive correlations the internalizing disorders factor\num and these health disease outcomes which is very consistent with phenotypic work\nand within anthropomorphic traits we do see i think a particularly interesting finding for the compulsive disorders\nfactor of a negative correlation between body mass index and waist to hip ratio\nand i mark that as interesting because anorexia loads on that factor and you might think that that is really\ndriving the relationship between bmi and waist to hip ratio given low weight status as\na diagnostic prerequisite for meeting criteria for anorexia but in fact we see\nthat there are actually shared pathways with ocd and tourette’s between\num these external correlates indicating that there’s some sort of shared general risk between um kind of these\nanthropomorphic traits and this compulsive disorders factor and then finally we see that there’s a lot of homogeneous relationships\nwith neuroticism but not with this internalizing factor in neuroticism which might come as some surprise but\nit’s largely driven by a much higher genetic correlation with\nthe mdd indicator and if we look at these correlations overall\nwe see that in particular um there’s the significant q trait metric for the\nneurodevelopmental and p factor indicating that a lot of the relationships with external traits are not\noperating through these factors and suggesting somewhat limited clinical utility for these two factors relative to these\ncompulsive psychotic and internalizing disorders factor\nwe when then went on to perform multivariate wasps and sort of two main steps um in the\nfirst one we looked at the snip effect as it concurrently predicted these four correlated factors and then a snip predicting this\noverarching general psychopathology factor um just to orient you to these miami\nplots i’m showing on the top half the factor snip effects and on the bottom half the q snip\neffects um that i’ve talked about earlier in this talk in black triangles i’m showing the 132\nhits across these four factors that were nld with hits that were identified for the individual disorders\nthat defined the factors in red i’m showing 20 novel hits that were not significant\nfor any of the individual disorders which highlights the ability of genomic stem to make novel discovery even without\ncollecting any new data and again i’ll say that with the caveat that i really\ntry to be careful to not frame genomic sam as a method for boosting power but for ultimately looking at\nshared genetic signal and divergent genetic signal across different traits but at the same time because you are\nleveraging shared power you are going to get some kind of boost in signal for a lot of these disorders and then in\npurple i’m showing the purple diamonds the significant q snip effects so for the compulsive disorders factor\nthese particular disorders were not super well powered so there’s not much signal we see a lot of shared signal for\nthe psychotic disorders factor for the neurodevelopmental disorders factor we see a sort of\num somewhat even balance of factor and q snip signal and internalizing disorders factor a\nmuch stronger signal for the factor and if we look at the qq plots for these different disorders\nwhat i’m showing here in blue is the signal for the factor and in pink the signal for q-snip and\nwhat we would expect if the factor is generally capturing the relationship between the individual\nsnips and the individual disorders that define the factor is that this blue line would sit kind of nicely above this pink line\nand that is what we observe for the compulsive psychotic and internalizing disorders factor but\non the other end we see that the qcnt metric is actually much stronger\nthan the factor signal for the neural developmental disorders factor which indicates that a lot of the\ngenetic signal is not operating through the factors so similar to what we observed for the patterns with external traits\nfor individual steps we find that they are often not operating through that neurodevelopmental factor\nand if we look at the individual snip effects for some of the snips that were particularly q snip significant we see\nthat a lot of that is due to divergent effects for autism which also loads on that factor it may\ncome as no surprise so that’s not to say that a neurodevelopmental factor in a kind of\nhypothetical sense might not prove to be useful but at least in the way that we’ve defined it based on the data that we had it doesn’t\nseem to be sufficiently kind of explaining the shared signal across its disorders\nand then if we look at the signal for the p factor we see a really striking imbalance\nbetween the factor signal and the q snip sniggle where we see only one hit um that was in ld with an individual\ndisorder hit no new loci and 69 cue snip hits and just a really\nelevated pink line relative to that blue line indicating that almost none of the snip effects are\noperating through that p factor which really suggests that there is kind of limited utility\nto a p factor for actually understanding shared biology across disorders\num finally i just want to kind of go over the um stratified genomic stem findings\nfor this particular 11 disorders model and i want to focus on\nsome findings for enrichment of brain cell types so we know that psychiatric disorders\nare generally enriched for genes expressed in the brain which is sort of i feel like more of a\nsanity check to see that it’s not expressed in you know the spleen or the stomach but is really\nin the central nervous system like we’d expect but with more recent single cell sequencing\num efforts coming out people have been able to pair that up with gywas summary data to actually look\nat enrichment of specific brain cell subtypes which gives us maybe a little bit better target in terms of\nwhere that signal is coming from within the brain we also know that protein truncating\nvariant intolerant genes which is referring to specific genes that\ngenerally do not show mutations are enriched across disorders so these\ngenes might be particularly relevant for just kind of functioning in general and so what we did is we created annotations\nfor pi genes brain cell subtypes and their intersection to examine whether or not prior\nenrichment patterns actually reflect some sort of pliotropic signal across disorders\nand so here i’m showing those stratified genomic sem findings for examining enrichment of the factors in\norange i’m sewing the glial cell category in dark blue\nslash purple the excitatory brain cell subtypes and then the gabaergic subtypes the pi\ngenes and then their intersection over here in the right most part of the plot for the different factors\nand what we see is that there’s a really unique signal for the psychotic disorders factor with an excitatory and\ngabaergic um genes um within the pi genes and a signal that is particularly enriched for\ntheir intersection and so i think this starts to give us some real traction\nin terms of being able to understand what it is that the psychotic disorders factor is capturing in terms of where\nthat genetic signal is coming from and also starts to point towards some reasons that these disorders might\nactually diverge and actually look both phenotypically different and not share genetic signal at the level of this p\nfactor over here um [Music] and i also would just highlight that if\nyou were to go into google scholar right now and type in any of these um sub neuronal sub cell types for\nbipolar disorder schizophrenia you could really make a case based on the prior literature that really any of these are relevant to\nthe factor and of course this is just one study among many um but we’re using a pretty high powered\nset of g-was summary statistics paired with some pretty cutting-edge rna-seq data that shows that really these glial cells\nare not particularly relevant for the psychotic disorders factor as we’ve defined it here\nso in conclusion um we’ve really expanded the genetic factor model of psychiatric risk to identify\nfour major factors of genetic risk sharing we find relatively high utility of the\npsychotic and internalizing disorders factor when we go on to really kind of stress test these factors\nusing different follow-up analyses including those q-snip and q-trait findings i talked about earlier\nwhere we find that the associations between relevant external traits and individual snips\nare generally captured by the factor for the compulsive disorders factor i\nwould really frame this as a factor where the jury is still out in terms of its utility\nnamely because the gwas summary statistics that define that factor are still relatively low powered\nwe do see that it does a pretty good job of explaining relationships with external traits but at the level of\nindividual snips there’s just not really the signal there to see whether or not it’s operating through the factor or through the\nindividual disorders and for the neurodevelopmental factor we really see\nsome pretty low utility where a lot of the relations and the disorders that define this\nfactor are relatively unique in terms of their patterns of genetic correlations with external traits and a lot of the snip\nassociations did not operate through the factor in a way that seemed largely attributable attributable to\na signal that was unique to autism relative to ptsd and adhd that also\nloaded on this factor um we also know that we can model a p\nfactor using a genetic correlation matrix um in line with a lot of phenotypic and family based work that’s been done\nbut when we stress test this factor we find that this in particular has incredibly low utility to the extent\nthat it obscures relationships with external correlates and sniff associations\nthis might be due to the fact that there are sort of kind of unique um bivariate associations between\ndifferent factors that are not captured across the factors as a whole so for example there might be some sort of\nshared signal between the psychotic disorders factor and compulsive disorders factor\nthat results in that genetic correlation between those two factors that is really dissociable from the signal between a psychotic disorders\nfactor and an internalizing disorders factor um so it’s kind of this\ncomplex venn diagram across these factors that does not include this kind of p factor at the center at\nleast in the way that we’ve examined it here which we think has pretty broad implications for a pretty rapidly expanding p factor\nliterature with um a lot of articles coming out all the time about this p factor\nusing stratified genomic sem which is that new genomics sum edition that is live on our\ngithub we find that prior enrichment findings generally reflect broad pathways of risk and we find in particular the\nintersection of pi excitatory and gabaergic genes are in which for this psychotic disorders\nfactor which gives some real insight into the biological pathways that might underlie the really high\ngenetic correlation between these two very debilitating disorders of bipolar disorder and schizophrenia\nand again as rna-seq methods get even better and the corresponding univariate g-wasps become even better powered\nwe’re going to be able to make these categories even more refined and even put them within developmentally specific windows such as excitatory\nneurons expressed during a specific period of development which i think is\nyou know just at face value could be a really exciting set of findings\num and just to end on a kind of sales pitch note um a lot of you have heard me talk about genomic sem but i hope that\nit’s clear that this is a pretty flexible method in terms of its ability to ask a number\nof different interesting questions whether you’re looking at something like g was by subtraction just doing some general factor modeling\num or trying to examine systems with relationships across traits that you might not be able to otherwise examine\nbecause of how rare or mutually exclusive they are it is an open source r package that’s publicly available with\na github um and a uh google group that you can ask\nquestions on and stratify genomics m is now live on the github and\nyou know one of the reasons that it’s exciting to talk to a group like this is not just to talk about the work that i’m doing which in and of\nitself is you know fun to do but it’s also great to hear what kind of questions people have and also\npotentially develop collaborations projects or grant ideas and\njust as an aside i’m on my internship my clinical internship year at mass general hospital in boston but i will be\nstarting as an assistant professor at cu boulder in the fall so again if people have projects or grants that they want to\nwork on this is obviously a group that i would be particularly interested in collaborating with\nand so i’ll just end by um [Music] naming and thanking a number of different people in in particular elliot\ntucker drove and michelle navarre who have been really central to working with me to develop genomics\nits extensions and a number of different people named here and of course also thanking groups like the\npsychiatric genomics consortium isec and uk biobank that really contributed to the data sets that i presented here\nin terms of the application of genomic stem to psychiatric traits so that’s all i\nhave um but again i just want to thank everyone for inviting me and for your time and curious to hear what\nquestions people may have andrew thank you for a fascinating talk how i’d like to structure it\nthe q a is that sam trejo who suggested you come talk to us and we really appreciate that suggestion will give us\nthe first question and for those who have follow-up questions in that conversation i suggest you jump\nright in but for those who have other questions if you could just queue up in the chat i’ll moderate that to tell who\nto tell us who’s next so sam’s going to give us the first question and then we’ll go from there sounds great hey andrew really cool talk\ni sam thank you um my question is is kind of about this idea that i think is true for this earlier\nstuff you talked about with the geos by subtraction and the sort of non-cognitive cognitive\nparts of educational attainment but but it seems like it extends later on into the stuff that you’re doing with um\nlike the p factor and all these different sort of um kind of like psychiatric factors uh so so the first\nquestion is like with did you us by subtraction if like if i were just to take the linear\ndifference between the uh ea some steps like you know beta weights and the um\niq beta weights uh you know i would do like a less sophisticated version of i think what you guys do in that paper\nright like i’m just sort of taking the bits in one g-wash that aren’t in the other gus and i was kind of curious like how\nsimilar of an answer would i get do you think or do you know to you know what the genomics m model fits\num and then what are the advantages using genomic sams and then my kind of second question that’s related to all\nthis is like um well actually i’ll just let you into that one\nfirst yeah now that’s a great question um so michelle navarre actually has\nan alternative method that people are probably aware of um geoist you know wide inferred summary statistics that\ndoes something very similar to what you’re describing that just kind of takes the summary statistics\nand i’m at that level just kind of pulls out um the shared and unique genetic signal i think the advantage of\ngenomics m is that um is two things one that you\ncan kind of actually depict the relationship between these two different sets of traits within like a classic\nkoleski model in a way that’s sort of intuitive in terms of how the genetic relationships are shared\nacross these different traits that you’re including um and the other is that my sense is\nthat if you do um just kind of what you’re describing\nyou’re potentially gonna get biased results because of that sample overlap piece that\ngenomic sem is able to account for using the v matrix so if you know that you have two\nentirely independent samples and you have no concern about that then i don’t know i think it’s kind of\nan open question whether or not the the answer would be pretty similar but my sense is that people are generally\npretty concerned about some level of unknown sample overlap and genomic stem is going to give you\num some ease of mind in that case that you’re actually appropriately accounting for that\nyeah that’s helpful that’s helpful and so then kind of the next question is like in both cases i think with the cognitive\nnon-cognitive and then all these psychiatric factors like we’re now able to basically you know isolate and generate polygenic scores\nfor traits that just don’t exist and and i guess i mean by that i mean they don’t exist it’s like they don’t they’ve never been measured and it’s not\nclear that we would ever able to be able to measure them um and i was just curious if you you know what you thought about that and\nwhether those sort of apologetic scores um should we like you know we should use them differently or think about them differently\num yeah that’s another great question i mean i think about that too in terms of the factors that we pull out\ntoo i mean this reflects some sort of shared genetic signal across these disorders that isn’t actually directly observed and so\ni think it’s always really important to kind of do some follow-up just for yourself um\nand for the people who are reading this kind of hypothetical article that you’re putting together to actually characterize what that\nkind of new polygenic score looks like so for example by taking non-cognitive\nsummary statistics and looking at the genetic correlation between a bunch of external correlates so you get a sense of what that signal\nis actually picking up on in a sort of kind of multi-dimensional space so um\ni totally agree i don’t think we should just start like kind of removing things from one another and just start kind of kitchen sinking things without\nany kind of hypothetical guiding force or sense of what these these new summary statistics are picking\nup on at the end of the day but i do think there’s a number of things within genomics m um that you can do to kind of clarify\nthat thanks super helpful yeah thank you everyone\noh yes um thank you for the talk it’s really comprehensive pretty much covered everything about genomicsm um so so i have questions\nabout some technical details but for the follow-ups probably you can continue in our\nindividual session after this but for now i i’m very interested in this annotation stratified janome icm so i’m\nwondering because if you’re interested in the heritability enrichment of factors in certain regions you actually\ndon’t have to fit annotation stratified version of genomic sem right you can just run the standard xiaomi sem and get the\njuva summary set for those factors and then test if their herd ability uh is enriched in\nin certain annotation categories i wonder empirically do you actually see differences when you run the annotation stratified\nversion of gsem and then characterize you know annotation enrichment\num you do see some differences in general you just see kind of\ndeflation of signal um i think that’s a great question you know i think that’s one thing that comes\nup a lot is just this general idea of like couldn’t you do this in a much simpler kind of more straightforward way um by\nyeah just taking the summary statistics for the factors and um running partitioned heritability\non that um the reason that you see deflated signal is that the factor summary statistics are estimated with\nerror and what i mean by that is those summary statistics are going to include some of that q signal\nor signal that is not actually operating through the factor itself so you could think\nabout kind of pruning based on significant q-snips and then feeding those summary statistics into\npartitioned heritability um but i think it’s kind of unclear what threshold to use when you do that\nwhereas if you’re doing it strictly within a genomic sam sem framework by looking\nat enrichment using partition covariance matrices you’re not including that kind of error\nthat gets introduced into the summary statistics in that way um another thing i would say\nis that you can also look at enrichment of the uniquenesses and enrichment of things like factor\ncorrelations so that is sort of a tangent to the question that you’re asking but just to\nsay that stratified genomic stem i think is kind of more broadly useful in the sense that you can look at things\nthat you couldn’t really feed into a partitioned heritability analysis like enrichment of factor correlations\num or kind of concurrently looking at enrichment of the uniquenesses within the same model\nyeah this is very helpful so a quick follow-up would be that um do you have a sense about how big the\nannotation need to be for this to work because my intuition would be that if you have a very small function annotation\num the the annotation stratified genetic covariance estimate will become very noisy so then when you fit a separate genomic\nsem in in that you know genomic region alone maybe maybe it will be challenging to\nconverge right so empirically based on the sample size in your analyze summary stats how big can annotation be\num one thing as far as the model converging is that and the way that the estimation procedure is coded\nis that we kind of fix the estimates from the annotation that includes all snips\nand then re-estimate the model with those fixed estimates and the partition specific covariance matrices\nso that helps a little bit with just kind of wonky model estimation in terms of just like um genetic\ncovariance estimates that are really imprecise you know i know that this is i’m clear\nto you but just to sort of um put out there that you know we have the partitioned sampling covariance\nmatrices too so you don’t run a danger of false positives in the sense that if a covariance estimate\nand the partition space is really imprecise then it’s also going to have a huge standard error that ports over to the enrichment\nestimate so there are certain annotations that are really small where you get like a huge enrichment estimate but with\na a confidence interval around it that is humongous at the same time um\nso big point estimate but not significant in that sense in terms of annotation size i don’t have\nlike a great sense of how small um it needs to be\ni mean these pi by brain cell subtype annotations are not humongous um\n[Music] but um yeah i don’t i don’t have a concrete answer for that\nthe other thing i’ll say is that when we first started this project my interest was in actually looking at whether or not the factor structure changes across\nannotations and that we are not powered to do for the reason that you’re\nkind of highlighting that there’s a lot of just kind of random noise in the covariance estimates so that you get these kind of fluctuating\nfactor structures that don’t actually reflect something that seems to actually be changing in the population\nso that’s why we’re not doing kind of partition specific factor structures but\nmore kind of fixing the model in the genome-wide sense and then looking at enrichment of particular model parameters\nyes great thank you of course we had an overlapping question that\nphilip and james asked about modelfit i think the general question is how you think about model fit\nbut maybe james could fill in a more precise targeted question\nsure yeah thanks for the talk andrew that was extremely helpful and informative and exciting um\nso my question i i think it’s similar to phillips although he can certainly jump in here is\nis that when we’ve tried to fit these more complicated models in gsam\nwhere we have more than just the five right like you did with the 11 the problem that we ran into was that\nthe the more complicated your model gets the harder it is to actually produce a strong fitting model\nso i’m just wondering since you didn’t really talk about it in your slide is is uh when you were able to successfully\nextract these uh four different latent factors how well would you say that the overall\nmodel actually fit the data to the extent that you feel confident that this neurodevelopmental factor is\nin fact a truly unique factor can you speak a little bit about that yeah so um\nthat four factor model does fit the data well by conventional standards so cfi i think is\nlike .96 um [Music] srmr is i think .06 for that model so\nusing like these kind of arbitrary cutoffs um we find that the model fits pretty well\nwe see really clear increase in model fit as we kind of move from a common factor model to this more nuanced four\nfactor model um of course you know there’s the trade-off of\nyou know a more complicated factor model is always going to fit the data better um but it it kind of hit this point\nwhere hypothetically the factors that we were pulling out made a lot of sense um it seemed to fit the data pretty well\nand as we kind of included you know updated summary statistics we were consistently pulling\nout the same factors using um a kind of restarted exploratory procedure\nso you know at the same time i think that you can have genetic correlations\nare such a broad kind of 10 000 foot metric that in the way that we showed with these kind of q\nfollow-up analysis that’s important to really stress test these factors because you know we can model a p factor we can\nmodel a neural developmental factor but that might be that it’s kind of aggregating across these really\ndissociable pathways um at different levels of analysis so i think that model fit is just one\npiece of this puzzle i agree entirely and i’ll follow up with you in our individual meeting but\none one additional thing i just wanted to add was was the the alignment of the models that you\nextracted and with theoretical uh alignment with our theory of what these\nfactors should be so for instance in your neural developmental factors disorders you had ptsd\nautism adhd and i think tourette’s right so you know to what extent does that align\nwith for instance high top models of neurodevelopmental disorders or or even dsm\nperspectives of what neurodevelopmental disorders are classified as not that we necessarily agree with the dsm but\nbut i’m just saying that you know there’s this the quantitative piece in which the the disorders fit these this type of\nstructure but then there’s also how well does that fit with our theoretical understanding of how these disorders should look or how they\npresent yeah i mean um it’s sort of a mix of\nboth and i think that that’s you know in some ways kind of nice like you would hope that after spending\nmillions of dollars to do genomics we don’t just recapitulate what we already knew in some way but actually get some kind\nof novel insight and the real interesting thing within this neurodevelopmental factor is ptsd\nand the reason i highlight that is we actually see that ptsd and adhd are correlated\ngreater than 0.7 across multiple separate cohorts which is not something that you would\nget from reading the dsm or just from practicing clinically i don’t think\nand so there’s sort of a separate project going on that’s really trying to tease that apart adhd and autism i would argue you know\nthere’s some good reason to think they would load together um but in a purely kind of exploratory sense that particular factor\ni think is is the odd one out particularly because ptsd is is loaded so strongly due to its\nrelationship with adhd but that relationship is there and we\nfeel pretty confident that it’s there because of how consistently we see that across independent cohorts\nis one of your questions short enough that it would be a minute or two long um oh my god\ni’m not really sure but but anyways let me just briefly say andrew i’m so glad that i that i heard that you accepted this job\noffer from boulder so i i know that matt keller is like super excited about you know that that you’re coming there\nand i only heard it from him like very recently that this is finally working out so thumbs up congratulations\nso actually i had a lot of like very far drifting like comments and questions that may actually be really better if\ni talk to you separately about this but maybe just one very concrete thing so um i was just wondering if genomic\nsem actually makes use of the standard errors that uh ld score regression rg estimates spit\nout or are you only losing using the point estimates themselves we’re also using the standard errors\nin that in that v matrix okay well so then there is this um there is a\nslightly weird thing that um if you construct the the genetic correlation matrix with a bivariate\nmethod that doesn’t take the entire multivariate structure into account like lesd ldsc right so that’s\nreally just bivariate by variant then the resulting um matrix may actually not be a\npositive semi-definite and the uh the standard errors are actually not necessarily theoretically correct\nso um i mean there is a way how you can actually uh uh get the correct standard errors but\nit would be a completely different method it will basically be a multivariate version of of grammar\nuh which would have some advantage of some disadvantages but i’m just thinking is it theoretically\npossible that you fit that that you built your genomic sem model\nbased on standard errors and rg estimates that are not coming\nfrom ldsc but let’s say from multivariate grammar um it is and i think at one point\nronald was maybe working on that even um exactly yes this is where this is\ncoming from exactly yeah yeah um and you know there’s it’s sort of\nlike this um kind of constant thing that is like you know these different kind of bivariate methods come out\nto how we could maybe incorporate that to kind of build out these matrices i will say that we\num you know the sampling covariance matrix is estimated within our multi-variable\nversion of ldsc um but and the block jackknife kind of precedes\nin a way that is similar but different in that we’re also populating those off diagonal elements but that actually doesn’t answer your\nquestion so your point still holds um and i think yeah um\ntrying to incorporate some of the stuff that ronald’s been working on as a real interest cool all right thanks of course\ni think we could keep you for a long time with these uh with this question some answers andrew but we thank you again for coming i know you’re going to\nstick around for the beginning of your individual meetings yes i want to just on behalf of the group thanks for such a clear\nand interesting uh discussion and presentation yeah thanks everyone really great\nquestions and it was a real privilege to present to you all."
  },
  {
    "objectID": "toc.html",
    "href": "toc.html",
    "title": "Table of Contents",
    "section": "",
    "text": "Welcome and Introduction\n\n\nChapter 1: Introduction\n\n1.1: What are psychiatric disorders?\n1.2: Epidemiology\n1.3 History\n1.4 Psychiatric Genomics: State-of-the-Science\n\n\n\nChapter 2: The Genome\n\n2.1: Organization of the genome\n2.2: Types of genetic variation\n2.3: Evolutionary signatures\n2.4: Linkage disequilibrium\n\n\n\nChapter 3: Technologies\n\n3.1: SNP array genotyping\n3.2: Next Generation Sequencing\n\n\n\nChapter 4: Study designs\n\n4.1: Epidemiological study design\n4.2: Confounding, Chance, and Bias\n4.3: Genetic study designs\n\n\n\nChapter 5: GWAS analysis\n\n5.1: Genotyping Quality Control\n5.2: Imputation\n5.3: Association testing\n5.4: Meta-analysis\n\n\n\nChapter 6: Polygenic Scores\n\n6.1: Polygenic Risk Scores\n\n\n\nChapter 7: Ancestry-specific analysis\n\n7.1: Cross-ancestry analysis\n7.2: Ancestry-specific PRS\n7.3: Local ancestry and Admixed populations\n\n\n\nChapter 8: Post-GWAS Bioinformatics\n\n8.1: SNP Heritability\n8.2: Genetic correlations and partitioned LDSC\n8.3: Gene-association analysis\n8.4: Gene-set analysis\n8.5: Fine-mapping\n8.6: Quantitative Trait Loci (QTLs)\n8.7: TWAS/PWAS/MWAS\n8.8: pheWAS\n\n\n\nChapter 9: Advanced Topics\n\n9.1: Copy Number Variation\n9.2: Mendelian Randomization\n9.3: Genomic Structural Equation Modeling\n9.4: Interactions with Environmental Factors\n9.5: Family-based analysis\n9.6: Therapeutic Implications\n\n\n\nChapter 10: Other considerations\n\n10.1: A Career in Psychiatric Genetics\n10.2: Caution in Genetic Prediction\n10.3 Small Effect Sizes\n10.6 GDPR for Dummies\n\n\n\nSoftware tutorials\n\nCNVs\nConditional Analysis\nCross-Disorder Analysis\nDatasets\nEpigenome-Wide Association Studies\nGene Set Identification\nGenome-Wide Association Studies\nGenomic SEM\nMendelian Randomization\nPolygenic Risk Score\nSNP Heritability and Genetic Correlation\n\n\n\nGlossary\n\n\nSoftware Resources\n\n\nAdditional Reading"
  },
  {
    "objectID": "chapter2.3_transcript.html",
    "href": "chapter2.3_transcript.html",
    "title": "Chapter 2.3: Evolutionary signatures (Video Transcript)",
    "section": "",
    "text": "Origins of Genetic Variation\nTitle: Origins of Genetic Variation\nDescription:\nPresenter(s): Jessica Pamment, Professor, DePaul University\n[Music]\nNext time you’re in the classroom look around you and you’ll see that although all your peers to humans the same species as you know to individuals in the class will look exactly the same unless you have identical twins in the room this variation in traits is true not only for human population but for any species some of the differences observed within a population are caused by the environment and experiences of each individual for example hormonal changes brought on by cooler temperatures result in the fur of an arctic fox turning from brown to white although the environment definitely plays a role in introducing variation in a population most of the variations seen in populations are caused by differences in genes for example one gene is responsible for determining where the rats will have brown or black fur with exception of clones such as identical twins each individual within a population carries a unique set of genes half of which were received from one parent and half from the other the total set of genes of all individuals in a given population is called the gene pool a gene is a discrete unit of hereditary information consisting of a specific nucleotide sequence in DNA so nucleotides are the building blocks of DNA and therefore of genes differences between individuals can be measured all the way down to the level of individual nucleotides however measuring differences within a gene pool at this level is not particularly useful because much of the variation lives within non-coding regions of the DNA meaning that these variations don’t result in an observable difference it’s often better to measure variation at the gene level because there is at this level that both quantitative and discrete traits are coded so how does genetic variation arise in a population well one of the ways is as a result of mutation which result in a change in the original DNA sequence mutations can occur as mistakes during DNA replication however if the mutation does not happen in a Cell that is passed down to offspring such as an agro sperm cell the change cannot lead to nearly Oh which is an alternative version of a gene variation Canosa rice at the chromosome level during the process of meiosis this is a modified type of cell division found only in sexually reproducing organisms which results in the production of gametes the two ways in which variation is introduced in meiosis crossing over an independent assortment crusted OVA happens early on in meiosis in prophase one and results in the exchange of DNA between homologous chromosomes so between the paternal and maternal chromosome of each chromosome pair this results in recombinant chromosomes the second way in which variation is introduced is as a result of the random arrangement of chromosome pairs on the cell plate during metaphase one in humans the random assortment of chromosomes gives rise to over 8.4 million possible combinations of chromosomes and this is without taking crossing event account which introduces even more variation another mechanism that contributes to genetic variation in sexually reproducing organisms is random fertilization as I just mentioned in humans each male and female gamete represents one of about 8.4 million possible chromosome combinations due to independent assortment the fusion of a male gamete with a female gamete during fertilization is completely random and will produce a cell with any of about 70 trillion chromosome combinations if we factor in variation brought in by crossing over the number of combinations is even higher hopefully you can see how unique you really are now that we’ve learned how genetic variation is introduced into a population of sexually reproducing organisms it isn’t to remember the evolutionary significance of this natural selection is a driving force behind evolution and natural selection results and the accumulation of genetic variations favored by the environment another way of thinking about this is that genetic variation is the raw material needed for evolution to occur\n[Music]\n\n\n\nMPG Primer: Natural selection & human genetic variation\nTitle: MPG Primer: Natural selection & human genetic variation\nDescription:\nPresenter(s): Stephen Schaffner, Computational Biologist, Broad Institute\ngood morning hello everyone thank you for coming I have the privilege of introducing myself I’m Steve Schaffner I’m a staff scientist here computational biologist and I’ve been here for a long time I used to be part of mpg back in the thousand genomes and half map Iran beating before that these days I work more in pathogens malaria and viruses but I’m still interested in human genetics and natural selection humans which is why I’m talking to you today about natural selection and in humans and particularly its effect on human genetic variation and what we can learn from from this and how we can detect it so I think you probably all know what natural selection is I’ll just stated for the record that it’s the principle that alleles that make an organism more\nNatural selection\nsuccessful in terms of survival and reproduction are likely to be transmitted more to the next generation and therefore likely to increase in frequency while they’re being selected and we can for convenience we can divide the kind of selection into several different kinds one first is balancing selection which is selection that maintains multiple alleles in the population at some intermediate frequency and there’s purifying selection which is the elimination of new mutations that are deleterious and finally there’s positive selection there’s a selection for some beneficial trade and actually the last two are really sort of flip sides of the same things you’re selecting for one trait you’re selecting against some other trait you only have you have to be choosing one but it’s convenient to distinguish them based on what’s the but starting out is rare you can think of purifying selection as eliminating new rare things and I’ll start by talking about balancing selection this is probably the rarest kind of selection it’s kind of cool and you can find it but it’s a little difficult to detect and probably doesn’t happen very often there is one very well-known case in humans sick of the sickle cell trait sickle cell allele which was one of the first cases probably I\nSickle cell\nthink is the first case of identified natural selection in humans it it was holding in the 50s noticed that certain diseases involving hemoglobin were much more common in places where there was a lot of malaria and yep hypothesis that there was natural selection was playing some sort of role and he was correct and these two maps the top map shows where malaria occurs worldwide and the bottom map shows where there’s a high prevalence of the sickle cell variant sickle cell trait and the reason is is quite clearly quite clear even it’s easy\nHeterozygous individuals survive best\nto find out to check you can actually measure the difference in Fitness if you have one copy of if you’re heterozygote if you have one copy of this allele then you have concern against malaria so if you have no copies you’re exposed to malaria or likely to die younger because malaria is a big killer if you have one copy you have a real benefit if you have two copies then you get sickle-cell disease and you’re also likely to die young because without modern medical care it’s very serious disease and so you could look at the the survival this was without that kind of medical care you look at the survival it’s clearly clear that that hair is I gets in this case have an advantage and that means that there’s selection pressure to maintain this allele at some intermediate level so that the maximum number of heterozygotes in the population it’s not very pleasant solution but it is an evolutionarily stable solution in toilets with severe selective pressure you can get balancing\nBalancing selection selection for diversity\nselection in other ways by selection for diversity there are a lot of cases in which it’s good to be different from other members of your species like escaping from predators if the predator is used to finding a purple people and eating them if you’re green that’s it can be kind of good to be green then and it’s quite common in resistance disease if a new virus enters the village and it can infect most of the people in the village and you’re different and you know have a different genotype than most of the people then you have an advantage so the whole thing that that’s spreading through the rest of halation you’re immune from and you can see the effects of this of the selection for diversity in terms of disease resistance in the HLA region which is critical for immune response to\nSelection for diversity HLA\npathogens and if you just look on chromosome six that the density of snips in that region it’s much higher there than elsewhere in the genome because there’s a lot of selection obviously we’re constantly exposed to different kinds of infectious disease so there’s a lot of selection for having diversity there and there’s the HLA region if it couldn’t guess where it was and this is actually an example of frequency dependent selection that is whether decree allele is ’event Aegeus depends on how frequent it is when it’s rare it’s good to be different so that’s advantageous if it increases in frequency then it becomes it can become less advantageous and also it obviously can vary quite a lot with your local environment what pathogens happen to be nearby or what predators and in humans it’s more mostly pathogens we worry about not so much predators these days so it can fluctuate quite a bit on small geographic scales and on temporals short temporal time scale as a side note pathogens also evolve and they do similar some similar things this is one of the chromosomes of Plasmodium falciparum which causes the most severe kind of malaria and where that sharp spike in diversity is showing diversity across the chromosome that short sharp spike in diversity is a gene that codes for protein that’s exposed to the immune system a lot of malaria proteins are not exposed because they hide in red blood cells but this this protein is exposed to the human immune system and again there’s a lot of pressure to be diverse so that if you are the first parasite entering a village you want to be different than all the other parasites that the populations have been exposed to so that you can happily infect people without being you know having that unpleasant immune system triggered immediately and they also they’ve all faster than we do by the way so they’re constantly evolving as well so that’s balancing selection purifying selection is the most common kind of selection it’s sort of a little dull because all it is is its removal of new Dilla Tereus mutations most organisms are pretty well adapted to their environment and a functional change you know something that changes their phenotype in an important way is probably bad for them it’s gonna be eliminated by natural selection it’s not gonna be passed on for very long and that’s it’s very clear to see this in humanity that humanity netic data this is a plot of the top figure here is from thousand genomes data it’s the diversity across the whole genome looking at different parts of genes and I’ve worked where some exons are this percents on the middle exon and the last exon within genes throughout the genome and you can see the diversity is much lower within these coding exons because changes to the to a protein are probably bad and they tend to be eliminated and you can see it see that the effect is strong and just read off from there how many mutations have occurred in these exons and that have been eliminated by selection over time and although they’re eliminated they may not be eliminated immediately obviously if something makes you non-viable then you’re not going to see in the population but lots of mutations are mildly d latias they might make you more likely be sick or stupid or less attractive whatever which might be bad in certain circumstances and so you can see these these alleles are hanging around the population but they they won’t rise to very high frequency\nPurifying selection eliminates deleterious mutations\nbecause they are bad and they’re less successful so if you just plot the allele frequency of nonsynonymous mutations and compare it to synonymous mutations that’s when I show here on the left as a function of frequency in the 1% being that first bin you see there’s an excess of nonsynonymous mutations compared to the synonymous mutations because they look more of these or functional and so that access represents mildly deleterious allele that are going to be eliminated eventually by selection but haven’t been eliminated yet even within the nonsynonymous you can break it down further on the right has plotted different categorization of nonsense mutations as to whether or not whether or not they’re likely to damage the protein has changed the effect of the protein so in pink and red those are changes that are have a pretty good chance of changing the proteins function and those are the ones that are see more of them at low frequency so those are the the ones are going to be eliminated so in terms of evolutionary biology these are not interesting this is going to sludge that’s against being eliminated by a natural selection all the time in terms of medical genetics or human genetics of disease these are probably a lot of the ones that were interested in because these are one of the ways of being dilatoriness it does it makes you more likely to get sick and so these are some of the things that are causing genetic diseases or increasing your risk for early onset diseases of various kinds so there are great medical interest in some cases but not of tremendous interest to evolutionary biologists the effect of purifying selection can be seen not only in the allele itself that’s that’s being selected against it can also be seen on some of the surrounding variation the effect of purifying selection is that it reduces diversity in that region and you can think of like there’s a say there’s a gene where deleterious mutations keep happening when Julie - this mutation happens that from the chromosome it happens on is going to be removed from the population eventually and so effectively it’s not part of the population now in terms of the long term success so you have a right around genes or other functional elements you effectively have a smaller population size and that means you can sustain less diversity because some of the diversity gets taken you know any diversity that’s on that chromosome that gets the bad mutation is going to be removed from the population and it’s very easy to see this - this again a thousand genomes data here they plotted the distance from the beginning from the start or the stop of a gene across the whole genome the three colors are three different populations and you can see in what’s plotted this diversity and there’s a dip significant dip in diversity around a gene it’s on the order of fifty or hundred KB so it’s a substantial stretch where there’s a notable reduction in diversity and this is you know affecting the diversity distribution of diversity throughout the genome this is an ongoing effect why why is the absolute level well there’s just less diversity the red and the blue are the two non-african populations and there’s just less diversity outside of Africa because they pass through a bottleneck leading Africa and so there’s isn’t that isn’t that much there there may also be a small difference in how much diversity is reduced there there’s no question of whether you’re pure of like selection has been less effective outside of Africa because the effective population size was smaller but that’s a pretty minor effect and finally there’s positive selection which is what we usually think of as selection and it’s natural selections but sir what Darwin was famous for it’s the basis for adaptation pretty much all adaptation we think and so it’s kind of a sexy thing to look for and people have looked for it I’m gonna focus it initially on selective sweeps which is you know it’s selection where the mutation starts out basically as a new mutation and increases in frequency I’ll add some few complications later and so the question is gonna help what’s the effect of positive selection on genetic variation how is what that means how can we detect it just by looking at genetic variation and so let’s take let’s take the out of the case where a selection doesn’t happen this is neutral evolution so suppose there’s a mutation that happens in this blue guy here and as time goes on the frequency of that allele it may increase it may decrease it kind of bops around a little bit it’s it drifts and technical to detect alarm is it drifts its genetic experiences genetic drift but on average it doesn’t actually change in frequency so very slowly it might change over time if there’s positive selection for something if this new allele this mutation provides a benefit well then over time it can rapidly increase in frequency and that’s that rapid increase is what leaves the genetic signature that we can we can look for it relieves a number of different genetic signatures and I’ll kind of describe some of them so let’s consider what the genetic situation looks like before selection happens and after so if we this is a cartoon version of some chromosomes in the population this new red mutation is beneficial and there’s a lot of diversity there there are different alleles you have different combinations of those different haplotypes present in the population because it’s just been sitting yeah look it’s just me behaving normally after selection has happened this has this allele has increased rapidly in frequency and is present in a large fraction of the population and so this signature we can look for well one signature happens suppose this selection has happened only in one geographic region like it happened in I know the Boston area something that’s I know being a Red Sox fan me or maybe even exude there’s an allele for that might make you more reproductively successful here let’s compare it to New York and you know if that’s the case then you will find that allele very at very high frequency in that region and very low frequency elsewhere so it’s unusual one signature of selection then is that there’s an unusually large difference in frequency between populations at that locus and this can indeed be seen here both I’ve just mentioned that one way of measuring a common way of measuring frequency differences in populations is a statistic called FST but there are other statistics too you can use and a classic example of this is is the Duffing olive oil it’s the Duffy protein is a blood antigen one of the mini blood antigens sits on the outside of red blood cells and does something or other not entirely clear one but one one of us because it’s not intended for it’s a is that it’s also the way by which Plasmodium vivax another cause of malaria enters red blood cells and so it’s the only way of entering the but red blood cell so it’s critical for an invasion and for infection and if you lose that protein then you are pretty much immune to by vechs and malaria and if you there is a mutation that knocks out that gene it’s called the Duffy null allele and audits shown there is the density the the frequency of that mutation around the world and see it’s a very high frequency in sub-saharan Africa and as a consequence people in sub-saharan Africa are largely immune effects of malaria so much so though there’s almost over maximal area across Africa so this was a highly successful case of natural selection in humans suggesting that there was a very large cost from the my max malaria at some point and outside of Africa it’s it’s basically not there at all and so this is sort of a classic case is in fact I believe this is how it was discovered that this allele that this protein was the invasion route for this this parasite so it’s a it can be a useful to be looking at natural selection a more recent case in terms of studying it come\nHigh altitude adaptation in Tibet\ncame from people who are looking for adaptation for higher handling high altitude or there was very low oxygen and so what this research group did was they compared allele frequencies between a Tibetan population and sample from Tibet and a han chinese sample very closely related populations very similarly low frequencies so bottom of two axes are the Tibetan frequency and the Han Chinese frequency and you can see almost everything has this pretty much the same frequency in both populations and there are two alleles that are very high frequency in tibet and not at very high frequency in the han chinese and they’re on the low to in the lower right hand corner there are two alleles in the same gene you pass one and it turns out this doesn’t does indeed confer adaptation to handling hand up into low oxygen levels present there so this was a very easy way of finding this particular gene so it’s in principle this is a powerful way of detecting where selection has happened regionally in practice there aren’t very many low-hanging fruit like that somebody took the trouble of plotting of comparing you know how how many real outliers are there they took a whole bunch of different population pairs so each one of these dots is a pair of populations and on the x-axis is the average difference in the little frequency measured by FST between those two populations and on the y axis is y axis shows the most extreme single allele in that pair of populations and most of the time you can predict really well what the extreme is going to be just from what the averages so all you’re seeing is sort of the tail the distribution up at the high end there are a few cases here up here where there there are clearly outliers the one I was just talking about you pass one is is one of them all the other colored dots here are known pitov pigmentation genes so it’s not a there’s actually information in there but by itself it may not be a very easy way of finding out what’s been selected all right so that’s that’s that’s just the first signature I won’t focus much about all the others another signature is low diversity if everyone or a large fraction population is the same now around this new selected allele well that means if you’re the same you’re not different there’s not a lot of genetic diversity there and so if you just plot generic diversity across chromosome you’ll find regions where diversity has dips because there’s been a selective sweep there so what that means is that you know I said purifying selection produces reduced diversity around of functional elements positive selection can also reproduce to reduce diversity so they have somewhat similar signatures here which is inconvenient the signature the the loss of diversity can be more profound in the case of positive selection because of positives we if a sweep goes to all the way to fixation everybody is the same and so there’s virtually no diversity present another thing that’s a little bit easier to look for and to serve another a different way of looking at the same thing is that if everyone is the same you have the same haplotype they’re basically if you can predict if you have that red allele you can predict what other alleles everyone will have in the population up to the point where recombination is broken and down and if it’s if this election happened recently then recombination hasn’t had time to break it down yet and so you can look forward on long haplotypes that aren’t at high frequency in the population and there’s a whole series of statistical tests that have been developed for detecting that there’s an unbroken long haplotype present and this turns out to be a very powerful way of detecting selection that’s happened within the last 20,000 years roughly and here a\nLong haplotype: LCT\nvery very clear signature force in a signal selection can be seen at one of the now poster children for selection humans which is lactase persistence there’s a normal state from amell’s is that as you get older you lose the ability to digest lactose because lactose is present mostly only in milk and normal normal mammals don’t drink milk as adults but in human populations where they practiced hurting for a long time you know many adults can in fact digest lactose they’re lactose tolerant most Northern Europeans I’m lactose tolerant because I’m from my ancestors of northern Europe and if you look around the lactase gene there is a an enormous Li long haplotype that’s hardly broken at all and in Northern Europe it’s around 70 percent frequency it extends for more than a megabase is result a very strong natural selection positive selection for this trait and the the mutation that gives the biscuit for digesting lactose is in fact on that half of the time it’s just plot just shows that if you plot the strength of you know this is the strength of the length of the haplotype versus the frequency like lactase sits way up here on the right this is the strongest signal selection by this test in in Western Europe okay finally one other signature that’s a little bit harder to visualize is that in regions they’ve been under undergoing recent positive selection there will be an excess of high frequency quran derived alleles normally the the derived allele the newer the newer allele is at low frequency it’s occur it tends to stay at low frequency but when this kind of sweep happens towards higher frequency it can bring any other mutations that are nearby to higher frequency and so it can bring more of these these rare mutations up to high frequency along with it so that’s a different just a different sort of thing you can look at in data and all these signatures have been looked at they’ve been known for a while and they’ve been looked at and various looking throughout in sweeps scans across the genome looking for places where selection has happened and you can do some interesting things with them it turns out that there’s independent information in each of these\nComposite of Multiple Signals (CMS)\nsignatures and so one thing you can do and it’s an approach that was pioneered by pretty savetti here is combine the information of the different signals and here you know in a cartoon version we’ve got all the evidence from a long haplotype evidence from dr allele frequency and evidence from differentiation in populations and they’re they’re giving a different information but the actual selected one you know where a selection actually happened it will be you can get enhanced information about that which is important because some of these signatures tend to cover very large regions of a chromosome and not really tell you very much information about which allele or ease in which gene was actually selected for so if you want to get and downs with details then it’s better to combine information and here’s an actual case I think this is\nCMS pinpoints candidate variants\nchromosome 5 in humans and I don’t remember what data set this is it might be a few thousand genomes these are different signatures of selection the top one is a long haplotype second one is population differentiation and then the bottom one is derived alleles and a lot of these are raised or elevated so somewhere in here there’s evidence that selection happened but they’re very noisy and it’s very hard to know exactly where the selection happens but if you combine them then you might as if by magic the bottom distribution shows the the score for where you think selection happened and if you see it they’re only a handful that has elevated score indicating that right here is where selection probably happened and it turns out the biggest signal there is a nonsynonymous mutation so it was a good chance it’s it’s functional it’s sitting in a gene that’s important for skin pigmentation this in fact is an allele that that contributes to European skin color that’s one of the major alleles for that good question isn’t how strong it is you know I’d say with four really strong signals 100 is fine for as you’re getting to more subtle things thousands are better above that the problem isn’t so much sample size as knowing being able to distinguish between background stuff and what’s actually selection it’s not just not a statistical problem it’s if anytime basically we’re doing is we’re looking for the weirdest part of the genome and lots of weird stuff can be happening in the genome and there’s other various things that confound these you know for instance the long haplotype test that can be founded if there’s an inversion where that suppresses recombination in that region and I think in you know that’s one of those sorts of things that can confuse you so there’s a variety of things that can’t affect you so I said these it’s tests have been known for a while there was a big spate of genome scans about ten or twelve years ago as data you know genome-wide data became available but there is still work going on in this area this is a figure from my paper that was published two weeks ago in science which a new signature for selection for\nA new signature singleton density\nrecent selection which I thought was cool enough but I will try to explain it to you the basic idea is that if so we’ve got two alleles here at a site each of the along the bottom pointer there we go well no these are all the samples and if you just construct a gene tree of which which samples are related to which at this particular locus yeah and this is the gene tree and is that this in blue here is a derived allele that the mutation happened at some point in the past and it’s been selected for and if it’s selected for its increased in frequency and what that means is that that it’s younger that on average you have more recent common ancestors and if you compare people they have a recent common ancestor that’s what it means for it to have increased in frequency recently and so if you just look at the the terminal branches the terminal lines here leading to all of these samples they tend to be longer for the one that wasn’t selected for because and any mutation that occurs on one of these is going to appear as a singleton in your sample it’s just that’s one mutation so all this test does is is count how many Singleton’s there are around each you look on the little eel from the genome just count how many Singleton’s there are nearby and look at the density of single things and then the the density will be lower fewer Singleton’s around recently selected things and one of the nice things about this test is that unlike some of the other tests I’ll say in a minute it works on not just selective sweeps but it works in some other more complicated situations and it’s according to the authors who they they’ve figured out you know what is this sensitive to if you have a reasonable sample size of like a few thousand then it’s probably sensitive to select it’s occurred in like the last 2000 years so in genetic terms that’s a very recent selection so this is this is cool and it’s worth reading that paper ok so I think there are some complications selective sweep is nice and pure if you have one but it’s making some assumptions the some assumes that the beneficial mutation occurred once or at least was so rare you treated as just there was just a one copy and then that rises to frequency and has all these effects and it as soon as there’s just that that one it’s one mutation of pretty large effect but suction can happen in other ways a suction can occur on standing variation that’s variation the variants that have been in the population for a long time you know if it’s been there for half a million years or a million years recombination has been happening all the time so that that variant is hitting on all lots of different haplotype backgrounds they’re all going to rise in frequency you’re not going to see most of these signals the same mutation can happen more than once that’s happened in the sickle cell trait if this identical mutation has occurred on multiple occasions been selected for but you’re in and I’ve again have multiple haplotypes increasing in frequency and finally there may be lots of different alleles that contribute to the same trait and each one may only shift a little bit in frequency and you can still have a substantial effect on that trait and so these signatures of selection or are pretty much hopeless there are are attempts to use sort of modifications of these tests while I’m looking at allele frequency distributions in the case of selection on the standing variation but it’s just harder than signature the the signal is isn’t isn’t as easy to find you can still do some interesting things though and find some interesting stuff if you stop thinking just about alleles but\nFocus on traits rather than alleles\nrather than think about the trait if you know what genes or what alleles that are contributing to variation in a trait then you can you know aggregate different alleles that are involved in that trade and so this is an example that was carried over studied to carry out by Joel Hirsch orange group here or here in Harvard and looking at height sure in europeans we know from aji wasps many of variants that affect stature we also know just from looking at europeans that there’s a client in height across europe from north to south Northern Europeans tend to be taller than southern Europeans and so what they did in this figure they plot they ordered they ranked the snips from the G wasps in terms of how large an effect it had each snip had hung stature and then on the y-axis they plot the difference in frequency between northern Europe and southern Europe and what they find is that the alleles that have the biggest effect on the stature are also the ones that have the biggest frequency difference in Europe and this is consistent across you know all of the you know the major alleles here they all have higher frequency in Europe this isn’t just randomly a few alleles happen to be higher all of them have higher frequency in the increased height direction in northern Europe or alternatively there all have you know the lower stature allele in southern Europe and this is pretty good evidence and it’s been substantiated by further studies by others that selection was acting on stature in Europeans in some way not it not clear exactly how but in some way in others there have been other studies this is a similar sort of study looking at a variety of traits correlated with geography in this case is finding a significant selection for stronger UV stronger damage respond stronger response to damage from ultraviolet radiation if you live near the equator if your population comes from your new equator which isn’t maybe isn’t too surprising but the different colors are different continents and on different continents the seems of selection pressure has been happening so you can indeed extract a fair bit of information about the trait that’s been selected for even if you may not necessarily know which particular allele has been selected so that leaves the question what traits have been selected for what have we actually found what’s been going on in humans in the last 20,000 years or so because that’s pretty much all the data I’m talking about or from selection that’s occurred within the last twenty or twenty to forty thousand years because that’s where that’s the easy place to look lots of interesting selection happened before like what made us anatomically modern humans but that’s a lot harder to find or to study so we’re studying all the relatively easy things so as I said you can scan the genome for these signatures of selection and many people did here’s a result of one of those scans because it was one one done here and here are a lot of places in the genome where there’s\nResults of a selection scan\nevidence that selection happened there probably some false positives in there but there are lots of actual cases of selection occurred there and of questions what do they do what was involved and the answer is mostly we have no idea something happened there somewhere and we don’t know what the trade was we don’t know what the selection pressure was and it’s just sort of going from something happened at this locus to what’s you know the phenotype is hard work that’s biology that’s not you know sitting around looking at that was genomes data you actually have to do some biology then and I’ll have one example here of\nFrom candidate to function: EDAR\nthe kind of work that can can elucidate this sort of thing this was this is a little segment of one of those genome scans this is chromosome 2 in East Asians and this this is a test for long haplotype so it’s very high in high score here across this entire region indicating that selection happened here where exactly you know probably somewhere in here but you can’t really tell little relative to say the size of the single gene so a long run and also the sheer number of variants here is a problem if you want to see who was one of these was one of these variants had a phenotypic effect actually drove this because one of these variants was probably actually important and the rest didn’t so if you’re now knowing what to test for say do some functional work this by itself is kind of daunting this is just too many variants there to put into a model organism say and see born into a cell and see what it did but you can do the trick I mentioned before if combining information from multiple Cygnus signal signatures and that actually cleans up this particular locus very nicely there are only a handful then candidates you want to look at and one of those turns out to be a nonsynonymous change and as I said you can see that sorry here you can see that there are in fact multiple genes across this region and that’s a month it was announced anonymous gene in nonsynonymous change in a particular gene known to be involved in development of hair and sweat glands it’s known have been under selection in other organisms so that then gives you something you can you can look at in more detail you can take that variant and the study it in the lab and somebody did a postdoc at Harvard and working with a broad and I\nMouse model EDARV370A\nsaid this was selection that parently happened in East Asia this increased in frequency and and she stuck it in a mouse this particular variant stuck it in a mouse and seal so what it did and well it did several different things it produced thicker hair it produced smaller and denser memory glands and it produced a higher density of eccrine sweat glands and it turns out that at least the the first and third of these the hair in the sweat glands this has the same thing as a big affected humans and the thicker hair is actually even is something you can actually see you know the East Asian hair doesn’t look the same as European hair say typically so this is this is a mutation that was selected for its illustrative in that by focusing in on something that was selected for we found something that clearly has a notable phenotypic effect on humans that distinguishes between humans it’s also illustrates one of the problems which is that mutations don’t always have one effect and so an affected hair and affected sweat glands and we don’t know why you can guess well it maybe it’s something temperature regulation and sweat glands are important or maybe hair was important we don’t know so it’s not you know a final answer but it was a major clue to finding something that’s phenotypically important and that distinguishes humans from one another if you look more broadly this is a this is from a very recent review article Sarah Tishkoff is group in science of what we’ve learned about selection in regional selection in humans and you can see on the map we’ve learned a number of different things and several of the cases I’ve talked about are on this map and I’ll just mention them selection for changes to diet the lactase persistence case up there in the Europe that’s one of the you know classic ones you can’t find lactase when you’re doing something wrong and you’re studying a selection but that’s also been studied in East Africa and the Middle East where there are other have been other hurting populations turns out to be independent mutations that in the same regulatory region that have the same phenotypic effect so it’s exactly the same pathway exactly the same mechanism but that’s occurred in multiple times in different places there are other populations around the world that have lactase persistence in South Asia in West Africa and these have not been studied in any depth at this point so exactly what the mechanisms are there I mentioned skin pigmentation there are a bunch of genes that are known to where alleles are known to contribute to the paler skin color that Europeans have and which varies with with latitude and so they’ve been well studied other genes are known to have mutations in Asia some of them independent some of them are shared so as different but partially different set of mutations other parts of the world where there’s also variation in like within Africa there’s quite a lot of variation and probably someone has been under selection but again the studies haven’t been done there there’s plenty of things still to study I mentioned the polygenic selection on on stature in Europe turns out there’s also been selection on stature elsewhere in in rainforest environments the selection for smaller stature in humans pygmies that tend to have small stature well known in the Central African rainforest and selection there has happened is is operated in a rather different way in Europe it was very polygenic selection on lots of different alleles in Central Africa it apparently was strong selection operating on a handful of low sigh for shorter stature might be because the pressure was of a different kind it may just be that’s the way it happened to work don’t really know and I think I think the last one is circle here is the one for high altitude and a patient that’s been studied in Himalayas it’s also been studied in other high places in the Ethiopian highlands and in the end these and you find in independent mutations most of them in the same pathway leading to similar phenotype so the same selection pressure different mutations but similar outcome I don’t know much about the thinking it might just be a matter of resources of you’re better off you know if you need to eat less because it’s not a very highly nourishing environment but I don’t really it’s not something I’ve looked at in any detail so that you know that those are the cases we’ve understood something about we don’t always necessarily know exactly what with exactly what the selection pressure was but least we have some idea what the phenotype might be one question that might occur to people is well how much of this is there how much has natural selection been operating in humans recently and it turns out to be kind of tricky to figure out because alright so there’s you know\nHow much positive selection is there, anyway?\ndifferences frequency differences between populations it varies across the genome how much of that is naturally occurring and how much of that is I mean is neutrally occurring how much of that is the result of selection especially since we don’t really know the demographic history of humans in detail we can’t exactly model it and tell you what the distribution should look like so it’s kind of an open question of how much positive selection has happened and one way of trying to address this is to look for look at that reduction in diversity like this particular reduction of diversity around the genes and I said that can be caused either by selective sweeps happening repeatedly in positive selection or by background selection so you need to find a way to to distinguish them and so one group what they did was they looked at genes so we’re only looking at gene so this is presumably background selection is happening all the time and they compared cases where there’s been a nonsynonymous change in humans over the last million years or so the places where there’s only been a nonsynonymous change and the idea is if if they’ve been a lot of selective sweeps you should see reduced diversity around the nonsynonymous case and here’s their bluff from their of their paper this is Hernandez at all from 2011 since a few years ago now here’s where the substitution happened at some point and they plot diversity for synonymous in blue the substitutions and for around nonsynonymous substitutions in red and those distributions are basically the same there’s no obvious difference between the nonsynonymous and the synonymous and their conclusion was we don’t see any evidence for lower\nConclusion 1\ndiversity around functional changes classic selective sweeps have no I mean they’re sure they’ve been some but they’re not a big you know they’re not a major factor maybe it’s been you know this polygenic selection other kinds of selection are happening and basically said the conclusions you should stop wasting your time looking for these things which was a little annoying for those those who were looking for them in particularly those of us who wanted to get funding to look for them because then you know this you know the reviewers of the grant proposal say why are you looking for this Hernandez it all just showed that there aren’t any selective sweeps or you know we’ve already found them all don’t don’t waste your time and then a couple years later there was another paper from a different group Davida Nord Petrov Petrov group groups and they came to a very different conclusion I think I’ve time to go through this and they said they looked at the same data and they said no you’ve drawn the wrong conclusion because the problem is you’re you’re looking at nonsynonymous mutations well let’s think about that a little bit let’s think about you’ve got two genes gene a is highly constrained every amino acid is a precious jewel and if you change one of these is bad so anytime there’s a missense mutations deleterious there you will find very few missense mutations that happen in neutrally so if there’s any mutations in missense mutations they will have been beneficial and what that means is lots of background selection happens there’s lots of purifying selection going on all the time because all of these mutations are bad so if you look at diversity around that gene well it’s going to be very low diversity around that gene now gene be in the other hand this is a weakly constrained gene there’s a lot of them a lot of the amino acids don’t do anything so a lot of neutral changes and consequences have got to be less background selection because they’re just there’s fewer opportunities for deleterious mutations and so around those genes there’s going to be you know a smaller reduction in diversity and there’s what they say is that when you look at nonsynonymous me to pick and nonsynonymous substitutions that’s happened which is what the Hernandez a tall group did you’re picking primarily from this group from these genes on the other hand when you’re picking nonsynonymous mutations you’re picking from both of them so you’re biased yourself to finding places where there’s not a lot of change and so when if there are cases a lot of cases of positive selection going on selective sweeps you’re going to completely lose it found because of this bias and so their conclusion was that there’s actually strong\nConclusion 2\nevidence for lots of selective sweeps in recent human history there’s been a high rate of strongly adaptive substitutions near amino acid change and has been even more sweeps and driven around regulatory changes which I think you know independently so that same data very different conclusions and a little heartening to those of us who are interested in selection I mentioned that I introduced that new test for selection is this density of Singleton’s they conclude that lots of different traits if you just accept their data at face value lots of different traits quantitative traits in humans that have been studied under by G wasps a lot of them show evidence for having been undergoing selection in the last 2000 years like more than half I don’t know if I believe that but it’s certainly a bit there’s Evans has suggested that lots of slow selection of some kind is going on all the time ok so that’s so that’s where we are in terms of studying selection from just looking at genetic variation and what its effect is but there’s one other topic that’s really interesting because it gives us a whole new way of looking at this and that’s ancient DNA because in all these things were inferring what happened in the past all these kinds of studies we look at genetic just variations in a bunch of Norwegians and you try to figure out what happened ten thousand years ago but now we can actually look at DNA from ten thousand years ago five thousand years ago and see directly what’s changed between then and now and people have done this we’ve now are at the point where we’ve sequenced enough ancient genomes we can compare our little frequencies in the past to allele frequencies now and this is data from a\n230 ancient genomes\npaper of last year David Reich’s group at Harvard and this is plots the colored dots are frequency of several particular alleles that are of interest in ancient about several ancient populations and the dashed lines are there what the frequency is in modern modern populations modern European populations is basically Europe and West Asia there are these ancient populations and I’ll point out a couple cases that the top left one is lactase again this is this classic example you can see that the frequencies in these ancient populations was at or near zero the the allele that gives you the ability at just lactose where as in its modern southern European populations it can is still fairly low but in northern European populations it’s very high so this is selection that’s happened just in the last few thousand years you can see that it’s been dramatically increased in frequency or at that time here on the other hand is of pigmentation allele one of the major ones that contributes to a European paleness and you can see that was at different levels and different populations it was they were apparently the steppe peoples these are the people of the Western steps in Asia they were apparently probably paler than than these other Europeans down here but they’re they’re all a lower frequency than and in modern Europeans so this is a selection that was ongoing at this time and has continued into the present and if you look across the genome this is the I don’t know if you’re used to Manhattan plots but the entire genome you spread out and the places where there are signature signals that suction has happened just from comparing the ancient DNA to the modern DNA you find a lot of the same things you find out that there’s skin pigmentation genes that are already known they find the dye of genes including lactase and like fatty acid dehydrogenase which is has been seem to be under selection and other populations selection for resistance to infectious disease at the HLA at Toolik receptors so the basic story we get from ancient DNA is it’s very similar which is\nAncient DNA tells a similar selection story\nheartening because you know we were reconstructing the past based on my computer models and it’s nice to see that you actually when you actually can look at the ancient DNA it tells you we were right in a lot of these cases we were really incorrectly inferring that selection happened but you get a lot more ten get a lot more detail when you look at the ancient DNA because our model our computer models are simple so I’ll give you a few examples of the difference in the story and to end here you know I talked about this this gradient of stature in Europe and there’s this north-south gradient so we concluded selection happened well when looking at the ancient DNA they concluded a little more detail they conclude that selection happened for shorter statute in stature in southern Europe that selection for taller stature happened in West Asia in these step populations released that’s where we see it and that we see this greater height in northern Europe because those people then moved into northern Europe it wasn’t necessarily selection happening in northern Europe through a greater height it was just people having to migrate in and you know looking at modern popular modern population we have no idea we would have no idea about this it’s very hard to figure out all of these you know where all these people have been moving around we tend to assume that well if we’re looking at you know Chinese people today that’s that’s who you know their ancestors were living there 10,000 years ago no people move they move a lot turns out you’re look at ancient DNA sometimes in ways that we couldn’t tell it all from modern DNA all right a second case the pigmentation there are two main genes that contribute to European pigmentation typical color with very similar names slc24a5 and SLC four five A two and we know from the genetic evidence modern Europeans that they’ve been both in under strong selection risen there’s an allele that’s risen to high frequency in both cases turns out the two genes have somewhat different histories looking at the ancient DNA one of them actually it’s the one I just showed this skin pigmentation gene boys you can see rising in frequency within Europe as selection was occurring for lighter skin the other allele actually entered the Europe at very high frequency with farmers when the first farmers moved in from Anatolia Evelyn’s mom Turkey they largely replace the European population and they already had the lighter skin presumably as the result of earlier selection but you know you get a much more detailed picture of the history of this one and finally that the last case is the case of Adar and the one thing that gives you more sweat glands and thicker hair well you know when we were studying it the story was it seemed pretty simple this plot shows where that allele this this sucker lily is is present in East Asia and as president the new world because people from eastern Asia populated the new world and so it was clear you know you can do lots of they did very detailed modeling for this your model ahead was clearly this is estimating where this allele originated and originating in central China 30,000 years ago so it was a nice simple story there was a cell paper and it’s a great paper but this particular conclusion the problem is you look at the ancient DNA it turns out this allele was at high frequency and Swedish hunter-gatherer 6,000 years ago which is not something you would guess from looking at the modern distribution and it says I mean it’s it was stolen it was under selection in East Asia and that that story hasn’t changed but the details of what happened historically if something come more complicated happened and probably arose in Western Asia and happened to be selected for an eastern Asia so and this is the kind of information to get from it really is a tiny snapshot of ancient DNA as we get more DNA we’ll be able to learn a great deal more at least from places where there is ancient DNA a lot of the world my own tropics DNA does not preserve well I’m gonna conclude with a couple of comments so recent positive selection it’s clearly has had an impact a significant impact on human phenotypic diversity both individuals and within groups exactly how much you know some traits many traits at least two traits have you know been changed by this and many of these traits are medical interest or biological interest it’s a great way of finding out one way of finding out where these inter important phenotypic changes are so if you study natural selection you can learn you identify places where things have changed it’s not by itself it’s not a certainly not an all-purpose tool it’s it’s a clue it has to be combined with functional work with jiwa you know with association studies with all kinds of other things but it is one one tool in the toolkit else out there thanks"
  },
  {
    "objectID": "chapter10.html",
    "href": "chapter10.html",
    "title": "Chapter 10: Other Considerations",
    "section": "",
    "text": "10.1 A Career in Psychiatric Genetics\nTitle: How does Genetics Affect our Mental Health?\nDescription:\nPresenter(s): Cathryn Lewis, Alex Curmi\nLevel: Beginner\nLength: 58:54\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\n\n\n10.2 Caution in Genetic Prediction\nTitle: Predicting the likelihood of future psychiatric disorders: a closer look, and some cautions.\nDescription:\nPresenter(s): Howard Edenberg\nLevel: Beginner\nLength: 4:30\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\n\n\n10.3 Small Effect Sizes\nTitle: Small effect sizes\nDescription:\nPresenter(s): Howard Edenberg\nLevel: Beginner\nLength: 4:05\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\n\n\n10.4 Language in Genetics Research\nTitle:\nDescription:\nPresenter(s): Carina Seah, Kayla Townsley\nLevel: Beginner\nLength:\n\n\n\n\n\n\n\n\n\nLink to video transcript\n\n\n\n10.5 Equitable Collaboration for LMIC Researchers\n\n\n\n10.6 GDPR for Dummies: A Survival Guide for Genetics Research\nTitle: GDPR for dummies: A survival guide for genetics research\nDescription:\nPresenter(s): Heidi Beate Bentzen\nLevel: Beginner\nLength: 23:46\n\n\n\n\n\n\n\n\n\nLink to video transcript here."
  },
  {
    "objectID": "chapter9.4_transcript.html#sec-video1",
    "href": "chapter9.4_transcript.html#sec-video1",
    "title": "Chapter 9.4: Interactions with Environmental Factors (Video Transcript)",
    "section": "Dummy variables: interaction terms explanation",
    "text": "Dummy variables: interaction terms explanation\nTitle: Dummy variables: interaction terms explanation\nPresenter(s): Ben Lambert\nBen Lambert:\nso let’s think back to our example we had in the last video so let’s say we were interested in how wage rates varied between let’s say male and female people so the idea is that we regress wage on let’s say now that we sort of implicitly assuming that we have all these other variables so I’m not going to include them explicitly we’re just gonna have alpha plus let’s say beta 1 times the number of years of education plus beta 2 times our sort of sex variable where our sect variable takes on the value of 1 if the individual is a female and it takes on a value of naught if the individual is a male but then we included a further term which was let’s say B 2 3 where we multiplied sex times education so the idea is that we have collected all these variables across our individual or individuals or in the population or in our sample rather and we have included a multiplicative term in our regression specification so what does this multiplicative term mean what how do we interpret this beta 3 well let’s think about again what this sort of average wage rate would be for a female and compare it with the average wage rate for a male so the average wage rate for a female if they had a given number of years of education would be alpha plus beta 1 times the number of years of education which they had plus well this sector level now takes on the value of 1 so I’ve just got plus beta 2 and now our sex parable here is taking on a value of 1 as well so I’ve got plus beta 3 times the number of years of education ok and then we can sort of simplify this if we should’ve noticed that our alpha and our beta 2 of those constants here so you’re writing those both the stars of the model we get sort of alpha plus beta 2 let’s say and then we recognize that we have essentially got to education terms we’ve got this one and this one so I couldn’t simplify these as well by writing them or by combining them I just get beta 1 plus beta 3 times the number of years of education okay so that’s for the female case what do we have for the male so the idea for the sort of males in our sample is that the average wage rate is given by alpha plus beta 1 times the number of years of education because our sex pheromone takes on a value of zero so these sort of second two terms actually cancel or don’t don’t exist for the male so now we can think about what the effects of our sex variable has been on our sort of specification and our interpretation so what does b2t represent here well Pizza 2 represents the additional premium which females would have over males if they had zero years worth of education because if they had two years worth of education then both of these two terms would disappear and the only difference between males and females within factually and when it would in fact actually be our Peter 2 so just like your proves in the last video that is actually the wage premium which females do so it’s over males in the case in this case of having zero years worth of education okay so what does b23 represent well notice that the only difference between these two specifications in terms of the education variable is that essentially the partial effect of education for females has been boosted by an amount b23 relatives to the males so what does that mean well if beta 3 was greater than zero it means that the additional effect of one more year of education for females was in fact greater than that for males if it was less than zero then it would be the other way around so the additional effects of having one more year of education on average would tend to cause a smaller increase in female wage than it would do for males so we can sort of think about what these cross terms mean in our regression specification well essentially what they mean is if I’m interacting a dummy variable with a continuous variable is that it allows us to have different slopes of that particular continuous variable across the two different values which are dummy variable can take on so and that’s quite a sort of an inappropriate assumption to make in a whole host of different situations in this particular situation it kind of then you might suppose that there might be in a different effect of educate between males and females but there are a whole host of other ways in which this could be true across other sort of types of model"
  },
  {
    "objectID": "chapter9.4_transcript.html#sec-video2",
    "href": "chapter9.4_transcript.html#sec-video2",
    "title": "Chapter 9.4: Interactions with Environmental Factors (Video Transcript)",
    "section": "Continuous variables: interaction term interpretation",
    "text": "Continuous variables: interaction term interpretation\nTitle: Continuous variables: interaction term interpretation\nPresenter(s): Ben Lambert\nBen Lambert:\nhi there in this video I wanted toexplain what the sort of interpretation is when we have two continuous variables multiplied together in some sort of regression model okay so let’s think about a particular example the only deal here is that let’s say we were trying to explain a company’s level of sales but we are trying to do that in terms of let’s say the effective price and let’s say the effect of advertising yes a this might be the company’s level of advertising spend and this is just the company’s price is set for a particular product so traditional some Theory would sort of expect us to have a downward sloping demand curve so we would expect b21 to be less than zero because if you lower the price then in sales increase and we would sort of expect that if we spend more on advertising then sales tend to be higher as well so we’ve got Peter 1 being less than 0 and beta 2 being greater than zero but let’s say we included a third term here which is beta 3 and now included the product of price and the company’s spend on advertising what interpretation can we actually give to this piece of three well let’s think about this in two different situations so let’s say that the company was vet so spending a hundred thousand on advertising and let’s think about what the company sort of expected sales would be under that situation well the idea is that the company’s level of sales we would expect if advertising 100 would be equal to alpha plus beta 1 times the price plus now we’re going to get 100 beta 2 for this third term and then we’re going to get plus 100 and be two three times the price okay so what does this shown us well we can actually sort of think about the effective price because price is appearing twice in our model here we can sort of combine the price variables to create a sort of aggregate effective price so here we would have the aggregate effect the price would be beta 1 plus 100 Peter 3 yeah and then that would all be multiplied by the price so one interpretation is beta 3 and what sign would we expect Peter 3 to have in this case well we would actually respect that beta3 would be greater than zero why would we expect that well the idea here is that if you spend more money on advertising then that tends to decrease the sensitivity of your consumers to price changes in that product so notice that this appears because beta 1 is less than zero so if we’re adding 100-meter three where beats of three is greater than zero then the idea is actually we have decreased the sort of sensitivity of consumers to price changes or we should have made our customers less reactive to price changes which is something which you might expect accompanies shows to exhibit right you’ve might expect if you spend more money on advertising you increase this in a brand value or the sort of non tangible effect which consumers consider when they’re thinking about your brand so that might make them less price sensitive okay so that’s kind of what beetle 3 is representing in this case let’s think about another example whereby let’s say we had the price level be four to ten and let’s sort of say what we might predict the company sales to be and that’s that case so the idea is that the company sales on average when the price was ten would be equal to alpha plus ten beta 1 plus beta 2 times the level of advertising which we haven’t specified Plus now we can have 10 beat in three times the level of advertising so notice that again here we have two terms here which essentially have the same variable so we can combine these so we now have an aggregate effective advertising being put to beta 2 plus 10 beta 3 yeah so what does beta 3 represent in this case well remember that we found from the first example the beta 3 by theory should be greater than zero well what does it say in this case well it says if your price is higher so remember the prices are represented by this 10 here then the effect of advertising tends to be greater so that might be the case right if you have a higher to the premium price product you might have to demonstrate to consumers that it’s worthwhile buying show the effect of advertising is greater than if you have let’s say a low price product which consumers would flock towards anyway so Pizza three generally what does it mean what are we learned from these considering these two cases well it shows that the effects or price depends on the level of advertising spend and the effect of advertising tends to be determined by or tends to be affected by the level of price so beta3 is sort of a way of adjusting the effect of price and advertising to take into account their sort of multiplicative effects on one another"
  },
  {
    "objectID": "chapter8.7_transcript.html",
    "href": "chapter8.7_transcript.html",
    "title": "Chapter 8.8: TWAS/PWAS/MWAS (Video Transcript)",
    "section": "",
    "text": "Author: Sasha Gusev (alexander_gusev@dfci.harvard.edu)\nLength: 16:39\nI am Sasha Gusev this is my first time at cgsi so thanks everybody for having me and giving me the opportunity to give this tutorial as with the other ones please feel free to interrupt or ask questions throughout and I’ll try to sort of break things down in a way that’s accessible.\nI’m going to be talking about genome-wide Association studies and specifically trying to make sense of genomewide association studies as a way to understand human disease and complex traits and so just to sort of start it at the at a very basic level this is the output of a genomewide association study or GWAS.\nThe procedure is very straightforward - you collect a lot of genetic data on individuals with the disease and without the disease or with a quantitative trait and then you test each genetic variant (and that’s what each of these dots is here) for association with the phenotype. The variants that are significantly associated are above a predefined threshold here, and if they replicate we treat those as genetic variants that are causal for the disease and this is sort of a study design that I think initially almost seemed too simple to work but now over time and with very large sample sizes has produced thousands if not hundreds of thousands of associations for nearly every complex trait that it’s been applied to when there were sufficient sample size.\nIn fact the challenge is now that these Association studies are almost producing too many results and what we would rather have than sort of this figure which is a real plot from geological prostate cancer is something more like this which is a systemic or systematic understanding of the disease of which genes are involved in the disease how they interact what contexts they’re relevant in and so forth and so uh whereas initially there is sort of a challenge of just fleshing out this side of the plot getting these associations um I think a key challenge now is in Connect going from this side of the plot over here to an actual understanding of the disease and one of the sort of most basic pieces of getting to that understanding is connecting variants to the genes associated variants to the genes that they likely operate through and then operate on the trait and so we can break it down into this very simple structure we have a variant we want to know its Target Gene and the effect that it has on that disease and so in particular we can break this down even further and first just ask whether we can identify variants that influence the expression of genes in a systematic way and this is something that was observed some time ago is that in fact if you take gene expression and you you basically kind of run a G was but on expression as your outcome gene expression measured in the past through microarrays or now through rna-seq and test variants typically near the gene in CIS with the gene for association with expression across individuals you will find that the expression of many genes is often highly heritable and so there’s an estimate here in 2011 that the CIS Locus for an average Gene contributed to between 37 and 24 of the variance of expression and again once you have a heritable phenotype in a population you can sort of apply the GEOS Paradigm to that phenotype and instead we call that an eqtl analysis and I’m sure you folks have seen work from the gtex Consortium over many years applying ettl studies and identifying thousands of variants associated with the expression of many genes in many tissues and in fact again this is one of those cases where as the sample sizes have grown this study design has actually yielded a very large number of associations that are almost like too difficult too many to fully process and I think the most recent Gtech study showed that if you sort of relax the significance threshold for these associations nearly every Gene has at least one eqtl in some tissue and in fact I think that if you continue as the sample sizes have grown even further we see that genes then start to have secondary eqtls and tertiary eqtls and this sort of curve uh does not is not even hitting diminishing returns so that’s the piece about identifying genetic variants that influence gene expression and then there’s been a lot of work in trying to understand how these eqtls connect to disease and I’ll highlight a couple studies in particular which basically asked in a couple different ways whether an eqtl is more likely to be a GEOS variant or is more likely to be associated with a complex trait and so the results on the left show that eqtls specifically as you get more confident about them being the causal eqtl are more enriched for heritability across many complex traits from gwas and then this figure on the right from gamazon at all showing that if you just sort of try to partition the amount of disease heritability that could be explained by eqtls those estimates are also quite High across a large number of complex traits again ranging from maybe 10 percent up to 35 percent so there’s this sort of incidental evidence that eqtls are enriched for disease heritability and may therefore give us an instrument to understand um the likely causal genes and eventually go back to that sort of big system-wide understanding of the phenotype so that’s the first part of the arrow the other part of this of this network is we want to understand how this genetic mechanism of gene expression actually goes on to influence the trait and for which traits and this is where the approach of a transcriptomide Association study or a tiwas comes in and I’ll just start with a very basic sort of thought experiment of what would we um want to do if we had the ideal data set how would we in sort of with infinite resources try to relate gene expression genetics and disease together and I think one way that we could do this is we could estimate expression in the hundreds of thousands of individuals that we have genetics and case control status in here like this represents case control status and then we could ask what genes are genetically correlated meaning the effect sizes on expression are also shared with the effect sizes on disease we could do this for every single Gene across the genome and that would give us an estimate of the genes that in principle could be linked to this phenotype and the the hurdle here is that we we uh very rarely or pretty much never have data at this scale what we typically have is a relatively small study of genotypes and measure gene expression usually as in the case of the G tax in a sort of healthy relatively healthy population that was convenient to sample and then we also have very large disease studies that also have genotypes but no gene expression measured and so the basic Insight of the transcript and mind Association study or tiwas is kind of thinking about the fact that we have what is shared across these two these two studies is the genetics and we know previously that gene expression is itself a heritable trait and if it’s a heritable trait then in principle it should be a predictable trait and so what we want to do is use the genetics to predict expression into this study over here where we haven’t measured it and then use the predicted expression as a sort of proxy to estimate the relationship between the genetic component or the predicted component of expression um and the uh in the in the phenotype um and again this is all I’m sort of presenting everything in the context of a single Gene but the idea is to use this methodology and scan across every Gene in the genome and identify the set of genes that are significantly uh genetically correlated or uh For Whom the predicted expression is significantly associated with the phenotype uh and so right then we do the test so the first question is can we actually predict gene expression in this way and the fact that we’ve observed significant eqtls or individual variants that affect expression basically tell us that tells us that that we can and in work that we’ve done and others have done we’ve shown using a number of different prediction schemes that I sort of won’t go into but that are various forms of penalized or Bayesian regression that you can in fact predict gene expression with a substantial degree of accuracy and in particular when you use models that incorporate all of the genetic variation around the gene you typically have substantial gains in the predictive accuracy so even though the single topic ETL explains a large fraction of the CIS effect or of sort of the total heritability near the gene um there is a very large number of genes for which additional variants contribute substantially to the predictive accuracy and so simply going from a single snip Paradigm to a sort of locus wide Paradigm increases our predictive accuracy and that’s going to translate into better Association statistics in the in the eventual guas study um now one additional constraint is that we typically don’t really even have this design where there’s individual level data in both studies what we actually have more frequently is this design where we have individual level data for the gene expression study and then we have summary statistics for the gys and the summary statistics are basically for every snip the marginal the marginal sorry I think this turned off dead battery I think I know what that means um is there a does anyone know if there’s a backup or can you guys just hear me like this I don’t know if this can you hear does that does that work okay let me try to decouple one sorry okay all right I’ll try to work with this and then let me know if you can get another one for that one cool okay so what we typically have is these summary statistics oh sorry do you need anything else it’s perfect okay let’s do it great all right that’s easier thank you okay so summary statistics uh this is what we actually have and they are the marginal Association statistics for every variant and what we want to know from this kind of data is what would the gene the predicted Gene trade Association have been if we could get to the individual level data and measure it and so this is really where the the uh the tus methodology comes in again because this is the type of data we have most of the time um and I’ll just sort of sketch out how this uh this parameter is estimated and the basic idea is that we think about what we would want to do with individual level data and then we kind of move terms around and try to identify pieces that can be estimated from the summary level data and so we start with predicted expression over here x are the genotypes that we use for the prediction W are the weights that we’ve trained in the gene expression data that gives us this this term G that’s the predict expression and then what we want to know is the association between y the phenotype and G the predicted expression so specifically we want to know this orange beta T was so we can kind of plug in the terms into a basic ordinarily squares regression and then decompose these terms and you can start to see pieces here that you can actually estimate from summary level data and in particular you’ll see that this covariance between the genotype and the phenotype actually corresponds to this these G was summary statistics that we get the association between each snip and and the phenotype and then this term down here the covariance between the Snips themselves is also something that we in principle can get from reference panels because it doesn’t rely on knowing the phenotype and so these two pieces we can get externally we plug them back in and now this is a summary based estimate of the beta tus that only requires the z-scores the reference LD and then these weights which we have we sort of assume that we have a priority and then I won’t go into the details of how we derive the variance for this Statistics it’s very very similar and the final Association statistic that we get looks like this where again in the numerator you have you can think of this as a weighted sum of the gyc scores that’s weighted by the uh the predictors of expression and then in the denominator we have essentially the variance of that predicted expression that accounts for the correlation across these Snips so Snips that are correlated are going to add to the variance and Snips that are independent or not so this is basically the the score and I think this is also kind of a useful framework to think about how you can go from Individual level data to estimates of quantities we’re interested in with summary level data uh when we apply this technique to summary based data and individual based data it works really well correlation is nearly perfect and again we didn’t really make any assumptions going through it uh going through that previous derivation except for the fact that the LD is well matched to the Target population and also there’s sort of a hidden assumption that the effect sizes can’t be so enormous that we need to account for changes in the environmental variants and those assumptions are very easily satisfied in most studies so now thinking about when does this approach actually uh lead to associations we ran some simulations where we considered three different study designs under the model where there is a causal Gene and we’ve observed the predictors of that causal Gene so you could imagine in that scenario just running your standard G was to try to identify the association um you could imagine testing only the top snipped the top eqtl that’s associated with expression or you can imagine running a full tus test and when we do that we see that in this scenario because we’re testing fewer features we’re only testing each gene instead of each snip then the power of the tuos or the eqtl only approach is higher than the guas approach so this is one case where not only are we getting a parameter that we’re interested in on its own we also have some increase in power because the multiple testing burden is effectively is effectively lower furthermore if we expand the model and say additionally consider genes with multiple causal variants where now the to us approach of applying a penalized model to the entire Locus is giving us more signals more predictive accuracy than the top eqtl we see that the power of these single snip approaches drops but the power of the tiwas locus-wide approach remains effectively the same and so again this is another scenario where when we have many causal variants for expression that all lead to disease then we can substantially boost power uh and where the truth is is is is sort of in between or maybe a little bit off the page there’s going to be some loci where we don’t have the measured expression uh at all so these uh expression based approaches will just fail there’s going to be some loci where there’s only a single variant for the Gene and will be up here and there’s going to be some loci where there are many causal variants for the Gene and the tiwas will then maximize power relative to other approaches uh so this is all in simulations under very specific presumed models we can also ask how well does this approach perform in real data and this this has actually been quite a challenging question to answer because as it stands we have very few well-established causal genes for disease so I showed you that plot at the beginning that had over 200 known associations for prostate cancer but the number of well-established really definitively established causal genes for prostate cancer for that study is extremely small and that’s sort of the case for most complex traits so we don’t actually have kind of working in a regime where we don’t really have a ground truth there was a study that was done in this pre-print by weeks at all from The Phoenician lab which I thought was an interesting attempt to to try to get at a ground truth and the basic idea was that if we look at data in their case they looked at data from the UK biobank where you had associations both with common sort of standard gwas and also a rare variant coding variant based set of tests and you identify a Locus where there’s both common non-coding associations and rare coding variant associations you can assume maybe it’s not a safe assumption but they assume that the rare coding variant is telling you the right causal Gene and so under this model they basically have a kind of ground truth which is what is the rare coding variant tell you the causal Gene is and then they can ask how various other approaches do based on just the blue stuff just the common variant associations for identifying that causal Gene and so now they have a ground truth um they can plot Precision recall curves and they used this approach to evaluate a bunch of different methods listed here and then also to propose their method which is uh conceptually quite different I won’t go into it but it’s sort of like an ensemble that integrates many many different features at the locus to make the predictions but I think the the what’s relevant here is how these other approaches perform and what you can see is that there’s quite a lot of heterogeneity in their performance uh the tiwas is here in blue and as one point it has the highest uh recall in this model relative to the other approaches aside from their sort of Ensemble based approach and then additionally they integrated each of these method methods together with their model and in that scenario the tiwas had the highest Precision together with their approach but again I think an important takeaway here is that this is far from a solved model with a clear optimal method the TOs provides you an estimate of a certain statistical quantity but this is biology in biology is complicated and so lots of different approaches have different trade-offs for what they’re able to identify and at what levels of precision and recall and then you know one thing I should mention that maybe people are noticing is that if you take a very simple model of just what is the nearest gene or what’s the distance or how far away is the potential causal Gene that actually performs really well and in fact it performs about as well as the method that they that they develop developed and also when combined has very good Precision so again I think that there’s many explanations for this one is that in fact it may be that the nearest Gene oftentimes is the correct Gene it may also be the case that this specific model tends to emphasize genes that are close to the association statistics but again I think it’s also important to keep in mind that probably some hybrid of all of these methods to that also consider proximity is going to eventually be this sort of optimal solution okay uh so that’s kind of where we stand with um with with 2s applications um I you know coming back to this figure you can sort of wonder why the Precision uh of tiwas is relatively low compared to these other methods and I think again there’s an important set of caveats which were sort of highlighted in this paper from Mike Weinberg at all a couple years ago nature genetics which essentially come down to the fact that tiwas is an association study it’s not a causal inference technique and as an association study it’s going to be susceptible to tagging and correlation in the same way that genome-wide Association studies are and so in this paper they proposed a number of sort of alternative models which could still identify a significant T was hit one alternative model is that you can have co-regulate Mission at a Locus where the same genetic variant or set of genetic variants Drive the expression of multiple genes both the causal and non-causal genes and this is a real phenomenon that it’s not that uncommon that you will identify loci with multiple genes with very high cyst genetic correlation and high genetic correlation to the trait another case is you can imagine some part of the genetic effect on a non-causal gene is tagged because of LD between variants with the effect on the causal Gene and this would produce a false positive Association or it would sort of induce some effect on both the causal the non-causal Gene and the causal Gene and then likewise you can imagine a scenario where the effect on the causal Gene was missed in the gene expression study because it was not sufficiently powered or it didn’t get the right context and so this would lead to a false negative association and so again I think this is important to keep in mind that this is a test that is expected to tag the causal mechanism when these assumptions are met but in the real world these assumptions should also be sort of interrogated uh the other I think important limitation and one that’s potentially solvable uh is to consider is the fact that as we’ve seen with other genetic predictors the uh uh these expression predicted expression models do not uh generalize well to other um to different populations and really because most of the data has so far been collected in individuals of European ancestry this is particularly a problem for generalizing to data from non-european populations um or in admix populations with low European ancestry and so this paper showed models that were trained in European individuals that had high accuracy predicted into held out European industry individuals and had significant drops in accuracy when predicting into individuals of African ancestry and again I think that they’re potentially interesting ways to address this problem you know probably the the most basic is just to start collecting more data in other populations we should definitely be doing that but also there’s methodological approaches that could potentially leverage all of the training data that we have available or think about the differences between populations to improve the prediction of these of these models okay I’m gonna drink so I also wanted to talk a little bit about methods I think these are all methods that we did not develop and had no hand in but that I think are interesting approaches to moving beyond just that beta tus that I described for the association between expression and disease and I’ll sort of walk through them briefly you know to to give you guys a flavor of methodologically what else can be done in this space there’s a great method called utmost that came out a couple years ago in nature genetics which uh thought about how um gene expression data that’s measured in multiple tissues in the same individuals could potentially be used to improve these predictive models so everything that I’ve been talking about so far sort of assumed that there was a population with some single modality of gene expression but you could imagine and this is exactly what the how the gtex was designed that you’ve measured multiple tissues for every individual this Y is now a matrix instead of a vector for a given Gene and then the approach that utmost proposes is to actually try to learn the expression for each tissue together with all of the other tissues observed and so again you see some similarities here these bees the what they’re learning are sort of the W’s that I talked about earlier now are being learned for all tissues at once and they do that by using again a form of penalized regression where they have a penalty within each tissue where they want the weights to generally be sparse and then they also have a penalty across the tissues where they don’t want to see a lot of differences between tissues they sort of assume that if a snip is important for one tissue it should also be important for another tissue and this approach particularly for tissues that had relatively small sample sizes substantially increased the prediction accuracy basically by borrowing signal from other tissues that were available and that’s sort of shown here in in purple is the increase in prediction accuracy and held out data um another approach thinking about this sort of multi-tissue framework is instead of learning weights using multiple tissues we may be interested in testing multiple tissues where each set of Weights were learned individually and so there was this work a method called multi-skin by barbaridol in 2019 which essentially showed that if what we’re interested in is this relationship here between now we have many G’s for a single Gene we have the predictive model from one tissue a second tissue a third tissue and we want to know if there’s an association for any of these features in a joint model so a multi-degree of Freedom test for Association what we actually have again because we don’t have the individual level data we’ve actually observed is these marginal tiwas individual tus statistics um but if we know the correlation between these statistics then we can actually approximate the relationship or the effect under the three degree of Freedom or end degree of Freedom a test from these marginal effects the multi-scan paper also did some clever stuff where you have many tissues with highly correlated expression and you don’t want to just throw them all into this model by using principal components analysis to First reduce the dimensionality of the expression down then just test the leading components of expression for Association in this P degree of Freedom test and this also in practice showed that it produces a much larger number of significant Gene trait associations again now we’re sort of saying that if there’s a little bit of signal here and a little bit of signal here and here then that can add up to a lot of signal across the the three degrees of freedom um the other I think interesting method or any other interesting method in this space is uh is now thinking about how to integrate together um many tiwas associations across a given Locus and so you’ve probably seen methods for GEOS fine mapping that try to identify the set of causal variants or variants that contain the causal variant with some predefined probability the same kind of methodology or the same sort of concept can be applied to tiwa statistics and so instead this work of Mancuso at all in 2019 reformulated this problem in terms of having multiple tiwas associations at a single Locus and then fine mapping these down to the likely to the set of likely causal genes and this is actually sort of starting to address some of the caveats that I outlined earlier when you have co-regulation of multiple genes or you have some tagging across multiple genes this is now an approach to put probabilities on which genes out of many are likely to be causal whereas which are likely to just be tags and to sort of estimate posterior probabilities of causality for a given Gene so just in the last couple of minutes I wanted to mention a bit about what else can be done with this framework and so everything so far that I’ve been talking about has involved gene expression or transcription but really the idea is that any molecular trait that is heritable and that can be predicted from data that we’ve measured is amenable to this sort of approach and this way of integrating with gwas and in particular we can go back to this model which maybe we’ve we’ve solved now in some sense and and sort of we can observe that this is this is also an over simplification in that most of the time for non-coding variants what we expect is that there’s some regulatory element that sits in between the variant and the express Gene that maybe excuse me is the modifier or is the mediator of this gene expression so really there’s probably an enhancer or a transcription factor or a combination of those features um through which this snip has an effect on the expression of the gene which then goes on to have an effect on the trait and with sufficient data we can actually start modeling these regulatory elements and the genetic predictors of these regulatory elements and we have some recent work to that end which we call a regulome or a system-wide Association study so we’re sort of padding out the the the the letters of the alphabet here but the idea is that instead of learning predictors of a given Gene you can learn predictors of a some biochemical activity including transcription Factor binding chromatin state or chromatin accessibility and and additionally in this regime we can also leverage some allele specific information of variants that are inside these Peaks that we suspect to be modifying their activity and so we can boost power even further because because we can we can sort of Leverage signal within each individual in addition to Across the individuals and again we’ve shown that this approach is fairly robust that you can identify a very large number of predictive models and when we’ve applied this approach specifically to cancer gwas phenotypes so again this is cancer risk we’re just talking about the predisposition to develop cancer we see going back to this plot that I started with we’ve now characterized each of these loci where we see that there were this Inner Circle here is the number of lots that it had a significant tiwas association with a gene but then actually when we incorporate these epigenetic features we see a much larger number of loci that additionally have associations through chromatin accessibility in this case many of which are do not actually exhibit a direct transcriptomic Association and so this is actually sort of interesting and somewhat mysterious in that we’re able to identify loci where there seems to be a genetic regulatory effect that we don’t see have a downstream Cascade on expression we do capture most of the low side that have the the the the tiwas association those were able to characterize but then we have this number of additional loci and we’ve sort of started to think about what those loci could be telling us one observation is that if you look at uh um if you look at the sort of distribution of evolutionary constraint across the genome you will see that as in regions with higher evolutionary constraint we see fewer tiwas models that can be built probably because selection is making it more difficult to detect or is decreasing the observed effect on expression making it harder to pick up the eqtls but we actually see more of these arwas or chromatin was models observed in those loci so this is maybe this Gap could potentially explain that sliver in the previous figure of these are low side that are very difficult to pick up in the expression framework but are not as difficult to pick up when we look directly at the intermediate chromatin phenotype and a sort of related observation that we’ve made is if you look at genes in terms of their tissue specificity so as we move from here to here from the left to the right these genes are more specifically expressed in prostate tissue we’re looking at a prostate cancer GEOS again we see that the tus models there’s fewer of them they’re harder to fit for more tissue specific expression but for the chromatin based models and the transcription Factor based models there’s more of them and they’re easier to fit and so again this could be pointing towards a phenomenon where more tissue specific expression has lower power for the sort of eqtl and transcriptome-based models but higher power for these epigenetic based models and so with that I’ll conclude um I hope I’ve been able to uh convince you at the very least that gene expression is a complex heritable and predictable trait and this predictability is something that we can leverage to integrate that trait into other data sets or we don’t have it measured and specifically we derive this tiwas statistic which is a measure of the cisgenetic correlation between the gene expression and the disease as I noted at the end this is not just limited to transcription other molecular phenotypes can be used within the same framework and and again I want to sort of emphasize the caveats that go along with any kind of Association is that it’s not a a causal inference and in fact causal inference in this space is I think a really interesting and sort of ongoing open problem how do we disentangle all of those different arrows that I was showing earlier and then also just to remind you that all the prediction here has been within the cislocus of the gene that’s where we have power at the current sample size but there’s a whole world of trans effects which we haven’t uh really we sort of just barely scratched the surface in understanding and so that’s something that I think as studies get larger and as we have more experimental data we’ll also be able to fold into this framework and so with that I’ll take your questions thanks thank you"
  }
]