---
title: "Software Tutorials: Genomic SEM (Video Transcript)"
---

**Title**: Genomic SEM Tutorial

**Presenter(s)**: Andrew Grotzinger

I'm Andrew Grotzinger and in this video we're going over how to run genomic SEM using psychiatric traits as examples genomics stem is a general framework for modeling genetic covariance matrices produced by methods like LD score regression to then estimate any number of structural equation models that can be used to test hypotheses about the processes that gave rise to the data that we observe it only requires you want summary statistics and those summary statistics can come from samples with unknown and varying degrees of sample overlap what that means is that you can now estimate models for really rare traits that you would not otherwise observe in the same sample it's going to be split into two parts for the Practical

the first is how to estimate a user-specified model using genome-wide estimates the second part is how to incorporate the effects of individual Snips to do things like estimated multivariate g-was all the Practical materials are available at this box link here and this includes an R script and all the files needed to run that on screen the top of that our script is going to include some code to actually download genomics Sam if you haven't done so already throughout the presentation we're going to be using gy summary statistics for schizophrenia bipolar major depressive disorder and I'm moving on to how to actually estimate user specified model this takes three primary steps the first is munging the summary statistics the

second is running Elder Scroll regression and the third is using that output from LD score regression to then run the model that you specify munge is a general procedure to format the summary stats in the way the LD score regression is expecting for the sake of this practical we're using a subset of 10 000 steps for schizophrenia bipolar major depression in general you can download and use a full set of summary stats on a personal laptop for the purpose of the Practical and restricting file size to produce using a subset here it takes four arguments the first is the name of the files the second is a reference file that you to a lot to the same reference allele across trades third is the name of the traits and the force is the total sample size and putting it all together in this last line here where I'm specifying all these arguments and running munch

again to really highlight that we are only using the restricted subset for the purpose of the Practical when munge runs it's going to produce a DOT log file that you should inspect to make sure things like column headers are interpreted correctly and in particular I've highlighted this section here where it prints how the effect column was interpreted for schizophrenia it's an odds ratio and it's interpreted correctly but you'll want to make sure to read the corresponding readme files for the summary statistics and then look at this log file to cross-reference whether or not that effect column is being interpreted right in this second

step we run LD score regression within the context of genomics M the first argument is traits and that's the name of these now munge summary stats which will all enter that.sumstats.gc ending then for binary traits we do the liability threshold correction that requires inputting the sample prevalence which reflects the cases over the total sample size and then the population prevalence which you can get from either epidemiological papers or from looking at the paper from the corresponding unit variate G was the fourth and fifth argument are the holder of LD scores and the LD score weights in almost all cases this is going to be the same folder and here we're using the European LD scores because we're using the European only summary stats and then finally we specify the names of the traits and this is how the traits are going to be named in your actual model and on this last line of code we are at ldscore aggression in The Next Step we're actually going to specify the model but before we do that I want to switch over to R and actually run through the codes you can see it first

I'm going to load in this package and I'm going to set the working directory to where I downloaded those Workshop materials and then for munge I'm going to set those files the Hat map 3 list that reference file the trade names the sample size and then run Munch and the Second Step I'm going to take those month summary stats and put the sample on population prevalence for the liability correction the folder of LD scores and all the score weights which are the same and then the trade names and finally actually run reality score aggression this is going to produce results that are actually interpretable because we use that subset of 10 000 Snips so when we now go on to step three of running the model I've created an LD score regression object that uses the full set of summary statistics that we're now going to load in so you can actually produce integral results in the context of the model we're gonna load that in and now switch back over to the PowerPoint to talk about how you specify a model in genomic stem we use the little Bond Syntax for running the model and in this case we would specify a regression relationship of a using a tilde b or for those of you that think of it in the format of Y till DX or outcome till the predictor for covariances you would specify two and of course for a variance of a variable it would be the covariance with itself so a totally tilde a for a factor you specify the factored name here followed by equal sign utility and then the factor indicators to fix a parameter you would put a number followed by an asterisk on the right hand side of the parameter

that you're estimating so this would fix the covariance between A and B to 1 and to name a parameter you would write the parameter label using some set of letters here I'm naming the covariance between A and B Cove a b and what this does is it allows you to use models transfer this parameter let's say the covariance is estimating as negative but you have some really sense that it should be positive so you put them as parameter constraint to keep it above zero we loaded in that pre-made ldsc data which again is using the full set of summary statistics this is not simulated data because we're using summary stats this is often something you can readily download online than to actually run the model this takes two necessary arguments and two optional arguments the first is Cove struck which is that output from LD score regression the second is the model so we're running a common factor model here and we're telling Lavon using this n a star that we want to freely estimate the first loading and then we want to fix the variance of the factor to one so we're using what's known as unit variance identification for this model an optional third argument is what estimation method you want to use we offer DWS at maximum likelihood but the default is DWS and then another optional argument is std.lb and that's whether or not you want to automatically specify the variances of variable sort of one and you would run the model so this is going to produce this set of results and

I'll show that in R here in a second but to walk you through what those results mean this first three columns or the parameters being estimated the fourth and fifth column here the unstandardized estimate and standard error for those parameters so it's the senator applied to the genetic covariance Matrix and the standardized estimate in standard error is the estimate standard error for the model applied to the genetic correlation Matrix where the heritabilities are scaled to one the model fit here is all going to print as an a because when you're estimating a common factor divided by three indicators uses it up all of your degrees of freedom so it perfectly fits the model but I want to walk through how you would interpret these model fit statistics more generally so chi-square is the model chi-square that reflects the exact index of fit I'm in the degrees of freedom and p-value for the model chi-square in the next two columns note that this will almost always be highly significant for your model because chi-square is sensitive to sample size which by definition is massive in G wasp space the next piece is AIC which can be used to compare models regardless of whether they are nested with lower values indicating better fit CFI is the comparative fit index which has these General heuristic cutoffs with higher being better and then finally SMR is the standardized rootme Square residual or lower is better going back over to R we're going to specify that argument specify the model the estimation method and std.lb we're going to run the model and you'll see that we produce the same results that we produce over here not in the slides but in the code I've included some alternative ways that you could specify this model so for example if you wrote scd.lb equals true you don't have to write that n a star F1 star F1 or you could use what's the common factor function to estimate the same model and produce the same results moving on to part two I want to talk about how you would ride multivariate gwas and genomics n and to be clear you don't have to do both of these parts you can certainly run a genome-wide model in part one and publish that alone you don't have to bring in these individual simple facts but I did want to show that because oftentimes for people the multivariate G wasp space is what's of

most interesting this includes four primary steps the first two of which mirror what we already did which is to write a monginelli score regression and you don't need to redo that we're only going to go over the last two steps of running the sunsets function and the multivariate gwas functions common factor g w acid user G was some stats takes a number of different arguments it can be a little bit confusing and I'll note that we have a flow chart on our GitHub to help you figure out how the argument should be specified again we're using the drastically subset Snips for the purpose of the presentation and we're also using a subset of reference file which is used to align the alleles and to pull out the snip minor allele frequency the third argument is the trait names and then the fourth argument here we're letting the subsets function know the standard errors for these traits are on the logistics scale and you'll want to be really careful with this because oftentimes for binary traits they might be effect column on an odds ratio scale but then the standard error will be on a logistic scale and that's something that you can determine oftentimes from the readme file if you have continuous traits or binary traits analyzed using a continuous model it's going to be a different set of arguments for the sake of time I'm not going to go over all of that here but we do have that flowchart on the GitHub and of course you can reach out to us if you're confused or getting strange results so putting that all together you'd run the subsets function here and much like the Munch function this will produce a log file that you want to make sure you go over to make sure everything's interpreted correctly and then once that's done you can use that output in combination with the LD score regression output to run models that include the effects of individual Snips I'm going to go over both the common factor G was at user gwas functions starting with the common factor G bus function this takes two necessary arguments the first is the output from LD score regression and the second is the output from the substance function and then finally once again you can specify dwbls or ml for the estimation method and then you can also specify whether you want to run in parallel so in practice if you're running a g-was bat you cannot run on a laptop unlike the models that we were showing in part one so for this you really will want to be running in parallel on a Computing cluster but for the purposes of this because we're using a small subset of the Snips we're going to run not in parallel so you get a sense of how these functions work I'm going to switch now over to R to go through that substance function where again we read the files the reference file the trait names whether the standard errors on a logistics scale we're writing t for true for all three running some stats and now taking that output and specifying the LD score regression output the output from some sets that we created and actually running the common factor gwas function

so this is going to produce output that looks like this there's a lot of columns it's got the sent chromosome base pair monolithal frequency A1 and A2 the parameter being estimated which is the effect of the snip on the factor and the estimate sandwich corrected Standard air Z estimate p-value and then this other metric that we call qsnip which indexes whether or not that snip really does not fit the model so this is the chi-square distributed test assist with degrees of freedom they're going to depend on the number of indicators in your model so that indexes the extent to which a common factor model is insufficient for accounting for the pathways from an independent Pathways model so if this common pathway of the snip on the factor is really a poor representation of the independent effects of the Snips on these individual indicators then Q is going to be significant so that's going to happen when you have things like a simp that has directionally opposing effects on the indicators or let's say the Sim has a really strong effect on one of the indicators but not the other if we go now over to these results you'll see the estimates here one thing you want to inspect is these last two columns of fail and warning zero means it's good to go but if you look at the warning messages you'll see that a handful of the variances are estimated as negative

I want to use this as an opportunity to talk about how you might troubleshoot a warning like this and then also go over how you would use user G was in this context for user gwas it takes those same first two arguments of the output from multi-score regression and the subsets output but then you're also going to specify the actual model that you want to run that's now going to include the effect of this individual snip so we're running that same model of the common factor model the common factor G was is automatically specifying behind the scenes where the stand predicts that common factor but then we're additionally adding in these model constraints we're renaming these parameter labels at the residual variances your schizophrenia bipolar and MDD and we're constraining them to be above zero because of that warning that we got when running common factor G bus

if we now go over and run that I want to mentioned that another optional argument for user G wasp is this sub-argument this is whether or not you want to save a particular piece of the model output the common factor guas is automatically only saving the effect of the sniff on the common factor what the sub argument does is it allows you to tell user gwas that look for each of these Snips I don't want to save all of the model output including the factor loadings and the residual variances are really for the sake of memory I want you to save the effect of this snip on the common factor so sub does not change at all how the model is estimated it changes how large the output file is and I would highly recommend setting this argument so now we're going to run this and while that's running I'd want to make some final notes that parallel processing are available for both user gwas and common factor g-was parallels executing the exact same code serial processing except that it takes advantage of additional cores an ideal runtime scenario you would split your jobs across Computing nodes on a cluster and run in parallel there is also MPI functionality available so again all runs are completely independent of one another I've listed a number of resources here including the GitHub and I'm not going to go through this but I've included some slides about some things to keep in mind so the final

thing I'd want to go back over here and show you that this produces the same set of results common factor Gus are very nearly so now that we've added those residual variance constraints but now if we look at the warnings we see that those warnings are now gone so with that I'll add it you can reach out to us with questions and I hope this was helpful to you
