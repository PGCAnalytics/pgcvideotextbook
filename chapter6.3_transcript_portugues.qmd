---
title: "Capítulo 6.3: Métodos de escore poligênico"
---

# ***Métodos de escore poligênico: comparação e implementação*** {#sec-section1}

**Título:** Métodos de escore poligênico: comparação e implementação

**Apresentador(es):** Oliver Pain (Social, Genetic, and Developmental Psychiatry Centre, King’s College London)

Olá, meu nome é Oliver Pain e sou pesquisador de pós-doutorado e trabalho com a professora Cathryn Lewis no King's College London. Hoje falarei sobre métodos de escore poligênico, comparando vários métodos entre si e também descrevendo recursos que desenvolvemos para o cálculo de escores poligênicos para fins clínicos e de pesquisa. Não tenho conflitos de interesse a declarar. Minha apresentação está dividida em três seções. Primeiro, darei uma breve introdução ao escore poligênica, depois descreverei nosso trabalho comparando diferentes métodos de escore poligênico e, finalmente, apresentarei um pipeline fácil e confiável para escore poligênico que desenvolvemos, chamado GenoPredPipe.

Então, primeiro, uma breve introdução ao escore poligênico. Um escore poligênico é uma forma de resumir o risco genético ou a propensão de um indivíduo para um determinado resultado, normalmente calculado com base na variação genética de um indivíduo e nas estatísticas resumidas de estudos de associação de todo o genoma (GWAS), conhecidas como sumstats do GWAS. Os escores poligênico são uma ferramenta de pesquisa útil e também podem ser úteis para a medicina personalizada, à medida que sua utilidade preditiva aumenta.

Para escore poligênico, as estatísticas resumidas do GWAS são normalmente processadas para identificar variantes que se sobrepõem à amostra alvo, levar em conta o desequilíbrio de ligação (LD) entre as variantes e ajustar os tamanhos dos efeitos do GWAS para otimizar a relação sinal-ruído nas estatísticas resumidas do GWAS. Portanto, o escore poligênico de um indivíduo pode variar devido às variantes genéticas consideradas e à frequência alélica e às estimativas de desequilíbrio de ligação usadas para ajustar os sumstats do GWAS. Frequentemente, na pesquisa, esses fatores variam dependendo da amostra-alvo, usando a variância intersectiva entre a amostra-alvo e o GWAS e usando estimativas de desequilíbrio de ligação e frequência alélica da própria amostra-alvo. Agora, isso não é ideal, e uma estratégia alternativa é usar uma abordagem padronizada de referência, em que um conjunto comum de variantes é considerado para todas as amostras-alvo, e as estimativas de ligação e frequência alélica são derivadas usando uma amostra de referência comum correspondente à ancestralidade. Esta abordagem padronizada de referência é vantajosa ao usar escores poligênico em contextos clínicos e de pesquisa, pois permite que escores poligênico sejam calculadas para um único indivíduo e evita variação no escore poligênico de um indivíduo devido a propriedades específicas da amostra-alvo, que podem influenciar o resultado.

Agora apresentarei nosso trabalho comparando métodos de escore poligênico. Esta tabela mostra os principais métodos de escore poligênico conforme relatado na literatura.

A abordagem pT+clump é a abordagem tradicional, em que a aglomeração baseada em LD é usada para remover o desequilíbrio de ligação entre as variantes principais e o GWAS, e uma faixa de limites de valor p é usada para selecionar as variantes consideradas. Os outros são métodos desenvolvidos mais recentemente que utilizam estimativas de LD para modelar conjuntamente o efeito de variantes genéticas e normalmente realizam a redução para levar em conta as diferentes arquiteturas genéticas.

Assim como a abordagem de limiar de valor p, vários dos métodos mais recentes aplicam vários parâmetros de redução para otimizar o escore poligênico. Quando vários parâmetros são usados, um procedimento de validação, como validação cruzada de 10 vezes, é necessário para evitar overfitting, em que a estimativa explicada pela variância é artificialmente alta devido à tentativa de muitos limites de valor p ou parâmetros de redução diferentes.

Em contraste, alguns métodos fornecem uma abordagem de pseudo-validação, em que o parâmetro ideal é estimado apenas com base nas estatísticas resumidas do GWAS, não exigindo uma amostra de validação. Outra abordagem que não requer uma amostra de validação é assumir uma arquitetura genética infinitesimal. Porém, esta abordagem funciona melhor para fenótipos altamente poligênicos.

Uma terceira opção é modelar múltiplos escores poligênicos com base em uma série de parâmetros, usando métodos como a rede elástica para explicar a correlação entre os escores poligênicos.

Comparamos métodos usando uma série de resultados medidos em duas amostras-alvo: UK Biobank e Twins Early Development Study, conhecido como TEDS. Usamos duas amostras para garantir que nossos resultados não sejam específicos da amostra-alvo e selecionamos as características à direita, pois elas possuem GWAS disponíveis publicamente e representam uma variedade de arquiteturas genéticas.

Como descrevi anteriormente, utilizamos a abordagem padronizada de referência ao calcular os escores poligênicos. As frequências de LD e alelos foram estimadas usando duas amostras de referência de diferentes tamanhos de amostra para avaliar a importância do tamanho de amostra de referência entre os métodos. E essas amostras de referência foram os subconjuntos europeus do 1000 Genomes Fase 3 e um subconjunto aleatório independente de 10.000 participantes europeus do UK Biobank. Variantes do HapMap 3 foram usadas, pois essas variantes são normalmente bem capturadas por arrays de genotipagem, após imputação, fornecem boa cobertura do genoma e também reduzem o tempo de computação.

Vários métodos já fornecem matrizes LD, incluindo apenas variantes do HapMap 3, para uso com seu software. Em linha com a abordagem padronizada de referência, a utilidade preditiva de cada abordagem de escore poligênico foi avaliada com base na correlação de Pearson entre o resultado observado e os valores previstos. A significância estatística das diferenças entre as correlações previstas e observadas foi determinada pelo teste de Williams, que contabiliza a correlação entre as previsões de cada método.

Esta figura mostra o desempenho médio de cada método em comparação com o melhor escore poligênico pT+clump, conforme identificado usando validação cruzada de 10 vezes. Os pontos transparentes na figura mostram os resultados para cada fenótipo alvo separadamente. Estou apenas mostrando os resultados com base na amostra alvo do UK Biobank ao usar a referência 1000 Genomes, pois os resultados foram altamente concordantes ao usar o TEDS ou a referência maior. Ao comparar métodos que você usa validação cruzada de 10 vezes para otimizar parâmetros (mostrados em vermelho), você pode ver que os melhores métodos são LDpred2, lassosum e PRScs. Todos superam a abordagem pT+clump, proporcionando uma melhoria relativa de 16 a 18% na predição.

O LDpred2 mostrou outras melhorias nominalmente significativas em relação ao lassosum e ao PRScs, em média.

Ao comparar métodos que usam uma abordagem de pseudovalidação ou um modelo infinitesimal (que não requer uma amostra de validação), destacado em azul e roxo, você pode ver que os métodos PRScs e DBSLMM apresentam bom desempenho, fornecendo pelo menos uma melhoria relativa de 5% em relação aos outros métodos de pseudo-validação. PRScs é melhor que DBSLMM, proporcionando uma melhoria relativa de 4% em relação a DBSLMM. Vale ressaltar que o desempenho do SBayesR melhorou em média ao usar uma referência LD maior do que ao usar o DBSLMM.

Ao comparar o desempenho do PRScs pseudovalidado com os resultados da validação cruzada de 10 vezes em vermelho, o escore poligênico do PRScs tem um desempenho apenas 3% pior do que o melhor escore poligênico identificado pela validação cruzada de 10 vezes para qualquer outro método, e tem um desempenho melhor do que a abordagem pT+clump para todos os fenótipos testados, destacando a confiabilidade desta abordagem.

A abordagem multi-PRS, mostrada em verde, que usa um modelo de rede elástica para modelar múltiplos escores poligênicos com base em uma gama de limites de valor p ou parâmetros de redução, supera consistentemente o melhor escore poligênico selecionada por validação cruzada de 10 vezes, mostrado em vermelho, com a maior melhoria para a abordagem pT+clump, de 13%.

Por último, testamos se o ajuste de escores poligênicos em vários métodos melhorou a predição, e descobrimos que isso aconteceu em um pequeno grau, embora a carga computacional seja obviamente substancial porque então você terá que executar todos os métodos. Em termos de tempo de execução, esses métodos variam substancialmente. Este gráfico mostra o número de minutos para executar o método apenas no cromossomo 22, sem qualquer computação paralela. Você pode ver que o PRScs e o LDpred2 demoram muito mais do que outros métodos. No entanto, desde o nosso estudo, os desenvolvedores do LDpred2 melhoraram substancialmente a eficiência do seu software, reduzindo pela metade o tempo de execução, diminuindo para pouco menos de 25 minutos para o cromossomo 22. É importante notar que o tempo necessário para PRScs ao usar um único parâmetro de encolhimento é um quinto do tempo mostrado aqui, o que significa que sua abordagem de pseudovalidação é razoavelmente rápida. O DBSLMM é de longe o mais rápido dos métodos mais recentes, demorando apenas alguns minutos quando executado em todo o genoma sem qualquer computação paralela, o que é impressionante considerando o seu desempenho em relação a outros métodos em termos de previsão.

Algo que gostaria de destacar é que ao utilizar o SBayesR, tenho utilizado a opção robusta de parametrização disponível na versão mais recente do SBayesR. Como descobri que o SBayesR não estava convergindo corretamente para alguns GWAS, usar essa parametrização robusta foi o mais confiável e não piorou o desempenho do SBayesR, exceto para depressão, usando a referência europeia menor do 1000 Genomes, você pode ver isso aqui no figura.

O segundo ponto é que, desde nosso estudo publicado, também comparei métodos PRScs usando estatísticas resumidas do GWAS para 17 fenótipos diferentes derivados usando apenas a amostra do UK Biobank, evitando possíveis problemas de controle de qualidade que ocorrem em meta-análises em larga escala. Os resultados são altamente concordantes com os resultados quando se utilizam os maiores resultados do meta-GWAS disponíveis publicamente, o que garante que nossas descobertas são confiáveis. No entanto, o desempenho do SBayesR melhorou novamente, destacando que o SBayesR é especialmente sensível à qualidade do GWAS do que outros métodos. Mas quando a qualidade é boa, o SBayesR tem um desempenho muito bom e parece ser o melhor método nesta análise.

Também estávamos interessados ​​em ver se um método específico tinha melhor desempenho na predição entre diferente ancestralidades, já que alguns métodos podem destacar melhor os efeitos genéticos causais do que outros. Usando os mesmos 17 GWAS que mencionei brevemente no slide anterior apenas na amostra do UK Biobank, testei o desempenho dos métodos de PRS entre populações. Portanto, utilizando o GWAS europeu, mas predizendo em subconjuntos não europeus do UK Biobank. Como pode ver, os resultados são muito semelhantes em cada população, sendo o melhor método identificado numa amostra-alvo europeia também o melhor método na amostra-alvo não europeia.

Portanto, o conselho para pesquisas futuras sobre métodos de escore poligênico é: para uma predição ideal, recomendamos modelar vários parâmetros de LDpred2, lassosum ou PRScs. Mas se você estiver procurando uma opção mais simples, talvez para observar a sobreposição genética, eu recomendaria usar o método de pseudovalidação PRScs, também conhecido como método “totalmente bayesiano”. Alternativamente, se você precisar calcular escores poligênicos para muitos GWAS ou tiver tempo ou capacidade de computador limitados, o DBSLMM é uma alternativa rápida e boa. Embora o SBayesR tenha um bom desempenho quando a qualidade das estatísticas resumidas do GWAS é boa, sua sensibilidade a essa qualidade significa que ele nem sempre tem um bom desempenho ao usar os maiores resultados do meta-GWAS.

Ok, agora vou passar para a última seção da minha palestra, que descreve nosso pipeline recentemente desenvolvido para escore poligênico, chamado GenoPredPipe. Portanto, a maior parte do trabalho que acabei de apresentar está contida no estudo mostrado no canto superior esquerdo, onde também descrevemos o escore poligênico da abordagem padronizada de referência. No estudo, fornecemos uma representação esquemática desta abordagem padronizada de referência mostrada na figura à direita. Assim, os dados genéticos alvo são primeiro imputados, se ainda não o foram; então, apenas as variantes do HapMap 3 são retidas, pois normalmente são bem imputadas e fornecem uma cobertura decente do genoma. E então, a amostra é dividida em superpopulações ancestrais determinadas por comparação com a referência da fase três do 1000 Genomes. E, por último, os escores poligênicos são calculados com base nas estatísticas resumidas do GWAS processadas antecipadamente usando uma referência de correspondência de ancestralidade do GWAS.

Agora, quando estávamos realizando este estudo e escrevendo o código para a abordagem padronizada de referência, queríamos garantir que os resultados fossem totalmente reproduzíveis e queríamos desenvolver um recurso que outros pesquisadores pudessem usar para calcular escores poligênicos padronizados de referência. Portanto, usamos uma combinação de R Markdown, Git, GitHub e GitHub Pages para criar um site disponível publicamente contendo uma série de documentos legíveis que descrevem o código e os resultados de nossos vários estudos.

O site se chama GenoPred porque se concentra na previsão baseada em genótipos. E há um QR code aqui se você quiser dar uma olhada. E agora vou mostrar brevemente o site. \[https://opain.github.io/GenoPred/\]. Então, esta é a página inicial do site, que mostra uma lista das páginas disponíveis. Clicarei agora no link que descreve como preparar os dados de referência para a abordagem padronizada de referência. Na parte superior, ele fornece algumas informações e lista o software necessário abaixo delas. E então, ele passa por cada etapa, uma por uma, com pedaços de código para os usuários usarem para reproduzir resultados ou criar seus próprios dados.

Agora, embora pensemos que este é um ótimo recurso para outros verem o que fizemos e replicarem os resultados, você pode ver que há muitas etapas e muitos pedaços de código separados a serem seguidos, tornando seu uso bastante demorado e possivelmente sujeito a erros do usuário. Então, recentemente escrevi todas as etapas para calcular escores poligênicos padronizados de referência em um pipeline usando Snakemake, junto com o que é chamado de ambiente Conda, que baixará e instalará automaticamente todo o software necessário, o que significa que criará totalmente resultados reproduzíveis. O pipeline requer apenas três arquivos de entrada. Primeiro, uma lista de estatísticas resumidas do GWAS que você deseja usar, indicando a população na qual foram derivadas e, opcionalmente, informações sobre a distribuição do fenótipo na população geral (ou seja, prevalência ou média e desvio padrão). Em seguida, você fornece uma lista de amostras alvo para as quais deseja calcular escores poligênicos, com o pipeline atualmente aceitando binários PLINK1 (por exemplo, BED, BIM, FAM) ou arquivos BGEN e também o formato 23andMe. Por último, você precisará de um arquivo chamado config.yaml, que descreve a localização do GWAS e arquivos de lista de destino. Então, executar todo o pipeline requer apenas uma única linha de código. Eu forneci alguns dados de teste para novos usuários experimentarem, que podem ser baixados. E a etapa dois, aqui, é onde o pipeline é realmente executado. Basta escrever Snakemake, em seguida, dois parâmetros indicando os recursos computacionais que você deseja usar e que deseja usar o Conda ambiente e, em seguida, o nome da saída desejada. Neste exemplo, estou solicitando o resultado final, que é um relatório em RMarkdown da identificação de ancestralidade e resultados de escore poligênico para todas as amostras alvo que envolve a execução de todo o pipeline.

Aqui está uma versão atualizada do esquema que mostrei anteriormente, com base no que está implementado no pipeline GenoPred. A única diferença é que ao final agora geramos um relatório resumindo os resultados da identificação de ancestralidade e escore poligênico. Vou mostrar esses relatórios brevemente agora.

Este é o relatório produzido para descrever os resultados de um único indivíduo. Primeiro, existem algumas descrições sobre o número de SNPs \[polimorfismos de nucleotídeo único\] antes e depois da imputação e o número de SNPs de referência disponíveis. Em seguida, o relatório descreve os resultados da análise de identificação de ancestralidade, mostrando primeiro a probabilidade de ser de cada subpopulação na referência 1000 Genomes. Neste exemplo, o indivíduo tem quase 100% de probabilidade de pertencer a uma população europeia. Você também pode ver a posição do indivíduo nos primeiros componentes principais projetados por referência em relação à população do 1000 Genomes que estou mostrando agora, e o indivíduo-alvo neste caso é aquele círculo preto.

Então, a ancestralidade do indivíduo é dividida em populações mais específicas dentro da superpopulação designada. Neste caso, mostrar que o indivíduo era mais semelhante à população da Grã-Bretanha e aos indivíduos em geral do Norte e da Europa Ocidental. E na parte inferior, mostra os escores poligênicos do indivíduo, neste caso para índice de massa corporal e doenças cardiovasculares, mostrando os resultados em termos relativos em comparação com uma referência de ancestralidade correspondente. E então, em termos absolutos para melhor interpretação, considerando a variância explicada pelos escores poligênicos e a distribuição do fenótipo na população geral.

Então, este é um exemplo de relatório produzido para resumir os resultados para uma amostra de indivíduos. Novamente, começando com algumas descrições sobre a sobreposição com os SNPs de referência, mostrando então o número de indivíduos atribuídos a cada superpopulação usando um limite máximo de 50%, com as probabilidades subjacentes mostradas como um histograma abaixo. Novamente, você pode ver a posição desses indivíduos nos componentes principais projetados em comparação com a referência do 1000 Genomes. E então, cada indivíduo é atribuído a populações específicas dentro de cada superpopulação atribuída. Porém, vou pular isso e mostrar os escores poligênicos, que estão resumidas na parte inferior, simplesmente usando histogramas para mostrar a distribuição dos escores poligênicos para cada população e GWAS.

Então, concluirei explicando por que acho que as pessoas deveriam usar esse pipeline. Em primeiro lugar, realiza a classificação de ancestralidade, o que é realmente importante, uma vez que os não-europeus são frequentemente excluídos dos estudos e, à medida que o tamanho das amostras aumenta, as populações não-europeias utilizáveis ​​podem ser identificadas e analisadas. Além disso, é importante considerar a ancestralidade de um indivíduo ao calcular o escore poligênico. Em segundo lugar, fornece escores poligênicos padronizados por referência, que são dimensionados de acordo com uma referência correspondente à ancestralidade e são independentes de quaisquer propriedades específicas da amostra-alvo, o que é útil para fins de pesquisa e previsão clínica. Terceiro, ele pode implementar com eficiência qualquer um dos métodos de escore poligênico de melhor desempenho usando uma única linha de código, economizando tempo e reduzindo erros do usuário. Quarto, foi experimentado e testado. Analisei o UK Biobank e outras amostras e comparei o PRS com os fenótipos observados, garantindo-me que está funcionando como deveria. Finalmente, está bem documentado online e produz resultados totalmente reproduzíveis, sendo que ambos são importantes para o progresso da ciência. Estou mostrando novamente o QR code do site GenoPred à direita. Por favor, verifique se você estiver interessado.

Por fim, gostaria de agradecer aos meus incríveis colegas pela ajuda neste trabalho, em particular a Cathryn, pela brilhante supervisão durante todo o processo. Obrigado por ouvir.

------------------------------------------------------------------------

# ***Escores de risco poligênico: comparação de PGS*** {#sec-section2}

**Título:** Escores de risco poligênico: comparação de PGS

**Apresentador(es):** Guiyan Ni (Institute for Molecular Bioscience, University of Queensland)

Bem-vindo! O tópico deste vídeo é como executar uma comparação de escore poligênico de risco. Até agora, já assistimos palestras individuais sobre PRScs, LDpred2, SBayesR e outros métodos. Acredito que todos vocês tenham um bom conhecimento do escore poligênico de risco e de cada um desses métodos.

Então, esses slides aqui são apenas para preparar o cenário, para ter certeza de que estamos na mesma página. Um escore poligênico de risco de um indivíduo é uma soma ponderada das contagens de alelos de risco. Com base nos resultados estatísticos resumidos do GWAS, o método básico para o escore poligênico de risco é a aglomeração e limiar do valor p. Este método é simples, mas não modela completamente diferentes arquiteturas genéticas. Portanto, existem muitos métodos novos que tentam modelar a arquitetura genética para a característica de interesse. Por exemplo, usando diferentes parâmetros na regressão Bayesiana, como modelo LDpred-infinitesimal, LDpred2, SBayesC, PRScs e SBayesR. E também, métodos como o lassosum usam a regressão LASSO. MegaPRS é outro método executado em diferentes priors. Por exemplo, se for executado a priori usando uma mistura de quatro distribuições normais, será igual a SBayesR ou similar. E se um SNP tiver uma contribuição para a variância do fenótipo, então será semelhante ao modelo infinitesimal LDpred ou \*SBLUP. Ele também pode executar um prior como o BOLT-LMM e também executar a regressão lassosum. Portanto, a diferença entre o MegaPRS e outros métodos é que a herdabilidade esperada por SNP \[polimorfismo de nucleotídeo único\] pode variar de acordo com LD e frequência alélica menor. Então, nesta palestra, compararemos todos esses métodos. E sabemos que quando um método é proposto, os criadores do método já o comparam com outros métodos. Mas a questão fundamental que estamos tentando responder aqui é: Qual método devemos usar nos dados do PGC?

Então usamos a validação cruzada. Queremos um grupo de validação cruzada para responder a esta pergunta e comparar o desempenho de diferentes métodos. Aqui está o exemplo de teste mostrando, para a validação cruzada, cada célula aqui é uma coorte, e a célula rosa é para a coorte de descoberta, e a célula verde é para a coorte alvo.

Na primeira rodada da análise, usamos as quatro coortes de descoberta rosa como um conjunto de descobertas e, em seguida, avaliamos o desempenho de cada método na amostra alvo. E então repetimos esse processo em cada uma dessas células, cada uma dessas coortes serve como coorte alvo. Se for necessário pelo método, temos outra coorte que servirá como amostra de ajuste para selecionar os hiperparâmetros ideais.

Assim, na análise de dados reais, usamos as estatísticas resumidas do GWAS da Esquizofrenia 2 \[PGC Schizophrenia GWAS data freeze 2\] como uma amostra de descoberta, e todos os dados aos quais realmente temos acesso através de um serviço, através da coorte, onde o genótipo do indivíduo está disponível. Usamos cada um deles como amostra-alvo. Para a coorte de ajuste, usamos essas quatro coortes para ajustar os hiperparâmetros. E então, podemos prever o escore poligênico em cada uma das amostras alvo.

Aqui, usamos algumas estatísticas para medir o desempenho de diferentes métodos. Uma é a AUC \[área sob a curva ROC\], outra é a proporção explicada na escala de responsabilidade e a terceira é a razão de chances. Analisarei cada um deles para mostrar como calcular cada uma dessas estatísticas.

Então, vamos primeiro começar com AUC. Aqui está um exemplo de como calcular a AUC manualmente. Portanto, AUC é na verdade a abreviação de área sob a curva ROC \[característica operacional do receptor\], que está sombreada em rosa aqui. A curva ROC é feita traçando a taxa de verdadeiros positivos em relação à taxa de falsos positivos em cada ponto de corte possível. Então, o que isso significa? Isso significa assumir que este é o gráfico de densidade para o escore poligênico na amostra de controle, e aqui é para as amostras de caso. Esta linha vertical é o corte atual. Neste caso, este gráfico pode ser dividido em quatro grupos: verdadeiro negativo, falso negativo, falso positivo e verdadeiro positivo. E então podemos calcular a proporção de cada grupo, e então podemos calcular as taxas de verdadeiros positivos e as taxas de falsos positivos, que são as coordenadas usadas na curva ROC. Portanto, no corte atual que usamos aqui, significa que temos cerca de 17% de casos que são corretamente classificados como casos, e depois há cerca de 10% de controles que são erroneamente classificados como casos, o que nos dá as coordenadas para este ponto aqui. E então, à medida que variamos essa linha vertical (esse corte), vamos obter essa curva ROC, como mostra esse slide aqui. E esta, você vê, é a primeira estatística que usamos para medir o desempenho de diferentes métodos.

E a segunda é a variância explicada na escala de responsabilidade quando se utilizam estudos de caso-controle verificados. Então, essa variância é função da variância explicada na escala observada, desse R2 observado em um estudo caso-controle, e de outros dois parâmetros, C e teta (θ). A variância explicada na escala observada é na verdade uma função de duas verossimilhanças, do modelo nulo e do modelo completo, que é desenhado nessas duas equações.

E esse parâmetro C é uma função de K, z e P. Esse parâmetro K é na verdade a proporção da população que está doente, isso também significa a prevalência da doença. O parâmetro z é a densidade neste limite t aqui, e esta curva é uma distribuição normal padrão. E o P é a proporção de casos nos resultados do GWAS ou no seu estudo de caso-controle. E o parâmetro θ é uma função do mesmo K, z, t e limite t, mas com uma combinação diferente.

Então, nesses slides eu apenas dou o resultado final de como calcular a variância explicada na escala de responsabilidade. A derivação completa desta equação pode ser encontrada nesta referência.

A terceira estatística é chamada de razão de chances. Uma razão de chances é uma razão entre duas probabilidades, onde uma probabilidade é a probabilidade de ser um caso sobre a probabilidade de ser um controle.

Então aqui está um exemplo de teste que mostra como calcular a razão de chances manualmente. Digamos que estamos ordenando os indivíduos com base em seu escore poligênico de risco, do mais baixo para o mais alto, e estamos interessados ​​na razão de chances entre o 10º decil e o 1º decil. Assim, com uma série de casos e controles mostrados nesta tabela, a probabilidade de ser o caso no 1º decil é de 23 sobre 103. E a probabilidade de ser o caso no 10º decil é de 83 dividido por 40. a razão de chances entre os dois decis é de 9,3. Este valor significa que quando ordenamos os indivíduos com base no seu escore poligênico, os indivíduos nos 10% superiores, ou no 10º decil, têm 9,3 vezes mais probabilidades de serem um caso em comparação com os indivíduos nos 10% inferiores. E essa razão de chances pode ser facilmente estimada a partir da regressão logística usando a função logit link.

Assim, usando a estratégia de uma coorte, podemos acessar a AUC, a variância explicada e também a razão de chances para cada uma dessas coortes alvo. Aqui está o resultado de AUC e variância explicado para cada método, e cores diferentes aqui representam diferentes métodos que usamos. O eixo y aqui é a diferença de AUC em comparação com p+T, que é uma referência que usamos. E como você pode ver, com diferentes coortes de validação, há muitas variações. E é por isso que achamos que nossa comparação é mais robusta em comparação com outras comparações quando usam apenas uma coorte-alvo.

Se resumirmos esses gráficos de barras por cada grupo por método, podemos observar esse gráfico de barras. O eixo y aqui é AUC, e cada um dos grupos representa cada um dos métodos que comparamos. E cada uma das barras em cada um dos grupos representa uma coorte de ajuste diferente que usamos. E notamos que os métodos que modelaram formalmente diferentes arquiteturas genéticas, na verdade, têm desempenho bastante semelhante. Isso ocorre porque a arquitetura genética dos transtornos psiquiátricos é bastante poligênica.

Se olharmos para os resultados da doença de Alzheimer, que é menos poligênica em comparação com os transtornos psiquiátricas, observaremos uma grande diferença entre os diferentes métodos. E depois também observamos um padrão semelhante de variância explicada na escala de responsabilidade e na razão de chances entre os 10% superiores e os 10% inferiores, bem como a razão de chances entre os 10% superiores e os médios. Mas observamos que LDpred2, SBayesR e MegaPRS têm a classificação mais alta entre a maioria das comparações.

Para resumir, nesta palestra, mostrei como calcular a AUC, a variância explicada na escala de responsabilidade e também a razão de chances manualmente. E com base nas comparações que fizemos, observamos que para transtornos psiquiátricos, que são muito poligênicos, todos os métodos têm desempenho semelhante, mas alguns têm classificação superior a outros, por exemplo, LDpred2, SBayesR e MegaPRS.

Os resultados que mostro aqui fazem parte deste estudo, que foi publicado recentemente. Neste artigo, também fizemos a comparação para depressão maior e também outras análises de sensibilidade. Também fornecemos o código para executar cada método e para cada comparação, bem como cada uma das estatísticas utilizadas para comparação.

Com isso, gostaria de agradecer muito à professora Naomi Wray, que sempre me deu grande apoio sempre que precisei, e a todos os demais membros do PCC. E obrigada a todos.
