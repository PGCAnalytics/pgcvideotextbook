<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.353">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>PGC Video Textbook - Chapter 6.1: Polygenic Risk Scores (Video Transcript)</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>


<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar docked nav-fixed">


<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">PGC Video Textbook</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link active" href="./index.html" rel="" target="" aria-current="page">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./contact.html" rel="" target="">
 <span class="menu-text">Contact us</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://pgc.unc.edu/" rel="" target="">
 <span class="menu-text">PGC Website</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools ms-auto">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item">Chapter 6.1: Polygenic Risk Scores (Video Transcript)</li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
      <a href="./index.html" class="sidebar-logo-link">
      <img src="./pgc_logo_website_v3.jpeg" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
      <div class="sidebar-tools-main">
    <div class="dropdown">
      <a href="" title="" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" aria-label=""><i class="bi bi-github"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="">
            Source code
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="">
            Report a bug
            </a>
          </li>
      </ul>
    </div>
</div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./welcome.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome to the PGC Video Textbook!</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./toc.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Table of Contents</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Chapters</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 1: Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 2: The Genome</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 3: Technologies</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 4: Study designs</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 5: GWAS analysis</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 6: Polygenic Scores</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter7.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 7: Ancestry-Specific Analyses and Considerations</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter8.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 8: Post-GWAS bioinformatics</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter9.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 9: Advanced Topics</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 10: Other Considerations</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
 <span class="menu-text">Software Tutorials</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./software_cnvs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">CNVs</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./software_conditional.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Conditional Analysis</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./software_crossdisorder.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Cross-disorder Analysis</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./software_ewas.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">EWAS</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./software_geneset.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Gene Set Identification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./software_gwas.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">GWAS</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./software_genomicSEM.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Genomic SEM</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./software_imaging.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Imaging</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./software_MR.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Mendelian Randomization</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./software_mtag.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">MTAG</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./software_prs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">PRS</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./software_correlation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">SNP Heritability and Genetic Correlation</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
 <span class="menu-text">Additional Resources</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./glossary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Glossary</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./resources.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Software Resources</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./addreading.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Additional Reading</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="https://pgc.unc.edu/for-researchers/download-results/" class="sidebar-item-text sidebar-link">
 <span class="menu-text">PGC Summary Statistics</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#sec-video1" id="toc-sec-video1" class="nav-link active" data-scroll-target="#sec-video1"><em>Polygenic Risk Scores</em></a></li>
  <li><a href="#sec-video2" id="toc-sec-video2" class="nav-link" data-scroll-target="#sec-video2"><em>What is a Polygenic Risk Score?</em></a></li>
  <li><a href="#sec-video3" id="toc-sec-video3" class="nav-link" data-scroll-target="#sec-video3">Polygenic Risk Scores: In detail</a></li>
  <li><a href="#sec-video4" id="toc-sec-video4" class="nav-link" data-scroll-target="#sec-video4">Polygenic Scoring Methods</a></li>
  <li><a href="#sec-video5" id="toc-sec-video5" class="nav-link" data-scroll-target="#sec-video5">Polygenic risk scores: PGS comparison</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Chapter 6.1: Polygenic Risk Scores (Video Transcript)</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<hr>
<section id="sec-video1" class="level1">
<h1><em>Polygenic Risk Scores</em></h1>
<p><strong>Title</strong>: Polygenic Risk Scores</p>
<p><strong>Presenter(s)</strong>: CPM Oxford</p>
<p>Our genes provide valuable insight into our family history, our ancestry, and also our health. As we learn more about our DNA, we identify new opportunities for healthcare. One such opportunity comes from using polygenic risk scores. In this video, you will learn what a polygenic risk score is, see how they can be used in healthcare, and find out how you can participate in research studies that use them.</p>
<p>Polygenic risk scores: Why might you care about whatever they are? Knowing our risk of developing particular diseases can be a really powerful way to help us live healthier lives. Imagine you knew that you were at a high risk of having a heart attack. That’s pretty clear motivation to do something about it, such as changing your diet or taking risk-lowering medication. But how could you know that you’re at a high risk?</p>
<p>A major risk factor for common diseases such as heart disease, cancer, and diabetes is our own genetic makeup. New studies show that we can now analyze an individual’s genes and actually measure that risk using something called polygenic risk scores.</p>
<p>Our genes vary from person to person; it’s why we’re not all the same. But some of these genetic differences can contribute to our risk of complex diseases. There are a few rare diseases that are caused by changes in a single gene, but we now know that for the most common diseases such as heart disease and diabetes, it’s often not just one or two of these genetic changes that are important; it’s many of them, each having a small effect on the polygenic risk (hence poly (“many”) genic (“to do with genes”) risk scores - scoring a risk.</p>
<p>Scientists have compared DNA among gigantic groups of people with and without disease and have identified thousands of genetic variations that influence risk for hundreds of diseases. This important reference DNA from the studies can then be compared with individual patients’ DNA to calculate their risk.</p>
<p>So, how could polygenic risk scores actually help you? A doctor’s usual assessment of a patient might indicate one course of action, but when armed with a polygenic risk score, a different approach may become the better way to go. These scores could help doctors better identify specific medicines or therapies a patient is likely to respond well to, or whether they’re likely to benefit from earlier screening for specific cancers. And because our genes don’t change, as more of these variations are identified, your polygenic risk scores can be updated without having any more tests. Also, remember this same genetic information can be used to give you a polygenic risk score for lots of different diseases.</p>
<p>So, we’re stuck with the genes we have all throughout our lives. But even when they do mean our polygenic risk score of a particular disease is high, that doesn’t mean our fate is sealed. There’s lots we all can and should do to reduce our overall risk of disease, particularly if our polygenic risk is high. For example, patients with a high polygenic risk of type 2 diabetes can significantly reduce their overall risk through exercise and eating a healthy diet.</p>
<p>So, what’s next? Well, you can see how much promise polygenic risk scores have, but there’s still work to be done. Further studies are needed to assess the clinical impact of PRS in improving the diagnosis, treatment, or prevention of specific diseases. Also, studies must be extended to cover a wider global population. But already, polygenic risk scores are a promising new tool. By taking them into consideration along with other risk factors, they have the potential to help us live longer, healthier, and happier lives.</p>
<p>To learn more about polygenic risk scores and how you can take part in research studies involving PRS, please visit us at cpm.well.ox.ac.uk/prs.</p>
<hr>
</section>
<section id="sec-video2" class="level1">
<h1><em>What is a Polygenic Risk Score?</em></h1>
<p><strong>Title</strong>: What is a polygenic risk score?</p>
<p><strong>Presenter(s)</strong>: Till Andlauer</p>
<p>In this video, I’m going to introduce you to polygenic risk scores (PRS), also often now called only polygenic scores (PGS) because you can also calculate them on quantitative traits like, for example, brain volumes.</p>
<p><em>Polygenic disorders</em></p>
<p>All complex common disorders are polygenic. If you want to quantify genetic risk for a complex disorder, you thus have to assess the effects of many genetic variants at the same time. The basis for a PRS is a GWAS, and there’s a separate video explaining what that is.</p>
<p><em>Calculating PRS</em></p>
<p>For the calculation of PRS, this GWAS is considered the discovery or training dataset. The GWAS effect sizes are used as the weights when calculating a PRS. To get stable effect size estimates, you need GWAS generated on large samples, as, for example, conducted by the PGC. But there’s one problem: neighboring variants are correlated, because they get inherited together and thus show similar associations. The SNP density could thus bias the PRS. At any given locus, classical PRS thus only use the variant with the lowest p-value. Correlated SNPs are removed using a method called LD clumping.</p>
<p><em>LD clumping</em></p>
<p>How many SNPs to use? That’s a difficult question. You might only want to use the SNPs that show, genome-wide significance - 44 in this study on depression. But what about these ones here? Are they not relevant? Likely, they would get significant if a larger sample size was used for the GWAS. Therefore, typically p-values of 0.05 or 0.01 are used as thresholds for the calculation of classical PRS.</p>
<p><em>Other methods</em></p>
<p>In recent years, many other methods have been published that use Bayesian regression frameworks to model the linkage disequilibrium and thereby calculate LD-corrected weights. These methods, like LDpred, PRS-CS, and SBayesR, they have been benchmarked in a recent manuscript, and they perform much better than classical PRS.</p>
<p>No matter how you choose your weights, the next step is always the same. For each SNP, you multiply the weight by the number of effect alleles. If “A” is the effect allele, then the multiplication factor would be “0” in this case, here “1”, and here “2”. You do this multiplication for each SNP, and you sum over all variants to get a single score. A PRS by itself, for a single person, is just an arbitrary number that cannot be interpreted well.</p>
<p>In order to be able to interpret the PRS, you need a large group of people that share the same ancestry and possibly even the same socio-demographic background among each other and with the people used for the training GWAS. Only with this group, you’re able to interpret the PRS of an individual relative to that group. And if you calculate PRS for many people, you will see that the scores follow a normal distribution. The PRS of a single individual may then map to a lower, an average, or a higher percentile within that distribution.</p>
<p>Patients suffering from a disorder will, on average, have higher PRS for that disorder than healthy controls, but only on average. Individual-level risk predictions from PRS thus have a very poor sensitivity and specificity. Basically, you can only say anything about the people mapping to the lowest or highest percentiles; they have significantly lower or higher risk for the disorder. And for everyone in between, you can’t say much.</p>
<p>You can, however, get fairly good predictions if you contrast the extreme ends of the distribution, but the odds ratios achieved are, of course, still much lower than for monogenic disorders. Nevertheless, PRS can be used for risk stratification to identify people at high risk that might need more medical attention.</p>
<p><em>Applications</em></p>
<p>In research, PRS have many highly useful applications. From the assessment of the polygenic risk load for common variants in families, especially affected by psychiatric disorders, to the genetic characterization of bipolar disorder subtypes, and of course, many, many more. Also, many reviews and methods articles on PRS have been published recently, and you will easily find a lot of material to keep on reading.</p>
<hr>
</section>
<section id="sec-video3" class="level1">
<h1>Polygenic Risk Scores: In detail</h1>
<p><strong>Title</strong>: Polygenic Risk Scores</p>
<p><strong>Presenter(s)</strong>: Adrian Campos</p>
<p><strong>Adrian Campos</strong>:</p>
<p>Hello, my name is Adrian Campos and today I’m going to talk to you about polygenic risk scores for the Boulder Workshop 2021. Special thanks to Sarah Medland, Lucia Colodro Conde, and Baptiste Couvy Douchesne, for all the input for preparing these slides.</p>
<p><em>Outline</em></p>
<p>This is the layout for today’s talk. We’re going to start with a brief introduction on GWAS and allele effect sizes. Then I will give a brief overview of what a PRS is and how it’s calculated with the graphical example. After that, we will discuss which variants to include and how to account for linkage disequilibrium when estimating a polygenic risk score. We will discuss the more traditional approach named clumping and thresholding which is widely used in the area. Then we will discuss some applications for polygenic risk scores, other methods for polygenic risk scores and a brief summary at the end.</p>
<p><em>GWAS</em></p>
<p>So without further ado, let me get out of the picture and let’s start with this talk. As we have previously seen, a Genome Wide Association Study (GWAS) allows us to identify which genetic variants are linked to a trait of interest. A GWAS allows us to identify not only which genetic variants are linked to a trait of interest, but also their effect size. If we imagine, for example, this to be a height GWAS and we focus on the highlighted genetic variant, we would identify an effect size of two centimeters per copy of G allele. So the effect size of this variant would be approximately 2, which is also the slope of a linear regression between the mean height and the genotype groups. What this basically means is that if we had access to a new sample with genotype data and height data, we would expect AG individuals to be an average 2 centimeters taller than AA individuals and two centimeters shorter than GG individuals. But we know that complex traits like height, are highly polygenic. There are many genetic variants that contribute to the phenotype. Furthermore, we know that common variants have a small effect size and that the example that we were using was an exaggeration. This would cause this single loci-based prediction to be basically useless. However, we can combine the information that we gain from several genetic variants to estimate an overall score and gain a better estimate of the trait. This is essentially what a PRS does.</p>
<p>Now let’s keep using this example to understand what a PRS really is. In our previous example, we started by focusing on this genetic variant. Which had an effect size of two centimeters per [copy of the] G allele. So if we were to score this participant based on this genetic variant, we would sum 0 given that he has no copies of the G allele. The same would be true for all participants that are homozygous for the A allele at this [locus]. Participants that are heterozygous at this [locus], have one copy of the G allele. So in order to score them we multiply 2, which is the effect size times the number of copies of the G allele, which is one. Finally, we score participants with two copies of the effect [allele] by two times the effect size, which also happens to be two in this example. For polygenic risk scores we want to aggregate the information of several genetic variants. So now let’s focus on another one. This one has an effect size of minus one per effect allele. Following the same process, we would score participants by weighting the number of copies of the T allele they have, times the effect size of that allele. So in this example, participants with a TT genotype will have a score of minus two for this [locus]. Participants with a CT genotype will have a score of minus one and participants with the reference CC Genotype will have a 0. We can do that for a third genetic variant. This one has an effect size of 0.5 per G allele. Again, we proceed to score this [locus] by multiplying the effect size times the number of copies of the effect allele. We can repeat this process, including all other variants and sum across all loci. These will give you an estimate of the polygenic risk for the trait of interest.</p>
<p>So a working definition of polygenic risk score is a weighted sum of alleles which quantifies the effect of several genetic variants on an individual’s phenotype. As a word of caution, the sample for which PRS will be calculated should be independent from that of the discovery GWAS. That means there has to be no sample overlap between the sample with which you calculated the effect sizes for the variants, and the sample in which you were calculating a polygenic risk score. Although in this example we have focused on a quantitative trait, which is height, it is important to mention that polygenic risk scores can also be used to calculate the genetic risk for a disease or a binary trait. It is important to remember that genetic material is organized on two complementary strands of DNA, which is made up by nucleotide bases. These bases are four: basically A T C G, but they are complementary to each other. That means that if one of the strands has an A, the other strand will have a T in the position that is complementary to that A. The same is true for C and G. Whenever the reference and alternative alleles of a genetic variant are not complementary to each other, we can tell which strand they came from. However, when the reference and alternative alleles are complementary to each other, it is hard to tell which of the strands we are actually measuring and therefore which is the effect allele. That can have severe consequences on polygenic risk scores. Sometimes it is possible to solve this ambiguity using information on allele frequency, but this can be tricky if the allele frequencies are close to 0.5. Now we’re going to discuss how to decide which variants to include for a PRS, and also how to account for linkage disequilibrium. I know that I previously said that we should repeat including all the other variants and sum across all loci. However, there are things to consider. The first one is that we know that there are many GWAS that are underpowered. That means that there are many more true associations that those that are reaching genome wide significance. The second one is that linkage disequilibrium creates a correlation structure within the variants and it is important to use independent SNPs for polygenic risk scores, or to account for their correlation somehow. To do this, we try to identify near independent SNPs using a method called clumping. Clumping basically selects all the SNP are significant at a certain P value threshold and forms clumps of SNPs within a certain distance to the index SNP only if they are in LD with the index SNP.</p>
<p>After clumping, genetic variants are approximately independent, but there’s still a question of whether we [should] include only genetic variants that reach genome wide significance, or do we relax the P value threshold for including them? One solution is to calculate many PRS, including more and more variants, by becoming more lenient on the P value threshold that we use to filter them out. Here is an example of eight P value thresholds, starting with the most strict, which would be just genome wide significant variants, all the way to the most relaxed which would be including all variants. After “solving” the problem of which genetic variants to include, the polygenic risk score can proceed as we previously saw and then we will end up with a set of scores that depict the genetic liability or the genetic risk. in an independent [population] (sample) for a certain trait of interest, then we can perform PRS-trait association analysis. For this, it is important to think about your sample. If it is a family based sample like a twin registry, it is important to adjust for relatedness. If it is homogeneous in terms of ancestry, even then it is always a good idea to adjust for genetic principal components to make sure you’re getting rid of population stratification effects. It’s also important to think whether the target sample matches the GWAS [sample] ancestry, because there are known issues of portability between ancestries.</p>
<p>Then you also have to consider your trait of interest. Is it continuous? Then you can use a linear regression to perform PRS-trait association. If it is binary, you can rely on logistic or probit regressions and if it is ordinal, you will have to find something like a cumulative linked model or cumulative linked mixed models for family based samples. Always remember that there are potential confounders of the trait and of the discovery GWAS and you have to think about them and adjust for them. Before performing a polygenic risk score analysis is important to keep in mind that the power of PRS depends on the power of the GWAS that will be used to estimate the PRS. In this example. The same target sample was used to calculate polygenic risk scores for depression. And they are comparing the variance explained [by] a polygenic risk score based on the first MDD PRS by the PGC and a subsequent update; and what they found is that there was a substantial increase on variance explained which was sample size dependent. The clumping and thresholding approach allows us to explore the pattern of variance explained and its relationship to the number of genetic variants that we are including. For example, here we can see that using the most strict cutoff is not having a significant variance explained, and the more variants we include, the more variance explained we are getting. This is a typical pattern [of a GWAS] of a PRS. constructed from a GWAS that was that still not fully powered upon a fully powered GWAS we expected a pattern were including just the genomewide SNPs performs really well in terms of variance explained and then when we start including more and more noisy SNPs, we are losing variance explained.</p>
<p><em>Applications</em></p>
<p>Now we will discuss some of the applications for polygenic risk scores. I have listed here some of them, but I think you can use your imagination and think of others. The first one is something very typical. To test for the GWAS association and quantify variance explained. It is basically a safety check in a genome wide association study where you want to demonstrate that your GWAS is really predictive of the trait of interest. Polygenic risk scores can also be used for risk stratification, which would be identifying people to test later for a specific disease. That should reduce the burden to a health service system. It can also help in clinical diagnosis of rare diseases. We can also use polygenic risk scores for testing genetic overlap between traits. For example, is a genetic risk for depression predictive of cardiovascular disease and vice versa?</p>
<p>We could also think of using PRS for trait imputation when a trait is not measured. For example, if you wanted to impute a smoking phenotype in a [population] sample. This is obviously imperfect and dependent on the heritability of the trait, but it might start gaining traction as polygenic risk scores become more and more predictive of the trait of interest. As there are many more GWASes of treatment response and they’re gaining power, personalized treatment based on polygenic risk scoring could become a reality. And basically any hypothesis where you rely on a genetic risk or a genetic liability. There’s been many publications where polygenic risk scores are used to examine gene by environment interactions.</p>
<p><em>Software</em></p>
<p>So far we have discussed the traditional clumping and thresholding approach. However, there are other methods that are worth mentioning. But first, let me mention the software that you can use to calculate clumping and thresholding polygenic risk scores. The first one is PLINK [and PLINK2]. The second one is PRSice2, and then there is an R library called bigsnpR which contains not only clumping and thresholding, but many other options. There are other types of PRS which we will discuss briefly like LDpred2, which is implemented in bigsnpR BbaseR which is implemented in GCTB. Lasso sum and lasso sum two, which are implemented also in bigsnpR and there’s PRS-CS, and JAMPred. Which I believe are standalone software, but I’m not quite sure. All of these methods share a common motivation, which is to substitute the clumping step with something more elegant. We basically want to approximate the effect sizes that we would obtain if we would have run a multiple linear regression GWAS. That is, a GWAS that simultaneously estimated the joint effects of all the SNPs. The problem is that we cannot do that. So what we do in a GWAS is we run m regressions. And we obtain the SNP ‘marginal’ effect sizes, that is, the effect size of each SNP without taking into account the correlation with other SNPs. And the lack of adjustment for these correlation is obvious from the Manhattan plots having these well defined towers.</p>
<p>To solve this problem, we need to find a method that approximates the multiple linear regression results based on the GWAS summary statistics. There are many methods that implement estimating the multiple regression, SNP effect sizes, and we really don’t have time to cover them all in detail. So today I’m going to quickly mention some of them, and then I’m going to give a couple of details in the two that I considered the most commonly being used in the area, which is LDPred2 and SBayesR. LDpred2 is implemented in bigsnpR, and it uses a Gibbs sampler to estimate the joint SNP effects. SBayesR is implemented in GCTB and it estimates the joint SNP effects using Bayesian, multiple regression. Lasso sum and lasso sum two are also implemented in bigsnpR and they are based on performing a penalized regression that basically shrinks the SNP effect sizes. Then there’s PRS-CS, which also uses a Bayesian regression to estimate the joint SNP effects and then JAMPred, which uses a two step Bayesian regression framework. In SBayesR they combine a likelihood function that connects the joint effect sizes with the GWAS summary statistics coupled with a finite mixture of normal distribution priors Underlying the marker [variant] effects. This basically means that we are modeling the SNP effect sizes as a mixture of normal distributions with mean zero and different variances. This is typically done using four normal distributions all with mean zero and distinct variances. The first one is variance of zero, which basically captures all the SNPs with a 0 effect, and then from there we allow increasing values of effect sizes to exist in this model. What this does then is performing Markov chain Monte Carlo Gibbs sampling for the model parameters which are basically: The joint effect sizes, the probabilities of the mixture components and error terms. Of course, the parameter that is of our main interests are those joint effect sizes that then we can use [for] as effect sizes or weights in our polygenic risk score analysis.</p>
<p><em>LDPRED II</em></p>
<p>LDpred2 is a recent update to LDpred, which was a method that also derived an expectation of the joint effects given the ‘marginal effects’ and the correlation (LD) between SNPs. This method assumes that there is a proportion P of causal variants on that trait of interest, and then assumes that the joint effect sizes are normally distributed with mean zero and variance proportional to the heritability of the trait. Importantly, the proportion of causal variants and the heritability of the trait are estimated independently, at least in the classical approach and for P there is a grid of values that are explored, whereas for heritability or SNP based heritability, it is estimated using LD score regression. Then it uses a Bayesian Gibbs sampler to also estimate the joint effect sizes for the GWAS. However, LDpred2 adds 2 new models to the traditional LDpred approach. The first one estimates P and the heritability from the model. Instead of testing several values and using LD score regression. This is useful because before P and h2 squared were estimated through a validation data set and this new approach which is called ‘LDPred2 auto’, doesn’t require this intermediate validation data set anymore. And there is another one called LDpred2 sparse which allows for effect sizes to be exactly 0, which would be similar to the first mixture component of SBayesR. LDPred2 is also good for modeling long range linkage disequilibrium such as that that is found near the HLA region. Other methods rely on removing these regions to account for this problem. However this method (it’s authors) adequately points out that these regions are important for certain traits, and that removing them would reduce power in them.</p>
<p><em>Key take home messages</em></p>
<p>As key take home messages, these approach is usually perform better than, or at least as well as, clumping and thresholding; and when they don’t, it is important to be weary as sometimes the models don’t converge and they might fail silently. Performing better PRS is still an area of active research and there is a clear battle between complexity and power versus scalability and ease of use. There’s many publications comparing across these methods, so try to read them and pick one that best fits your needs.</p>
<p><em>Summary</em></p>
<p>So in summary: A polygenic risk score is a weighted sum of alleles. It’s basically a tool that estimates the genetic liability or risk to traits. It can be done for both quantitative and binary traits. Before performing PRS, it is essential to have quality controlled your GWAS (discovery) summary statistics. To have quality controlled the (target) genotype dataset and to be wary of the potentially problematic ambiguous SNPs. Furthermore, practically you find that you need to match the SNP identifiers between your GWAS data and the genotype data. Remember that discovery and target samples need to be independent and to consider statistical power before starting any analysis. When using polygenic risk scores do remember to be aware of related individuals in the sample and to properly adjust for them. As well as for population stratification. Also consider that differences in ancestry might underlie differences in predictive ability of a polygenic risk score and always be wary of jumping too fast to conclusions. Always consider potential biases in the discovery GWAS and in the target sample.</p>
<p><em>Further reading</em></p>
<p>If you’re interested, here are some further reading on polygenic risk scores. Some of these are historical papers that mark the milestones of actually achieving polygenic prediction in complex traits, and some of them are discussion on the possible biases and the different methods that exist for polygenic risk scoring. And that’s it for the intro to polygenic risk scores. Thank you and see you next time.</p>
<hr>
</section>
<section id="sec-video4" class="level1">
<h1>Polygenic Scoring Methods</h1>
<p><strong>Title</strong>: Polygenic Scoring Methods: Comparison and Implementation</p>
<p><strong>Presenter(s)</strong>: Oliver Pain</p>
<p>hello my name is oliver payne and i’m a postdoctoral researcher working with professor catherine lewis at king’s college london today i’ll be talking about polygenic scoring methods comparing various methods to one another and also describing resources we’ve developed for the calculation of polygenic scores for research and clinical purposes i have no conflicts of interest to declare so my presentation is split into three sections first i’ll give a brief introduction to polygenic scoring then describe our work comparing different polygenic scoring methods and finally introduce an easy and reliable pipeline for polygenic scoring that we’ve developed called genopred pipe</p>
<p>so first a brief introduction to polygenic scoring a polygenic score is a way of summarizing an individual’s genetic risk or propensity for a given outcome typically calculated based on an individual’s genetic variation and genome-wide association studies summary statistics referred to as g-words sum stats polygenic scores are a useful research tool and could also be useful for personalized medicine as their predictive utility increases for polygenic scoring the dual summary statistics are typically processed to identify variants overlapping with the target sample account for linkage to equilibrium between variants and adjust the g word’s effect sizes to optimize the signal to noise ratio in the geoserum statistics therefore an individual’s polygenic score can vary due to the genetic variance considered and the alloy of frequency and linkages equilibrium estimates used to adjust the dual sumstats often in research these factors vary depending on the target sample using the intersective variance between the target sample and the g was and using estimates of linkages equilibrium and alloy frequency from the target sample itself now this isn’t ideal and an alternative strategy is to use a reference standardized approach whereby a common set of variants are considered for all target samples and the linkage and algebra frequency estimates are derived using a common ancestry matched reference sample this reference standardized approach is advantageous when using polygenic scores in both clinical and research contexts as it allows polygenic scores to be calculated for a single individual and it avoids variation in an individual’s polygenic score due to target sample specific properties which might influence the result</p>
<p>now i’ll be presenting our work comparing polygenic scoring methods this table shows the leading polygenic scoring methods as reported in the literature the pt plus clump approach is the traditional approach whereby ld based clumping is used to remove the linkage to equilibrium between lead variants and g-was and a range of p-value thresholds are used to select the variants considered the others are more recently developed methods that use ld estimates to jointly model the effect of genetic variants and typically perform shrinkage to account for the different genetic architectures like the p-value thresholding approach several of the newer methods apply multiple shrinkage parameters to optimize the polygenic score when multiple parameters used a validation procedure such as 10-fold cross-validation is required to avoid overfitting whereby the variance explained estimate is artificially high due to trying many different p-value thresholds or shrinkage parameters in contrast some methods provide a pseudo-validation approach whereby the optimal parameter is estimated based on the g-word summary statistics alone not requiring a validation sample another approach that doesn’t require a validation sample is to assume an infinitesimal genetic architecture though this approach works best for highly polygenic phenotypes a third option is to model multiple polygenic scores based on a range of parameters using methods such as elastic net to account for the correlation between the polygenic scores</p>
<p>we compared methods using a range of outcomes measured in two target samples uk biobank and the twins early development study referred to as teds we used two samples to ensure our results not target sample specific and we selected the traits on the right as they have a well-powered publicly available g was and represent a range of genetic architectures as i described previously we used the reference standardized approach when calculating the polygenic scores ld and allele frequencies were estimated using two reference samples of differing sample size to evaluate the importance of reference sample size across methods and these reference samples were the european subsets of 1000 genomes phase 3 and an independent random subset of 10 000 european uk biobank participants hatmap three variants we used the route as these variants are typically well captured by genome-wide arrays after imputation provide good coverage of the genome and also reduce the computation times several methods already provide ld matrices including only hatmap 3 variants for use with their software in line with the reference standardized approach the predictive utility of each polygenic score approach was evaluated based on the pearson correlation between the observed outcome and predicted values the statistical significance of differences between predicted and observed correlations was determined using the williams test which counts for the correlation between predictions from each method this figure shows the average performance of each method compared to the best pt plus clump polygenic score as identified using 10-fold cross-validation the transparent points in the figure show the results for each target phenotype separately i’m only showing the results based on the uk biobank target sample when using the thousand genomes reference as the results were highly concordant when using teds or the larger reference when comparing methods that you use 10-fold cross-validation to optimize parameters shown in red you can see that the best methods are ld probe 2 lasso sum and pure scs all outperforming the pt plus clump approach providing a 16 to 18 relative improvement in prediction ld pro 2 showed further nominally significant improvements over the su-sum and pure scs on average when comparing methods that use a pseudo-validation approach or infinitesimal model not requiring a validation sample highlighted in blue and purple you can see that pscs and dbs lmm methods perform well providing at least a five percent relative improvement over the other pseudo-validation methods pure scs is better than dbs lmm providing a four percent relative improvement over dbs lmm it’s worth mentioning that especially’s</p>
<p>performance improved when using a larger ld reference on average then doing as well as dvs lmm when comparing the pseudo-validated pscs performance to 10-fold cross-validation results in red the prcs polygenic score performs only three percent worse than the best polygenic score identified by 10-fold cross-validation for any other method and performs better than the pt plus comp approach for all phenotypes tested highlighting the reliability of this approach the multi-prs approach shown in green which uses an elastic net model to model to model multiple polygenic scores based on a range of p-value thresholds or shrinkage parameters consistently outperforms the single best polygenic score selected by tenfold cross foundation shown in red with the largest improvement for the pt plus clump approach of 13</p>
<p>lastly we tested whether fitting polygenic scores across multiple methods improved prediction and we found it did to a small degree though the</p>
<p>computational burden is obviously substantial because then you have to run all of the methods in terms of runtime these methods very substantially this graph shows the number of minutes to run the method on chromosome 22 alone without any parallel computing you can see the prcs and ld pro 2 take a lot longer than other methods however since our study ld pro 2 developers have substantially improved the efficiency of their software halving the runtime moving that down to around here just under 25 minutes for chromosome 22. it’s worth noting that the time taken for pure scs when using a single shrinkage parameter is one-fifth of the time shown here meaning its pseudo-validation approach is actually reasonably quick dbs lmm is by far the fastest of the newer methods taking just a few minutes when run genome-wide without any parallel computing which is impressive considering how well it performed against other methods in terms of prediction something i wanted to highlight is that when using sbaza i’ve been using the robust parameterization option available in the newest version of esbazar as i found sbasal was not converging properly for some g was using this robust parameterization was most reliable and did not make the esbaza performance worse except for depression using the smaller 1000 genomes european reference you can see this here in the figure the second point is that since our published study i’ve also compared prcs methods using geosummary statistics for 17 different phenotypes derived using only the uk biobank sample avoiding possible quality control issues that occur from large scale meta analyses the results are highly concordant to the results when using the largest publicly available meta was results which is reassuring that our findings are are reliable however the performance of essbase r did improve again highlighting that especially is more sensitive to g1’s quality than other methods but when the quality is good esposa performs very well appears to be the best method in this analysis we were also interested to see whether one particular method did better when predicting across ancestries as some methods might highlight causal genetic effects better than others using the same 17 g was i briefly mentioned on the previous slide within the uk biobank sample alone i’ve tested the performance of prs methods across populations</p>
<p>so using the european g-wars but predicting in non-european subsets of uk biobank as you can see the results are very similar in each population with the best method identified in a european target sample also being the best method in a non-european target sample</p>
<p>so the advice for the future research regarding polygenic score methods is for optimal prediction we recommend modeling multiple parameters from ld pred 2 lasso sum or pure scs but if you’re looking for a simpler option for looking at genetic overlap perhaps i’d recommend using the pure scs pseudo-validation method also referred to as its fully bayesian method alternatively if you need to compute polygenic scores for many g-words or have limited time or computer power then dbs lmm is a fast and good alternative although especially does very well when the g was summary statistic’s quality is good its sensitivity to that quality means it doesn’t always perform well when using the largest meta g was results</p>
<p>okay so now i’m going to move on to the last section of my talk which describes our recently developed pipeline for polygenic scoring called genome pred pipe so most of the work i’ve just presented is contained in the study shown in the top left where we also described the reference standardized approach polygenic scoring in the study we provide a schematic representation of this reference standardized approach shown in the figure on the right whereby target genetic data is first imputed if it hasn’t been already then only happened three variants are retained as these are typically well imputed and provide decent coverage of the genome and then the sample is split into ancestral superpopulations determined by comparison to the thousand genomes phase three reference and lastly polygenic scores are then calculated based on jewish summary statistics processed in advance using a g was ancestry matched reference when we were carrying out this study and writing the code for the reference standardized approach we wanted to ensure the results were fully reproducible and we wanted to develop a resource that other researchers could use to calculate reference standardized polygenic scores so we used a combination of our markdown git github and github pages to create a publicly available website containing a series of readable documents describing the code and results of our various studies</p>
<p>the website is called genopred as it focuses on genotype-based prediction and there is a qr code here if you would like to take a look i’ll now briefly show you the website so this is the home page of the website which shows a list of the pages available i’ll now click on the link describing how to prepare the reference data for the reference standardized approach at the top it provides some information and then lists the required software below that and then it goes through each step uh one by one with code chunks for users to use to reproduce the results or create their own data now whilst we think this is a great resource for others to see what we’ve done and replicate the results you can see there are many steps and many separate code chunks to follow making its use quite lengthy and possibly prone to user error so i’ve recently written all of the steps to calculate reference standardized polygenic scores into a pipeline using snakebig along with what is called a condor environment which will automatically download and install all of the required software meaning it’ll create fully reproducible results the pipeline just requires three input files first a list of g was summary statistics that you want to use indicating the population in which they were derived and optionally information regarding the distribution of phenotype in the general population i.e prevalence or mean and standard deviation then you provide a list of target samples you want to polygenic scores four with the pipeline currently accepting pink one binaries so bed bim fam or b gen files and also the 23andme format lastly you’ll need a file called config.yaml which describes the location of the gwas</p>
<p>and target list files then actually running the whole pipeline just takes a single line of code i provided some test data for new users to experiment with which can be downloaded and then step two here is where the pipeline is actually run just writing snake make then two parameters indicating the computational resources you want to use and that you want to use condor environments and then the name of the output that you want in this example i’m requesting the final output which is an r markdown report of the ancestry identification and polygenic score results for all target samples which involves running the full pipeline</p>
<p>here is an updated version of the schematic i showed you earlier based on what is implemented in the geno pred pipeline the only difference is that at the end now we generate a report summarizing the results of the ancestry identification and polygenic scoring i’ll show you these reports briefly now this is the report produced to describe the results of a single individual first there are some descriptives about the number of snips before and after invitation and the number of reference snips available then the report describes the results of the ancestry identification analysis first showing the probability of being from each suit population in 1000 genomes reference in this example the individual has a probability of almost 100 of being from a european population you can also see the individual’s position on the first few reference projected principal components relative to the thousand genomes population i’m showing now and the individual individual in this case is that black circle then the individual’s ancestry is broken down into more specific populations within the assigned superpopulation in this case showing the individual was most similar to the great british population and individuals broadly from northern and western europe and at the bottom shows the individual’s polygenic scores in this case for body mass index and cardiovascular disease showing the results in relative terms compared to an ancestry matched reference and then in absolute terms for improved interpretation by considering the variance explained by the polygenic scores and the distribution of the phenotype in the general population then this is an example of the report produced to summarize the results for a sample of individuals again starting with some descriptives about the overlap with the reference snips then showing the number of individuals assigned to each superpopulation using a hardcore threshold of 50 with the underlying probabilities shown as a histogram below again you can see the position of these individuals on projected principal components compared to the thousand units reference and then each individual is assigned to specific populations within each assigned super population i’ll skip past these though and show you the polygenic scores which is summarized at the bottom just simply using histograms to show the distribution of polygenic scores for each population and g was</p>
<p>so i’ll conclude with why i think people should use this pipeline first it performs ancestry classification which is really important as non-europeans are often discarded from studies and as sample sizes increase usable non-european populations can be identified and analyzed also it’s important to consider the ancestor of an individual when calculating the polygenic score second it provides reference standardized polygenic scores which are scaled according to an ancestry matched reference and are independent of any target sample specific properties which is useful for research and clinical prediction purposes third it can efficiently implement any of the top performing polygenic scoring methods using a single line of code saving time and reducing user error fourth it’s been tried and tested i’ve put uk biobank through it and other samples and compared the prs to the observed phenotypes assuring me that it’s working as it should finally it’s well documented online and produces fully reproducible results both of which are important for progressing science i’m showing the qr code again for the juno pro website on the right please do check it out if you’re interested lastly i’d like to thank my amazing colleagues for their help with this work in particular catherine for her brilliant supervision throughout thank you for listening</p>
<hr>
</section>
<section id="sec-video5" class="level1">
<h1>Polygenic risk scores: PGS comparison</h1>
<p><strong>Title</strong>: Polygenic risk scores: PGS comparison</p>
<p><strong>Presenter(s)</strong>: Guiyan Ni</p>
<p>welcome the topic of this video is how</p>
<p>to run polygenic risk score comparison</p>
<p>until now we already have watched the</p>
<p>individual talk on prscs LD Pro 2 and</p>
<p>spsr and other methods I believe you all</p>
<p>have a good understanding about</p>
<p>polygenic score and each of those</p>
<p>methods</p>
<p>so this slides here is just to set up</p>
<p>the scene to make sure that we are on</p>
<p>the same page</p>
<p>collagen risk score of an individual is</p>
<p>a weighted sum of the counties of risk</p>
<p>allele So based on the jivas summer</p>
<p>statistical results the basic method for</p>
<p>polygen resource score is pwalu clamping</p>
<p>on the thresholding</p>
<p>this method is simple but it doesn’t</p>
<p>firmly model different genetic</p>
<p>architecture so there are many new</p>
<p>methods try to model the genetic</p>
<p>architecture for the trade of Interest</p>
<p>for example using different paraders in</p>
<p>the base in regression like our LD</p>
<p>printing infinitesimal model LD Pro 2 SP</p>
<p>SBC</p>
<p>prscs and acid base arm</p>
<p>and also matter like La Susan is using</p>
<p>the lasso regression</p>
<p>a mega PRS is another method is runs on</p>
<p>different prayers for example if it runs</p>
<p>on a probably using a mixture of four</p>
<p>normal distribution it will be the same</p>
<p>like a space r</p>
<p>or similar</p>
<p>and if a zoom order snip have a</p>
<p>contribution to the phenotype variance</p>
<p>then it will be similar to LD product</p>
<p>infinitesimal model or S block it can</p>
<p>also run a prayer like both a similar</p>
<p>like both LM and also can run also some</p>
<p>regression</p>
<p>so the difference between Mega PRS and</p>
<p>other method is the expected first name</p>
<p>heritability can vary between</p>
<p>um can vary by LD and manual frequency</p>
<p>so in this talk we will compare all</p>
<p>those message and we know that when the</p>
<p>method is proposed they already compared</p>
<p>with other methods but the fundamental</p>
<p>question we are trying to answer here is</p>
<p>which method should we use in the PGC</p>
<p>data</p>
<p>then we use the course validation we</p>
<p>want a cohort out of course validation</p>
<p>to</p>
<p>um to answer this question and to</p>
<p>compare the performance of different</p>
<p>um methods here’s the toy example</p>
<p>showing uh for the course of validation</p>
<p>each cell here is one cohort and the</p>
<p>pink cell is for the discovery cohort</p>
<p>and the target cell is as green cell is</p>
<p>for the Target cohort</p>
<p>and in first uh first round of the</p>
<p>analysis we’re using the four pinks</p>
<p>discover cohort as a discovery says and</p>
<p>then while it is a performance of each</p>
<p>method in in the Target</p>
<p>sample and then we repeat this process</p>
<p>into each of those cells or each of</p>
<p>those cohorts serve as a Target cohort</p>
<p>if it’s needed by the by the um some</p>
<p>method we have another cohort will serve</p>
<p>as a tuning sample to select the optimal</p>
<p>hyper parameters</p>
<p>so in the real data analysis we use the</p>
<p>Geo summer statistic from schizophrenia</p>
<p>2 as a discover sample and all those</p>
<p>data we actually have access to uh</p>
<p>through a service through cohort where</p>
<p>the individual level genotype is</p>
<p>available and we use each of them as a</p>
<p>Target sample</p>
<p>for the tuning cohort we use uh these</p>
<p>four cohorting term to tune the hyper</p>
<p>parameters</p>
<p>and then we can predict the polygenic</p>
<p>score</p>
<p>into each of the targets sample</p>
<p>here we used a search statistic to</p>
<p>measure the performance of different</p>
<p>methods one is AUC another one is</p>
<p>proportion explaining</p>
<p>in the liability scale and third one is</p>
<p>all the ratio</p>
<p>I will go through each of them to show</p>
<p>how to calculate each of those</p>
<p>statistics</p>
<p>so first start with AUC here is a toy</p>
<p>example</p>
<p>on how to calculate AUC back hand</p>
<p>so AUC is actually a short for the area</p>
<p>under the RC curve which is shaded by</p>
<p>the Ping here so the ROC curve is made</p>
<p>by plotting</p>
<p>um</p>
<p>against the true positive rate to the</p>
<p>false positive rates at each possible</p>
<p>cut off</p>
<p>so what that mean</p>
<p>it means as</p>
<p>assume that</p>
<p>um this is the density of plot for the</p>
<p>polygenic score in the control sample</p>
<p>and here is for the case samples</p>
<p>and this vertical line is the current</p>
<p>cutoff</p>
<p>and in this case this graph can be</p>
<p>divided into four groups and true</p>
<p>negative</p>
<p>false negative</p>
<p>true positive</p>
<p>and a false positive and true positive</p>
<p>and then we can calculate the proportion</p>
<p>of each each group and then we can</p>
<p>calculate the true positive rates and</p>
<p>the false positive positive rate</p>
<p>which other coordinates used in the RC</p>
<p>curve so in the in the current cutoff we</p>
<p>use here it means that we have roughly</p>
<p>about 17 percent of cases the the uh</p>
<p>correctly classified as case</p>
<p>and then there are about 10 percent of</p>
<p>control they are running classified as</p>
<p>case</p>
<p>and which give us the coordinates</p>
<p>for this dot here</p>
<p>and then we’ll vary the this vertical</p>
<p>line This cut off we will uh we will get</p>
<p>this Roc curve as shown in this slides</p>
<p>here</p>
<p>and this you see the first statistic we</p>
<p>use to measure the performance of</p>
<p>different methods</p>
<p>and the second one is variance explained</p>
<p>in the liability scale when using a</p>
<p>certain case control studies</p>
<p>so this variance is a function of</p>
<p>variance explaining the observed scale</p>
<p>this r squared observed</p>
<p>case control study and another two</p>
<p>parameters C and Theta</p>
<p>so the variance explained on The</p>
<p>observed scale is actually a function of</p>
<p>two likelihoods from the new model and</p>
<p>the phone full mode which is designed</p>
<p>in this two equation</p>
<p>and this parameter C is a function of k</p>
<p>z and P and this K parameter is actually</p>
<p>the proportion of the population that</p>
<p>are diseased</p>
<p>is also means the prevalence of the</p>
<p>disease and Z parameter is a density at</p>
<p>this threshold T here and this curve is</p>
<p>a standard normal distribution</p>
<p>and the p is a proportional case in your</p>
<p>G was a result or in your case control</p>
<p>study</p>
<p>and Theta parameter is a function of the</p>
<p>same</p>
<p>kzt and threshold T but with different</p>
<p>combination</p>
<p>so in this slides I just give the final</p>
<p>result of how to calculate the variance</p>
<p>explained in the liability scale the</p>
<p>full Direction with of this equation can</p>
<p>be found in this reference</p>
<p>foreign</p>
<p>statistic is called Oz ratio</p>
<p>and also ratio is a ratio between two OS</p>
<p>and the OS is a probability being a case</p>
<p>over the probability being a control</p>
<p>so here is a toy example showing how to</p>
<p>calculate all the visual backhand</p>
<p>and that</p>
<p>saying that we are ordering the</p>
<p>individual based on their polygen risk</p>
<p>score from a lowest to highest and we</p>
<p>are interested in the observation</p>
<p>between the uh 10 stairs and</p>
<p>um first day so with a number of cases</p>
<p>and the controls show in this table so</p>
<p>the always being a case in the first day</p>
<p>so is 23 over 103 and us being a case in</p>
<p>the 10 states of is 83 divided by the</p>
<p>43. the old ratio of between the two</p>
<p>decimals is 9.3</p>
<p>so this this value means um when we</p>
<p>order individual based on their</p>
<p>polygender score</p>
<p>the individual in the top 10 days in the</p>
<p>top 10 percent or in the 10 states or</p>
<p>have 9.3 Times Higher of us being a case</p>
<p>compared to the individual in the bottom</p>
<p>10 percent</p>
<p>and this this old version can be easily</p>
<p>estimated from the logistical version</p>
<p>using the logic link function</p>
<p>so using the U1 cohort strategy we can</p>
<p>access the AUC variance plan and also</p>
<p>ratio for each of those Target cohort</p>
<p>and here show the result for AUC and for</p>
<p>cohort of each method and different</p>
<p>colors here stand for different method</p>
<p>we used and the y-axis here is a UC</p>
<p>difference compared to the P plus T</p>
<p>which is a benchmark we used</p>
<p>and as you can see of course different</p>
<p>validation cohorts there are lots of</p>
<p>variations</p>
<p>and</p>
<p>that’s why we think our comparison is</p>
<p>more robust compared to other comparison</p>
<p>when they are only use one target cohort</p>
<p>if we summarize these uh bar products</p>
<p>by each group by method</p>
<p>we can see we can observe this bar plot</p>
<p>the y-axis here is AUC and</p>
<p>each of the group stands for each of the</p>
<p>method we compared and each of the bar</p>
<p>in each of the group stands for</p>
<p>different tuning cohort we use</p>
<p>and we noticed that the methods that</p>
<p>have formally modeled different genetic</p>
<p>architecture they actually have quite</p>
<p>similar performance this is because the</p>
<p>genetic architecture of psychiatric</p>
<p>disorders they are quite polygenic</p>
<p>if we look at the results for the</p>
<p>Alzheimer’s disease Which is less</p>
<p>polygenic compared to psychiatric</p>
<p>disorder we will observe a big</p>
<p>difference across different methods</p>
<p>and then we also observed the similar</p>
<p>pattern for our various explained in the</p>
<p>laminate scale and also ratio between</p>
<p>the top 10 percent and bottom 10 also</p>
<p>the alteration between top 10 medium</p>
<p>but uh we observed that LD player 2 as</p>
<p>base R and mega PRS they rank the</p>
<p>highest amount in most of the comparison</p>
<p>and to summarize in this talk I show how</p>
<p>to calculate AUC various explaining</p>
<p>liability skill and also ratio backhand</p>
<p>and based on the comparison we made we</p>
<p>observed that</p>
<p>for security disorders which are very</p>
<p>polygenic all the methods are perform</p>
<p>similar but some are rank higher than</p>
<p>others for example I would refer to as</p>
<p>base R and mega PRS</p>
<p>so the result actually here is part of</p>
<p>this study which is published recently</p>
<p>published and in this paper we also did</p>
<p>the comparison for major depression and</p>
<p>also other sensitivity analysis and we</p>
<p>also provide the code for each to run</p>
<p>each other method and for each</p>
<p>comparison and also each of the</p>
<p>statistics</p>
<p>statistic use for comparison</p>
<p>and with this I would like to give a big</p>
<p>thank you to Professor Norman Ray who</p>
<p>always give me a huge support of</p>
<p>whatever I needed and thanks to all</p>
<p>other pccg members</p>
<p>and thank you all</p>
<p>foreign</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
          // target, if specified
          link.setAttribute("target", "_blank");
          // default icon
          link.classList.add("external");
      }
    }
});
</script>
</div> <!-- /content -->



</body></html>