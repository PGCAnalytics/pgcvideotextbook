---
title: "Software Tutorials: GWAS (Video Transcript)"
---

# Genome-wide Association Studies {#sec-video1}

**Title**: Genome-wide Association Studies

**Presenter(s)**: Hailiang Huang

My name is Hailiang Huang. I'm a statistical geneticist at the Broad Institute. I'm very happy today to do this she was tutorial with you so the first thing uh let's clone the GitHub I have created for you the GitHub repository that I have created for you with the data and tutorial materials please type this in your comment line in your command line in the terminals repeat clone and the location of the repository under my first and last name and G1 tutorial it will take a while but it should be very fast all right this is done let's get into the location

uh the folder you just cloned and have a look at all the files I would recommend you to open the gos on the squad tutorial and you know text editor so that you can just simply copy and paste everything from the text editor to your terminal all right I have opened this in one of my text editors and just notice uh the few information here the information here uh so first where you can download this GitHub repository in case you have any questions feel free to contact me and this for this tutorial you have to install the p-link 2 which can be retrieved here and R which can be downloaded from the r project website and also we use the qqman package in R so please have it installed in your R program

all right let's get started so first let's take a look at the data so this is the one of the two data fires it has individual level genotype if you use the head command you will see the first few the first few rows in that file uh the first two columns are the individual ID and the family ID the next two columns are the IDS for the parents of that individual and the sex and the diagnosis after the first six columns the rest of the columns have the genotype of that individual which you can easily tell and in case that genotype is missing we have zero to indicate the missing genotype and the other file is called a map file it basically has a list of snips and each row here corresponds to the you know every two columns here uh the map one-on-one so that uh peeling will know which snip this data set is

all right so the first thing we typically do after we see this data set is to convert this to Binary what you saw is very convenient for you to you know take a look at what the data is but there is a format called binary format that is more efficient in terms of the storage and Analysis all you need to do is to indicate your input file which can be done by the dash dash file and you can use the command dash dash makebet to convert this text file to Binary format and then indicates you know where you want this binary format to be written to in the dash dash out command all right this is done and if you compare the size of the fire before and after uh the conversion to the binary file here is how you can do that so you can see that before conversion you have 77 megabyte for the fire with the genotypes and after the conversion you only have 4.7 megabytes so the conversion feels still more than uh you know 10 times space

all right so let's start some QC the first thing we want to do is to remove the individuals that has a high missing rate you can do that by using the appealing uh dash dash missing command the dash dash B file simply it has p-link where you know the input files should be the dash dash out Health Link where you know peeling has to write the output bias to so you use the dash dash missing command to calculate the business and here is what the output file looks like it basically has a list of individuals with their missing rate and here uh you know the auto command filtering on the mixing rate in the fixed column uh you can tell we have two individuals with more than two percent missing rates missing rate typically a high mixing rate typically indicates problems in the sample quality so it's not a bad idea to remove the samples with high missing rate

all right so uh the next we want to do is to remove samples with high heterozygosity rate and this can be done by calculating the hydrode using the dash dash head command and if you take a look this is the output from that calculation it's basically a list of individuals with their heterozygosity rate I have prepared a r script for you to put together individuals that fail either the missing weight test or the heterozygosity test and for considering the time I'm not going to go into the details of that R script but you can just call that our script by doing this and that script where we will write a list of individuals with extremely high missing rate or with heterozygosity rate that are either too high or too low to this fire and all you need to do is to use a dash dash remove command to remove that list of individuals and tell pilling that you want to write the new data file to this file

all right so we have removed uh individuals that have a high missing rate but we also have to do that for the Snips for to remove the Snips that have a high missing uh genotype rate uh we can do that by leveraging the other output file from the dash dash missing EP link and that is a high missing bio which has a list of the Snips with their missing weight and we decide to use five percent as a threshold here we write all these things we want to exclude in this fire and we use the p-link dash dash exclude to remove such sleeves so pilling remove removes individuals and the p-link dash dash exclude exclude the snips all right this is done we now have the second iteration of QC with the Snips with high missing weight removed the next we want to do is we want to find Snips with high with differential missing rate uh the missing rate is different in cases versus in controls we do so by using the dash dash test Dash missing here and take a look at what is inside you have a list of the Snips with their missing weight in cases as affected and controlled as unaffected in this case we don't want to use the p-value because the sample size is too small for the p-value to be helpful we simply want to remove the Snips with five percent difference between cases and controls and this can be done by subtracting the difference of subtracting for the difference between the missing weight in cases versus controls and using five percent as a threshold we write everything we want to exclude here and we will Ex we will uh you know remove that lift up the Snips here

all right the last thing we want to do on the Smiths is the Hardy Weinberg equilibrium test and this can be done by using the dash dash Hardy command this is a list of snips with their Hardy Weinberg marker P value and we want to only use the Hardy Weinberg test for the unaffected samples and we want to use the p-value of 10 to the minus six as a cutoff we filter for such Snips and we use peeling to remove those snips

all right now you have a data set that are relatively clean for individuals and for sleeps what's next the next is to remove individuals that are related to each other but to do that we need to create a data set that is uh independent in terms of genetic variance we can do so by using the dash dash independent pairwise and this is a bunch of current parameters go into the LG pruning process including the window size the step of the window that is moving and also the threshold you want to use for configuring two variants that are in LD with each other

so after you run this command you will have a list of snips that peeling would recommend you to keep as independent Snips this is in the fire called Dash prune Dash in all you need to do is to generate a new data set with only these variants by using the dash dash extract command that will give you a independent you know set of markers and in a in a new p-link binary format and then you want to use the dash dash genome file to calculate the relativeness between samples and take a look at what is in that output file which is just a pair list of pairs of individuals with their relatedness typically a related means of hi-hat greater than 0.2 indicates something concerning we want to capture that pairs using this command write them to this file and we use the peeling dash dash remove as we did before to remove such individuals

all right so let's take a look at how many variants and how many individuals we have removed it thinks that with the data is pretty good before QC we had over 100k 100 individual Snips and 200 subjects after QC we got rid of less than 1000 variants and eight variants

all right so now let's take a look how well we did in the QC uh we use the word count Dash L to count how many variants we have before an after QC and how many individuals we have before and after QC so it looks like the data was pretty good um his accusing removed roughly 1000 variants which is very normal uh we started with over a little over a hundred thousand and we ended with uh 99.7000 which is not bad we removed eight individuals from the 200 to 192 which is not bad as well all right so far so good so what next the

*Principal Components Analysis (PCA)*

next is to generate uh the principal components so that we can include them as covariates to control the population structure so this is how you do that you need to create a new independent data set independent in terms of the variance and you want to generate that based on the latest file you have created remove removing the related individuals you do that similarly as you did before using the dash dash independent pairwise command then you create a new file uh the file that only has the independent variance like this and then you print you create the principal component plot sorry you created the print you perform the principal component analysis using the dash dash PCA command all right uh this is output from that command the uh dot angle value and Dot eigenvector the dot eigenvector has what you need for the downstream analysis so let's take a look it has a list of individuals with principle components a lot of them

*Running GWAS association test*

so what you do next is to perform your G was with these principal components and this can be done using the dash dash logistic because this is a case control study you want to use the logistic regression the height cover basically asks appealing not to write the coefficient for the covariance because you for this study you only hear about the variance uh the coefficient for the variance not the covariance um this is uh this is to help the peeling where your covariance is which file has a covariance and this is to calculating how many components how many principal components you should include as the covariance and here we do one to ten all right let's do it

all right it's done so the dot logistic file has a list of all the variants with their auth ratio standard error the confidence interval and the p-value this is exactly what you need from a givas and we could use this little script I wrote in R to do some visualization that script also calculates the genomic genomic inflation factor which is only 1.06 looks like the genome has been very well controlled and let's look at the two figures it has generated the Manhattan plot let's open it and the qqplot all right so this is a Manhattan plot this is the QQ plot you can tell that the genome The genome has been well controlled you don't really have no inflation but unfortunately for this particular given study we don't have a genome-wide significant signal uh for now we do have you know one signal uh surpassing the suggestive significance threshold uh suggesting that we may need more samples to uh do to to to for this device to identify the same significant Locus all right that is a very quick tutorial and I hope it helps for free our research thank you for your attention

------------------------------------------------------------------------

# Genotype QC {#sec-video2}

hello I'm Johnny Cohen and in this brief presentation I'm going to discuss some key points concerned with running quality control on genome-wide genotype data which is a common first step in running a g was I'm going to provide a theoretical overview addressing the overarching reasons why we need to do QC highlighting some common steps and discussing a few pitfalls the data might throw up I'm not going to talk about conducting imputation or G was analyzes or secondary analyzes nor am I going to talk a great length about the process of genotyping and ensuring the quality of genotyping calls I'll similarly not go into any deep code or maths however if you are starting to run your own QC and analyzes I recommend the pgc's rickapili automated pipeline as the starting point there are also some simple scripts on my group's GitHub that may be useful as well they follow a step-by-step process with codes and explanations we're currently updating this repository so look out for some video tutorials there as well so here is our starting point I'll be using this graph on the top right several times through this talk and this is a genotype calling graph with common homozygotes in blue heterozygous in green and rare homozygotes in red hopefully your data will already have been put through an automated genotype calling Pipeline and if you're really lucky and overworked and underappreciated bioinformatician may have done some manual recalling to ensure the quality of the data is as high as possible but in point of fact the data you'll be using won't be in this visual form but rather as a numeric Matrix like the one below that Snips and individuals this might be in the form of a blink genotype file or its binary equivalent or it's in some similar form that can be converted to the blink format and where we want to go is clean data with variants that are called in the majority of participants in your study and won't cause biases in Downstream analyzes that should give a nice clean Manhattan pot from G was like the one below rather than the Starry Night effect of this poorly Q Siege Manhattan blunt above however something I'd like to emphasize across this talk is that QC is a data informed process and what works for one cohort won't necessarily be exactly right for another good QC requires the analyst to investigate and understand the data often the first step is to remove rare variants and this is because we cannot be certain of variant calls consider the variance in the circle on the right are these outlying common homozygates or are they heterozygates we cannot really tell because there aren't enough of them to form a recognizable cluster typically we might want to exclude variants with a low minor allele count for example five there are many excellent automated calling methods to increase the amount of certainty you have in these variants but it's also worth noting that many analytical methods don't deal well with rare variance anyway again the demands of your data determine your QC choices it may be more useful for you to call rare variants even if you're uncertain of them or you may wish to remove them and be absolutely certain of the variance that you retain next we need to think about missing data genotyping is a biochemical process and like all such processes it goes wrong in some cases and a call cannot be made this can be a failure of the genotyping probe or poor quality of DNA or a host of other reasons but such calls are unreliable and they need to be removed missingness is best dealt with iteratively to convince you of that let's examine this example data we want to keep only the participants which are the rows in this example with complete or near complete data on the eight variants we're examining which here are shown in the columns so we could remove everyone with fewer than seven snips but when we do that oh dear we've obliterated our sample size so instead let's do things iteratively so we'll remove the worst snip again variant 7 goes and then we remove the worst participant bye bye Dave then we remove the next word snip so that's snip two and now everyone has near complete data and we've retained nearly all of our cohort so this was obviously a simple example how does this look with real data so here we have some real data and it's it's pretty good data most variants are only missing in a small percentage of the cohort but there are some that are missing in as much as 10 of the cohort so let's do that iterative thing removing variants missing in 10 of the individuals and then individuals who have more than 10 missing variants and then nine percent and so on down to one percent when we do this the data looks good nearly all of the variants are zero percent messiness and those that aren't are present in at least 578 to the 582 possible participants and we've lost around 25 participants for about 22 and a half thousand snips but what if we didn't do the iterative thing and we just went straight for 99 complete data so when we do that the distribution of variance looks good again arguably it looks even better and we've retained an additional 16 000 variants but we've lost another 40 participants which is about six percent more of the original Total than we lost with the iterative method typically participants are more valuable than variants which can be regained through imputation anyway but this again is a data-driven decision if coverage is more important than cohort size in your case you might want to prioritize well genotyped variants over individuals so we've addressed rare variants where genotyping is uncertain and missingness where the data is unreliable but sometimes calling is simply wrong and again there are many reasons that could be we can identify some of these implausible genotype calls by using some simple population genetic theory so from our observed genotypes we can calculate the allele frequency at any bioelix nip we've called so here the frequency of the a allele is twice the frequency of the AAA calls those are our common homozygots in blue plus the frequency of a b calls are heterozygous in green and we can do the equivalent as you see on the slide for the frequency of the blade knowing the frequency of the AE and the B allele we can use Hardy and weinberg's calculation for how we expect alleles at a given frequency to be distributed into genotypes to generate an expectation for the genotype to be expect to observe at any given allele frequency we can then compare how our observed genotypes I.E the blue green and red clusters fit to that expectation and we can test that using a chi-squared test now Harley Weinberg equilibrium is an idealized mathematical abstraction so there are lots of plausible ways it can be broken most notably by evolutionary pressure as a result in case Control Data it's typically best to assess it just in controls or to be less strict with defining violations of Highly Weinberg in cases that said in my experience genotyping errors can produce very large violations of Harvey Weinberg so if you exclude the strongest violations you tend to be removing the biggest genotyping errors the previous steps are mostly focused on problematic variants but samples can also be erroneous one example is the potential for sample swaps either through sample mislabeling in the lab or correctly entered data in phenotypic data these are often quite hard to detect but one way to detect at least some of these is to compare self-reported sex with X chromosome psygosity which is expected to differ between males and females in particular males have One X chromosome they're what's known as hemizygous so when you genotype then they appear to be homozygous on all Snips on the X chromosome females on the other hand have two X chromosomes they are holozygous and they have a normal X distribution centered around zero which is the sample mean in this case you could also look at chromosome why Snips for the same reason however Y chromosome genotyping tends to be a bit sparse and is often not of fantastic quality so there are benefits to using both of these methods it's also worth noting that potential errors here are just that potential where possible it's useful to confirm these with further information for example if there isn't a distinction between self-reported sex and self-reported gender in your phenotype data then known transgender individuals may be being removed unnecessarily the aim here is to determine places where the phenotypic and genotypic data is discordant as these May indicate a sample Swap and this might indicate the genotype to phenotype relationship has been broken and that data is no longer useful to you average variant homozygosity can also be applied across the genome where this metric is sometimes referred to as the breeding coefficient it's called that because High values of it can be caused by consanguinity related individuals having children together which increases the average homozygosity of the genome there can also be other violations of expected homozygosity so it's worth examining the distribution of values and investigating or excluding any outliers that you see examining genetic data also gives us the opportunity to assess the degree of relatedness between samples for example identical sets of variants imply duplicates or identical twins 50 sharing implies a parent Offspring relationship or siblings and those two things can be separated by examining how often both alleles of a variants are shared specifically we would expect parents and Offspring to always share one allele at each variant whereas whereas siblings may share No alleles they may share one allele or they measure to it lower amounts of sharing imply uncles and aunts and then cousins and grandparents and so on down to more and more distant relationships in some approaches to analysis individuals are assumed to be unrelated so the advice used to be to remove one member of each pair of related individuals however as mixed linear models have become more popular in G was and mixed linear models are able to retain and include related individuals in analyzes related individuals therefore should be retained if the exact analysis method isn't known again it's worth having some phenotypic knowledge here unexpected relatives are a potential sign of sample switches and need to be examined confirmed and potentially removed if they are truly unexpected and once again it's important to know your sample the data shown in this graph does not despite what the graph appears to suggest come from a sample with a vast amount of cousins instead it comes from one in which a minority of individuals were from a different ancestry and that biases this metric I'll talk a little more about that in just a moment relatingness can also be useful for detecting sample contamination contamination will result in a mixture of different dnas being treated as a single sample and this results in an overabundance of heterozygote calls this in turn creates a signature pattern of low level relatedness between the contaminated sample and many other members of the cohort these samples should be queried with the gene typing lab to confirm whether or not a contamination event has occurred and potentially be removed if an alternative explanation for this odd pattern of inter-sample relatedness can't be found finally a word on genetic ancestry because of the way in which we have migrated across our history there is a correlation between the geography of human populations and their genetics this can be detected by running principal component analyzes on genotype data pruned for linkage to equilibrium for example this is the UK biobank data you can see subsets of individuals who cluster together and who share European ethnicities other subsets who share African ethnicities and subsets who share different Asian ethnicities and in a more diverse cohort you would be able to see other groupings as well this kind of 2D plot isn't the best way of visualizing this for example here it isn't really possible to distinguish the South Asian and add mixed American groupings and you don't get the full sense of the dominance of European ancestry data in this cohort Europeans in this case account for around 95 of the full cohort but because of over plotting I.E the same value as being plotted on top of each other in this 2D plot you don't really appreciate that looking across multiple principal components helps for that ancestry is important to QC many of the processes I've talked about rely on the groups being assessed fairly being fairly homogeneous as such if your data is multi ancestry it's best to separate those ancestries out a rerun Key C in each group separately so that was a brief run through of some of the key things to think about when running QC I hope I've got across the need to treat this as a data informed process and to be willing to rerun steps and adjust approaches to fit cohorts although we've got something resembling standard practice in genotype QC I think there are still some unresolved questions so get hold of some data look online for guides and automated pipelines and enjoy your QC thank you very much for listening I'm doing a q a at 9 30 EST otherwise please feel free to throw questions at me on Twitter where I live or at the email address on screen which I occasionally check thank you very much

------------------------------------------------------------------------

# Tractor: GWAS with admixed individuals {#sec-video3}

**Title**: Tractor: Enabling GWAS in admixed cohorts

**Presenter(s)**: Elizabeth Atkinson

hi thanks so much for your interest in using tractor I hope this tutorial helps get you started in doing ancestry where g-was admix cohorts so here's just a brief outline I'll start with some motivation go into the overview of our statistical model and then talk about implementation of the actual tractor code

so to start off with our motivation I want to reiterate a point that is now thankfully becoming common knowledge in the gwas community that is that the vast majority of our association studies are actually conducted on European cohorts and if we look further at this breakdown of the small kind of wedge of the pie of who's not European we can see that there's actually only a few percent from Recently admixed populations such as African-American and Hispanic Latino individuals groups that collectively make up more than a third of the US populace and just to make sure we're all on the same page and add mixed individual refers to somebody whose ancestry is not homogeneous but rather has components from several different ancestral groups there are actually many more advanced individuals out there whose samples have already been collected and genotyped or sequenced alongside phenotypes but they're not making it into this figure due to being kind of intentionally excluded for being admixed so there's really a pressing need for novel methods to allow for the easy incorporation of admix people into Association studies so admix people are generally removed due to the challenges of accurately accounting for their complex ancestry such that population substructure and stratification can seep in and bias your results and in the context of GEOS this means the potential for false positive hits that are due to ancestry rather than a real relationship to the phenotype

so in a global collection such as this example from the PGC even controlling for PCs which is the standard way that admixture is attempted to be kind of controlled for there's still so much variability in the data that there's a lot of concern over false positives it's because of this researchers will often sort of draw a circle around people who are deemed homogeneous enough to included in study and everyone else is excluded so this nearly always ends up resulting in European individuals being included as they generally represent the bulk of samples that have been collected it's the main motivation of the project I'm talking about today was to try to rectify this issue and develop a framework to allow for the easy incorporation admits people into Association studies

so with tractor we are handling this concern by directly incorporating local ancestry local ancestry tells us the answers for origin of each particular haplotype tract in an advics individual at each spot in their genome so in this three-way admix individual the y-axis are the autosomes the position along them is on the x-axis and because humans are diploid the top and bottom half of each chromosome is painted differently and you can see that each tract in this individual is colored according to the ancestry from which it derived

so the intuition behind tractor is that to correct from population structure we effectively scoop out the tracks from each component ancestry to be analyzed along alongside other ancestrally similar segments so we do this by tracking the ancestry context of each of the alleles for each person at each spot in their genome so note that here I'm only showing this red ancestry but the same thing is going to be happening for the blue and green ancestry the statistical model built into tractor tests each submit for an association with the phenotype by splitting up the ancestry context of the minor alleles so in the two-way advics example we have our intercept of course and then we include terms for how many copies of the index ancestry there are for this person at that spot in the genome so say if you have zero one or two for example African alleles at this location as well as how many minor alleles fall on your ancestry a versus ancestry B backbone so those are the first three x's in these terms this is what correct for the fine scale population structure if there were differing allele frequencies for the ancestries at this spot in the genome as we expect there to kind of routinely be to the different demographic histories modern day human populations then we're no longer going to be confounded by this as we've sort of deconvolved them um so the mind really you'll see it on each ancestry backbone are kind of properly scaled to their background expectations note that while I'm showing two-way admixed example here this model can also scale easily to an arbitrary number of ancestries by addition of terms as well as allowing for the ready inclusion of all of your necessary covariants

so I already went over how our model corrects for the fine scale population structure at the genotype or haplotype level which is what's allowing the inclusion of atomix people in a well-calibrated manner in geost but it also has some other nice benefits I'd like to briefly mention we've built in an optional step to recover long-range tracks that we find to be disrupted by statistical phasing by using our local ancestry information and with respect to gwas our novel local ancestry aware model can improve your results in a variety of ways most notably through boosting GEOS power generating ancestry-specific summary statistics and helping to localize your G1 signals be closer to the causal snips so we really hope that this framework should Advance existing methodologies for studying advics individuals and allow for better calibrated understanding of the genics of complex disorders in these underrepresented populations

so I'll now shift to giving a brief walkthrough of the steps involved in implementing the tractor pipeline so there are three different steps the first one being optional for efforts that require complete haplotypes I'm going to go through each of them kind of individually but the first three steps are implemented as Python scripts and step three the gbos model is sort of the recommended implementation is in the cloud native language scale there's a lot more documentation and description of these steps on our GitHub page so please check out the wiki there for more information as well as a Jupiter notebook to kind of walk through the steps of the pipeline

so before running local ancestry aware GEOS you need to call your local ancestry we recommend the program RF mix for this which is described in their paper maples at all 2013 as well as their GitHub repo which I'm linking to here so the success of tractor really relies on good local ancestry calls so it's really important to ensure that your local ancestry inference performance is highly accurate before you try to run a tractor-gos

um just to make sure you know we're kind of all ready to go I wanted to show an example command for launching an RF mix run on one chromosome using some publicly available data um so this is sort of the you know basic parameter settings that we've come up with as being kind of Ideal for our tractor pipeline again more details on the Wiki page

but what I'd like to spend more time on is the actual tractor scripts so this first step um in our pipeline is what's going to detect and correct for switch errors from phasing by you using our local ancestry calls you can see our supplementary information and extended data figures one and two in our manuscript for some additional context around what we mean by this um so this step uses RF mix ancestry calls as input and is implemented with the script unkink MSP files.pi which tracks those the switch locations and corrects them

so the output will consist of two files a text file that documents the switch locations for each individual so that'll be sort of your input MSP file name suffixed with switches and a corrected local ancestry file again the same input name but this time suffixed with unkinked as in unkinking a garden hose we're kind of straightening out those switch errors here's your kind of example usage you should just need to point to the MSP file name without this msp.tsv suffix I'm going to call this script so this is one of the default output um files from RF mix

next we also need to correct switch errors in our phase genotype file and we're expecting VCF format for this so this is what's going to actually recover the Fuller haplotypes and improve our long-range track distribution this step is implemented with the script unkink genophile.pi and expects his input the phased VCF file that went into RF mix as well as that switches file that we generated in the previous step so the switches file is basically used to determine the positions that need to be flipped in your VCI file tractor also expects all of your VCF files to be phased and it's recommended to strip their info and format annotations prior to running just to ensure good parsing and again Step One is optional and won't affect your GEOS results but can be useful for efforts that require complete haplotypes

all right so in step two we extract the tracks that relate to each component ancestry into their own VCF files and calculate the ancestry haplotype and Alternate wheel counts for each component answer for your admixed cohort um so this step is implemented with the script extract tracts.pi and expects as input the stem name of your VCF you can have it just be unzipped or gzipped is also allowed as well as your again your RF mix MSP files so you can input those unkinned files from the previous step if you chose to correct switch errors or you can go straight from your original data if you did not this script now accepts uh an arbitrary number of ancestry calls which is very exciting so you can specify the number of component ancestries with the dash dash num inks flag um the default is two you can change it to however you know multi-way your population was if your VCF file is g-zipped also include the flag dash dash zipped if not leave that flag off

um so there are multiple files output from this step including the counts for each individual at each position for these three different pieces of information I'm listing here so firstly you'll get the number of copies of each ancestry at that spot so if you have zero one or two copies of you know ancestry a b c however many ancestors are in your data um secondly you'll get the number of alternate allele counts on each of those ancestral backgrounds and thirdly you'll get VCF files for each of those ancestries containing only genotype information for that ancestry's tracks so these will be sort of um BCF files that will contain a bunch of missing data will kind of blank out the other ancestories and it'll also contain some half calls for instances where there's you know sort of heterozygous ancestry at a spot

all right so our ancestry dosage output can now be used in our local ancestry aware g-was model the recommended implementation of the tractor joint analysis model uses the free scalable software framework for kale hail can be run locally or on the cloud I'm going to be showing a cloud implementation here and it can be useful to build and test your you know your pipeline in a jupyter notebook when applying it to your own cohort data so for this reason we Supply an example Jupiter notebook written in hail on our GitHub which can be adapted for user data the specific commands to load in and run linear linear regression on our ancestry D consult file will be as follows so here is the specific sort of Suite of commands in hail that will read in and format our tracker dosage files just to make it a little nicer running things Downstream this is a little bit more involved in terms of loading files in than a standard VCF or Matrix table this is because we need to run g-was on entries rather than rows so for each ancestry and dosage Capital type file will be kind of annotating that information in rather than just sort of one piece of information per variant and this will be come a little bit clearer in the following slides but that's why there's like you know a couple extra steps in this reading in dosage files um proportion

so next we'll join our ancestry and haplotype dosage files together onto a hail Matrix table so we do this by basically annotating the haplotype counts and any of the other ancestry information we have onto our first ancestry ancestry zero and here we're creating a new combined hail Matrix table called Mt which will have all of this information in it basically we'll have kind of our original ancestry zero dosage of information and then structs for each additional term that we're annotating on so here I'm showing examples for two-way and three-way administ example and the two way we're annotating structs for the ancestry one dosages as well as our hap counts for our ancestry zero so our index ancestry haplotek counts these are both um these all three of these are default outputs from our earlier tractor steps so if you want to have more multi-way admixed populations included you just need to add additional terms for each new ancestry so one new dosage term for each additional ancestry and N minus one terms for the haplotype counts

so now we can load in and annotate our Matrix table with our phenotype and covariate information and from here on we are now utilizing standard hail commands the only thing to be you know careful about with loading in your phenotypes is to make sure that you're keying by the same ID field that will match the names in your genotypes file and again note that I'm doing this in the kind of Google Cloud example so I'm pointing to the location of this file in my Google bucket and finally we can run the linear regression so for a single phenotype for example total cholesterol or TC as shown here along with our relevant covariance so here in addition to our intercept of one we have our half counts term and our two terms for our two-way admix population so our ancestry zero and one dosages and then we also are putting in all of our covariates that we want here so in this case I'm using sex age blood dilution level which is kind of a phenotype specific covariate and then an estimate of the Global African ancestry fraction that we got prior to this from using the program and mixture so we recommend putting in an estimate of your Global ancestry in addition to these sort of local ancestry specific terms just to account for if there's sort of a relationship to the phenotype with their overall ancestry proportions and we find that the direct measure of global ancestry is a bit more accurate than PCS so this is sort of the recommended strategy so doing that we are basically annotating each of the variants in our Matrix table with the results of the G wasps the results are again saved as a struct here named TC our phenotype um and this will contain pieces of information for each of the summary statistics that we listed um you know here including the beta effect size the standard error and the p-value within each of these there's going to be an array of indexable values that's in the order of your terms so the index of zero is going to be your Intercept in the example I listed you know prior to this one is going to be your haplotype dosage two is your ansys dosage three would be ancestry one and then covariance would be four until however many covariates that you have so you can pull out summary statistics for every term in your model I'm going to allow for kind of easy comparison they're all going to be annotated into kind of one big struct that's just indexable by what position they were in your model to run multiple phenotypes in batch you can make a list of phenotypes first and then cycle through them annotating The Matrix table with a multiple structs that will each be named um you know the names of your genotypes so again all the information can be stored just in that one Matrix table in our Jupiter notebook we also provide some code to generate Manhattan and QQ plots you can select the term you would like to plot by its index as we talked about in the last slide and you can use the hail commands plot dot Manhattan and plot dot QQ to visualize the results so here we're pulling in our TC struct from the p-value we want the um the third term or the index of two and here is what those plotting commands will produce with those you know the the settings by launching those commands you'll end up with figures that look exactly like this um great so with that I hope that this was helpful thanks so much again for your interest in tractor and feel free to leave questions on our GitHub repo or email me with any thoughts I also have many people to thank of course for this project including my ko1 funding from NIMH and I'll also direct you to our our paper published several months ago on nature genetics which has a much more thorough description of our method and some other considerations again our code is freely available up on GitHub alongside a Wiki tutorial to help you get started with tractor there as well so thanks very much and I hope that you enjoy using tractor

------------------------------------------------------------------------

# Genotype Calling and Imputation with MoChA {#sec-video4}

**Title**: Genotype calling/imputation on the cloud: MoChA pipleine

**Presenter(s)**: Giulio Genovese

hello everybody today I'm going to talk about the bunker Pipeline and how you can use it to perform phasing and imputation of a DNA microwave data in the cloud so as an introduction MoChA pipeline what is phasing and imputation well um for every genomine Association study where we work with DNA Micro Data um we always have to go or process the data through three main steps the first step is to call genotypes from microarray intensity data then we have to retrieve the Apple type structure of the samples by phasing and then we have to impute the missing variants that were not present in the immigrate by imputing by comparing the Apple types that we face with apple types of another reference panel of samples for which we have more information and the phasing and imputations in

particular are two steps that require a fair amount of computations and that parallelization of computations um do they have the high computation requirements and there have been many pipelines out there to perform these two steps but today I'm going to talk about the mocha pipeline which consists of two different uh scripts one is the mocha riddle and one is the Widow the first one we performs uh calling genotypes and basically in the second one performs imputation these clips are written in Widow which is a language for workflows and allows you to Define very complex uh segments of tasks um and we know in particular corresponds to a set of tasks and a description of the order in which tasks have to be performed and in the writing the task we have to Define which input file we need which commands we're going to run on those files and which output file so you expect and the nice thing about the writing pipelines in the the video language is that they can be run on any computational environment um in particular uh you can do this thanks to the Cromwell workflow mg which is a a an engine developer the broad Institute and the idea is that you can input your Widow pipeline into the Cromwell server and the Chromebook server will take care of dispatching all the required jobs to different nodes in a highly parallelized fashion uh and this could be nodes on an HPC cluster like an sglsx.com cluster that are common in academic environments or maybe in cloud computing cluster like Google Cloud Amazon web services or Microsoft and this is all transparent to the user that doesn't have to worry about the technicalities of the cluster implementation when riding a window pipeline um and so what features does the mocha pipeline for phasing reputation have that might be not available in other similar pipelines well the first feature is that it follows a minimalistic approach so the user always has to provide a minimal amount of necessary inputs to run the pipeline it can take a very different input files but in particular it can process a raw Illumina input files also known as either files without requiring uh to be a processor in alternative ways it uses a very state-of-the-art software for phase in a relationship between five it allows you to use the reference Genome of your choice so you can just request the grch38 as the reference that you want to use and once you have set up a Chromebook server or you can actually even run the pipeline in Terra if you if you want it's very easy to use and it scale so very nicely and seamlessly to a course of a biobank size and so in particular you can run it on the UK biobank and the phasing part will run for less than 100 dollars and facing imputation overall will run in less than 40 hours if you run it in Google Cloud um and the reason I actually developed this phase English program was a support analysis related to music chromosome alterations so in particular when you run this pipeline you're also want to get a as an output a list of constitutional mosaicanism alterations in the samples new process

*Running the pipeline*

so how complicated is it to run the mocha pipeline hopefully I can convince you that it's not that complicated um so the running a pipeline mostly means that you need to define a set of input files let's say that we're going to run the pipeline in either mode and we're going to provide a set of either files um which are again illuminate intensifies we're gonna have to provide that and you're also going to have to provide a table with a list of samples and a table with a list of batches and then in particular I'm going to have to provide a configuration file in Json format indicating in which way the pipeline needs to be run and once we input that into the Chrome wall server together with the mocha Widow escape the output we expect is going to be a set of VCF phase files and an output table with the list of these files so this is an example of what the input for a run is going to look like here we have a sample table each row is a sample here we have a batch table on the bottom left where each row is a batch and for each sample we have to Define which batch it belongs to and which green and red eye dot file uh it corresponds to and for each batch we have to define the three different manifest files from Illumina which are the BPM CSV and EGT which inform the the software about which variants have been tested and about what the Clusters original type look like and the configuration file which is in Json format is going to define a just a minimum set of parameters like the name of the cohort the mode which in this case is the ielt mode because we are inputting either files um in this case because it manifest files are in grch37 uh um coordinates we will ask the pipeline to realign the Manifest files this comes very handy when Illumina does not provide a manifest file for glch 38 and then we can provide information about how much parallelization we want to achieve in phasing so we can Define the window size the maximum window size for phasing which in this case would be 50 centimorgan but we can decrease that if we want to parallelize the computation even more and then after that we need to provide a

file uh URL addresses for where to find all the files for the dimension of reference for the Illumina manifest files for the islands and also for the two tables and and that's it once we have these three files so we can just submit the mocha with all together with the configuration file into the Chromebook server with a command like that and then if everything goes well we would obtain as an output a table with a list of VCF files and the VCF files that will be generated will be uh divided across batches and so for each batch we're going to get a VCF file with the phase genotypes and probe intensity and then also gives you a file just with phase genotypes useful for imputation then we'll also get some summary plots that will visualize the call rate across samples and maybe some statistics that will indicate the possible contaminations and then also other summary plots indicating probe intensities for the sex chromosome and and other that I'm not sure enough the cost that I've observed if you run it in Google cloud is of about one dollar or five thousand samples for GSA like Andries um similarly when I'm in the position pipeline is not that different especially after you've run the mocha riddle pipeline uh we're gonna have to input a set of phase UCF files one per batch we have to gonna input a table listing the batches and then again a configuration file listing some important option and again this will be input into the Chromebook server together with the impute Widow script and we're going to expect a bunch of inputed VCF files out if everything runs correctly uh this is an example of what an input would look like and again a batch table file in WS format where each row contains information for one batch and then a configuration file here on the right where again we can Define uh parameters like the name of the cohort which chromos we wanna impute if we don't want to talk chromosome if you omit these all 20 chromosomes will be included and the information about for example which kind of information we want in the output to included BCF files do we want the dosages and then we can ask for dosages or do we want you know type probabilities or maybe we don't want either and we just want the genotypes by default the limitation pipeline will use the pathogenomic coverage reference panel but you can also input a reference pen of your choice if you want the only requirement is that you have access to the to the reference panel and that it's the structure in VCF format to split across the 23 different chromosomes and again a simple command like this will submit the job to a running chroma server where we indicate that we're gonna run the middle script together with the Json file with the configuration up here if everything goes well we're gonna get as an output again a table with a list of VCF and it's gonna we're gonna obtain one VCF for each batch and for each chromosome that we have asked to compute um the pipeline like as I said it uses input5 to run optional you use bigger five it should need to run it in a commercial settings and you know don't have a commercial license for compute file fine but input file is free for academic research and the cost if you use the pathogens project reference panel is about one dollar for a thousand samples when you run it in Google Cloud

*Applications*

now I'm going to show you very quickly some applications of the mocha pipeline uh and so first of all uh this is a list of biobanks across the world where we were able to run the scripts um thanks to collaborators that have access to the data on some of these biobanks uh I personally have access to the UK biobank the Massachusetts General brigant biobank data um and then something you might notice that because of different restrictions with different biobanks we actually have to run the pipeline on very different array of computational environments from lsf and slurm clusters to Google Cloud European clusters um but this is a testament the fact that the pipeline that we structure is actually can run uh almost any biobank

um moving on there will be more pipelines in development there is actually a collision score pipeline that can be run right on the output of the application pipeline if you're interested in computer scores uh and there is a working progress Association pipeline that will run regioning in a very distributed fashion and will take again as an input the output of the more computation pipeline part of the reason why I've been developing this set of pipelines is uh that about five years ago I started to work with Polo on a project um in the UK biobank and to our surprise we found that there was a lot of interest in biology to be discovered by doing so and I've niched Furniture paper after that I started to realize that there was actually quite some interest in running this kind of analysis with machine clubs and alterations on biobands but the technical skills to actually do that was a really prohibitive for many users and so I decided to try to package the analysis that people were done into a easy to reproduce uh framework and that led to development of the mocha pipeline however although it is designed to scale to biobank size data set it can be applied also to a smaller cohort and this is an example of a work from Metro Sherman that applied the framework on autism problems and he was able to show that Jose comes alterations are more common in autism programs compared to their unaffected siblings even if this explains all a very small fraction of autism problems

and um to conclude the mocha with the pipeline supports virtual real computational environments it follows a minimalistic design and it can input either files it can be used on grch38 without further corridors and they can scale to biobanks of any size and if you're interested in CMV analysis it actually will provide germline and somatic outputs for further starting that I would like just to acknowledge the many people that have helped in developing this framework to start I want to thank my supervisor Stephen McCarroll that has supported me for many years uh for a law that has run many of the initial analysis in the UK by Bank showing that this framework actually is important and then the many many many collaborators and users of the pipeline that have provided the incredibly important feedback with that I'll conclude and understand that will be going to be a q a session after this so I'll be available for questions if you have any thank you for listening and I hope this pipeline might be useful to you

------------------------------------------------------------------------

# SAIGE: Family-based GWAS {#sec-video5}

**Title**: Genetic association studies in large-scale biobanks and cohorts

**Presenter(s)**: Wei Zhou

Hi, I'm Wei Zhou. In this presentation I'mgoing to give a quick tutorial on a program called sage which is for scalable and accurate implementation of generalized mixed models it was developed for conducting genetic Association studies in large-scale biobanks and cohorts we will firstly go through several challenges of g-was in large-scale cohorts and biobanks mostly for binary phenotypes followed by our Solutions we have implemented Sage for single variant Association tests because the single variant solution tests are usually underpowered for rare variants more recently we have implemented the region or Gene based Association tests called sage Gene for rare variants in the same Sage r package lasting this talk we're going to use some examples to demonstrate how to run the sage r package here three main challenges of G was in large-scale biobanks and cohorts are listed including sample relatedness large-scale data sizes and unbalanced case control ratio of binary phenotypes

first let's take a look at the simple relating needs simple relatingness is an important confounding factor in a genetic Association tests that needs to be accounted for for example in the UK power bank data almost one-third of individuals have a third degree or closer relative in a cohort however linear and logistic regression models assume individuals are unrelated therefore instead we use mixed models for g-was with relative samples mixed models use the random effect denoted as B here to account for sample relativeness B follows the multivariant normal distribution whose variance contains the genetic relationship Matrix called grm each of diagonal elements in grm is corresponding to the relativeness coefficient of a simple pair which can be estimated using the genetic data in a linear and logistic mixed models B which is the random effect is included to account for the simple relating needs through the grm

in 2015 look at all has developed a linear mix model method called both omm both IMM uses several asymptotic approaches to achieve the scalability of the mixed models and it is the first linear mix model method feasible for G was in biobank scale data so can we use the linear mix model for binary phenotypes in this figure the variance against the mean of residuals are plotted for the linear model and logistic model the linear model assumes constant variance as the Orange Line shows while the true mean variance relationship for binary trades is represented by the green line therefore for binary phenotypes instead of using linear mix model we would like to use the logistic mix model in 2016 Chen at all developed an r package called GMAT in which the logistic mix model has been implemented for jiwas on binary phenotypes while accounting for simple relating needs

so we use logistic mixed models to account for simple relating needs in jiwas binary phenotypes when we apply the logistic mix model as implemented in GMAT to one example phenotype colorectal cancer in the UK biobank we found that it would take more than 600 gigabytes of memory and 184 CPU years to finish 1G was in the UK biobank this brings out our next challenge large-scale data to be able to run logistic mixed models in biobank skill data for binary phenotypes we need to optimize implementation to make logistic mix models computationally practical for large-scale data sets we use similar approaches that have been previously used in both lmm which is the first linear mixed model method that are scalable for large-scale biobanks several approaches have been used to reduce the computation memory and time we have successfully reduced the computation memory from more than 600 gigabytes to less than 10 gigabytes and the computation time becomes nearly linear as the sample size increases the program optimization allows us to apply logistic mix model for binary phenotypes in the UK power bank here again we have the same example for colorectal cancer after we apply logistimix model to this phenotype we still see many Superior signals as we can tell from the Manhattan plot so what is missing here

we later find out the reason is the unbalanced case control ratio of binary phenotypes unbalanced kiss control ratio is widely observed in biobanks this plot is showing a distribution of case control ratio of 1600 binary phenotypes in the UK biobank and around 85 percent of them have case control ratio lower than 1 to 100 which means there are 100 times more cases than controls previous Studies have shown that unbalanced case control ratios can cause inflated type 1 errors of the score test which is represented by the Blue Line in the plot the y-axis is for type 1 error rates and the x-axis is for the minor Leo frequency of the testing genetic markers the score test is widely used in chiwas because of its relatively low competition burden as we can see from the left to the right as the study becomes more unbalanced the inflation becomes more severe also as

the myelofrequency becomes lower and the inflation becomes more severe the reason is that when the case control ratio is unbalanced the score test the statistic does not follow the normal distribution in a plot the dotted line is for the normal normal distribution and the black solid line is for the score test statistic so when we try to get a p-value based on the normal approximation we'll see inflation to solve this problem day at all in 2017 proposed to use the saddle point approximation to approximate the empirical distribution of the score test statistic instead of using the normal approximation and they have implemented this as the in the spa test art package so for our third challenge we're going to use the spa test to account for the unbalanced case control ratio so in summary Sage can work for GEOS with related samples based on the mixed models and it can work for large-scale data with those optimization strategies and it can account for unbalanced case control ratio of binary phenotypes through the saddle point approximation again back to our example phenotype character cancer and we see Sage successfully corrects the inflated type 1 error

we have applied Sage to three other phenotypes in the UK power bank on white British temples from coronary artery disease which is relatively common with the case control ratio 1 to 12 to the less prevalent disease thyroid cancer with kiss control ratio 1 to 1000 as we can tell from the third column of the Manhattan plots and sage has well corrected type 1 error rates Sage contains two steps in the first step the now logistic mix model was fit with phenotype covariates and genotypes that were used to construct the grm on the fly as input in a step 2 score tests were conducted for each genetic marker with Saddle Point approximation applied to account for the unbalanced case control ratio of the binary phenotypes sage has been implemented as an open source r package and has been used to

run gwas for UK power bank data and for other biobanks and large Consortium V web has been created for some of the projects for browsing the Phenom white g-was results as more and more sequencing data are being generated more rare variants are being detected however single variant Association tests by g-was are usually underpowered for rare variants and set-based tests can be used to increase the power in which rare variants are grouped and tested together based on some functional units such as genes and Regulatory regions we then extended Sage for region or Gene based tests for rare variants we implemented different set-based Association tasks including burden scat and scad all and searching also allows for incorporation of multiple minor Leo frequency cutoffs and functional annotations here are some useful papers that you can read if you want to know more details about different methods and the first three are Sage Sage ching and the sage Gene plus which is an improved version of the sage Gene that we have recently implemented and all the three methods have been implemented in a sage r package and the fourth one is the GMAT which is an r package implemented the logistic mix model and the bottom map is the first linear mixed model method that is scalable for biobank skill data and regini is a new method based on some machine learning approaches it is very computationally efficient method for biobank scale G was I would like to thank Dr John Chen Zhao who co-developed Sage Gene with me and my PhD advisors Dr Sean Lee and Dr Christy Weller my postdoc advisors Dr Mark Deli and Dr Benjamin nail and now my colleagues from the University of Michigan now fantastic collaborators from the hunt study in Norway next we have prepared some sage and staging examples as Wiki pages in a GitHub and we will go through some of them together if you click on the first link it will bring you to this Sage Hands-On practice to get ready you can Store install the sage r package following the instructions that you may find link here on the stage GitHub and all the example data can be found in the EXT data

so sage has two steps the first one is to fit a now logistic or linear mix model and the second one is to perform Association tasks for each genetic marker that you want we want to test and while applying the spa to score test to account for unbalanced case control ratios here's the for step one you can use the trade type option to specify whether it is a binary or quantitative trait for binary trade Sage will automatically fit a non-legistic mix model and for quantitative trade it'll fit a non-linear mix model and the step one takes two input files

the first input file is the in the link format containing genotypes of genetic markers that we want to use to estimate the genetic relationship Matrix and which contains the relatingness coefficient of simple pairs and the second file is the called phenotype file which contains non-genetic covariates if there is any and phenotype and simple IDs here's a snapshot for the phenotype file and you can see there are multiple columns corresponding to phenotype you want to test covariates included in the model and simple IDs so here's an example command line to fit the now logistic mix model for binary phenyl types and as we can see here we have the pilling file and we have to specify the final tab file and which column is for phenotype and which columns are for covariates and which comes for simple ID

so there are way more options available for step one and for more details and you can use call this R script and dash dash help and to see all of them and then if you run the example step one comment in a previous slide and you expect to see the screen output ends with the following text if the job above has been run successfully and step one will generate three output files as you can see model file ending with the dot RDA and the variance file which contains a number for the various ratio and then Association result file which is an intermediate file and so the first two will be used as input for step two

in Step 2 we perform single variant association test for each genetic marker so step two requires four input files and first of all is the dosage or genotype file containing dosage or genotypes of markers we want to test Sage supports four formats for dosages or genotypes is VCF BCF bgm file or the sap file so we'll use speech and file in the example today but you can click on the links to see more details about those different file formats and the second one is the simple file so

this file contains one column for sample IDs corresponding to the simple order in the dosage file and no header is included in a simple white simple file and this file is only required for BJ input as for some Vision files there is no simple ID information included in a region and but for VCF and the VCF and the staff simple IDs are already included in those files so this simple file is not required for those file format and the third and fourth input files are output from Step One here's the example code to run step two as we can see we specify bgm file Vision file index and Sample file and we specify the chromosome number because only one chromosome can be tested in each job and we use minimum value of frequency or minimum value account to specify some cutoffs for markers to be tested and GMAT model file and variance ratio file both of them are output by step one and then we use a um line output so we use 1000 here to tell Sage

to Output the results of every 1000 markers to the output file we don't want to use a very low number here because it will generate heavier overhead writing to the output file and then the last one is is output AF in case on control you we specify this one to be true and Stage will output the allele frequencies in cases and controls if we're testing a binary Trace to the output so here's an um Header information in the output file by step 2 which contains the association results for the genetic markers we're testing so Association results are all with regard to allele 2. as we see in the output file and I want to point out the column called P dot value.na so these p-values are obtained under the normal approximation so if we generate a QQ plot for p dot value Dot N A we're very um it's very likely for us to see the inflation for binary phenotypes with unbalanced case control ratios so um the P dot value are the p-values that we want to use next if you are interested in conducting Qing or region based tests for rare variants you can click on the second link here which will take you to the Wiki page we created for examples of searching jobs so here we can see this page contains the similar formats as Sage that we have went through for the Hands-On practice searching also contains the similar Step 1 and step 2 as Sage does and it it contains the extra step called Step Zero in which we construct the sparse grm based on the provided link file and this tab only needs once for each data set and it does not change according to different phenotypes

so in Step 0 we're creating a sparse grm based on the genotypes stored in the p-link file as we use for step one for the full grm the difference is that we create this sparse grm and store it in a file and so it can be reused in step one for different phenotypes so the input file will be the link file same as the plink file for step one and then the output file will be a file storing the sparse grm and a file storing IDs of samples in the sparse grm so if you rather example code and you expect the screen output ends with the following text if the job has been run successfully then we're running Step 1 again it is to fit a now logistic or linear mix model so step one asks for four input files instead of two as we see in Sage so the first two are the same as step one for single parent association test in stage it's the appealing file storing

The genome types and phenotype file containing a phenotype covariates and Sample ID information and the last two the third and the fourth ones are output by step 0 which are file storing the sparse CRM and the file storing the simple IDs in sparse grm so here's the example code for step one job and as we can see we can use the two options that highlighted here to specify the sparse grm the sparse grm simple IDs that we have obtained from Step Zero again this test help can be used to see more detailed parameter list and if the step one job has been run successfully and the screen output will end with the following text as we see there are multiple values corresponding to multiple variance ratios and there are actually four different my little count categories that we are using so for the specific cutoffs for different milio count categories and you can use Dash help to have a look

so this step one will generate four output files and the first three are the same as the step one output by stage and when we try to run single variance Association tests their model file and the various ratio file and the intermediate file for the association tests of some randomly selected markers and the fourth file here is is specific to Sage Gene which is a sparse Sigma file so this file will be used as input for step two along with the first and the second output file which are the model file and the variance ratio file in Step 2 we're performing The set-based Association tasks it asks for five input files and the first three are similar to Sage and there are dosage or genotype file in different formats and model file and variance ratio file generated by step one note that if you're using Vision to for dosages or genotypes and you need to use the simple file to specify the simple IDs in the beginning file and then let's look at the fourth and fifth file they are specific to Sage Gene and the fourth file is the group file we can take a closer look at this one

so each line in a group file is for one gene or one set of variants that you want to test them together as a set and the first element is for the Jing or set name and the rest of the line is for variant IDs included in this jingle set for VCF or save input the genetic marker IDs should be in the in the format Chrome Dome position the ref alternate allele and for bgm file the genetic marker IDs should be matched should match the IDS in a pigeon file and each element in the line is separated by tab and the fifth file is called sparse Sigma file it is also um output by the step one with all those input files and we can run Step 2 job to perform set-based association test and I want to point out this option called Max MAF for group test and this can be used to specify the maximum value frequency of genetic markers that you want to include in a group test so by default this value is one percent but if you want to test rare variance only and you can lower this number again uses dash dash help to see more detailed parameter list and information and if your step 2 job runs successfully and you see this screen output ends with the text as we show here then we let's take a look at the output files by step two Step 2 will provide a file with region or gene-based Association has the results for each Shing or region we're testing will provide p-value of scan o test and also p-value from burden and scat test and there are a couple of columns there showing the number of markers following each monthly account category and the category information can be found in the step 1 script and if we specify that we want to Output the single variance association test result for genetic markers in the genes or regions we're testing and the second file will be generated to provide those information

last if we want to use multiple value frequency cutoffs and multiple functional annotations to test each gene for example and we can use different group files and to run Step 2 for thatching for multiple times so to combine those different p-values for the same gene and we can use the gauchy distribution to combine them so here's the code that you can use to combine multiple p-values for Jing or testing region

------------------------------------------------------------------------

# CC-GWAS {#sec-video6}

**Title**: Tutorial to use CC-GWAS to identify loci with different allele frequency among cases of different disorders.

**Presenter(s)**: Wouter Peyrot

hi thank you very much for your interest toapply ccgwas to identify loci with different allele frequency among cases of different disorders my name is wautre piro in the next 15 minutes or so i'll show you how to run the ccgwa software as we also conduct this analysis in the cross-disorder working group of the pgc for the next wave of analysis elcas price and i developed the software and if you're interested in any further reading please see our paper in naturegenetics2021 or visit my github page and if you have any questions or suggestions please do not hesitate to contact me i'm most welcome to explain further or to receive any suggestions you may have and so just at the end first here i'll show a couple of slides to give a big picture overview of the method and then i'll get on my hands on some real data to show you how the analysis are done with the our package software so ccg was is intended to compare cases of two different disorders and when you like to extend for example to compare 10 disorders you just need to repeat the ccgwos analysis accordingly to compare two disorders at a time and so ccgwas compares the cases of two disorders based on the respective case control geos results and the case control geos results are of course widely publicly available most of the time and ccg was works with taking a weighted difference of these case control geos results and ccg was combines two components which have a distinct function so the first component is what we refer to as the ccg was ols component and this component optimizes the power and controls for type 1 error at no null snips and with null null snips we refer to snip snips that have no impact on either this order so there is no case case difference and if you're interested in any more details about the ccg was os component i refer to our paper and then there is a second component which we refer to as the ccg was exact component and the ccg was exact component controls the type 1 error at stress test snips and stress test snips are a very specific set of snips that impact both disorders but have no case case difference so that the impact on both disorders is exactly the same so there's no case case difference and this set of stress test snips is a very tricky set of snips because they can get type 1 error quickly that's why we have this additional set of weights

to control for type 1 error at stress test snipped and so ccg was says that a snip is significantly associated with case case status when the p-value of the ols component is smaller than 5 times 10 to the minus 8 and when the p-value of the exact component is smaller than 10 to the minus 4. and ccg also has an additional filter to protect against false positives and that is a very specific set of false positive that may may result from differential taking of a causal stress test snip so again the stress test snip is a snip that impacts both disorders but there's no difference in allele frequency between the cases and here in the right column you can see the causal stressed snip so it has an impact on this order a it has an impact on this order b but there is no case case difference so there's no case case effect but now suppose that you have a tagging snip which stacks in population a in which you stutter study disorder a with a r of 0.6 and this order b it takes a snip with an r of 0.3 then when you look at the difference of the of the this order a and this or the b effects you do find the case case difference so this is a very specific set of snips for which you can find uh false positive and uh to protect against this snip ccg was has an additional built-in step to filter evidence to filter snips that show evidence of differential tagging once again when you're interested in any more details of this step plea i refer to our paper and its supplements

so now we get to how to run ccgwas um so i first suggest to visit the github page ccg was github and at this github page i will download all the results and in particular i'm now going to already download the files in the test folder there are the two dummy input files for the for the case control uh input geos results so in return to the previous page and here you can see an explanation of how to run so running ccg was how to get started there's a detailed description of all the input parameters where you can with that you can look up for reference and there's a description of the output files and there's here this there is this example which i'll go through with you

so let's first get to running ccgwas so first here i have a terminal opened and i can show you i'm in a folder where i have already downloaded these two files so the testcase control g was for bib 10 snips and testcase control g was for schizophrenia 10 steps so we'll look at the example of comparing bipolar to schizophrenia so back to the github page so first we open r and i'm working on my macbook but i know this is exactly the same as working on a linux based server and it also works on windows here and you first need to load a couple of r libraries and install the ccgwas package i'll just copy paste it from here paste it here and wait for r to run it and now ccg was just loaded and you can see that ccgwash is now a functioning rso yeah you can see that ccgos is now loaded into r okay and i've already loaded these two files so now i go to this example

here at the top bottom of the github page i've loaded this ccg was example and you can see if it all works and i'll copy paste it and you can see that all is loaded but before looking to the results i'll first get to you to see the input parameters so probably best do it on this github page so here you can ccg was first you set the name of the output file and then you set the name of the first disorder you set the name of the second disorder which is bipolar disorder you say where where the sum stats fell of the first disorder and the some stats value of the second disorder case control of the case control subsets results and then note the six columns of these results are very specific so they need to be exactly as i've described it here

so let me show it's at the github page so here it says yeah so the columns should be the snip name the chromosome number the base pair position the effective allele the non-effective allele the allele frequency of the effective allele the odds ratio for the risk of the disorder for the effect per effective allele the standard error of the log alt of the the log of the alternate show the p value for this snip and n effective and when you don't have an effective you can impute imputed as i also described here on the github page so four divided by one over n case plus one over n control and i note that the results for the for the case control uh disorder and disorder b will be based merged on the snip name so before you run ccg was make sure that the snip names between those two sets are well aligned now a return to the example that we just ran

so that's where we left off i just showed you what the columns should look like of these two gz files and then there are more input parameters so k a1 a0 means it's the it's the population prevalence of disorder a so for schizophrenia that's approximately 0.4 percent and then because of course it's very hard to know exactly the population prevalence of a disorder you can also put boundaries to it and so for schizophrenia we set it at one percent and the low at the point four percent and it's important to add these ranges because there's also again protects against type one error at stress test snips and then for this order b you set the exact same three values so the population preference of this or the b and the high estimate and the low estimate and then you specify results that typically come from ldl discord regression but of course you can use different methods for it that's the liability skill heritability for this order bay the liability scale heritability for this order b and how to get to these estimates i refer to the software package that provide these estimates then you want to know the genetic correlation between this and this order b and also the the intercept that you get from biferit or discord regression and this is very important to set this value because it helps you to increase the power of cctvs and you also set an estimate of the approximated number of causal snips and so this m value you need to specify it and i also show here at the input files here i give some extra explanation around about it and also some papers where you can see if this m is already estimated or how you can estimate it or if you want to make an approximation how to do it so details on setting m is here on the github page under the input parameters

so we were left here with specifying the number of snips and then at last you need to give the number of cases for this order bay this order this order a the number of cases for this order b the number of controls for this order a and the number of controls for this order b and then you also need to specify the overlap of controls and once again when you set this overlap of controls it really helps you to increase the power of ccgwas and so it has kind of the same function as the intercept so these are two ways to to to increase the power of ccg was and also to double check that you don't risk any type one error result so when you know these values it's important to set it to increase the power of gcg os so i hope this clarifies the input parameters of ccgos and now our return to the analysis that we just did by running this command line um yeah so we just ran it and you can see that the that the r function gives kind of an output file which is also saved in the log file so it says well

when the analysis was started and that you run ccg was and not ccg was plus and for ccg was plus i refer to my get it to the github page and this is when you also have a direct case case comparison available and then it shows some summary about the data that were read from the gzip file and it does some very rough qc on the odds ratios and then how it merges the data and so this kind of the just the log file that's that's plotted and you also get provided in an fst plot for this and let me see if i can quickly yeah so this is what the fst plot looks like it's an output of ccg was and here you can see the fst again for the details i refer to our paper but it gives intuition of the genetic distance between cases and controls so here you can see the controls for for bipolar disorder the cases for bipolar disorder the controls for schizophrenia and the cases for schizophrenia and i don't know what just happened and these numbers gives the relative difference between them so you can see that even though schizophrenia and bipolar disorder have a very large genetic correlation of 0.7 the genetic distance is still considerable with a relative value of 0.49 compared to 0.66 or 0.6 for the case control difference

okay um and then when we look at the output files i always like this system where from r you can also use the command line in the terminal yeah you can see that by running ccg ones these these are the input summer statistics for these tens these 10 snips this is the output valve this is the fst plot that i just showed this is the log file which is also printed here on screen so i'm not going to open it but it's good for a later reference if you're interested and i note this log file also gives you the ols and the exact weights that i just discussed so here in the in the in the log file you can see so for this example the ols weights were 0.54 for schizophrenia minus 0.3 for bipolar and the exact weights are also listed in the in the log file and may be good to report is in your in the description of your ccg was analysis and here there's also the the results that are \[Music\] displayed here

so now let's have a look at these results i'll just copy paste this also from the github page and for now i'll remove the all and i'll explain to you later what all means so so i need to give it the correct name test out and then let's have a look at d so once again i only have 10 snips now normally of course these are maybe 10 million snips or something and here you can see the columns so there's the snip name the chromosome number the base pair position the effective allele the non-effective allele and then you have three columns ols beta or standard error and ols p value and these represent the the effects from the from the oles component of ccgwas and then there are three more columns exact beta exact standard error and exact p value and these three three columns give the ccgs results of the exact component and now as we said and here there's also a column which labels ccg was significant so when ccg was significant this column is labeled as one right so and you can see for this specific snip in the ninth row so when we look here you can see that our lsp value is 9 times 10 to the minus 12 so it's smaller than 5 times 10 to the minus 8 and you can see that the exact p-value is 4.7 times 10 to the minus 11. so it's smaller than 10 to the minus four so this snip ccg was significant and so if you're interested

i think it's good to to note that you can also have a more detailed output file and to do this you need to add to this to this r script yeah so you need to add here an additional input parameter which was named if i remember correctly yeah so then you add save all is true here save always true and then when we look again at the files that are now in the folder here you can see that an additional output file popped up and when we when we have a look at this file so i open this file changing the file name a little bit at the all here so and now this file takes much more space of course for 10 snips it's not really a problem but when you have many snips it can be tedious

so and there are many many columns here so the first uh columns of d uh uh represent again the columns that i just disc that so the first columns are the snip chromosome base pair position effectively or non-effectively and then you have the case control effects and these are these are skilled to the standardized observed scale based on a 50 50 case control ascertainment again when you're interested in the details please see our paper and it supplements and then you have the case control results for for this for this order b then you have the oles results as we just discussed and you have the exact results but then there are additional columns so there is the potential tagging for the stress test snip potential differential taking of the stress test snip and it's not necessary to understand all the details if you're interested please have a look at the at our paper or at supplements but just know that behind the scenes we test for it and it's being excluded from the from the previous output file that i showed

and then here we also have this exact approach which which again so the exact component of ccg was protects against type 1 error of stress test snips and this is also based so that you don't know exactly the population preference of both these orders so here you can see the exact results when you take the low low values of both population prevalence disorders low high high low and the high high population prevalence bound and here's the ccg was significant folder so as you can see this d this output fell all if you're interested to look into more details of your results you can have a look at it and if you have any questions please don't hesitate to contact me and ask me for the details but in general advice not to look at these results because also in the in the trimmed results which are the default output and now i'm going to load it again we already removed the snips that may have a problem with differential tagging or some other issues right so if you use this file you're you're safe in that aspect and then finally and i see i extended the 15 minutes but i hope you don't mind so now we showed how to run ccgos in practice and for uh

yeah for follow-up analysis as i just said use ccg was resolved based on the save all is false results and i know this is a default so when you don't set save or this is what's being done and it's a trimmed set of results and then we advise to use the ccg was ols components so these are the columns that are labeled oles beta ols standard error and osp value for clumping and for example polygenic risk or analysis so to look at which loci have the different allele frequency and when you're interested in genetic correlation analysis or any other sort of ld score regression based analysis or methods that are alike we advise to use the ccgo's exact component so the exact beta exact standard error and the exact p-value so that brings me to the end of this tutorial once again please don't hesitate to contact me if you have any questions and i hope this tutorial was helpful
